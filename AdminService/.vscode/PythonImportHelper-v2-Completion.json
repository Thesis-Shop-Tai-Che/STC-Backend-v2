[
    {
        "label": "Depends",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Request",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Depends",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "status",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Request",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Request",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Request",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Depends",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "Request",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "ModelView",
        "importPath": "sqladmin",
        "description": "sqladmin",
        "isExtraImport": true,
        "detail": "sqladmin",
        "documentation": {}
    },
    {
        "label": "Admin",
        "importPath": "sqladmin",
        "description": "sqladmin",
        "isExtraImport": true,
        "detail": "sqladmin",
        "documentation": {}
    },
    {
        "label": "Admin",
        "importPath": "sqladmin",
        "description": "sqladmin",
        "isExtraImport": true,
        "detail": "sqladmin",
        "documentation": {}
    },
    {
        "label": "Tag",
        "importPath": "tag.models.tag_model",
        "description": "tag.models.tag_model",
        "isExtraImport": true,
        "detail": "tag.models.tag_model",
        "documentation": {}
    },
    {
        "label": "Request",
        "importPath": "starlette.requests",
        "description": "starlette.requests",
        "isExtraImport": true,
        "detail": "starlette.requests",
        "documentation": {}
    },
    {
        "label": "RedirectResponse",
        "importPath": "starlette.responses",
        "description": "starlette.responses",
        "isExtraImport": true,
        "detail": "starlette.responses",
        "documentation": {}
    },
    {
        "label": "Response",
        "importPath": "starlette.responses",
        "description": "starlette.responses",
        "isExtraImport": true,
        "detail": "starlette.responses",
        "documentation": {}
    },
    {
        "label": "RedirectResponse",
        "importPath": "starlette.responses",
        "description": "starlette.responses",
        "isExtraImport": true,
        "detail": "starlette.responses",
        "documentation": {}
    },
    {
        "label": "create_access_token",
        "importPath": "auth.utils",
        "description": "auth.utils",
        "isExtraImport": true,
        "detail": "auth.utils",
        "documentation": {}
    },
    {
        "label": "create_refresh_token",
        "importPath": "auth.utils",
        "description": "auth.utils",
        "isExtraImport": true,
        "detail": "auth.utils",
        "documentation": {}
    },
    {
        "label": "create_access_token",
        "importPath": "auth.utils",
        "description": "auth.utils",
        "isExtraImport": true,
        "detail": "auth.utils",
        "documentation": {}
    },
    {
        "label": "decode_and_verify_access_token",
        "importPath": "auth.utils",
        "description": "auth.utils",
        "isExtraImport": true,
        "detail": "auth.utils",
        "documentation": {}
    },
    {
        "label": "decode_and_verify_refresh_token",
        "importPath": "auth.utils",
        "description": "auth.utils",
        "isExtraImport": true,
        "detail": "auth.utils",
        "documentation": {}
    },
    {
        "label": "verify_password",
        "importPath": "auth.utils",
        "description": "auth.utils",
        "isExtraImport": true,
        "detail": "auth.utils",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Generator",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Iterable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Optional",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Tuple",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Union",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Dict",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "HTMLResponse",
        "importPath": "fastapi.responses",
        "description": "fastapi.responses",
        "isExtraImport": true,
        "detail": "fastapi.responses",
        "documentation": {}
    },
    {
        "label": "JSONResponse",
        "importPath": "fastapi.responses",
        "description": "fastapi.responses",
        "isExtraImport": true,
        "detail": "fastapi.responses",
        "documentation": {}
    },
    {
        "label": "JSONResponse",
        "importPath": "fastapi.responses",
        "description": "fastapi.responses",
        "isExtraImport": true,
        "detail": "fastapi.responses",
        "documentation": {}
    },
    {
        "label": "AuthenticationBackend",
        "importPath": "sqladmin.authentication",
        "description": "sqladmin.authentication",
        "isExtraImport": true,
        "detail": "sqladmin.authentication",
        "documentation": {}
    },
    {
        "label": "login_required",
        "importPath": "sqladmin.authentication",
        "description": "sqladmin.authentication",
        "isExtraImport": true,
        "detail": "sqladmin.authentication",
        "documentation": {}
    },
    {
        "label": "get_db",
        "importPath": "database",
        "description": "database",
        "isExtraImport": true,
        "detail": "database",
        "documentation": {}
    },
    {
        "label": "get_db",
        "importPath": "database",
        "description": "database",
        "isExtraImport": true,
        "detail": "database",
        "documentation": {}
    },
    {
        "label": "get_db",
        "importPath": "database",
        "description": "database",
        "isExtraImport": true,
        "detail": "database",
        "documentation": {}
    },
    {
        "label": "Base",
        "importPath": "database",
        "description": "database",
        "isExtraImport": true,
        "detail": "database",
        "documentation": {}
    },
    {
        "label": "get_db",
        "importPath": "database",
        "description": "database",
        "isExtraImport": true,
        "detail": "database",
        "documentation": {}
    },
    {
        "label": "Base",
        "importPath": "database",
        "description": "database",
        "isExtraImport": true,
        "detail": "database",
        "documentation": {}
    },
    {
        "label": "engine",
        "importPath": "database",
        "description": "database",
        "isExtraImport": true,
        "detail": "database",
        "documentation": {}
    },
    {
        "label": "get_db",
        "importPath": "database",
        "description": "database",
        "isExtraImport": true,
        "detail": "database",
        "documentation": {}
    },
    {
        "label": "URLPath",
        "importPath": "starlette.routing",
        "description": "starlette.routing",
        "isExtraImport": true,
        "detail": "starlette.routing",
        "documentation": {}
    },
    {
        "label": "jwt",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "jwt",
        "description": "jwt",
        "detail": "jwt",
        "documentation": {}
    },
    {
        "label": "settings",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "settings",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "settings",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "settings",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "settings",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "HTTPAuthorizationCredentials",
        "importPath": "fastapi.security",
        "description": "fastapi.security",
        "isExtraImport": true,
        "detail": "fastapi.security",
        "documentation": {}
    },
    {
        "label": "HTTPBearer",
        "importPath": "fastapi.security",
        "description": "fastapi.security",
        "isExtraImport": true,
        "detail": "fastapi.security",
        "documentation": {}
    },
    {
        "label": "HTTPBearer",
        "importPath": "fastapi.security",
        "description": "fastapi.security",
        "isExtraImport": true,
        "detail": "fastapi.security",
        "documentation": {}
    },
    {
        "label": "datetime",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "datetime",
        "description": "datetime",
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "datetime",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "timedelta",
        "importPath": "datetime",
        "description": "datetime",
        "isExtraImport": true,
        "detail": "datetime",
        "documentation": {}
    },
    {
        "label": "random",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "random",
        "description": "random",
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "sample",
        "importPath": "random",
        "description": "random",
        "isExtraImport": true,
        "detail": "random",
        "documentation": {}
    },
    {
        "label": "string",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "string",
        "description": "string",
        "detail": "string",
        "documentation": {}
    },
    {
        "label": "ascii_lowercase",
        "importPath": "string",
        "description": "string",
        "isExtraImport": true,
        "detail": "string",
        "documentation": {}
    },
    {
        "label": "jwt",
        "importPath": "jose",
        "description": "jose",
        "isExtraImport": true,
        "detail": "jose",
        "documentation": {}
    },
    {
        "label": "ExpiredSignatureError",
        "importPath": "jose",
        "description": "jose",
        "isExtraImport": true,
        "detail": "jose",
        "documentation": {}
    },
    {
        "label": "jwt",
        "importPath": "jose",
        "description": "jose",
        "isExtraImport": true,
        "detail": "jose",
        "documentation": {}
    },
    {
        "label": "Client",
        "importPath": "mailjet_rest",
        "description": "mailjet_rest",
        "isExtraImport": true,
        "detail": "mailjet_rest",
        "documentation": {}
    },
    {
        "label": "CryptContext",
        "importPath": "passlib.context",
        "description": "passlib.context",
        "isExtraImport": true,
        "detail": "passlib.context",
        "documentation": {}
    },
    {
        "label": "requests",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "requests",
        "description": "requests",
        "detail": "requests",
        "documentation": {}
    },
    {
        "label": "networkx",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "networkx",
        "description": "networkx",
        "detail": "networkx",
        "documentation": {}
    },
    {
        "label": "Graph",
        "importPath": "networkx",
        "description": "networkx",
        "isExtraImport": true,
        "detail": "networkx",
        "documentation": {}
    },
    {
        "label": "Graph",
        "importPath": "networkx",
        "description": "networkx",
        "isExtraImport": true,
        "detail": "networkx",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "mlab",
        "importPath": "mayavi",
        "description": "mayavi",
        "isExtraImport": true,
        "detail": "mayavi",
        "documentation": {}
    },
    {
        "label": "matplotlib.pyplot",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib.pyplot",
        "description": "matplotlib.pyplot",
        "detail": "matplotlib.pyplot",
        "documentation": {}
    },
    {
        "label": "Axes3D",
        "importPath": "mpl_toolkits.mplot3d",
        "description": "mpl_toolkits.mplot3d",
        "isExtraImport": true,
        "detail": "mpl_toolkits.mplot3d",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "collections",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "collections",
        "description": "collections",
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "defaultdict",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "namedtuple",
        "importPath": "collections",
        "description": "collections",
        "isExtraImport": true,
        "detail": "collections",
        "documentation": {}
    },
    {
        "label": "hierarchy",
        "importPath": "scipy.cluster",
        "description": "scipy.cluster",
        "isExtraImport": true,
        "detail": "scipy.cluster",
        "documentation": {}
    },
    {
        "label": "distance",
        "importPath": "scipy.spatial",
        "description": "scipy.spatial",
        "isExtraImport": true,
        "detail": "scipy.spatial",
        "documentation": {}
    },
    {
        "label": "networkx.algorithms.bipartite",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "networkx.algorithms.bipartite",
        "description": "networkx.algorithms.bipartite",
        "detail": "networkx.algorithms.bipartite",
        "documentation": {}
    },
    {
        "label": "multiprocessing",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "Pool",
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "isExtraImport": true,
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "Pool",
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "isExtraImport": true,
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "Manager",
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "isExtraImport": true,
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "Manager",
        "importPath": "multiprocessing",
        "description": "multiprocessing",
        "isExtraImport": true,
        "detail": "multiprocessing",
        "documentation": {}
    },
    {
        "label": "time",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "time",
        "description": "time",
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "time",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "perf_counter",
        "importPath": "time",
        "description": "time",
        "isExtraImport": true,
        "detail": "time",
        "documentation": {}
    },
    {
        "label": "itertools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "itertools",
        "description": "itertools",
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "product",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "chain",
        "importPath": "itertools",
        "description": "itertools",
        "isExtraImport": true,
        "detail": "itertools",
        "documentation": {}
    },
    {
        "label": "seaborn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "seaborn",
        "description": "seaborn",
        "detail": "seaborn",
        "documentation": {}
    },
    {
        "label": "PIL",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "PIL",
        "description": "PIL",
        "detail": "PIL",
        "documentation": {}
    },
    {
        "label": "matplotlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "matplotlib",
        "description": "matplotlib",
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "pyplot",
        "importPath": "matplotlib",
        "description": "matplotlib",
        "isExtraImport": true,
        "detail": "matplotlib",
        "documentation": {}
    },
    {
        "label": "itemgetter",
        "importPath": "operator",
        "description": "operator",
        "isExtraImport": true,
        "detail": "operator",
        "documentation": {}
    },
    {
        "label": "numpy.linalg",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy.linalg",
        "description": "numpy.linalg",
        "detail": "numpy.linalg",
        "documentation": {}
    },
    {
        "label": "gzip",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gzip",
        "description": "gzip",
        "detail": "gzip",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "zipfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "zipfile",
        "description": "zipfile",
        "detail": "zipfile",
        "documentation": {}
    },
    {
        "label": "io",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "io",
        "description": "io",
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "BytesIO",
        "importPath": "io",
        "description": "io",
        "isExtraImport": true,
        "detail": "io",
        "documentation": {}
    },
    {
        "label": "networkx.algorithms.approximation",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "networkx.algorithms.approximation",
        "description": "networkx.algorithms.approximation",
        "detail": "networkx.algorithms.approximation",
        "documentation": {}
    },
    {
        "label": "getaddresses",
        "importPath": "email.utils",
        "description": "email.utils",
        "isExtraImport": true,
        "detail": "email.utils",
        "documentation": {}
    },
    {
        "label": "parseaddr",
        "importPath": "email.utils",
        "description": "email.utils",
        "isExtraImport": true,
        "detail": "email.utils",
        "documentation": {}
    },
    {
        "label": "mailbox",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "mailbox",
        "description": "mailbox",
        "detail": "mailbox",
        "documentation": {}
    },
    {
        "label": "urllib.request",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "urllib.request",
        "description": "urllib.request",
        "detail": "urllib.request",
        "documentation": {}
    },
    {
        "label": "sys",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sys",
        "description": "sys",
        "detail": "sys",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "platform",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "platform",
        "description": "platform",
        "detail": "platform",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "einsum",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "einsum",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "Tensor",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "optim",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "optim",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "nn",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "optim",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "optim",
        "importPath": "torch",
        "description": "torch",
        "isExtraImport": true,
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "TTS",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "TTS",
        "description": "TTS",
        "detail": "TTS",
        "documentation": {}
    },
    {
        "label": "argparse",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "argparse",
        "description": "argparse",
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "RawTextHelpFormatter",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "RawTextHelpFormatter",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "RawTextHelpFormatter",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "RawTextHelpFormatter",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "RawTextHelpFormatter",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "RawTextHelpFormatter",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "RawTextHelpFormatter",
        "importPath": "argparse",
        "description": "argparse",
        "isExtraImport": true,
        "detail": "argparse",
        "documentation": {}
    },
    {
        "label": "importlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "importlib",
        "description": "importlib",
        "detail": "importlib",
        "documentation": {}
    },
    {
        "label": "torch.utils.data",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tqdm",
        "description": "tqdm",
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "load_config",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "load_config",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "load_config",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "load_config",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "load_config",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "load_config",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "load_config",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "load_config",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "register_config",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "load_config",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "register_config",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "load_config",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "load_config",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "register_config",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "load_config",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "BaseTrainingConfig",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "check_config_and_model_args",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "load_config",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "get_from_config_or_model_args_with_default",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "load_config",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "load_config",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "BaseTrainingConfig",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "BaseTrainingConfig",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "load_config",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "load_config",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "load_config",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "load_config",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "load_config",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config",
        "description": "TTS.config",
        "isExtraImport": true,
        "detail": "TTS.config",
        "documentation": {}
    },
    {
        "label": "TTSDataset",
        "importPath": "TTS.tts.datasets.TTSDataset",
        "description": "TTS.tts.datasets.TTSDataset",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets.TTSDataset",
        "documentation": {}
    },
    {
        "label": "setup_model",
        "importPath": "TTS.tts.models",
        "description": "TTS.tts.models",
        "isExtraImport": true,
        "detail": "TTS.tts.models",
        "documentation": {}
    },
    {
        "label": "setup_model",
        "importPath": "TTS.tts.models",
        "description": "TTS.tts.models",
        "isExtraImport": true,
        "detail": "TTS.tts.models",
        "documentation": {}
    },
    {
        "label": "setup_model",
        "importPath": "TTS.tts.models",
        "description": "TTS.tts.models",
        "isExtraImport": true,
        "detail": "TTS.tts.models",
        "documentation": {}
    },
    {
        "label": "setup_model",
        "importPath": "TTS.tts.models",
        "description": "TTS.tts.models",
        "isExtraImport": true,
        "detail": "TTS.tts.models",
        "documentation": {}
    },
    {
        "label": "setup_model",
        "importPath": "TTS.tts.models",
        "description": "TTS.tts.models",
        "isExtraImport": true,
        "detail": "TTS.tts.models",
        "documentation": {}
    },
    {
        "label": "setup_model",
        "importPath": "TTS.tts.models",
        "description": "TTS.tts.models",
        "isExtraImport": true,
        "detail": "TTS.tts.models",
        "documentation": {}
    },
    {
        "label": "make_symbols",
        "importPath": "TTS.tts.utils.text.characters",
        "description": "TTS.tts.utils.text.characters",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "phonemes",
        "importPath": "TTS.tts.utils.text.characters",
        "description": "TTS.tts.utils.text.characters",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "symbols",
        "importPath": "TTS.tts.utils.text.characters",
        "description": "TTS.tts.utils.text.characters",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "parse_symbols",
        "importPath": "TTS.tts.utils.text.characters",
        "description": "TTS.tts.utils.text.characters",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "BaseCharacters",
        "importPath": "TTS.tts.utils.text.characters",
        "description": "TTS.tts.utils.text.characters",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "BaseVocabulary",
        "importPath": "TTS.tts.utils.text.characters",
        "description": "TTS.tts.utils.text.characters",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "_characters",
        "importPath": "TTS.tts.utils.text.characters",
        "description": "TTS.tts.utils.text.characters",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "_pad",
        "importPath": "TTS.tts.utils.text.characters",
        "description": "TTS.tts.utils.text.characters",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "_phonemes",
        "importPath": "TTS.tts.utils.text.characters",
        "description": "TTS.tts.utils.text.characters",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "_punctuations",
        "importPath": "TTS.tts.utils.text.characters",
        "description": "TTS.tts.utils.text.characters",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "Graphemes",
        "importPath": "TTS.tts.utils.text.characters",
        "description": "TTS.tts.utils.text.characters",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "IPAPhonemes",
        "importPath": "TTS.tts.utils.text.characters",
        "description": "TTS.tts.utils.text.characters",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "BaseCharacters",
        "importPath": "TTS.tts.utils.text.characters",
        "description": "TTS.tts.utils.text.characters",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "BaseVocabulary",
        "importPath": "TTS.tts.utils.text.characters",
        "description": "TTS.tts.utils.text.characters",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "Graphemes",
        "importPath": "TTS.tts.utils.text.characters",
        "description": "TTS.tts.utils.text.characters",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "IPAPhonemes",
        "importPath": "TTS.tts.utils.text.characters",
        "description": "TTS.tts.utils.text.characters",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "Graphemes",
        "importPath": "TTS.tts.utils.text.characters",
        "description": "TTS.tts.utils.text.characters",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "IPAPhonemes",
        "importPath": "TTS.tts.utils.text.characters",
        "description": "TTS.tts.utils.text.characters",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "_blank",
        "importPath": "TTS.tts.utils.text.characters",
        "description": "TTS.tts.utils.text.characters",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "_bos",
        "importPath": "TTS.tts.utils.text.characters",
        "description": "TTS.tts.utils.text.characters",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "_eos",
        "importPath": "TTS.tts.utils.text.characters",
        "description": "TTS.tts.utils.text.characters",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "_pad",
        "importPath": "TTS.tts.utils.text.characters",
        "description": "TTS.tts.utils.text.characters",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "_phonemes",
        "importPath": "TTS.tts.utils.text.characters",
        "description": "TTS.tts.utils.text.characters",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "_punctuations",
        "importPath": "TTS.tts.utils.text.characters",
        "description": "TTS.tts.utils.text.characters",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "numpy_transforms",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio",
        "description": "TTS.utils.audio",
        "isExtraImport": true,
        "detail": "TTS.utils.audio",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "importPath": "TTS.utils.io",
        "description": "TTS.utils.io",
        "isExtraImport": true,
        "detail": "TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "copy_model_files",
        "importPath": "TTS.utils.io",
        "description": "TTS.utils.io",
        "isExtraImport": true,
        "detail": "TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "load_fsspec",
        "importPath": "TTS.utils.io",
        "description": "TTS.utils.io",
        "isExtraImport": true,
        "detail": "TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "save_fsspec",
        "importPath": "TTS.utils.io",
        "description": "TTS.utils.io",
        "isExtraImport": true,
        "detail": "TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "save_fsspec",
        "importPath": "TTS.utils.io",
        "description": "TTS.utils.io",
        "isExtraImport": true,
        "detail": "TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "copy_model_files",
        "importPath": "TTS.utils.io",
        "description": "TTS.utils.io",
        "isExtraImport": true,
        "detail": "TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "load_fsspec",
        "importPath": "TTS.utils.io",
        "description": "TTS.utils.io",
        "isExtraImport": true,
        "detail": "TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "load_fsspec",
        "importPath": "TTS.utils.io",
        "description": "TTS.utils.io",
        "isExtraImport": true,
        "detail": "TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "load_fsspec",
        "importPath": "TTS.utils.io",
        "description": "TTS.utils.io",
        "isExtraImport": true,
        "detail": "TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "load_fsspec",
        "importPath": "TTS.utils.io",
        "description": "TTS.utils.io",
        "isExtraImport": true,
        "detail": "TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "load_fsspec",
        "importPath": "TTS.utils.io",
        "description": "TTS.utils.io",
        "isExtraImport": true,
        "detail": "TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "load_fsspec",
        "importPath": "TTS.utils.io",
        "description": "TTS.utils.io",
        "isExtraImport": true,
        "detail": "TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "load_fsspec",
        "importPath": "TTS.utils.io",
        "description": "TTS.utils.io",
        "isExtraImport": true,
        "detail": "TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "load_fsspec",
        "importPath": "TTS.utils.io",
        "description": "TTS.utils.io",
        "isExtraImport": true,
        "detail": "TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "save_checkpoint",
        "importPath": "TTS.utils.io",
        "description": "TTS.utils.io",
        "isExtraImport": true,
        "detail": "TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "load_fsspec",
        "importPath": "TTS.utils.io",
        "description": "TTS.utils.io",
        "isExtraImport": true,
        "detail": "TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "load_fsspec",
        "importPath": "TTS.utils.io",
        "description": "TTS.utils.io",
        "isExtraImport": true,
        "detail": "TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "load_fsspec",
        "importPath": "TTS.utils.io",
        "description": "TTS.utils.io",
        "isExtraImport": true,
        "detail": "TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "load_fsspec",
        "importPath": "TTS.utils.io",
        "description": "TTS.utils.io",
        "isExtraImport": true,
        "detail": "TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "load_fsspec",
        "importPath": "TTS.utils.io",
        "description": "TTS.utils.io",
        "isExtraImport": true,
        "detail": "TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "load_fsspec",
        "importPath": "TTS.utils.io",
        "description": "TTS.utils.io",
        "isExtraImport": true,
        "detail": "TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "load_fsspec",
        "importPath": "TTS.utils.io",
        "description": "TTS.utils.io",
        "isExtraImport": true,
        "detail": "TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "save_checkpoint",
        "importPath": "TTS.utils.io",
        "description": "TTS.utils.io",
        "isExtraImport": true,
        "detail": "TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseTrainingConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.config.shared_configs",
        "description": "TTS.config.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "TTSDataset",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "TTSDataset",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "load_tts_samples",
        "importPath": "TTS.tts.datasets",
        "description": "TTS.tts.datasets",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets",
        "documentation": {}
    },
    {
        "label": "save_file",
        "importPath": "TTS.tts.utils.managers",
        "description": "TTS.tts.utils.managers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.managers",
        "documentation": {}
    },
    {
        "label": "BaseIDManager",
        "importPath": "TTS.tts.utils.managers",
        "description": "TTS.tts.utils.managers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.managers",
        "documentation": {}
    },
    {
        "label": "EmbeddingManager",
        "importPath": "TTS.tts.utils.managers",
        "description": "TTS.tts.utils.managers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.managers",
        "documentation": {}
    },
    {
        "label": "EmbeddingManager",
        "importPath": "TTS.tts.utils.managers",
        "description": "TTS.tts.utils.managers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.managers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "get_speaker_balancer_weights",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "get_speaker_manager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "get_speaker_balancer_weights",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "get_speaker_balancer_weights",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "importPath": "TTS.tts.utils.speakers",
        "description": "TTS.tts.utils.speakers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "glob",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "glob",
        "description": "glob",
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "glob",
        "importPath": "glob",
        "description": "glob",
        "isExtraImport": true,
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "glob",
        "importPath": "glob",
        "description": "glob",
        "isExtraImport": true,
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "glob",
        "importPath": "glob",
        "description": "glob",
        "isExtraImport": true,
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "glob",
        "importPath": "glob",
        "description": "glob",
        "isExtraImport": true,
        "detail": "glob",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "importPath": "TTS.tts.utils.text.tokenizer",
        "description": "TTS.tts.utils.text.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "count_parameters",
        "importPath": "TTS.utils.generic_utils",
        "description": "TTS.utils.generic_utils",
        "isExtraImport": true,
        "detail": "TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "count_parameters",
        "importPath": "TTS.utils.generic_utils",
        "description": "TTS.utils.generic_utils",
        "isExtraImport": true,
        "detail": "TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "remove_experiment_folder",
        "importPath": "TTS.utils.generic_utils",
        "description": "TTS.utils.generic_utils",
        "isExtraImport": true,
        "detail": "TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "set_init_dict",
        "importPath": "TTS.utils.generic_utils",
        "description": "TTS.utils.generic_utils",
        "isExtraImport": true,
        "detail": "TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "get_experiment_folder_path",
        "importPath": "TTS.utils.generic_utils",
        "description": "TTS.utils.generic_utils",
        "isExtraImport": true,
        "detail": "TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "get_git_branch",
        "importPath": "TTS.utils.generic_utils",
        "description": "TTS.utils.generic_utils",
        "isExtraImport": true,
        "detail": "TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "format_aux_input",
        "importPath": "TTS.utils.generic_utils",
        "description": "TTS.utils.generic_utils",
        "isExtraImport": true,
        "detail": "TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "format_aux_input",
        "importPath": "TTS.utils.generic_utils",
        "description": "TTS.utils.generic_utils",
        "isExtraImport": true,
        "detail": "TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "format_aux_input",
        "importPath": "TTS.utils.generic_utils",
        "description": "TTS.utils.generic_utils",
        "isExtraImport": true,
        "detail": "TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "get_import_path",
        "importPath": "TTS.utils.generic_utils",
        "description": "TTS.utils.generic_utils",
        "isExtraImport": true,
        "detail": "TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "import_class",
        "importPath": "TTS.utils.generic_utils",
        "description": "TTS.utils.generic_utils",
        "isExtraImport": true,
        "detail": "TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "get_user_data_dir",
        "importPath": "TTS.utils.generic_utils",
        "description": "TTS.utils.generic_utils",
        "isExtraImport": true,
        "detail": "TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "get_user_data_dir",
        "importPath": "TTS.utils.generic_utils",
        "description": "TTS.utils.generic_utils",
        "isExtraImport": true,
        "detail": "TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "get_user_data_dir",
        "importPath": "TTS.utils.generic_utils",
        "description": "TTS.utils.generic_utils",
        "isExtraImport": true,
        "detail": "TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "process_map",
        "importPath": "tqdm.contrib.concurrent",
        "description": "tqdm.contrib.concurrent",
        "isExtraImport": true,
        "detail": "tqdm.contrib.concurrent",
        "documentation": {}
    },
    {
        "label": "Gruut",
        "importPath": "TTS.tts.utils.text.phonemizers",
        "description": "TTS.tts.utils.text.phonemizers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.phonemizers",
        "documentation": {}
    },
    {
        "label": "DEF_LANG_TO_PHONEMIZER",
        "importPath": "TTS.tts.utils.text.phonemizers",
        "description": "TTS.tts.utils.text.phonemizers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.phonemizers",
        "documentation": {}
    },
    {
        "label": "get_phonemizer_by_name",
        "importPath": "TTS.tts.utils.text.phonemizers",
        "description": "TTS.tts.utils.text.phonemizers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.phonemizers",
        "documentation": {}
    },
    {
        "label": "DEF_LANG_TO_PHONEMIZER",
        "importPath": "TTS.tts.utils.text.phonemizers",
        "description": "TTS.tts.utils.text.phonemizers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.phonemizers",
        "documentation": {}
    },
    {
        "label": "get_phonemizer_by_name",
        "importPath": "TTS.tts.utils.text.phonemizers",
        "description": "TTS.tts.utils.text.phonemizers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.phonemizers",
        "documentation": {}
    },
    {
        "label": "ESpeak",
        "importPath": "TTS.tts.utils.text.phonemizers",
        "description": "TTS.tts.utils.text.phonemizers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.phonemizers",
        "documentation": {}
    },
    {
        "label": "Gruut",
        "importPath": "TTS.tts.utils.text.phonemizers",
        "description": "TTS.tts.utils.text.phonemizers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.phonemizers",
        "documentation": {}
    },
    {
        "label": "JA_JP_Phonemizer",
        "importPath": "TTS.tts.utils.text.phonemizers",
        "description": "TTS.tts.utils.text.phonemizers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.phonemizers",
        "documentation": {}
    },
    {
        "label": "ZH_CN_Phonemizer",
        "importPath": "TTS.tts.utils.text.phonemizers",
        "description": "TTS.tts.utils.text.phonemizers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.phonemizers",
        "documentation": {}
    },
    {
        "label": "ESpeak",
        "importPath": "TTS.tts.utils.text.phonemizers",
        "description": "TTS.tts.utils.text.phonemizers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.phonemizers",
        "documentation": {}
    },
    {
        "label": "pathlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pathlib",
        "description": "pathlib",
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "get_vad_model_and_utils",
        "importPath": "TTS.utils.vad",
        "description": "TTS.utils.vad",
        "isExtraImport": true,
        "detail": "TTS.utils.vad",
        "documentation": {}
    },
    {
        "label": "remove_silence",
        "importPath": "TTS.utils.vad",
        "description": "TTS.utils.vad",
        "isExtraImport": true,
        "detail": "TTS.utils.vad",
        "documentation": {}
    },
    {
        "label": "shutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "shutil",
        "description": "shutil",
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "copytree",
        "importPath": "shutil",
        "description": "shutil",
        "isExtraImport": true,
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "copyfile",
        "importPath": "shutil",
        "description": "shutil",
        "isExtraImport": true,
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "rmtree",
        "importPath": "shutil",
        "description": "shutil",
        "isExtraImport": true,
        "detail": "shutil",
        "documentation": {}
    },
    {
        "label": "librosa",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "librosa",
        "description": "librosa",
        "detail": "librosa",
        "documentation": {}
    },
    {
        "label": "magphase",
        "importPath": "librosa",
        "description": "librosa",
        "isExtraImport": true,
        "detail": "librosa",
        "documentation": {}
    },
    {
        "label": "pyin",
        "importPath": "librosa",
        "description": "librosa",
        "isExtraImport": true,
        "detail": "librosa",
        "documentation": {}
    },
    {
        "label": "soundfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "soundfile",
        "description": "soundfile",
        "detail": "soundfile",
        "documentation": {}
    },
    {
        "label": "TTS",
        "importPath": "TTS.api",
        "description": "TTS.api",
        "isExtraImport": true,
        "detail": "TTS.api",
        "documentation": {}
    },
    {
        "label": "CS_API",
        "importPath": "TTS.api",
        "description": "TTS.api",
        "isExtraImport": true,
        "detail": "TTS.api",
        "documentation": {}
    },
    {
        "label": "TTS",
        "importPath": "TTS.api",
        "description": "TTS.api",
        "isExtraImport": true,
        "detail": "TTS.api",
        "documentation": {}
    },
    {
        "label": "ModelManager",
        "importPath": "TTS.utils.manage",
        "description": "TTS.utils.manage",
        "isExtraImport": true,
        "detail": "TTS.utils.manage",
        "documentation": {}
    },
    {
        "label": "ModelManager",
        "importPath": "TTS.utils.manage",
        "description": "TTS.utils.manage",
        "isExtraImport": true,
        "detail": "TTS.utils.manage",
        "documentation": {}
    },
    {
        "label": "ModelManager",
        "importPath": "TTS.utils.manage",
        "description": "TTS.utils.manage",
        "isExtraImport": true,
        "detail": "TTS.utils.manage",
        "documentation": {}
    },
    {
        "label": "ModelManager",
        "importPath": "TTS.utils.manage",
        "description": "TTS.utils.manage",
        "isExtraImport": true,
        "detail": "TTS.utils.manage",
        "documentation": {}
    },
    {
        "label": "ModelManager",
        "importPath": "TTS.utils.manage",
        "description": "TTS.utils.manage",
        "isExtraImport": true,
        "detail": "TTS.utils.manage",
        "documentation": {}
    },
    {
        "label": "ModelManager",
        "importPath": "TTS.utils.manage",
        "description": "TTS.utils.manage",
        "isExtraImport": true,
        "detail": "TTS.utils.manage",
        "documentation": {}
    },
    {
        "label": "ModelManager",
        "importPath": "TTS.utils.manage",
        "description": "TTS.utils.manage",
        "isExtraImport": true,
        "detail": "TTS.utils.manage",
        "documentation": {}
    },
    {
        "label": "ModelManager",
        "importPath": "TTS.utils.manage",
        "description": "TTS.utils.manage",
        "isExtraImport": true,
        "detail": "TTS.utils.manage",
        "documentation": {}
    },
    {
        "label": "Synthesizer",
        "importPath": "TTS.utils.synthesizer",
        "description": "TTS.utils.synthesizer",
        "isExtraImport": true,
        "detail": "TTS.utils.synthesizer",
        "documentation": {}
    },
    {
        "label": "Synthesizer",
        "importPath": "TTS.utils.synthesizer",
        "description": "TTS.utils.synthesizer",
        "isExtraImport": true,
        "detail": "TTS.utils.synthesizer",
        "documentation": {}
    },
    {
        "label": "Synthesizer",
        "importPath": "TTS.utils.synthesizer",
        "description": "TTS.utils.synthesizer",
        "isExtraImport": true,
        "detail": "TTS.utils.synthesizer",
        "documentation": {}
    },
    {
        "label": "Synthesizer",
        "importPath": "TTS.utils.synthesizer",
        "description": "TTS.utils.synthesizer",
        "isExtraImport": true,
        "detail": "TTS.utils.synthesizer",
        "documentation": {}
    },
    {
        "label": "Synthesizer",
        "importPath": "TTS.utils.synthesizer",
        "description": "TTS.utils.synthesizer",
        "isExtraImport": true,
        "detail": "TTS.utils.synthesizer",
        "documentation": {}
    },
    {
        "label": "traceback",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "traceback",
        "description": "traceback",
        "detail": "traceback",
        "documentation": {}
    },
    {
        "label": "NoamLR",
        "importPath": "trainer.torch",
        "description": "trainer.torch",
        "isExtraImport": true,
        "detail": "trainer.torch",
        "documentation": {}
    },
    {
        "label": "DistributedSampler",
        "importPath": "trainer.torch",
        "description": "trainer.torch",
        "isExtraImport": true,
        "detail": "trainer.torch",
        "documentation": {}
    },
    {
        "label": "DistributedSamplerWrapper",
        "importPath": "trainer.torch",
        "description": "trainer.torch",
        "isExtraImport": true,
        "detail": "trainer.torch",
        "documentation": {}
    },
    {
        "label": "DistributedSampler",
        "importPath": "trainer.torch",
        "description": "trainer.torch",
        "isExtraImport": true,
        "detail": "trainer.torch",
        "documentation": {}
    },
    {
        "label": "DistributedSamplerWrapper",
        "importPath": "trainer.torch",
        "description": "trainer.torch",
        "isExtraImport": true,
        "detail": "trainer.torch",
        "documentation": {}
    },
    {
        "label": "DistributedSampler",
        "importPath": "trainer.torch",
        "description": "trainer.torch",
        "isExtraImport": true,
        "detail": "trainer.torch",
        "documentation": {}
    },
    {
        "label": "DistributedSamplerWrapper",
        "importPath": "trainer.torch",
        "description": "trainer.torch",
        "isExtraImport": true,
        "detail": "trainer.torch",
        "documentation": {}
    },
    {
        "label": "get_optimizer",
        "importPath": "trainer.trainer_utils",
        "description": "trainer.trainer_utils",
        "isExtraImport": true,
        "detail": "trainer.trainer_utils",
        "documentation": {}
    },
    {
        "label": "get_optimizer",
        "importPath": "trainer.trainer_utils",
        "description": "trainer.trainer_utils",
        "isExtraImport": true,
        "detail": "trainer.trainer_utils",
        "documentation": {}
    },
    {
        "label": "get_scheduler",
        "importPath": "trainer.trainer_utils",
        "description": "trainer.trainer_utils",
        "isExtraImport": true,
        "detail": "trainer.trainer_utils",
        "documentation": {}
    },
    {
        "label": "get_optimizer",
        "importPath": "trainer.trainer_utils",
        "description": "trainer.trainer_utils",
        "isExtraImport": true,
        "detail": "trainer.trainer_utils",
        "documentation": {}
    },
    {
        "label": "get_scheduler",
        "importPath": "trainer.trainer_utils",
        "description": "trainer.trainer_utils",
        "isExtraImport": true,
        "detail": "trainer.trainer_utils",
        "documentation": {}
    },
    {
        "label": "get_optimizer",
        "importPath": "trainer.trainer_utils",
        "description": "trainer.trainer_utils",
        "isExtraImport": true,
        "detail": "trainer.trainer_utils",
        "documentation": {}
    },
    {
        "label": "get_scheduler",
        "importPath": "trainer.trainer_utils",
        "description": "trainer.trainer_utils",
        "isExtraImport": true,
        "detail": "trainer.trainer_utils",
        "documentation": {}
    },
    {
        "label": "get_optimizer",
        "importPath": "trainer.trainer_utils",
        "description": "trainer.trainer_utils",
        "isExtraImport": true,
        "detail": "trainer.trainer_utils",
        "documentation": {}
    },
    {
        "label": "get_optimizer",
        "importPath": "trainer.trainer_utils",
        "description": "trainer.trainer_utils",
        "isExtraImport": true,
        "detail": "trainer.trainer_utils",
        "documentation": {}
    },
    {
        "label": "get_scheduler",
        "importPath": "trainer.trainer_utils",
        "description": "trainer.trainer_utils",
        "isExtraImport": true,
        "detail": "trainer.trainer_utils",
        "documentation": {}
    },
    {
        "label": "get_optimizer",
        "importPath": "trainer.trainer_utils",
        "description": "trainer.trainer_utils",
        "isExtraImport": true,
        "detail": "trainer.trainer_utils",
        "documentation": {}
    },
    {
        "label": "get_scheduler",
        "importPath": "trainer.trainer_utils",
        "description": "trainer.trainer_utils",
        "isExtraImport": true,
        "detail": "trainer.trainer_utils",
        "documentation": {}
    },
    {
        "label": "EncoderDataset",
        "importPath": "TTS.encoder.dataset",
        "description": "TTS.encoder.dataset",
        "isExtraImport": true,
        "detail": "TTS.encoder.dataset",
        "documentation": {}
    },
    {
        "label": "save_best_model",
        "importPath": "TTS.encoder.utils.generic_utils",
        "description": "TTS.encoder.utils.generic_utils",
        "isExtraImport": true,
        "detail": "TTS.encoder.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "save_checkpoint",
        "importPath": "TTS.encoder.utils.generic_utils",
        "description": "TTS.encoder.utils.generic_utils",
        "isExtraImport": true,
        "detail": "TTS.encoder.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "setup_encoder_model",
        "importPath": "TTS.encoder.utils.generic_utils",
        "description": "TTS.encoder.utils.generic_utils",
        "isExtraImport": true,
        "detail": "TTS.encoder.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "AugmentWAV",
        "importPath": "TTS.encoder.utils.generic_utils",
        "description": "TTS.encoder.utils.generic_utils",
        "isExtraImport": true,
        "detail": "TTS.encoder.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "setup_encoder_model",
        "importPath": "TTS.encoder.utils.generic_utils",
        "description": "TTS.encoder.utils.generic_utils",
        "isExtraImport": true,
        "detail": "TTS.encoder.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "setup_encoder_model",
        "importPath": "TTS.encoder.utils.generic_utils",
        "description": "TTS.encoder.utils.generic_utils",
        "isExtraImport": true,
        "detail": "TTS.encoder.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "setup_encoder_model",
        "importPath": "TTS.encoder.utils.generic_utils",
        "description": "TTS.encoder.utils.generic_utils",
        "isExtraImport": true,
        "detail": "TTS.encoder.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "setup_encoder_model",
        "importPath": "TTS.encoder.utils.generic_utils",
        "description": "TTS.encoder.utils.generic_utils",
        "isExtraImport": true,
        "detail": "TTS.encoder.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "init_training",
        "importPath": "TTS.encoder.utils.training",
        "description": "TTS.encoder.utils.training",
        "isExtraImport": true,
        "detail": "TTS.encoder.utils.training",
        "documentation": {}
    },
    {
        "label": "plot_embeddings",
        "importPath": "TTS.encoder.utils.visual",
        "description": "TTS.encoder.utils.visual",
        "isExtraImport": true,
        "detail": "TTS.encoder.utils.visual",
        "documentation": {}
    },
    {
        "label": "PerfectBatchSampler",
        "importPath": "TTS.utils.samplers",
        "description": "TTS.utils.samplers",
        "isExtraImport": true,
        "detail": "TTS.utils.samplers",
        "documentation": {}
    },
    {
        "label": "BucketBatchSampler",
        "importPath": "TTS.utils.samplers",
        "description": "TTS.utils.samplers",
        "isExtraImport": true,
        "detail": "TTS.utils.samplers",
        "documentation": {}
    },
    {
        "label": "BucketBatchSampler",
        "importPath": "TTS.utils.samplers",
        "description": "TTS.utils.samplers",
        "isExtraImport": true,
        "detail": "TTS.utils.samplers",
        "documentation": {}
    },
    {
        "label": "PerfectBatchSampler",
        "importPath": "TTS.utils.samplers",
        "description": "TTS.utils.samplers",
        "isExtraImport": true,
        "detail": "TTS.utils.samplers",
        "documentation": {}
    },
    {
        "label": "check_update",
        "importPath": "TTS.utils.training",
        "description": "TTS.utils.training",
        "isExtraImport": true,
        "detail": "TTS.utils.training",
        "documentation": {}
    },
    {
        "label": "gradual_training_scheduler",
        "importPath": "TTS.utils.training",
        "description": "TTS.utils.training",
        "isExtraImport": true,
        "detail": "TTS.utils.training",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "asdict",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "asdict",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "asdict",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "asdict",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "asdict",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "replace",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "replace",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "asdict",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "field",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerConfig",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerModel",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "Trainer",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "TrainerArgs",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "get_last_checkpoint",
        "importPath": "trainer",
        "description": "trainer",
        "isExtraImport": true,
        "detail": "trainer",
        "documentation": {}
    },
    {
        "label": "load_wav_data",
        "importPath": "TTS.vocoder.datasets.preprocess",
        "description": "TTS.vocoder.datasets.preprocess",
        "isExtraImport": true,
        "detail": "TTS.vocoder.datasets.preprocess",
        "documentation": {}
    },
    {
        "label": "load_wav_feat_data",
        "importPath": "TTS.vocoder.datasets.preprocess",
        "description": "TTS.vocoder.datasets.preprocess",
        "isExtraImport": true,
        "detail": "TTS.vocoder.datasets.preprocess",
        "documentation": {}
    },
    {
        "label": "load_wav_data",
        "importPath": "TTS.vocoder.datasets.preprocess",
        "description": "TTS.vocoder.datasets.preprocess",
        "isExtraImport": true,
        "detail": "TTS.vocoder.datasets.preprocess",
        "documentation": {}
    },
    {
        "label": "load_wav_data",
        "importPath": "TTS.vocoder.datasets.preprocess",
        "description": "TTS.vocoder.datasets.preprocess",
        "isExtraImport": true,
        "detail": "TTS.vocoder.datasets.preprocess",
        "documentation": {}
    },
    {
        "label": "load_wav_data",
        "importPath": "TTS.vocoder.datasets.preprocess",
        "description": "TTS.vocoder.datasets.preprocess",
        "isExtraImport": true,
        "detail": "TTS.vocoder.datasets.preprocess",
        "documentation": {}
    },
    {
        "label": "load_wav_data",
        "importPath": "TTS.vocoder.datasets.preprocess",
        "description": "TTS.vocoder.datasets.preprocess",
        "isExtraImport": true,
        "detail": "TTS.vocoder.datasets.preprocess",
        "documentation": {}
    },
    {
        "label": "load_wav_data",
        "importPath": "TTS.vocoder.datasets.preprocess",
        "description": "TTS.vocoder.datasets.preprocess",
        "isExtraImport": true,
        "detail": "TTS.vocoder.datasets.preprocess",
        "documentation": {}
    },
    {
        "label": "load_wav_data",
        "importPath": "TTS.vocoder.datasets.preprocess",
        "description": "TTS.vocoder.datasets.preprocess",
        "isExtraImport": true,
        "detail": "TTS.vocoder.datasets.preprocess",
        "documentation": {}
    },
    {
        "label": "load_wav_data",
        "importPath": "TTS.vocoder.datasets.preprocess",
        "description": "TTS.vocoder.datasets.preprocess",
        "isExtraImport": true,
        "detail": "TTS.vocoder.datasets.preprocess",
        "documentation": {}
    },
    {
        "label": "load_wav_data",
        "importPath": "TTS.vocoder.datasets.preprocess",
        "description": "TTS.vocoder.datasets.preprocess",
        "isExtraImport": true,
        "detail": "TTS.vocoder.datasets.preprocess",
        "documentation": {}
    },
    {
        "label": "load_wav_data",
        "importPath": "TTS.vocoder.datasets.preprocess",
        "description": "TTS.vocoder.datasets.preprocess",
        "isExtraImport": true,
        "detail": "TTS.vocoder.datasets.preprocess",
        "documentation": {}
    },
    {
        "label": "load_wav_data",
        "importPath": "TTS.vocoder.datasets.preprocess",
        "description": "TTS.vocoder.datasets.preprocess",
        "isExtraImport": true,
        "detail": "TTS.vocoder.datasets.preprocess",
        "documentation": {}
    },
    {
        "label": "load_wav_data",
        "importPath": "TTS.vocoder.datasets.preprocess",
        "description": "TTS.vocoder.datasets.preprocess",
        "isExtraImport": true,
        "detail": "TTS.vocoder.datasets.preprocess",
        "documentation": {}
    },
    {
        "label": "load_wav_data",
        "importPath": "TTS.vocoder.datasets.preprocess",
        "description": "TTS.vocoder.datasets.preprocess",
        "isExtraImport": true,
        "detail": "TTS.vocoder.datasets.preprocess",
        "documentation": {}
    },
    {
        "label": "load_wav_feat_data",
        "importPath": "TTS.vocoder.datasets.preprocess",
        "description": "TTS.vocoder.datasets.preprocess",
        "isExtraImport": true,
        "detail": "TTS.vocoder.datasets.preprocess",
        "documentation": {}
    },
    {
        "label": "preprocess_wav_files",
        "importPath": "TTS.vocoder.datasets.preprocess",
        "description": "TTS.vocoder.datasets.preprocess",
        "isExtraImport": true,
        "detail": "TTS.vocoder.datasets.preprocess",
        "documentation": {}
    },
    {
        "label": "setup_model",
        "importPath": "TTS.vocoder.models",
        "description": "TTS.vocoder.models",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models",
        "documentation": {}
    },
    {
        "label": "setup_model",
        "importPath": "TTS.vocoder.models",
        "description": "TTS.vocoder.models",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models",
        "documentation": {}
    },
    {
        "label": "setup_model",
        "importPath": "TTS.vocoder.models",
        "description": "TTS.vocoder.models",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models",
        "documentation": {}
    },
    {
        "label": "setup_discriminator",
        "importPath": "TTS.vocoder.models",
        "description": "TTS.vocoder.models",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models",
        "documentation": {}
    },
    {
        "label": "setup_generator",
        "importPath": "TTS.vocoder.models",
        "description": "TTS.vocoder.models",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models",
        "documentation": {}
    },
    {
        "label": "WaveGradDataset",
        "importPath": "TTS.vocoder.datasets.wavegrad_dataset",
        "description": "TTS.vocoder.datasets.wavegrad_dataset",
        "isExtraImport": true,
        "detail": "TTS.vocoder.datasets.wavegrad_dataset",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "check_argument",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "MISSING",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "check_argument",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "check_argument",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "Coqpit",
        "importPath": "coqpit",
        "description": "coqpit",
        "isExtraImport": true,
        "detail": "coqpit",
        "documentation": {}
    },
    {
        "label": "BaseEncoderConfig",
        "importPath": "TTS.encoder.configs.base_encoder_config",
        "description": "TTS.encoder.configs.base_encoder_config",
        "isExtraImport": true,
        "detail": "TTS.encoder.configs.base_encoder_config",
        "documentation": {}
    },
    {
        "label": "BaseEncoderConfig",
        "importPath": "TTS.encoder.configs.base_encoder_config",
        "description": "TTS.encoder.configs.base_encoder_config",
        "isExtraImport": true,
        "detail": "TTS.encoder.configs.base_encoder_config",
        "documentation": {}
    },
    {
        "label": "torchaudio",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchaudio",
        "description": "torchaudio",
        "detail": "torchaudio",
        "documentation": {}
    },
    {
        "label": "AngleProtoLoss",
        "importPath": "TTS.encoder.losses",
        "description": "TTS.encoder.losses",
        "isExtraImport": true,
        "detail": "TTS.encoder.losses",
        "documentation": {}
    },
    {
        "label": "GE2ELoss",
        "importPath": "TTS.encoder.losses",
        "description": "TTS.encoder.losses",
        "isExtraImport": true,
        "detail": "TTS.encoder.losses",
        "documentation": {}
    },
    {
        "label": "SoftmaxAngleProtoLoss",
        "importPath": "TTS.encoder.losses",
        "description": "TTS.encoder.losses",
        "isExtraImport": true,
        "detail": "TTS.encoder.losses",
        "documentation": {}
    },
    {
        "label": "AngleProtoLoss",
        "importPath": "TTS.encoder.losses",
        "description": "TTS.encoder.losses",
        "isExtraImport": true,
        "detail": "TTS.encoder.losses",
        "documentation": {}
    },
    {
        "label": "GE2ELoss",
        "importPath": "TTS.encoder.losses",
        "description": "TTS.encoder.losses",
        "isExtraImport": true,
        "detail": "TTS.encoder.losses",
        "documentation": {}
    },
    {
        "label": "SoftmaxAngleProtoLoss",
        "importPath": "TTS.encoder.losses",
        "description": "TTS.encoder.losses",
        "isExtraImport": true,
        "detail": "TTS.encoder.losses",
        "documentation": {}
    },
    {
        "label": "BaseEncoder",
        "importPath": "TTS.encoder.models.base_encoder",
        "description": "TTS.encoder.models.base_encoder",
        "isExtraImport": true,
        "detail": "TTS.encoder.models.base_encoder",
        "documentation": {}
    },
    {
        "label": "BaseEncoder",
        "importPath": "TTS.encoder.models.base_encoder",
        "description": "TTS.encoder.models.base_encoder",
        "isExtraImport": true,
        "detail": "TTS.encoder.models.base_encoder",
        "documentation": {}
    },
    {
        "label": "scipy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "scipy",
        "description": "scipy",
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "signal",
        "importPath": "scipy",
        "description": "scipy",
        "isExtraImport": true,
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "signal",
        "importPath": "scipy",
        "description": "scipy",
        "isExtraImport": true,
        "detail": "scipy",
        "documentation": {}
    },
    {
        "label": "LSTMSpeakerEncoder",
        "importPath": "TTS.encoder.models.lstm",
        "description": "TTS.encoder.models.lstm",
        "isExtraImport": true,
        "detail": "TTS.encoder.models.lstm",
        "documentation": {}
    },
    {
        "label": "LSTMSpeakerEncoder",
        "importPath": "TTS.encoder.models.lstm",
        "description": "TTS.encoder.models.lstm",
        "isExtraImport": true,
        "detail": "TTS.encoder.models.lstm",
        "documentation": {}
    },
    {
        "label": "ResNetSpeakerEncoder",
        "importPath": "TTS.encoder.models.resnet",
        "description": "TTS.encoder.models.resnet",
        "isExtraImport": true,
        "detail": "TTS.encoder.models.resnet",
        "documentation": {}
    },
    {
        "label": "ResNetSpeakerEncoder",
        "importPath": "TTS.encoder.models.resnet",
        "description": "TTS.encoder.models.resnet",
        "isExtraImport": true,
        "detail": "TTS.encoder.models.resnet",
        "documentation": {}
    },
    {
        "label": "hashlib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "hashlib",
        "description": "hashlib",
        "detail": "hashlib",
        "documentation": {}
    },
    {
        "label": "subprocess",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "subprocess",
        "description": "subprocess",
        "detail": "subprocess",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "logging",
        "importPath": "absl",
        "description": "absl",
        "isExtraImport": true,
        "detail": "absl",
        "documentation": {}
    },
    {
        "label": "logger_factory",
        "importPath": "trainer.logging",
        "description": "trainer.logging",
        "isExtraImport": true,
        "detail": "trainer.logging",
        "documentation": {}
    },
    {
        "label": "ConsoleLogger",
        "importPath": "trainer.logging.console_logger",
        "description": "trainer.logging.console_logger",
        "isExtraImport": true,
        "detail": "trainer.logging.console_logger",
        "documentation": {}
    },
    {
        "label": "umap",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "umap",
        "description": "umap",
        "detail": "umap",
        "documentation": {}
    },
    {
        "label": "torch.nn.functional",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn.functional",
        "description": "torch.nn.functional",
        "detail": "torch.nn.functional",
        "documentation": {}
    },
    {
        "label": "Lock",
        "importPath": "threading",
        "description": "threading",
        "isExtraImport": true,
        "detail": "threading",
        "documentation": {}
    },
    {
        "label": "parse_qs",
        "importPath": "urllib.parse",
        "description": "urllib.parse",
        "isExtraImport": true,
        "detail": "urllib.parse",
        "documentation": {}
    },
    {
        "label": "Flask",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "render_template",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "render_template_string",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "send_file",
        "importPath": "flask",
        "description": "flask",
        "isExtraImport": true,
        "detail": "flask",
        "documentation": {}
    },
    {
        "label": "BaseTTSConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseTTSConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseTTSConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseTTSConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseTTSConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseTTSConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseTTSConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseTTSConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseTTSConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "CapacitronVAEConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "GSTConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseTTSConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseTTSConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "CharactersConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "CharactersConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "CapacitronVAEConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "CapacitronVAEConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "CapacitronVAEConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseTTSConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "CapacitronVAEConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "GSTConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "CapacitronVAEConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "GSTConfig",
        "importPath": "TTS.tts.configs.shared_configs",
        "description": "TTS.tts.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "AlignTTSArgs",
        "importPath": "TTS.tts.models.align_tts",
        "description": "TTS.tts.models.align_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.align_tts",
        "documentation": {}
    },
    {
        "label": "AlignTTS",
        "importPath": "TTS.tts.models.align_tts",
        "description": "TTS.tts.models.align_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.align_tts",
        "documentation": {}
    },
    {
        "label": "AlignTTS",
        "importPath": "TTS.tts.models.align_tts",
        "description": "TTS.tts.models.align_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.align_tts",
        "documentation": {}
    },
    {
        "label": "ForwardTTSArgs",
        "importPath": "TTS.tts.models.forward_tts",
        "description": "TTS.tts.models.forward_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.forward_tts",
        "documentation": {}
    },
    {
        "label": "ForwardTTSArgs",
        "importPath": "TTS.tts.models.forward_tts",
        "description": "TTS.tts.models.forward_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.forward_tts",
        "documentation": {}
    },
    {
        "label": "ForwardTTSArgs",
        "importPath": "TTS.tts.models.forward_tts",
        "description": "TTS.tts.models.forward_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.forward_tts",
        "documentation": {}
    },
    {
        "label": "ForwardTTSArgs",
        "importPath": "TTS.tts.models.forward_tts",
        "description": "TTS.tts.models.forward_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.forward_tts",
        "documentation": {}
    },
    {
        "label": "ForwardTTS",
        "importPath": "TTS.tts.models.forward_tts",
        "description": "TTS.tts.models.forward_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.forward_tts",
        "documentation": {}
    },
    {
        "label": "ForwardTTS",
        "importPath": "TTS.tts.models.forward_tts",
        "description": "TTS.tts.models.forward_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.forward_tts",
        "documentation": {}
    },
    {
        "label": "ForwardTTS",
        "importPath": "TTS.tts.models.forward_tts",
        "description": "TTS.tts.models.forward_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.forward_tts",
        "documentation": {}
    },
    {
        "label": "ForwardTTS",
        "importPath": "TTS.tts.models.forward_tts",
        "description": "TTS.tts.models.forward_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.forward_tts",
        "documentation": {}
    },
    {
        "label": "ForwardTTS",
        "importPath": "TTS.tts.models.forward_tts",
        "description": "TTS.tts.models.forward_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.forward_tts",
        "documentation": {}
    },
    {
        "label": "ForwardTTS",
        "importPath": "TTS.tts.models.forward_tts",
        "description": "TTS.tts.models.forward_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.forward_tts",
        "documentation": {}
    },
    {
        "label": "ForwardTTS",
        "importPath": "TTS.tts.models.forward_tts",
        "description": "TTS.tts.models.forward_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.forward_tts",
        "documentation": {}
    },
    {
        "label": "ForwardTTS",
        "importPath": "TTS.tts.models.forward_tts",
        "description": "TTS.tts.models.forward_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.forward_tts",
        "documentation": {}
    },
    {
        "label": "ForwardTTS",
        "importPath": "TTS.tts.models.forward_tts",
        "description": "TTS.tts.models.forward_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.forward_tts",
        "documentation": {}
    },
    {
        "label": "ForwardTTSArgs",
        "importPath": "TTS.tts.models.forward_tts",
        "description": "TTS.tts.models.forward_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.forward_tts",
        "documentation": {}
    },
    {
        "label": "TacotronConfig",
        "importPath": "TTS.tts.configs.tacotron_config",
        "description": "TTS.tts.configs.tacotron_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.tacotron_config",
        "documentation": {}
    },
    {
        "label": "TacotronConfig",
        "importPath": "TTS.tts.configs.tacotron_config",
        "description": "TTS.tts.configs.tacotron_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.tacotron_config",
        "documentation": {}
    },
    {
        "label": "TacotronConfig",
        "importPath": "TTS.tts.configs.tacotron_config",
        "description": "TTS.tts.configs.tacotron_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.tacotron_config",
        "documentation": {}
    },
    {
        "label": "TacotronConfig",
        "importPath": "TTS.tts.configs.tacotron_config",
        "description": "TTS.tts.configs.tacotron_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.tacotron_config",
        "documentation": {}
    },
    {
        "label": "TacotronConfig",
        "importPath": "TTS.tts.configs.tacotron_config",
        "description": "TTS.tts.configs.tacotron_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.tacotron_config",
        "documentation": {}
    },
    {
        "label": "TortoiseArgs",
        "importPath": "TTS.tts.models.tortoise",
        "description": "TTS.tts.models.tortoise",
        "isExtraImport": true,
        "detail": "TTS.tts.models.tortoise",
        "documentation": {}
    },
    {
        "label": "TortoiseAudioConfig",
        "importPath": "TTS.tts.models.tortoise",
        "description": "TTS.tts.models.tortoise",
        "isExtraImport": true,
        "detail": "TTS.tts.models.tortoise",
        "documentation": {}
    },
    {
        "label": "VitsArgs",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "VitsAudioConfig",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "Vits",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "Vits",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "VitsAudioConfig",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "CharactersConfig",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "Vits",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "VitsArgs",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "VitsAudioConfig",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "Vits",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "VitsArgs",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "VitsAudioConfig",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "Vits",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "VitsAudioConfig",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "Vits",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "VitsArgs",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "VitsAudioConfig",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "CharactersConfig",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "Vits",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "VitsArgs",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "VitsAudioConfig",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "Vits",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "VitsArgs",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "VitsAudioConfig",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "amp_to_db",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "db_to_amp",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "load_audio",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "spec_to_mel",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "wav_to_mel",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "wav_to_spec",
        "importPath": "TTS.tts.models.vits",
        "description": "TTS.tts.models.vits",
        "isExtraImport": true,
        "detail": "TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "base64",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "base64",
        "description": "base64",
        "detail": "base64",
        "documentation": {}
    },
    {
        "label": "prepare_data",
        "importPath": "TTS.tts.utils.data",
        "description": "TTS.tts.utils.data",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.data",
        "documentation": {}
    },
    {
        "label": "prepare_stop_target",
        "importPath": "TTS.tts.utils.data",
        "description": "TTS.tts.utils.data",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.data",
        "documentation": {}
    },
    {
        "label": "prepare_tensor",
        "importPath": "TTS.tts.utils.data",
        "description": "TTS.tts.utils.data",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.data",
        "documentation": {}
    },
    {
        "label": "get_length_balancer_weights",
        "importPath": "TTS.tts.utils.data",
        "description": "TTS.tts.utils.data",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.data",
        "documentation": {}
    },
    {
        "label": "get_length_balancer_weights",
        "importPath": "TTS.tts.utils.data",
        "description": "TTS.tts.utils.data",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.data",
        "documentation": {}
    },
    {
        "label": "get_length_balancer_weights",
        "importPath": "TTS.tts.utils.data",
        "description": "TTS.tts.utils.data",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.data",
        "documentation": {}
    },
    {
        "label": "compute_energy",
        "importPath": "TTS.utils.audio.numpy_transforms",
        "description": "TTS.utils.audio.numpy_transforms",
        "isExtraImport": true,
        "detail": "TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "compute_f0",
        "importPath": "TTS.utils.audio.numpy_transforms",
        "description": "TTS.utils.audio.numpy_transforms",
        "isExtraImport": true,
        "detail": "TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "save_wav",
        "importPath": "TTS.utils.audio.numpy_transforms",
        "description": "TTS.utils.audio.numpy_transforms",
        "isExtraImport": true,
        "detail": "TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "save_wav",
        "importPath": "TTS.utils.audio.numpy_transforms",
        "description": "TTS.utils.audio.numpy_transforms",
        "isExtraImport": true,
        "detail": "TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "xml.etree.ElementTree",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "xml.etree.ElementTree",
        "description": "xml.etree.ElementTree",
        "detail": "xml.etree.ElementTree",
        "documentation": {}
    },
    {
        "label": "PositionalEncoding",
        "importPath": "TTS.tts.layers.generic.pos_encoding",
        "description": "TTS.tts.layers.generic.pos_encoding",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.generic.pos_encoding",
        "documentation": {}
    },
    {
        "label": "PositionalEncoding",
        "importPath": "TTS.tts.layers.generic.pos_encoding",
        "description": "TTS.tts.layers.generic.pos_encoding",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.generic.pos_encoding",
        "documentation": {}
    },
    {
        "label": "PositionalEncoding",
        "importPath": "TTS.tts.layers.generic.pos_encoding",
        "description": "TTS.tts.layers.generic.pos_encoding",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.generic.pos_encoding",
        "documentation": {}
    },
    {
        "label": "FFTransformerBlock",
        "importPath": "TTS.tts.layers.generic.transformer",
        "description": "TTS.tts.layers.generic.transformer",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.generic.transformer",
        "documentation": {}
    },
    {
        "label": "FFTransformerBlock",
        "importPath": "TTS.tts.layers.generic.transformer",
        "description": "TTS.tts.layers.generic.transformer",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.generic.transformer",
        "documentation": {}
    },
    {
        "label": "FFTransformerBlock",
        "importPath": "TTS.tts.layers.generic.transformer",
        "description": "TTS.tts.layers.generic.transformer",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.generic.transformer",
        "documentation": {}
    },
    {
        "label": "Conv1dBN",
        "importPath": "TTS.tts.layers.generic.res_conv_bn",
        "description": "TTS.tts.layers.generic.res_conv_bn",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.generic.res_conv_bn",
        "documentation": {}
    },
    {
        "label": "Conv1dBNBlock",
        "importPath": "TTS.tts.layers.generic.res_conv_bn",
        "description": "TTS.tts.layers.generic.res_conv_bn",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.generic.res_conv_bn",
        "documentation": {}
    },
    {
        "label": "ResidualConv1dBNBlock",
        "importPath": "TTS.tts.layers.generic.res_conv_bn",
        "description": "TTS.tts.layers.generic.res_conv_bn",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.generic.res_conv_bn",
        "documentation": {}
    },
    {
        "label": "Conv1dBN",
        "importPath": "TTS.tts.layers.generic.res_conv_bn",
        "description": "TTS.tts.layers.generic.res_conv_bn",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.generic.res_conv_bn",
        "documentation": {}
    },
    {
        "label": "ResidualConv1dBNBlock",
        "importPath": "TTS.tts.layers.generic.res_conv_bn",
        "description": "TTS.tts.layers.generic.res_conv_bn",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.generic.res_conv_bn",
        "documentation": {}
    },
    {
        "label": "ResidualConv1dBNBlock",
        "importPath": "TTS.tts.layers.generic.res_conv_bn",
        "description": "TTS.tts.layers.generic.res_conv_bn",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.generic.res_conv_bn",
        "documentation": {}
    },
    {
        "label": "WNBlocks",
        "importPath": "TTS.tts.layers.generic.wavenet",
        "description": "TTS.tts.layers.generic.wavenet",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.generic.wavenet",
        "documentation": {}
    },
    {
        "label": "WN",
        "importPath": "TTS.tts.layers.generic.wavenet",
        "description": "TTS.tts.layers.generic.wavenet",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.generic.wavenet",
        "documentation": {}
    },
    {
        "label": "RelativePositionTransformer",
        "importPath": "TTS.tts.layers.glow_tts.transformer",
        "description": "TTS.tts.layers.glow_tts.transformer",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.glow_tts.transformer",
        "documentation": {}
    },
    {
        "label": "RelativePositionTransformer",
        "importPath": "TTS.tts.layers.glow_tts.transformer",
        "description": "TTS.tts.layers.glow_tts.transformer",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.glow_tts.transformer",
        "documentation": {}
    },
    {
        "label": "RelativePositionTransformer",
        "importPath": "TTS.tts.layers.glow_tts.transformer",
        "description": "TTS.tts.layers.glow_tts.transformer",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.glow_tts.transformer",
        "documentation": {}
    },
    {
        "label": "RelativePositionTransformer",
        "importPath": "TTS.tts.layers.glow_tts.transformer",
        "description": "TTS.tts.layers.glow_tts.transformer",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.glow_tts.transformer",
        "documentation": {}
    },
    {
        "label": "ActNorm",
        "importPath": "TTS.tts.layers.generic.normalization",
        "description": "TTS.tts.layers.generic.normalization",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.generic.normalization",
        "documentation": {}
    },
    {
        "label": "LayerNorm",
        "importPath": "TTS.tts.layers.generic.normalization",
        "description": "TTS.tts.layers.generic.normalization",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.generic.normalization",
        "documentation": {}
    },
    {
        "label": "LayerNorm2",
        "importPath": "TTS.tts.layers.generic.normalization",
        "description": "TTS.tts.layers.generic.normalization",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.generic.normalization",
        "documentation": {}
    },
    {
        "label": "LayerNorm2",
        "importPath": "TTS.tts.layers.generic.normalization",
        "description": "TTS.tts.layers.generic.normalization",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.generic.normalization",
        "documentation": {}
    },
    {
        "label": "CouplingBlock",
        "importPath": "TTS.tts.layers.glow_tts.glow",
        "description": "TTS.tts.layers.glow_tts.glow",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.glow_tts.glow",
        "documentation": {}
    },
    {
        "label": "InvConvNear",
        "importPath": "TTS.tts.layers.glow_tts.glow",
        "description": "TTS.tts.layers.glow_tts.glow",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.glow_tts.glow",
        "documentation": {}
    },
    {
        "label": "ResidualConv1dLayerNormBlock",
        "importPath": "TTS.tts.layers.glow_tts.glow",
        "description": "TTS.tts.layers.glow_tts.glow",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.glow_tts.glow",
        "documentation": {}
    },
    {
        "label": "WN",
        "importPath": "TTS.tts.layers.glow_tts.glow",
        "description": "TTS.tts.layers.glow_tts.glow",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.glow_tts.glow",
        "documentation": {}
    },
    {
        "label": "GatedConvBlock",
        "importPath": "TTS.tts.layers.generic.gated_conv",
        "description": "TTS.tts.layers.generic.gated_conv",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.generic.gated_conv",
        "documentation": {}
    },
    {
        "label": "TimeDepthSeparableConvBlock",
        "importPath": "TTS.tts.layers.generic.time_depth_sep_conv",
        "description": "TTS.tts.layers.generic.time_depth_sep_conv",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.generic.time_depth_sep_conv",
        "documentation": {}
    },
    {
        "label": "DurationPredictor",
        "importPath": "TTS.tts.layers.glow_tts.duration_predictor",
        "description": "TTS.tts.layers.glow_tts.duration_predictor",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.glow_tts.duration_predictor",
        "documentation": {}
    },
    {
        "label": "DurationPredictor",
        "importPath": "TTS.tts.layers.glow_tts.duration_predictor",
        "description": "TTS.tts.layers.glow_tts.duration_predictor",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.glow_tts.duration_predictor",
        "documentation": {}
    },
    {
        "label": "DurationPredictor",
        "importPath": "TTS.tts.layers.glow_tts.duration_predictor",
        "description": "TTS.tts.layers.glow_tts.duration_predictor",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.glow_tts.duration_predictor",
        "documentation": {}
    },
    {
        "label": "sequence_mask",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "sequence_mask",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "sequence_mask",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "sequence_mask",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "sequence_mask",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "generate_path",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "maximum_path",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "sequence_mask",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "sequence_mask",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "average_over_durations",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "generate_path",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "maximum_path",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "sequence_mask",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "generate_path",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "maximum_path",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "sequence_mask",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "generate_path",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "maximum_path",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "rand_segments",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "segment",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "sequence_mask",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "sequence_mask",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "sequence_mask",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "average_over_durations",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "generate_path",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "rand_segments",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "segment",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "sequence_mask",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "sequence_mask",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "sequence_mask",
        "importPath": "TTS.tts.utils.helpers",
        "description": "TTS.tts.utils.helpers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "Version",
        "importPath": "packaging.version",
        "description": "packaging.version",
        "isExtraImport": true,
        "detail": "packaging.version",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "AvgPool1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Conv1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Conv2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "ConvTranspose1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Parameter",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "LayerNorm",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "AvgPool1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Conv1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Conv2d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "ConvTranspose1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Conv1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "ConvTranspose1d",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "functional",
        "importPath": "torch.nn",
        "description": "torch.nn",
        "isExtraImport": true,
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm.auto",
        "description": "tqdm.auto",
        "isExtraImport": true,
        "detail": "tqdm.auto",
        "documentation": {}
    },
    {
        "label": "Linear",
        "importPath": "TTS.tts.layers.tacotron.common_layers",
        "description": "TTS.tts.layers.tacotron.common_layers",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tacotron.common_layers",
        "documentation": {}
    },
    {
        "label": "Prenet",
        "importPath": "TTS.tts.layers.tacotron.common_layers",
        "description": "TTS.tts.layers.tacotron.common_layers",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tacotron.common_layers",
        "documentation": {}
    },
    {
        "label": "Linear",
        "importPath": "TTS.tts.layers.tacotron.common_layers",
        "description": "TTS.tts.layers.tacotron.common_layers",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tacotron.common_layers",
        "documentation": {}
    },
    {
        "label": "ConvBNBlock",
        "importPath": "TTS.tts.layers.tacotron.tacotron2",
        "description": "TTS.tts.layers.tacotron.tacotron2",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tacotron.tacotron2",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "importPath": "TTS.tts.layers.tacotron.tacotron2",
        "description": "TTS.tts.layers.tacotron.tacotron2",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tacotron.tacotron2",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "importPath": "TTS.tts.layers.tacotron.tacotron2",
        "description": "TTS.tts.layers.tacotron.tacotron2",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tacotron.tacotron2",
        "documentation": {}
    },
    {
        "label": "Postnet",
        "importPath": "TTS.tts.layers.tacotron.tacotron2",
        "description": "TTS.tts.layers.tacotron.tacotron2",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tacotron.tacotron2",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "importPath": "TTS.tts.layers.glow_tts.decoder",
        "description": "TTS.tts.layers.glow_tts.decoder",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.glow_tts.decoder",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "importPath": "TTS.tts.layers.glow_tts.decoder",
        "description": "TTS.tts.layers.glow_tts.decoder",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.glow_tts.decoder",
        "documentation": {}
    },
    {
        "label": "torch.distributions",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.distributions",
        "description": "torch.distributions",
        "detail": "torch.distributions",
        "documentation": {}
    },
    {
        "label": "checkpoint",
        "importPath": "torch.utils.checkpoint",
        "description": "torch.utils.checkpoint",
        "isExtraImport": true,
        "detail": "torch.utils.checkpoint",
        "documentation": {}
    },
    {
        "label": "Outputnet",
        "importPath": "TTS.tts.layers.overflow.common_layers",
        "description": "TTS.tts.layers.overflow.common_layers",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.overflow.common_layers",
        "documentation": {}
    },
    {
        "label": "OverflowUtils",
        "importPath": "TTS.tts.layers.overflow.common_layers",
        "description": "TTS.tts.layers.overflow.common_layers",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.overflow.common_layers",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "importPath": "TTS.tts.layers.overflow.common_layers",
        "description": "TTS.tts.layers.overflow.common_layers",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.overflow.common_layers",
        "documentation": {}
    },
    {
        "label": "OverflowUtils",
        "importPath": "TTS.tts.layers.overflow.common_layers",
        "description": "TTS.tts.layers.overflow.common_layers",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.overflow.common_layers",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "importPath": "TTS.tts.layers.overflow.common_layers",
        "description": "TTS.tts.layers.overflow.common_layers",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.overflow.common_layers",
        "documentation": {}
    },
    {
        "label": "OverflowUtils",
        "importPath": "TTS.tts.layers.overflow.common_layers",
        "description": "TTS.tts.layers.overflow.common_layers",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.overflow.common_layers",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "importPath": "TTS.tts.layers.overflow.common_layers",
        "description": "TTS.tts.layers.overflow.common_layers",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.overflow.common_layers",
        "documentation": {}
    },
    {
        "label": "Outputnet",
        "importPath": "TTS.tts.layers.overflow.common_layers",
        "description": "TTS.tts.layers.overflow.common_layers",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.overflow.common_layers",
        "documentation": {}
    },
    {
        "label": "OverflowUtils",
        "importPath": "TTS.tts.layers.overflow.common_layers",
        "description": "TTS.tts.layers.overflow.common_layers",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.overflow.common_layers",
        "documentation": {}
    },
    {
        "label": "betabinom",
        "importPath": "scipy.stats",
        "description": "scipy.stats",
        "isExtraImport": true,
        "detail": "scipy.stats",
        "documentation": {}
    },
    {
        "label": "MultivariateNormal",
        "importPath": "torch.distributions.multivariate_normal",
        "description": "torch.distributions.multivariate_normal",
        "isExtraImport": true,
        "detail": "torch.distributions.multivariate_normal",
        "documentation": {}
    },
    {
        "label": "functools",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "functools",
        "description": "functools",
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "partial",
        "importPath": "functools",
        "description": "functools",
        "isExtraImport": true,
        "detail": "functools",
        "documentation": {}
    },
    {
        "label": "fsspec",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "fsspec",
        "description": "fsspec",
        "detail": "fsspec",
        "documentation": {}
    },
    {
        "label": "LogitsWarper",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "GPT2Config",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "GPT2PreTrainedModel",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "LogitsProcessorList",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2CTCTokenizer",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2FeatureExtractor",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "Wav2Vec2ForCTC",
        "importPath": "transformers",
        "description": "transformers",
        "isExtraImport": true,
        "detail": "transformers",
        "documentation": {}
    },
    {
        "label": "ContinuousTransformerWrapper",
        "importPath": "TTS.tts.layers.tortoise.xtransformers",
        "description": "TTS.tts.layers.tortoise.xtransformers",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "RelativePositionBias",
        "importPath": "TTS.tts.layers.tortoise.xtransformers",
        "description": "TTS.tts.layers.tortoise.xtransformers",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "importPath": "TTS.tts.layers.tortoise.xtransformers",
        "description": "TTS.tts.layers.tortoise.xtransformers",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "scipy.io.wavfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "scipy.io.wavfile",
        "description": "scipy.io.wavfile",
        "detail": "scipy.io.wavfile",
        "documentation": {}
    },
    {
        "label": "read",
        "importPath": "scipy.io.wavfile",
        "description": "scipy.io.wavfile",
        "isExtraImport": true,
        "detail": "scipy.io.wavfile",
        "documentation": {}
    },
    {
        "label": "TorchSTFT",
        "importPath": "TTS.utils.audio.torch_transforms",
        "description": "TTS.utils.audio.torch_transforms",
        "isExtraImport": true,
        "detail": "TTS.utils.audio.torch_transforms",
        "documentation": {}
    },
    {
        "label": "TorchSTFT",
        "importPath": "TTS.utils.audio.torch_transforms",
        "description": "TTS.utils.audio.torch_transforms",
        "isExtraImport": true,
        "detail": "TTS.utils.audio.torch_transforms",
        "documentation": {}
    },
    {
        "label": "TorchSTFT",
        "importPath": "TTS.utils.audio.torch_transforms",
        "description": "TTS.utils.audio.torch_transforms",
        "isExtraImport": true,
        "detail": "TTS.utils.audio.torch_transforms",
        "documentation": {}
    },
    {
        "label": "TorchSTFT",
        "importPath": "TTS.utils.audio.torch_transforms",
        "description": "TTS.utils.audio.torch_transforms",
        "isExtraImport": true,
        "detail": "TTS.utils.audio.torch_transforms",
        "documentation": {}
    },
    {
        "label": "CausalLMOutputWithCrossAttentions",
        "importPath": "transformers.modeling_outputs",
        "description": "transformers.modeling_outputs",
        "isExtraImport": true,
        "detail": "transformers.modeling_outputs",
        "documentation": {}
    },
    {
        "label": "AttentionBlock",
        "importPath": "TTS.tts.layers.tortoise.arch_utils",
        "description": "TTS.tts.layers.tortoise.arch_utils",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.arch_utils",
        "documentation": {}
    },
    {
        "label": "TypicalLogitsWarper",
        "importPath": "TTS.tts.layers.tortoise.arch_utils",
        "description": "TTS.tts.layers.tortoise.arch_utils",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.arch_utils",
        "documentation": {}
    },
    {
        "label": "AttentionBlock",
        "importPath": "TTS.tts.layers.tortoise.arch_utils",
        "description": "TTS.tts.layers.tortoise.arch_utils",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.arch_utils",
        "documentation": {}
    },
    {
        "label": "Downsample",
        "importPath": "TTS.tts.layers.tortoise.arch_utils",
        "description": "TTS.tts.layers.tortoise.arch_utils",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.arch_utils",
        "documentation": {}
    },
    {
        "label": "Upsample",
        "importPath": "TTS.tts.layers.tortoise.arch_utils",
        "description": "TTS.tts.layers.tortoise.arch_utils",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.arch_utils",
        "documentation": {}
    },
    {
        "label": "normalization",
        "importPath": "TTS.tts.layers.tortoise.arch_utils",
        "description": "TTS.tts.layers.tortoise.arch_utils",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.arch_utils",
        "documentation": {}
    },
    {
        "label": "zero_module",
        "importPath": "TTS.tts.layers.tortoise.arch_utils",
        "description": "TTS.tts.layers.tortoise.arch_utils",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.arch_utils",
        "documentation": {}
    },
    {
        "label": "CheckpointedXTransformerEncoder",
        "importPath": "TTS.tts.layers.tortoise.arch_utils",
        "description": "TTS.tts.layers.tortoise.arch_utils",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.arch_utils",
        "documentation": {}
    },
    {
        "label": "AttentionBlock",
        "importPath": "TTS.tts.layers.tortoise.arch_utils",
        "description": "TTS.tts.layers.tortoise.arch_utils",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.arch_utils",
        "documentation": {}
    },
    {
        "label": "normalization",
        "importPath": "TTS.tts.layers.tortoise.arch_utils",
        "description": "TTS.tts.layers.tortoise.arch_utils",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.arch_utils",
        "documentation": {}
    },
    {
        "label": "TorchMelSpectrogram",
        "importPath": "TTS.tts.layers.tortoise.arch_utils",
        "description": "TTS.tts.layers.tortoise.arch_utils",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.arch_utils",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "importPath": "TTS.tts.layers.tortoise.transformer",
        "description": "TTS.tts.layers.tortoise.transformer",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.transformer",
        "documentation": {}
    },
    {
        "label": "enum",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "enum",
        "description": "enum",
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "enum",
        "description": "enum",
        "isExtraImport": true,
        "detail": "enum",
        "documentation": {}
    },
    {
        "label": "sample_dpmpp_2m",
        "importPath": "k_diffusion.sampling",
        "description": "k_diffusion.sampling",
        "isExtraImport": true,
        "detail": "k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "sample_euler_ancestral",
        "importPath": "k_diffusion.sampling",
        "description": "k_diffusion.sampling",
        "isExtraImport": true,
        "detail": "k_diffusion.sampling",
        "documentation": {}
    },
    {
        "label": "DPM_Solver",
        "importPath": "TTS.tts.layers.tortoise.dpm_solver",
        "description": "TTS.tts.layers.tortoise.dpm_solver",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.dpm_solver",
        "documentation": {}
    },
    {
        "label": "NoiseScheduleVP",
        "importPath": "TTS.tts.layers.tortoise.dpm_solver",
        "description": "TTS.tts.layers.tortoise.dpm_solver",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.dpm_solver",
        "documentation": {}
    },
    {
        "label": "model_wrapper",
        "importPath": "TTS.tts.layers.tortoise.dpm_solver",
        "description": "TTS.tts.layers.tortoise.dpm_solver",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.dpm_solver",
        "documentation": {}
    },
    {
        "label": "abc",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "abc",
        "description": "abc",
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "abstractmethod",
        "importPath": "abc",
        "description": "abc",
        "isExtraImport": true,
        "detail": "abc",
        "documentation": {}
    },
    {
        "label": "Tokenizer",
        "importPath": "tokenizers",
        "description": "tokenizers",
        "isExtraImport": true,
        "detail": "tokenizers",
        "documentation": {}
    },
    {
        "label": "english_cleaners",
        "importPath": "TTS.tts.utils.text.cleaners",
        "description": "TTS.tts.utils.text.cleaners",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.cleaners",
        "documentation": {}
    },
    {
        "label": "english_cleaners",
        "importPath": "TTS.tts.utils.text.cleaners",
        "description": "TTS.tts.utils.text.cleaners",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.cleaners",
        "documentation": {}
    },
    {
        "label": "phoneme_cleaners",
        "importPath": "TTS.tts.utils.text.cleaners",
        "description": "TTS.tts.utils.text.cleaners",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.cleaners",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "rearrange",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "repeat",
        "importPath": "einops",
        "description": "einops",
        "isExtraImport": true,
        "detail": "einops",
        "documentation": {}
    },
    {
        "label": "urllib",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "urllib",
        "description": "urllib",
        "detail": "urllib",
        "documentation": {}
    },
    {
        "label": "request",
        "importPath": "urllib",
        "description": "urllib",
        "isExtraImport": true,
        "detail": "urllib",
        "documentation": {}
    },
    {
        "label": "isfunction",
        "importPath": "inspect",
        "description": "inspect",
        "isExtraImport": true,
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "signature",
        "importPath": "inspect",
        "description": "inspect",
        "isExtraImport": true,
        "detail": "inspect",
        "documentation": {}
    },
    {
        "label": "Conv1d",
        "importPath": "torch.nn.modules.conv",
        "description": "torch.nn.modules.conv",
        "isExtraImport": true,
        "detail": "torch.nn.modules.conv",
        "documentation": {}
    },
    {
        "label": "DiscriminatorP",
        "importPath": "TTS.vocoder.models.hifigan_discriminator",
        "description": "TTS.vocoder.models.hifigan_discriminator",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.hifigan_discriminator",
        "documentation": {}
    },
    {
        "label": "MultiPeriodDiscriminator",
        "importPath": "TTS.vocoder.models.hifigan_discriminator",
        "description": "TTS.vocoder.models.hifigan_discriminator",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.hifigan_discriminator",
        "documentation": {}
    },
    {
        "label": "MultiPeriodDiscriminator",
        "importPath": "TTS.vocoder.models.hifigan_discriminator",
        "description": "TTS.vocoder.models.hifigan_discriminator",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.hifigan_discriminator",
        "documentation": {}
    },
    {
        "label": "piecewise_rational_quadratic_transform",
        "importPath": "TTS.tts.layers.vits.transforms",
        "description": "TTS.tts.layers.vits.transforms",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.vits.transforms",
        "documentation": {}
    },
    {
        "label": "SSIMLoss",
        "importPath": "TTS.tts.utils.ssim",
        "description": "TTS.tts.utils.ssim",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.ssim",
        "documentation": {}
    },
    {
        "label": "MDNBlock",
        "importPath": "TTS.tts.layers.align_tts.mdn",
        "description": "TTS.tts.layers.align_tts.mdn",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.align_tts.mdn",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "importPath": "TTS.tts.layers.feed_forward.decoder",
        "description": "TTS.tts.layers.feed_forward.decoder",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.feed_forward.decoder",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "importPath": "TTS.tts.layers.feed_forward.decoder",
        "description": "TTS.tts.layers.feed_forward.decoder",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.feed_forward.decoder",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "importPath": "TTS.tts.layers.feed_forward.decoder",
        "description": "TTS.tts.layers.feed_forward.decoder",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.feed_forward.decoder",
        "documentation": {}
    },
    {
        "label": "DurationPredictor",
        "importPath": "TTS.tts.layers.feed_forward.duration_predictor",
        "description": "TTS.tts.layers.feed_forward.duration_predictor",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.feed_forward.duration_predictor",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "importPath": "TTS.tts.layers.feed_forward.encoder",
        "description": "TTS.tts.layers.feed_forward.encoder",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.feed_forward.encoder",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "importPath": "TTS.tts.layers.feed_forward.encoder",
        "description": "TTS.tts.layers.feed_forward.encoder",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.feed_forward.encoder",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "importPath": "TTS.tts.layers.feed_forward.encoder",
        "description": "TTS.tts.layers.feed_forward.encoder",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.feed_forward.encoder",
        "documentation": {}
    },
    {
        "label": "BaseTTS",
        "importPath": "TTS.tts.models.base_tts",
        "description": "TTS.tts.models.base_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.base_tts",
        "documentation": {}
    },
    {
        "label": "BaseTTS",
        "importPath": "TTS.tts.models.base_tts",
        "description": "TTS.tts.models.base_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.base_tts",
        "documentation": {}
    },
    {
        "label": "BaseTTS",
        "importPath": "TTS.tts.models.base_tts",
        "description": "TTS.tts.models.base_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.base_tts",
        "documentation": {}
    },
    {
        "label": "BaseTTS",
        "importPath": "TTS.tts.models.base_tts",
        "description": "TTS.tts.models.base_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.base_tts",
        "documentation": {}
    },
    {
        "label": "BaseTTS",
        "importPath": "TTS.tts.models.base_tts",
        "description": "TTS.tts.models.base_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.base_tts",
        "documentation": {}
    },
    {
        "label": "BaseTTS",
        "importPath": "TTS.tts.models.base_tts",
        "description": "TTS.tts.models.base_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.base_tts",
        "documentation": {}
    },
    {
        "label": "BaseTTS",
        "importPath": "TTS.tts.models.base_tts",
        "description": "TTS.tts.models.base_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.base_tts",
        "documentation": {}
    },
    {
        "label": "BaseTTS",
        "importPath": "TTS.tts.models.base_tts",
        "description": "TTS.tts.models.base_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.base_tts",
        "documentation": {}
    },
    {
        "label": "plot_alignment",
        "importPath": "TTS.tts.utils.visual",
        "description": "TTS.tts.utils.visual",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_spectrogram",
        "importPath": "TTS.tts.utils.visual",
        "description": "TTS.tts.utils.visual",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_alignment",
        "importPath": "TTS.tts.utils.visual",
        "description": "TTS.tts.utils.visual",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_spectrogram",
        "importPath": "TTS.tts.utils.visual",
        "description": "TTS.tts.utils.visual",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_alignment",
        "importPath": "TTS.tts.utils.visual",
        "description": "TTS.tts.utils.visual",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_spectrogram",
        "importPath": "TTS.tts.utils.visual",
        "description": "TTS.tts.utils.visual",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_alignment",
        "importPath": "TTS.tts.utils.visual",
        "description": "TTS.tts.utils.visual",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_avg_energy",
        "importPath": "TTS.tts.utils.visual",
        "description": "TTS.tts.utils.visual",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_avg_pitch",
        "importPath": "TTS.tts.utils.visual",
        "description": "TTS.tts.utils.visual",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_spectrogram",
        "importPath": "TTS.tts.utils.visual",
        "description": "TTS.tts.utils.visual",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_alignment",
        "importPath": "TTS.tts.utils.visual",
        "description": "TTS.tts.utils.visual",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_spectrogram",
        "importPath": "TTS.tts.utils.visual",
        "description": "TTS.tts.utils.visual",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_alignment",
        "importPath": "TTS.tts.utils.visual",
        "description": "TTS.tts.utils.visual",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_spectrogram",
        "importPath": "TTS.tts.utils.visual",
        "description": "TTS.tts.utils.visual",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_alignment",
        "importPath": "TTS.tts.utils.visual",
        "description": "TTS.tts.utils.visual",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_spectrogram",
        "importPath": "TTS.tts.utils.visual",
        "description": "TTS.tts.utils.visual",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_alignment",
        "importPath": "TTS.tts.utils.visual",
        "description": "TTS.tts.utils.visual",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_spectrogram",
        "importPath": "TTS.tts.utils.visual",
        "description": "TTS.tts.utils.visual",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_alignment",
        "importPath": "TTS.tts.utils.visual",
        "description": "TTS.tts.utils.visual",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_spectrogram",
        "importPath": "TTS.tts.utils.visual",
        "description": "TTS.tts.utils.visual",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_alignment",
        "importPath": "TTS.tts.utils.visual",
        "description": "TTS.tts.utils.visual",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_alignment",
        "importPath": "TTS.tts.utils.visual",
        "description": "TTS.tts.utils.visual",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_spectrogram",
        "importPath": "TTS.tts.utils.visual",
        "description": "TTS.tts.utils.visual",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_spectrogram",
        "importPath": "TTS.tts.utils.visual",
        "description": "TTS.tts.utils.visual",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_spectrogram",
        "importPath": "TTS.tts.utils.visual",
        "description": "TTS.tts.utils.visual",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "copy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "copy",
        "description": "copy",
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "deepcopy",
        "importPath": "copy",
        "description": "copy",
        "isExtraImport": true,
        "detail": "copy",
        "documentation": {}
    },
    {
        "label": "TacotronLoss",
        "importPath": "TTS.tts.layers.losses",
        "description": "TTS.tts.layers.losses",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.losses",
        "documentation": {}
    },
    {
        "label": "GlowTTSLoss",
        "importPath": "TTS.tts.layers.losses",
        "description": "TTS.tts.layers.losses",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.losses",
        "documentation": {}
    },
    {
        "label": "BCELossMasked",
        "importPath": "TTS.tts.layers.losses",
        "description": "TTS.tts.layers.losses",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.losses",
        "documentation": {}
    },
    {
        "label": "L1LossMasked",
        "importPath": "TTS.tts.layers.losses",
        "description": "TTS.tts.layers.losses",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.losses",
        "documentation": {}
    },
    {
        "label": "MSELossMasked",
        "importPath": "TTS.tts.layers.losses",
        "description": "TTS.tts.layers.losses",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.losses",
        "documentation": {}
    },
    {
        "label": "SSIMLoss",
        "importPath": "TTS.tts.layers.losses",
        "description": "TTS.tts.layers.losses",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.losses",
        "documentation": {}
    },
    {
        "label": "MSELossMasked",
        "importPath": "TTS.tts.layers.losses",
        "description": "TTS.tts.layers.losses",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.losses",
        "documentation": {}
    },
    {
        "label": "L1LossMasked",
        "importPath": "TTS.tts.layers.losses",
        "description": "TTS.tts.layers.losses",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.losses",
        "documentation": {}
    },
    {
        "label": "synthesis",
        "importPath": "TTS.tts.utils.synthesis",
        "description": "TTS.tts.utils.synthesis",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.synthesis",
        "documentation": {}
    },
    {
        "label": "synthesis",
        "importPath": "TTS.tts.utils.synthesis",
        "description": "TTS.tts.utils.synthesis",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.synthesis",
        "documentation": {}
    },
    {
        "label": "synthesis",
        "importPath": "TTS.tts.utils.synthesis",
        "description": "TTS.tts.utils.synthesis",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.synthesis",
        "documentation": {}
    },
    {
        "label": "synthesis",
        "importPath": "TTS.tts.utils.synthesis",
        "description": "TTS.tts.utils.synthesis",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.synthesis",
        "documentation": {}
    },
    {
        "label": "synthesis",
        "importPath": "TTS.tts.utils.synthesis",
        "description": "TTS.tts.utils.synthesis",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.synthesis",
        "documentation": {}
    },
    {
        "label": "transfer_voice",
        "importPath": "TTS.tts.utils.synthesis",
        "description": "TTS.tts.utils.synthesis",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.synthesis",
        "documentation": {}
    },
    {
        "label": "trim_silence",
        "importPath": "TTS.tts.utils.synthesis",
        "description": "TTS.tts.utils.synthesis",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.synthesis",
        "documentation": {}
    },
    {
        "label": "synthesis",
        "importPath": "TTS.tts.utils.synthesis",
        "description": "TTS.tts.utils.synthesis",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.synthesis",
        "documentation": {}
    },
    {
        "label": "torch.distributed",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.distributed",
        "description": "torch.distributed",
        "detail": "torch.distributed",
        "documentation": {}
    },
    {
        "label": "WeightedRandomSampler",
        "importPath": "torch.utils.data.sampler",
        "description": "torch.utils.data.sampler",
        "isExtraImport": true,
        "detail": "torch.utils.data.sampler",
        "documentation": {}
    },
    {
        "label": "WeightedRandomSampler",
        "importPath": "torch.utils.data.sampler",
        "description": "torch.utils.data.sampler",
        "isExtraImport": true,
        "detail": "torch.utils.data.sampler",
        "documentation": {}
    },
    {
        "label": "BatchSampler",
        "importPath": "torch.utils.data.sampler",
        "description": "torch.utils.data.sampler",
        "isExtraImport": true,
        "detail": "torch.utils.data.sampler",
        "documentation": {}
    },
    {
        "label": "Sampler",
        "importPath": "torch.utils.data.sampler",
        "description": "torch.utils.data.sampler",
        "isExtraImport": true,
        "detail": "torch.utils.data.sampler",
        "documentation": {}
    },
    {
        "label": "SubsetRandomSampler",
        "importPath": "torch.utils.data.sampler",
        "description": "torch.utils.data.sampler",
        "isExtraImport": true,
        "detail": "torch.utils.data.sampler",
        "documentation": {}
    },
    {
        "label": "WeightedRandomSampler",
        "importPath": "torch.utils.data.sampler",
        "description": "torch.utils.data.sampler",
        "isExtraImport": true,
        "detail": "torch.utils.data.sampler",
        "documentation": {}
    },
    {
        "label": "BaseTrainerModel",
        "importPath": "TTS.model",
        "description": "TTS.model",
        "isExtraImport": true,
        "detail": "TTS.model",
        "documentation": {}
    },
    {
        "label": "BaseTrainerModel",
        "importPath": "TTS.model",
        "description": "TTS.model",
        "isExtraImport": true,
        "detail": "TTS.model",
        "documentation": {}
    },
    {
        "label": "BaseTrainerModel",
        "importPath": "TTS.model",
        "description": "TTS.model",
        "isExtraImport": true,
        "detail": "TTS.model",
        "documentation": {}
    },
    {
        "label": "TTSDataset",
        "importPath": "TTS.tts.datasets.dataset",
        "description": "TTS.tts.datasets.dataset",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets.dataset",
        "documentation": {}
    },
    {
        "label": "TTSDataset",
        "importPath": "TTS.tts.datasets.dataset",
        "description": "TTS.tts.datasets.dataset",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets.dataset",
        "documentation": {}
    },
    {
        "label": "_parse_sample",
        "importPath": "TTS.tts.datasets.dataset",
        "description": "TTS.tts.datasets.dataset",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets.dataset",
        "documentation": {}
    },
    {
        "label": "TTSDataset",
        "importPath": "TTS.tts.datasets.dataset",
        "description": "TTS.tts.datasets.dataset",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets.dataset",
        "documentation": {}
    },
    {
        "label": "LanguageManager",
        "importPath": "TTS.tts.utils.languages",
        "description": "TTS.tts.utils.languages",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.languages",
        "documentation": {}
    },
    {
        "label": "get_language_balancer_weights",
        "importPath": "TTS.tts.utils.languages",
        "description": "TTS.tts.utils.languages",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.languages",
        "documentation": {}
    },
    {
        "label": "LanguageManager",
        "importPath": "TTS.tts.utils.languages",
        "description": "TTS.tts.utils.languages",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.languages",
        "documentation": {}
    },
    {
        "label": "LanguageManager",
        "importPath": "TTS.tts.utils.languages",
        "description": "TTS.tts.utils.languages",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.languages",
        "documentation": {}
    },
    {
        "label": "get_language_balancer_weights",
        "importPath": "TTS.tts.utils.languages",
        "description": "TTS.tts.utils.languages",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.languages",
        "documentation": {}
    },
    {
        "label": "LanguageManager",
        "importPath": "TTS.tts.utils.languages",
        "description": "TTS.tts.utils.languages",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.languages",
        "documentation": {}
    },
    {
        "label": "LanguageManager",
        "importPath": "TTS.tts.utils.languages",
        "description": "TTS.tts.utils.languages",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.languages",
        "documentation": {}
    },
    {
        "label": "get_language_balancer_weights",
        "importPath": "TTS.tts.utils.languages",
        "description": "TTS.tts.utils.languages",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.languages",
        "documentation": {}
    },
    {
        "label": "LanguageManager",
        "importPath": "TTS.tts.utils.languages",
        "description": "TTS.tts.utils.languages",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.languages",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp.autocast_mode",
        "description": "torch.cuda.amp.autocast_mode",
        "isExtraImport": true,
        "detail": "torch.cuda.amp.autocast_mode",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp.autocast_mode",
        "description": "torch.cuda.amp.autocast_mode",
        "isExtraImport": true,
        "detail": "torch.cuda.amp.autocast_mode",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp.autocast_mode",
        "description": "torch.cuda.amp.autocast_mode",
        "isExtraImport": true,
        "detail": "torch.cuda.amp.autocast_mode",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp.autocast_mode",
        "description": "torch.cuda.amp.autocast_mode",
        "isExtraImport": true,
        "detail": "torch.cuda.amp.autocast_mode",
        "documentation": {}
    },
    {
        "label": "autocast",
        "importPath": "torch.cuda.amp.autocast_mode",
        "description": "torch.cuda.amp.autocast_mode",
        "isExtraImport": true,
        "detail": "torch.cuda.amp.autocast_mode",
        "documentation": {}
    },
    {
        "label": "AlignmentNetwork",
        "importPath": "TTS.tts.layers.generic.aligner",
        "description": "TTS.tts.layers.generic.aligner",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.generic.aligner",
        "documentation": {}
    },
    {
        "label": "GlowTTSConfig",
        "importPath": "TTS.tts.configs.glow_tts_config",
        "description": "TTS.tts.configs.glow_tts_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.glow_tts_config",
        "documentation": {}
    },
    {
        "label": "GlowTTSConfig",
        "importPath": "TTS.tts.configs.glow_tts_config",
        "description": "TTS.tts.configs.glow_tts_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.glow_tts_config",
        "documentation": {}
    },
    {
        "label": "GlowTTSConfig",
        "importPath": "TTS.tts.configs.glow_tts_config",
        "description": "TTS.tts.configs.glow_tts_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.glow_tts_config",
        "documentation": {}
    },
    {
        "label": "GlowTTSConfig",
        "importPath": "TTS.tts.configs.glow_tts_config",
        "description": "TTS.tts.configs.glow_tts_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.glow_tts_config",
        "documentation": {}
    },
    {
        "label": "GlowTTSConfig",
        "importPath": "TTS.tts.configs.glow_tts_config",
        "description": "TTS.tts.configs.glow_tts_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.glow_tts_config",
        "documentation": {}
    },
    {
        "label": "GlowTTSConfig",
        "importPath": "TTS.tts.configs.glow_tts_config",
        "description": "TTS.tts.configs.glow_tts_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.glow_tts_config",
        "documentation": {}
    },
    {
        "label": "GlowTTSConfig",
        "importPath": "TTS.tts.configs.glow_tts_config",
        "description": "TTS.tts.configs.glow_tts_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.glow_tts_config",
        "documentation": {}
    },
    {
        "label": "GlowTTSConfig",
        "importPath": "TTS.tts.configs.glow_tts_config",
        "description": "TTS.tts.configs.glow_tts_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.glow_tts_config",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "importPath": "TTS.tts.layers.glow_tts.encoder",
        "description": "TTS.tts.layers.glow_tts.encoder",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.glow_tts.encoder",
        "documentation": {}
    },
    {
        "label": "TensorboardLogger",
        "importPath": "trainer.logging.tensorboard_logger",
        "description": "trainer.logging.tensorboard_logger",
        "isExtraImport": true,
        "detail": "trainer.logging.tensorboard_logger",
        "documentation": {}
    },
    {
        "label": "TensorboardLogger",
        "importPath": "trainer.logging.tensorboard_logger",
        "description": "trainer.logging.tensorboard_logger",
        "isExtraImport": true,
        "detail": "trainer.logging.tensorboard_logger",
        "documentation": {}
    },
    {
        "label": "TensorboardLogger",
        "importPath": "trainer.logging.tensorboard_logger",
        "description": "trainer.logging.tensorboard_logger",
        "isExtraImport": true,
        "detail": "trainer.logging.tensorboard_logger",
        "documentation": {}
    },
    {
        "label": "TensorboardLogger",
        "importPath": "trainer.logging.tensorboard_logger",
        "description": "trainer.logging.tensorboard_logger",
        "isExtraImport": true,
        "detail": "trainer.logging.tensorboard_logger",
        "documentation": {}
    },
    {
        "label": "NeuralHMM",
        "importPath": "TTS.tts.layers.overflow.neural_hmm",
        "description": "TTS.tts.layers.overflow.neural_hmm",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.overflow.neural_hmm",
        "documentation": {}
    },
    {
        "label": "NeuralHMM",
        "importPath": "TTS.tts.layers.overflow.neural_hmm",
        "description": "TTS.tts.layers.overflow.neural_hmm",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.overflow.neural_hmm",
        "documentation": {}
    },
    {
        "label": "EmissionModel",
        "importPath": "TTS.tts.layers.overflow.neural_hmm",
        "description": "TTS.tts.layers.overflow.neural_hmm",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.overflow.neural_hmm",
        "documentation": {}
    },
    {
        "label": "NeuralHMM",
        "importPath": "TTS.tts.layers.overflow.neural_hmm",
        "description": "TTS.tts.layers.overflow.neural_hmm",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.overflow.neural_hmm",
        "documentation": {}
    },
    {
        "label": "TransitionModel",
        "importPath": "TTS.tts.layers.overflow.neural_hmm",
        "description": "TTS.tts.layers.overflow.neural_hmm",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.overflow.neural_hmm",
        "documentation": {}
    },
    {
        "label": "get_spec_from_most_probable_state",
        "importPath": "TTS.tts.layers.overflow.plotting_utils",
        "description": "TTS.tts.layers.overflow.plotting_utils",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.overflow.plotting_utils",
        "documentation": {}
    },
    {
        "label": "plot_transition_probabilities_to_numpy",
        "importPath": "TTS.tts.layers.overflow.plotting_utils",
        "description": "TTS.tts.layers.overflow.plotting_utils",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.overflow.plotting_utils",
        "documentation": {}
    },
    {
        "label": "get_spec_from_most_probable_state",
        "importPath": "TTS.tts.layers.overflow.plotting_utils",
        "description": "TTS.tts.layers.overflow.plotting_utils",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.overflow.plotting_utils",
        "documentation": {}
    },
    {
        "label": "plot_transition_probabilities_to_numpy",
        "importPath": "TTS.tts.layers.overflow.plotting_utils",
        "description": "TTS.tts.layers.overflow.plotting_utils",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.overflow.plotting_utils",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "importPath": "TTS.tts.layers.overflow.decoder",
        "description": "TTS.tts.layers.overflow.decoder",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.overflow.decoder",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "importPath": "TTS.tts.layers.overflow.decoder",
        "description": "TTS.tts.layers.overflow.decoder",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.overflow.decoder",
        "documentation": {}
    },
    {
        "label": "CapacitronVAE",
        "importPath": "TTS.tts.layers.tacotron.capacitron_layers",
        "description": "TTS.tts.layers.tacotron.capacitron_layers",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tacotron.capacitron_layers",
        "documentation": {}
    },
    {
        "label": "CapacitronVAE",
        "importPath": "TTS.tts.layers.tacotron.capacitron_layers",
        "description": "TTS.tts.layers.tacotron.capacitron_layers",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tacotron.capacitron_layers",
        "documentation": {}
    },
    {
        "label": "GST",
        "importPath": "TTS.tts.layers.tacotron.gst_layers",
        "description": "TTS.tts.layers.tacotron.gst_layers",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tacotron.gst_layers",
        "documentation": {}
    },
    {
        "label": "GST",
        "importPath": "TTS.tts.layers.tacotron.gst_layers",
        "description": "TTS.tts.layers.tacotron.gst_layers",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tacotron.gst_layers",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "importPath": "TTS.tts.layers.tacotron.tacotron",
        "description": "TTS.tts.layers.tacotron.tacotron",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tacotron.tacotron",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "importPath": "TTS.tts.layers.tacotron.tacotron",
        "description": "TTS.tts.layers.tacotron.tacotron",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tacotron.tacotron",
        "documentation": {}
    },
    {
        "label": "PostCBHG",
        "importPath": "TTS.tts.layers.tacotron.tacotron",
        "description": "TTS.tts.layers.tacotron.tacotron",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tacotron.tacotron",
        "documentation": {}
    },
    {
        "label": "CBHG",
        "importPath": "TTS.tts.layers.tacotron.tacotron",
        "description": "TTS.tts.layers.tacotron.tacotron",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tacotron.tacotron",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "importPath": "TTS.tts.layers.tacotron.tacotron",
        "description": "TTS.tts.layers.tacotron.tacotron",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tacotron.tacotron",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "importPath": "TTS.tts.layers.tacotron.tacotron",
        "description": "TTS.tts.layers.tacotron.tacotron",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tacotron.tacotron",
        "documentation": {}
    },
    {
        "label": "Prenet",
        "importPath": "TTS.tts.layers.tacotron.tacotron",
        "description": "TTS.tts.layers.tacotron.tacotron",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tacotron.tacotron",
        "documentation": {}
    },
    {
        "label": "BaseTacotron",
        "importPath": "TTS.tts.models.base_tacotron",
        "description": "TTS.tts.models.base_tacotron",
        "isExtraImport": true,
        "detail": "TTS.tts.models.base_tacotron",
        "documentation": {}
    },
    {
        "label": "BaseTacotron",
        "importPath": "TTS.tts.models.base_tacotron",
        "description": "TTS.tts.models.base_tacotron",
        "isExtraImport": true,
        "detail": "TTS.tts.models.base_tacotron",
        "documentation": {}
    },
    {
        "label": "alignment_diagonal_score",
        "importPath": "TTS.tts.utils.measures",
        "description": "TTS.tts.utils.measures",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.measures",
        "documentation": {}
    },
    {
        "label": "alignment_diagonal_score",
        "importPath": "TTS.tts.utils.measures",
        "description": "TTS.tts.utils.measures",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.measures",
        "documentation": {}
    },
    {
        "label": "CapacitronOptimizer",
        "importPath": "TTS.utils.capacitron_optimizer",
        "description": "TTS.utils.capacitron_optimizer",
        "isExtraImport": true,
        "detail": "TTS.utils.capacitron_optimizer",
        "documentation": {}
    },
    {
        "label": "CapacitronOptimizer",
        "importPath": "TTS.utils.capacitron_optimizer",
        "description": "TTS.utils.capacitron_optimizer",
        "isExtraImport": true,
        "detail": "TTS.utils.capacitron_optimizer",
        "documentation": {}
    },
    {
        "label": "contextmanager",
        "importPath": "contextlib",
        "description": "contextlib",
        "isExtraImport": true,
        "detail": "contextlib",
        "documentation": {}
    },
    {
        "label": "denormalize_tacotron_mel",
        "importPath": "TTS.tts.layers.tortoise.audio_utils",
        "description": "TTS.tts.layers.tortoise.audio_utils",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.audio_utils",
        "documentation": {}
    },
    {
        "label": "load_voice",
        "importPath": "TTS.tts.layers.tortoise.audio_utils",
        "description": "TTS.tts.layers.tortoise.audio_utils",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.audio_utils",
        "documentation": {}
    },
    {
        "label": "wav_to_univnet_mel",
        "importPath": "TTS.tts.layers.tortoise.audio_utils",
        "description": "TTS.tts.layers.tortoise.audio_utils",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.audio_utils",
        "documentation": {}
    },
    {
        "label": "UnifiedVoice",
        "importPath": "TTS.tts.layers.tortoise.autoregressive",
        "description": "TTS.tts.layers.tortoise.autoregressive",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.autoregressive",
        "documentation": {}
    },
    {
        "label": "AudioMiniEncoderWithClassifierHead",
        "importPath": "TTS.tts.layers.tortoise.classifier",
        "description": "TTS.tts.layers.tortoise.classifier",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.classifier",
        "documentation": {}
    },
    {
        "label": "CLVP",
        "importPath": "TTS.tts.layers.tortoise.clvp",
        "description": "TTS.tts.layers.tortoise.clvp",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.clvp",
        "documentation": {}
    },
    {
        "label": "SpacedDiffusion",
        "importPath": "TTS.tts.layers.tortoise.diffusion",
        "description": "TTS.tts.layers.tortoise.diffusion",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.diffusion",
        "documentation": {}
    },
    {
        "label": "get_named_beta_schedule",
        "importPath": "TTS.tts.layers.tortoise.diffusion",
        "description": "TTS.tts.layers.tortoise.diffusion",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.diffusion",
        "documentation": {}
    },
    {
        "label": "space_timesteps",
        "importPath": "TTS.tts.layers.tortoise.diffusion",
        "description": "TTS.tts.layers.tortoise.diffusion",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.diffusion",
        "documentation": {}
    },
    {
        "label": "DiffusionTts",
        "importPath": "TTS.tts.layers.tortoise.diffusion_decoder",
        "description": "TTS.tts.layers.tortoise.diffusion_decoder",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.diffusion_decoder",
        "documentation": {}
    },
    {
        "label": "RandomLatentConverter",
        "importPath": "TTS.tts.layers.tortoise.random_latent_generator",
        "description": "TTS.tts.layers.tortoise.random_latent_generator",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.random_latent_generator",
        "documentation": {}
    },
    {
        "label": "VoiceBpeTokenizer",
        "importPath": "TTS.tts.layers.tortoise.tokenizer",
        "description": "TTS.tts.layers.tortoise.tokenizer",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.tokenizer",
        "documentation": {}
    },
    {
        "label": "VocConf",
        "importPath": "TTS.tts.layers.tortoise.vocoder",
        "description": "TTS.tts.layers.tortoise.vocoder",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.vocoder",
        "documentation": {}
    },
    {
        "label": "VocType",
        "importPath": "TTS.tts.layers.tortoise.vocoder",
        "description": "TTS.tts.layers.tortoise.vocoder",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.vocoder",
        "documentation": {}
    },
    {
        "label": "Wav2VecAlignment",
        "importPath": "TTS.tts.layers.tortoise.wav2vec_alignment",
        "description": "TTS.tts.layers.tortoise.wav2vec_alignment",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.tortoise.wav2vec_alignment",
        "documentation": {}
    },
    {
        "label": "mel",
        "importPath": "librosa.filters",
        "description": "librosa.filters",
        "isExtraImport": true,
        "detail": "librosa.filters",
        "documentation": {}
    },
    {
        "label": "mel",
        "importPath": "librosa.filters",
        "description": "librosa.filters",
        "isExtraImport": true,
        "detail": "librosa.filters",
        "documentation": {}
    },
    {
        "label": "VitsDiscriminator",
        "importPath": "TTS.tts.layers.vits.discriminator",
        "description": "TTS.tts.layers.vits.discriminator",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.vits.discriminator",
        "documentation": {}
    },
    {
        "label": "PosteriorEncoder",
        "importPath": "TTS.tts.layers.vits.networks",
        "description": "TTS.tts.layers.vits.networks",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.vits.networks",
        "documentation": {}
    },
    {
        "label": "ResidualCouplingBlocks",
        "importPath": "TTS.tts.layers.vits.networks",
        "description": "TTS.tts.layers.vits.networks",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.vits.networks",
        "documentation": {}
    },
    {
        "label": "TextEncoder",
        "importPath": "TTS.tts.layers.vits.networks",
        "description": "TTS.tts.layers.vits.networks",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.vits.networks",
        "documentation": {}
    },
    {
        "label": "StochasticDurationPredictor",
        "importPath": "TTS.tts.layers.vits.stochastic_duration_predictor",
        "description": "TTS.tts.layers.vits.stochastic_duration_predictor",
        "isExtraImport": true,
        "detail": "TTS.tts.layers.vits.stochastic_duration_predictor",
        "documentation": {}
    },
    {
        "label": "rehash_fairseq_vits_checkpoint",
        "importPath": "TTS.tts.utils.fairseq",
        "description": "TTS.tts.utils.fairseq",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.fairseq",
        "documentation": {}
    },
    {
        "label": "HifiganGenerator",
        "importPath": "TTS.vocoder.models.hifigan_generator",
        "description": "TTS.vocoder.models.hifigan_generator",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.hifigan_generator",
        "documentation": {}
    },
    {
        "label": "plot_results",
        "importPath": "TTS.vocoder.utils.generic_utils",
        "description": "TTS.vocoder.utils.generic_utils",
        "isExtraImport": true,
        "detail": "TTS.vocoder.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "interpolate_vocoder_input",
        "importPath": "TTS.vocoder.utils.generic_utils",
        "description": "TTS.vocoder.utils.generic_utils",
        "isExtraImport": true,
        "detail": "TTS.vocoder.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "plot_results",
        "importPath": "TTS.vocoder.utils.generic_utils",
        "description": "TTS.vocoder.utils.generic_utils",
        "isExtraImport": true,
        "detail": "TTS.vocoder.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "plot_results",
        "importPath": "TTS.vocoder.utils.generic_utils",
        "description": "TTS.vocoder.utils.generic_utils",
        "isExtraImport": true,
        "detail": "TTS.vocoder.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "bangla",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "bangla",
        "description": "bangla",
        "detail": "bangla",
        "documentation": {}
    },
    {
        "label": "numerize",
        "importPath": "bnnumerizer",
        "description": "bnnumerizer",
        "isExtraImport": true,
        "detail": "bnnumerizer",
        "documentation": {}
    },
    {
        "label": "Normalizer",
        "importPath": "bnunicodenormalizer",
        "description": "bnunicodenormalizer",
        "isExtraImport": true,
        "detail": "bnunicodenormalizer",
        "documentation": {}
    },
    {
        "label": "jieba",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "jieba",
        "description": "jieba",
        "detail": "jieba",
        "documentation": {}
    },
    {
        "label": "pypinyin",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pypinyin",
        "description": "pypinyin",
        "detail": "pypinyin",
        "documentation": {}
    },
    {
        "label": "inflect",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "inflect",
        "description": "inflect",
        "detail": "inflect",
        "documentation": {}
    },
    {
        "label": "unicodedata",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "unicodedata",
        "description": "unicodedata",
        "detail": "unicodedata",
        "documentation": {}
    },
    {
        "label": "MeCab",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "MeCab",
        "description": "MeCab",
        "detail": "MeCab",
        "documentation": {}
    },
    {
        "label": "num2words",
        "importPath": "num2words",
        "description": "num2words",
        "isExtraImport": true,
        "detail": "num2words",
        "documentation": {}
    },
    {
        "label": "english_dictionary",
        "importPath": "TTS.tts.utils.text.korean.ko_dictionary",
        "description": "TTS.tts.utils.text.korean.ko_dictionary",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.korean.ko_dictionary",
        "documentation": {}
    },
    {
        "label": "etc_dictionary",
        "importPath": "TTS.tts.utils.text.korean.ko_dictionary",
        "description": "TTS.tts.utils.text.korean.ko_dictionary",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.korean.ko_dictionary",
        "documentation": {}
    },
    {
        "label": "hangul_to_jamo",
        "importPath": "jamo",
        "description": "jamo",
        "isExtraImport": true,
        "detail": "jamo",
        "documentation": {}
    },
    {
        "label": "normalize",
        "importPath": "TTS.tts.utils.text.korean.korean",
        "description": "TTS.tts.utils.text.korean.korean",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.korean.korean",
        "documentation": {}
    },
    {
        "label": "bangla_text_to_phonemes",
        "importPath": "TTS.tts.utils.text.bangla.phonemizer",
        "description": "TTS.tts.utils.text.bangla.phonemizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.bangla.phonemizer",
        "documentation": {}
    },
    {
        "label": "BasePhonemizer",
        "importPath": "TTS.tts.utils.text.phonemizers.base",
        "description": "TTS.tts.utils.text.phonemizers.base",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.phonemizers.base",
        "documentation": {}
    },
    {
        "label": "BasePhonemizer",
        "importPath": "TTS.tts.utils.text.phonemizers.base",
        "description": "TTS.tts.utils.text.phonemizers.base",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.phonemizers.base",
        "documentation": {}
    },
    {
        "label": "BasePhonemizer",
        "importPath": "TTS.tts.utils.text.phonemizers.base",
        "description": "TTS.tts.utils.text.phonemizers.base",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.phonemizers.base",
        "documentation": {}
    },
    {
        "label": "BasePhonemizer",
        "importPath": "TTS.tts.utils.text.phonemizers.base",
        "description": "TTS.tts.utils.text.phonemizers.base",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.phonemizers.base",
        "documentation": {}
    },
    {
        "label": "BasePhonemizer",
        "importPath": "TTS.tts.utils.text.phonemizers.base",
        "description": "TTS.tts.utils.text.phonemizers.base",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.phonemizers.base",
        "documentation": {}
    },
    {
        "label": "BasePhonemizer",
        "importPath": "TTS.tts.utils.text.phonemizers.base",
        "description": "TTS.tts.utils.text.phonemizers.base",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.phonemizers.base",
        "documentation": {}
    },
    {
        "label": "Punctuation",
        "importPath": "TTS.tts.utils.text.punctuation",
        "description": "TTS.tts.utils.text.punctuation",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.punctuation",
        "documentation": {}
    },
    {
        "label": "Punctuation",
        "importPath": "TTS.tts.utils.text.punctuation",
        "description": "TTS.tts.utils.text.punctuation",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.punctuation",
        "documentation": {}
    },
    {
        "label": "Punctuation",
        "importPath": "TTS.tts.utils.text.punctuation",
        "description": "TTS.tts.utils.text.punctuation",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.punctuation",
        "documentation": {}
    },
    {
        "label": "_DEF_PUNCS",
        "importPath": "TTS.tts.utils.text.punctuation",
        "description": "TTS.tts.utils.text.punctuation",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.punctuation",
        "documentation": {}
    },
    {
        "label": "Punctuation",
        "importPath": "TTS.tts.utils.text.punctuation",
        "description": "TTS.tts.utils.text.punctuation",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.punctuation",
        "documentation": {}
    },
    {
        "label": "logging",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "logging",
        "description": "logging",
        "detail": "logging",
        "documentation": {}
    },
    {
        "label": "gruut",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "gruut",
        "description": "gruut",
        "detail": "gruut",
        "documentation": {}
    },
    {
        "label": "IPA",
        "importPath": "gruut_ipa",
        "description": "gruut_ipa",
        "isExtraImport": true,
        "detail": "gruut_ipa",
        "documentation": {}
    },
    {
        "label": "japanese_text_to_phonemes",
        "importPath": "TTS.tts.utils.text.japanese.phonemizer",
        "description": "TTS.tts.utils.text.japanese.phonemizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.japanese.phonemizer",
        "documentation": {}
    },
    {
        "label": "japanese_text_to_phonemes",
        "importPath": "TTS.tts.utils.text.japanese.phonemizer",
        "description": "TTS.tts.utils.text.japanese.phonemizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.japanese.phonemizer",
        "documentation": {}
    },
    {
        "label": "korean_text_to_phonemes",
        "importPath": "TTS.tts.utils.text.korean.phonemizer",
        "description": "TTS.tts.utils.text.korean.phonemizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.korean.phonemizer",
        "documentation": {}
    },
    {
        "label": "korean_text_to_phonemes",
        "importPath": "TTS.tts.utils.text.korean.phonemizer",
        "description": "TTS.tts.utils.text.korean.phonemizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.korean.phonemizer",
        "documentation": {}
    },
    {
        "label": "chinese_text_to_phonemes",
        "importPath": "TTS.tts.utils.text.chinese_mandarin.phonemizer",
        "description": "TTS.tts.utils.text.chinese_mandarin.phonemizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.chinese_mandarin.phonemizer",
        "documentation": {}
    },
    {
        "label": "anyascii",
        "importPath": "anyascii",
        "description": "anyascii",
        "isExtraImport": true,
        "detail": "anyascii",
        "documentation": {}
    },
    {
        "label": "replace_numbers_to_characters_in_text",
        "importPath": "TTS.tts.utils.text.chinese_mandarin.numbers",
        "description": "TTS.tts.utils.text.chinese_mandarin.numbers",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.chinese_mandarin.numbers",
        "documentation": {}
    },
    {
        "label": "six",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "six",
        "description": "six",
        "detail": "six",
        "documentation": {}
    },
    {
        "label": "cleaners",
        "importPath": "TTS.tts.utils.text",
        "description": "TTS.tts.utils.text",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text",
        "documentation": {}
    },
    {
        "label": "MultiPhonemizer",
        "importPath": "TTS.tts.utils.text.phonemizers.multi_phonemizer",
        "description": "TTS.tts.utils.text.phonemizers.multi_phonemizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.phonemizers.multi_phonemizer",
        "documentation": {}
    },
    {
        "label": "MultiPhonemizer",
        "importPath": "TTS.tts.utils.text.phonemizers.multi_phonemizer",
        "description": "TTS.tts.utils.text.phonemizers.multi_phonemizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.phonemizers.multi_phonemizer",
        "documentation": {}
    },
    {
        "label": "bisect",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "bisect",
        "description": "bisect",
        "detail": "bisect",
        "documentation": {}
    },
    {
        "label": "_Loss",
        "importPath": "torch.nn.modules.loss",
        "description": "torch.nn.modules.loss",
        "isExtraImport": true,
        "detail": "torch.nn.modules.loss",
        "documentation": {}
    },
    {
        "label": "LogNorm",
        "importPath": "matplotlib.colors",
        "description": "matplotlib.colors",
        "isExtraImport": true,
        "detail": "matplotlib.colors",
        "documentation": {}
    },
    {
        "label": "scipy.signal",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "scipy.signal",
        "description": "scipy.signal",
        "detail": "scipy.signal",
        "documentation": {}
    },
    {
        "label": "tarfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tarfile",
        "description": "tarfile",
        "detail": "tarfile",
        "documentation": {}
    },
    {
        "label": "expanduser",
        "importPath": "os.path",
        "description": "os.path",
        "isExtraImport": true,
        "detail": "os.path",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "torch.utils.model_zoo",
        "description": "torch.utils.model_zoo",
        "isExtraImport": true,
        "detail": "torch.utils.model_zoo",
        "documentation": {}
    },
    {
        "label": "download_kaggle_dataset",
        "importPath": "TTS.utils.download",
        "description": "TTS.utils.download",
        "isExtraImport": true,
        "detail": "TTS.utils.download",
        "documentation": {}
    },
    {
        "label": "download_url",
        "importPath": "TTS.utils.download",
        "description": "TTS.utils.download",
        "isExtraImport": true,
        "detail": "TTS.utils.download",
        "documentation": {}
    },
    {
        "label": "extract_archive",
        "importPath": "TTS.utils.download",
        "description": "TTS.utils.download",
        "isExtraImport": true,
        "detail": "TTS.utils.download",
        "documentation": {}
    },
    {
        "label": "pickle",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pickle",
        "description": "pickle",
        "detail": "pickle",
        "documentation": {}
    },
    {
        "label": "Optimizer",
        "importPath": "torch.optim.optimizer",
        "description": "torch.optim.optimizer",
        "isExtraImport": true,
        "detail": "torch.optim.optimizer",
        "documentation": {}
    },
    {
        "label": "pysbd",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pysbd",
        "description": "pysbd",
        "detail": "pysbd",
        "documentation": {}
    },
    {
        "label": "VitsConfig",
        "importPath": "TTS.tts.configs.vits_config",
        "description": "TTS.tts.configs.vits_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.vits_config",
        "documentation": {}
    },
    {
        "label": "VitsConfig",
        "importPath": "TTS.tts.configs.vits_config",
        "description": "TTS.tts.configs.vits_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.vits_config",
        "documentation": {}
    },
    {
        "label": "VitsConfig",
        "importPath": "TTS.tts.configs.vits_config",
        "description": "TTS.tts.configs.vits_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.vits_config",
        "documentation": {}
    },
    {
        "label": "VitsConfig",
        "importPath": "TTS.tts.configs.vits_config",
        "description": "TTS.tts.configs.vits_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.vits_config",
        "documentation": {}
    },
    {
        "label": "VitsConfig",
        "importPath": "TTS.tts.configs.vits_config",
        "description": "TTS.tts.configs.vits_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.vits_config",
        "documentation": {}
    },
    {
        "label": "VitsConfig",
        "importPath": "TTS.tts.configs.vits_config",
        "description": "TTS.tts.configs.vits_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.vits_config",
        "documentation": {}
    },
    {
        "label": "VitsConfig",
        "importPath": "TTS.tts.configs.vits_config",
        "description": "TTS.tts.configs.vits_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.vits_config",
        "documentation": {}
    },
    {
        "label": "VitsConfig",
        "importPath": "TTS.tts.configs.vits_config",
        "description": "TTS.tts.configs.vits_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.vits_config",
        "documentation": {}
    },
    {
        "label": "VitsConfig",
        "importPath": "TTS.tts.configs.vits_config",
        "description": "TTS.tts.configs.vits_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.vits_config",
        "documentation": {}
    },
    {
        "label": "VitsConfig",
        "importPath": "TTS.tts.configs.vits_config",
        "description": "TTS.tts.configs.vits_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.vits_config",
        "documentation": {}
    },
    {
        "label": "VitsConfig",
        "importPath": "TTS.tts.configs.vits_config",
        "description": "TTS.tts.configs.vits_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.vits_config",
        "documentation": {}
    },
    {
        "label": "VitsConfig",
        "importPath": "TTS.tts.configs.vits_config",
        "description": "TTS.tts.configs.vits_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.vits_config",
        "documentation": {}
    },
    {
        "label": "VitsConfig",
        "importPath": "TTS.tts.configs.vits_config",
        "description": "TTS.tts.configs.vits_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.vits_config",
        "documentation": {}
    },
    {
        "label": "VitsConfig",
        "importPath": "TTS.tts.configs.vits_config",
        "description": "TTS.tts.configs.vits_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.vits_config",
        "documentation": {}
    },
    {
        "label": "setup_model",
        "importPath": "TTS.vc.models",
        "description": "TTS.vc.models",
        "isExtraImport": true,
        "detail": "TTS.vc.models",
        "documentation": {}
    },
    {
        "label": "BaseVCConfig",
        "importPath": "TTS.vc.configs.shared_configs",
        "description": "TTS.vc.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.vc.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseVCConfig",
        "importPath": "TTS.vc.configs.shared_configs",
        "description": "TTS.vc.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.vc.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "FreeVCArgs",
        "importPath": "TTS.vc.models.freevc",
        "description": "TTS.vc.models.freevc",
        "isExtraImport": true,
        "detail": "TTS.vc.models.freevc",
        "documentation": {}
    },
    {
        "label": "FreeVCAudioConfig",
        "importPath": "TTS.vc.models.freevc",
        "description": "TTS.vc.models.freevc",
        "isExtraImport": true,
        "detail": "TTS.vc.models.freevc",
        "documentation": {}
    },
    {
        "label": "FreeVCConfig",
        "importPath": "TTS.vc.models.freevc",
        "description": "TTS.vc.models.freevc",
        "isExtraImport": true,
        "detail": "TTS.vc.models.freevc",
        "documentation": {}
    },
    {
        "label": "FreeVC",
        "importPath": "TTS.vc.models.freevc",
        "description": "TTS.vc.models.freevc",
        "isExtraImport": true,
        "detail": "TTS.vc.models.freevc",
        "documentation": {}
    },
    {
        "label": "remove_weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "spectral_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "remove_weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "remove_weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "spectral_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "weight_norm",
        "importPath": "torch.nn.utils",
        "description": "torch.nn.utils",
        "isExtraImport": true,
        "detail": "torch.nn.utils",
        "documentation": {}
    },
    {
        "label": "TTS.vc.modules.freevc.commons",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "TTS.vc.modules.freevc.commons",
        "description": "TTS.vc.modules.freevc.commons",
        "detail": "TTS.vc.modules.freevc.commons",
        "documentation": {}
    },
    {
        "label": "get_padding",
        "importPath": "TTS.vc.modules.freevc.commons",
        "description": "TTS.vc.modules.freevc.commons",
        "isExtraImport": true,
        "detail": "TTS.vc.modules.freevc.commons",
        "documentation": {}
    },
    {
        "label": "init_weights",
        "importPath": "TTS.vc.modules.freevc.commons",
        "description": "TTS.vc.modules.freevc.commons",
        "isExtraImport": true,
        "detail": "TTS.vc.modules.freevc.commons",
        "documentation": {}
    },
    {
        "label": "get_padding",
        "importPath": "TTS.vc.modules.freevc.commons",
        "description": "TTS.vc.modules.freevc.commons",
        "isExtraImport": true,
        "detail": "TTS.vc.modules.freevc.commons",
        "documentation": {}
    },
    {
        "label": "init_weights",
        "importPath": "TTS.vc.modules.freevc.commons",
        "description": "TTS.vc.modules.freevc.commons",
        "isExtraImport": true,
        "detail": "TTS.vc.modules.freevc.commons",
        "documentation": {}
    },
    {
        "label": "TTS.vc.modules.freevc.modules",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "TTS.vc.modules.freevc.modules",
        "description": "TTS.vc.modules.freevc.modules",
        "detail": "TTS.vc.modules.freevc.modules",
        "documentation": {}
    },
    {
        "label": "BaseVC",
        "importPath": "TTS.vc.models.base_vc",
        "description": "TTS.vc.models.base_vc",
        "isExtraImport": true,
        "detail": "TTS.vc.models.base_vc",
        "documentation": {}
    },
    {
        "label": "mel_spectrogram_torch",
        "importPath": "TTS.vc.modules.freevc.mel_processing",
        "description": "TTS.vc.modules.freevc.mel_processing",
        "isExtraImport": true,
        "detail": "TTS.vc.modules.freevc.mel_processing",
        "documentation": {}
    },
    {
        "label": "SpeakerEncoder",
        "importPath": "TTS.vc.modules.freevc.speaker_encoder.speaker_encoder",
        "description": "TTS.vc.modules.freevc.speaker_encoder.speaker_encoder",
        "isExtraImport": true,
        "detail": "TTS.vc.modules.freevc.speaker_encoder.speaker_encoder",
        "documentation": {}
    },
    {
        "label": "get_wavlm",
        "importPath": "TTS.vc.modules.freevc.wavlm",
        "description": "TTS.vc.modules.freevc.wavlm",
        "isExtraImport": true,
        "detail": "TTS.vc.modules.freevc.wavlm",
        "documentation": {}
    },
    {
        "label": "struct",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "struct",
        "description": "struct",
        "detail": "struct",
        "documentation": {}
    },
    {
        "label": "binary_dilation",
        "importPath": "scipy.ndimage.morphology",
        "description": "scipy.ndimage.morphology",
        "isExtraImport": true,
        "detail": "scipy.ndimage.morphology",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "TTS.vc.modules.freevc.speaker_encoder.hparams",
        "description": "TTS.vc.modules.freevc.speaker_encoder.hparams",
        "isExtraImport": true,
        "detail": "TTS.vc.modules.freevc.speaker_encoder.hparams",
        "documentation": {}
    },
    {
        "label": "*",
        "importPath": "TTS.vc.modules.freevc.speaker_encoder.hparams",
        "description": "TTS.vc.modules.freevc.speaker_encoder.hparams",
        "isExtraImport": true,
        "detail": "TTS.vc.modules.freevc.speaker_encoder.hparams",
        "documentation": {}
    },
    {
        "label": "audio",
        "importPath": "TTS.vc.modules.freevc.speaker_encoder",
        "description": "TTS.vc.modules.freevc.speaker_encoder",
        "isExtraImport": true,
        "detail": "TTS.vc.modules.freevc.speaker_encoder",
        "documentation": {}
    },
    {
        "label": "Fp32GroupNorm",
        "importPath": "TTS.vc.modules.freevc.wavlm.modules",
        "description": "TTS.vc.modules.freevc.wavlm.modules",
        "isExtraImport": true,
        "detail": "TTS.vc.modules.freevc.wavlm.modules",
        "documentation": {}
    },
    {
        "label": "Fp32LayerNorm",
        "importPath": "TTS.vc.modules.freevc.wavlm.modules",
        "description": "TTS.vc.modules.freevc.wavlm.modules",
        "isExtraImport": true,
        "detail": "TTS.vc.modules.freevc.wavlm.modules",
        "documentation": {}
    },
    {
        "label": "GLU_Linear",
        "importPath": "TTS.vc.modules.freevc.wavlm.modules",
        "description": "TTS.vc.modules.freevc.wavlm.modules",
        "isExtraImport": true,
        "detail": "TTS.vc.modules.freevc.wavlm.modules",
        "documentation": {}
    },
    {
        "label": "GradMultiply",
        "importPath": "TTS.vc.modules.freevc.wavlm.modules",
        "description": "TTS.vc.modules.freevc.wavlm.modules",
        "isExtraImport": true,
        "detail": "TTS.vc.modules.freevc.wavlm.modules",
        "documentation": {}
    },
    {
        "label": "MultiheadAttention",
        "importPath": "TTS.vc.modules.freevc.wavlm.modules",
        "description": "TTS.vc.modules.freevc.wavlm.modules",
        "isExtraImport": true,
        "detail": "TTS.vc.modules.freevc.wavlm.modules",
        "documentation": {}
    },
    {
        "label": "SamePad",
        "importPath": "TTS.vc.modules.freevc.wavlm.modules",
        "description": "TTS.vc.modules.freevc.wavlm.modules",
        "isExtraImport": true,
        "detail": "TTS.vc.modules.freevc.wavlm.modules",
        "documentation": {}
    },
    {
        "label": "TransposeLast",
        "importPath": "TTS.vc.modules.freevc.wavlm.modules",
        "description": "TTS.vc.modules.freevc.wavlm.modules",
        "isExtraImport": true,
        "detail": "TTS.vc.modules.freevc.wavlm.modules",
        "documentation": {}
    },
    {
        "label": "get_activation_fn",
        "importPath": "TTS.vc.modules.freevc.wavlm.modules",
        "description": "TTS.vc.modules.freevc.wavlm.modules",
        "isExtraImport": true,
        "detail": "TTS.vc.modules.freevc.wavlm.modules",
        "documentation": {}
    },
    {
        "label": "init_bert_params",
        "importPath": "TTS.vc.modules.freevc.wavlm.modules",
        "description": "TTS.vc.modules.freevc.wavlm.modules",
        "isExtraImport": true,
        "detail": "TTS.vc.modules.freevc.wavlm.modules",
        "documentation": {}
    },
    {
        "label": "BaseGANVocoderConfig",
        "importPath": "TTS.vocoder.configs.shared_configs",
        "description": "TTS.vocoder.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseGANVocoderConfig",
        "importPath": "TTS.vocoder.configs.shared_configs",
        "description": "TTS.vocoder.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseGANVocoderConfig",
        "importPath": "TTS.vocoder.configs.shared_configs",
        "description": "TTS.vocoder.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseGANVocoderConfig",
        "importPath": "TTS.vocoder.configs.shared_configs",
        "description": "TTS.vocoder.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseVocoderConfig",
        "importPath": "TTS.vocoder.configs.shared_configs",
        "description": "TTS.vocoder.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseVocoderConfig",
        "importPath": "TTS.vocoder.configs.shared_configs",
        "description": "TTS.vocoder.configs.shared_configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "WavegradArgs",
        "importPath": "TTS.vocoder.models.wavegrad",
        "description": "TTS.vocoder.models.wavegrad",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.wavegrad",
        "documentation": {}
    },
    {
        "label": "Wavegrad",
        "importPath": "TTS.vocoder.models.wavegrad",
        "description": "TTS.vocoder.models.wavegrad",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.wavegrad",
        "documentation": {}
    },
    {
        "label": "Wavegrad",
        "importPath": "TTS.vocoder.models.wavegrad",
        "description": "TTS.vocoder.models.wavegrad",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.wavegrad",
        "documentation": {}
    },
    {
        "label": "Wavegrad",
        "importPath": "TTS.vocoder.models.wavegrad",
        "description": "TTS.vocoder.models.wavegrad",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.wavegrad",
        "documentation": {}
    },
    {
        "label": "WavegradArgs",
        "importPath": "TTS.vocoder.models.wavegrad",
        "description": "TTS.vocoder.models.wavegrad",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.wavegrad",
        "documentation": {}
    },
    {
        "label": "Wavegrad",
        "importPath": "TTS.vocoder.models.wavegrad",
        "description": "TTS.vocoder.models.wavegrad",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.wavegrad",
        "documentation": {}
    },
    {
        "label": "WavegradArgs",
        "importPath": "TTS.vocoder.models.wavegrad",
        "description": "TTS.vocoder.models.wavegrad",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.wavegrad",
        "documentation": {}
    },
    {
        "label": "WavernnArgs",
        "importPath": "TTS.vocoder.models.wavernn",
        "description": "TTS.vocoder.models.wavernn",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.wavernn",
        "documentation": {}
    },
    {
        "label": "Wavernn",
        "importPath": "TTS.vocoder.models.wavernn",
        "description": "TTS.vocoder.models.wavernn",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.wavernn",
        "documentation": {}
    },
    {
        "label": "Wavernn",
        "importPath": "TTS.vocoder.models.wavernn",
        "description": "TTS.vocoder.models.wavernn",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.wavernn",
        "documentation": {}
    },
    {
        "label": "Wavernn",
        "importPath": "TTS.vocoder.models.wavernn",
        "description": "TTS.vocoder.models.wavernn",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.wavernn",
        "documentation": {}
    },
    {
        "label": "WavernnArgs",
        "importPath": "TTS.vocoder.models.wavernn",
        "description": "TTS.vocoder.models.wavernn",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.wavernn",
        "documentation": {}
    },
    {
        "label": "WavernnArgs",
        "importPath": "TTS.vocoder.models.wavernn",
        "description": "TTS.vocoder.models.wavernn",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.wavernn",
        "documentation": {}
    },
    {
        "label": "discretized_mix_logistic_loss",
        "importPath": "TTS.vocoder.utils.distribution",
        "description": "TTS.vocoder.utils.distribution",
        "isExtraImport": true,
        "detail": "TTS.vocoder.utils.distribution",
        "documentation": {}
    },
    {
        "label": "gaussian_loss",
        "importPath": "TTS.vocoder.utils.distribution",
        "description": "TTS.vocoder.utils.distribution",
        "isExtraImport": true,
        "detail": "TTS.vocoder.utils.distribution",
        "documentation": {}
    },
    {
        "label": "sample_from_discretized_mix_logistic",
        "importPath": "TTS.vocoder.utils.distribution",
        "description": "TTS.vocoder.utils.distribution",
        "isExtraImport": true,
        "detail": "TTS.vocoder.utils.distribution",
        "documentation": {}
    },
    {
        "label": "sample_from_gaussian",
        "importPath": "TTS.vocoder.utils.distribution",
        "description": "TTS.vocoder.utils.distribution",
        "isExtraImport": true,
        "detail": "TTS.vocoder.utils.distribution",
        "documentation": {}
    },
    {
        "label": "MelganGenerator",
        "importPath": "TTS.vocoder.models.melgan_generator",
        "description": "TTS.vocoder.models.melgan_generator",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.melgan_generator",
        "documentation": {}
    },
    {
        "label": "MelganGenerator",
        "importPath": "TTS.vocoder.models.melgan_generator",
        "description": "TTS.vocoder.models.melgan_generator",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.melgan_generator",
        "documentation": {}
    },
    {
        "label": "MelganGenerator",
        "importPath": "TTS.vocoder.models.melgan_generator",
        "description": "TTS.vocoder.models.melgan_generator",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.melgan_generator",
        "documentation": {}
    },
    {
        "label": "DistributedSampler",
        "importPath": "torch.utils.data.distributed",
        "description": "torch.utils.data.distributed",
        "isExtraImport": true,
        "detail": "torch.utils.data.distributed",
        "documentation": {}
    },
    {
        "label": "DistributedSampler",
        "importPath": "torch.utils.data.distributed",
        "description": "torch.utils.data.distributed",
        "isExtraImport": true,
        "detail": "torch.utils.data.distributed",
        "documentation": {}
    },
    {
        "label": "DistributedSampler",
        "importPath": "torch.utils.data.distributed",
        "description": "torch.utils.data.distributed",
        "isExtraImport": true,
        "detail": "torch.utils.data.distributed",
        "documentation": {}
    },
    {
        "label": "GANDataset",
        "importPath": "TTS.vocoder.datasets.gan_dataset",
        "description": "TTS.vocoder.datasets.gan_dataset",
        "isExtraImport": true,
        "detail": "TTS.vocoder.datasets.gan_dataset",
        "documentation": {}
    },
    {
        "label": "GANDataset",
        "importPath": "TTS.vocoder.datasets.gan_dataset",
        "description": "TTS.vocoder.datasets.gan_dataset",
        "isExtraImport": true,
        "detail": "TTS.vocoder.datasets.gan_dataset",
        "documentation": {}
    },
    {
        "label": "DiscriminatorLoss",
        "importPath": "TTS.vocoder.layers.losses",
        "description": "TTS.vocoder.layers.losses",
        "isExtraImport": true,
        "detail": "TTS.vocoder.layers.losses",
        "documentation": {}
    },
    {
        "label": "GeneratorLoss",
        "importPath": "TTS.vocoder.layers.losses",
        "description": "TTS.vocoder.layers.losses",
        "isExtraImport": true,
        "detail": "TTS.vocoder.layers.losses",
        "documentation": {}
    },
    {
        "label": "WaveRNNLoss",
        "importPath": "TTS.vocoder.layers.losses",
        "description": "TTS.vocoder.layers.losses",
        "isExtraImport": true,
        "detail": "TTS.vocoder.layers.losses",
        "documentation": {}
    },
    {
        "label": "MelganFeatureLoss",
        "importPath": "TTS.vocoder.layers.losses",
        "description": "TTS.vocoder.layers.losses",
        "isExtraImport": true,
        "detail": "TTS.vocoder.layers.losses",
        "documentation": {}
    },
    {
        "label": "MultiScaleSTFTLoss",
        "importPath": "TTS.vocoder.layers.losses",
        "description": "TTS.vocoder.layers.losses",
        "isExtraImport": true,
        "detail": "TTS.vocoder.layers.losses",
        "documentation": {}
    },
    {
        "label": "STFTLoss",
        "importPath": "TTS.vocoder.layers.losses",
        "description": "TTS.vocoder.layers.losses",
        "isExtraImport": true,
        "detail": "TTS.vocoder.layers.losses",
        "documentation": {}
    },
    {
        "label": "TorchSTFT",
        "importPath": "TTS.vocoder.layers.losses",
        "description": "TTS.vocoder.layers.losses",
        "isExtraImport": true,
        "detail": "TTS.vocoder.layers.losses",
        "documentation": {}
    },
    {
        "label": "BaseVocoder",
        "importPath": "TTS.vocoder.models.base_vocoder",
        "description": "TTS.vocoder.models.base_vocoder",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.base_vocoder",
        "documentation": {}
    },
    {
        "label": "BaseVocoder",
        "importPath": "TTS.vocoder.models.base_vocoder",
        "description": "TTS.vocoder.models.base_vocoder",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.base_vocoder",
        "documentation": {}
    },
    {
        "label": "BaseVocoder",
        "importPath": "TTS.vocoder.models.base_vocoder",
        "description": "TTS.vocoder.models.base_vocoder",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.base_vocoder",
        "documentation": {}
    },
    {
        "label": "ResidualStack",
        "importPath": "TTS.vocoder.layers.melgan",
        "description": "TTS.vocoder.layers.melgan",
        "isExtraImport": true,
        "detail": "TTS.vocoder.layers.melgan",
        "documentation": {}
    },
    {
        "label": "MelganDiscriminator",
        "importPath": "TTS.vocoder.models.melgan_discriminator",
        "description": "TTS.vocoder.models.melgan_discriminator",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.melgan_discriminator",
        "documentation": {}
    },
    {
        "label": "MelganDiscriminator",
        "importPath": "TTS.vocoder.models.melgan_discriminator",
        "description": "TTS.vocoder.models.melgan_discriminator",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.melgan_discriminator",
        "documentation": {}
    },
    {
        "label": "PQMF",
        "importPath": "TTS.vocoder.layers.pqmf",
        "description": "TTS.vocoder.layers.pqmf",
        "isExtraImport": true,
        "detail": "TTS.vocoder.layers.pqmf",
        "documentation": {}
    },
    {
        "label": "PQMF",
        "importPath": "TTS.vocoder.layers.pqmf",
        "description": "TTS.vocoder.layers.pqmf",
        "isExtraImport": true,
        "detail": "TTS.vocoder.layers.pqmf",
        "documentation": {}
    },
    {
        "label": "ResidualBlock",
        "importPath": "TTS.vocoder.layers.parallel_wavegan",
        "description": "TTS.vocoder.layers.parallel_wavegan",
        "isExtraImport": true,
        "detail": "TTS.vocoder.layers.parallel_wavegan",
        "documentation": {}
    },
    {
        "label": "ResidualBlock",
        "importPath": "TTS.vocoder.layers.parallel_wavegan",
        "description": "TTS.vocoder.layers.parallel_wavegan",
        "isExtraImport": true,
        "detail": "TTS.vocoder.layers.parallel_wavegan",
        "documentation": {}
    },
    {
        "label": "ConvUpsample",
        "importPath": "TTS.vocoder.layers.upsample",
        "description": "TTS.vocoder.layers.upsample",
        "isExtraImport": true,
        "detail": "TTS.vocoder.layers.upsample",
        "documentation": {}
    },
    {
        "label": "LVCBlock",
        "importPath": "TTS.vocoder.layers.lvc_block",
        "description": "TTS.vocoder.layers.lvc_block",
        "isExtraImport": true,
        "detail": "TTS.vocoder.layers.lvc_block",
        "documentation": {}
    },
    {
        "label": "WaveGradDataset",
        "importPath": "TTS.vocoder.datasets",
        "description": "TTS.vocoder.datasets",
        "isExtraImport": true,
        "detail": "TTS.vocoder.datasets",
        "documentation": {}
    },
    {
        "label": "Conv1d",
        "importPath": "TTS.vocoder.layers.wavegrad",
        "description": "TTS.vocoder.layers.wavegrad",
        "isExtraImport": true,
        "detail": "TTS.vocoder.layers.wavegrad",
        "documentation": {}
    },
    {
        "label": "DBlock",
        "importPath": "TTS.vocoder.layers.wavegrad",
        "description": "TTS.vocoder.layers.wavegrad",
        "isExtraImport": true,
        "detail": "TTS.vocoder.layers.wavegrad",
        "documentation": {}
    },
    {
        "label": "FiLM",
        "importPath": "TTS.vocoder.layers.wavegrad",
        "description": "TTS.vocoder.layers.wavegrad",
        "isExtraImport": true,
        "detail": "TTS.vocoder.layers.wavegrad",
        "documentation": {}
    },
    {
        "label": "UBlock",
        "importPath": "TTS.vocoder.layers.wavegrad",
        "description": "TTS.vocoder.layers.wavegrad",
        "isExtraImport": true,
        "detail": "TTS.vocoder.layers.wavegrad",
        "documentation": {}
    },
    {
        "label": "DBlock",
        "importPath": "TTS.vocoder.layers.wavegrad",
        "description": "TTS.vocoder.layers.wavegrad",
        "isExtraImport": true,
        "detail": "TTS.vocoder.layers.wavegrad",
        "documentation": {}
    },
    {
        "label": "FiLM",
        "importPath": "TTS.vocoder.layers.wavegrad",
        "description": "TTS.vocoder.layers.wavegrad",
        "isExtraImport": true,
        "detail": "TTS.vocoder.layers.wavegrad",
        "documentation": {}
    },
    {
        "label": "PositionalEncoding",
        "importPath": "TTS.vocoder.layers.wavegrad",
        "description": "TTS.vocoder.layers.wavegrad",
        "isExtraImport": true,
        "detail": "TTS.vocoder.layers.wavegrad",
        "documentation": {}
    },
    {
        "label": "UBlock",
        "importPath": "TTS.vocoder.layers.wavegrad",
        "description": "TTS.vocoder.layers.wavegrad",
        "isExtraImport": true,
        "detail": "TTS.vocoder.layers.wavegrad",
        "documentation": {}
    },
    {
        "label": "WaveRNNDataset",
        "importPath": "TTS.vocoder.datasets.wavernn_dataset",
        "description": "TTS.vocoder.datasets.wavernn_dataset",
        "isExtraImport": true,
        "detail": "TTS.vocoder.datasets.wavernn_dataset",
        "documentation": {}
    },
    {
        "label": "WaveRNNDataset",
        "importPath": "TTS.vocoder.datasets.wavernn_dataset",
        "description": "TTS.vocoder.datasets.wavernn_dataset",
        "isExtraImport": true,
        "detail": "TTS.vocoder.datasets.wavernn_dataset",
        "documentation": {}
    },
    {
        "label": "Normal",
        "importPath": "torch.distributions.normal",
        "description": "torch.distributions.normal",
        "isExtraImport": true,
        "detail": "torch.distributions.normal",
        "documentation": {}
    },
    {
        "label": "http.client",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "http.client",
        "description": "http.client",
        "detail": "http.client",
        "documentation": {}
    },
    {
        "label": "tempfile",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "tempfile",
        "description": "tempfile",
        "detail": "tempfile",
        "documentation": {}
    },
    {
        "label": "wavfile",
        "importPath": "scipy.io",
        "description": "scipy.io",
        "isExtraImport": true,
        "detail": "scipy.io",
        "documentation": {}
    },
    {
        "label": "csv",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "csv",
        "description": "csv",
        "detail": "csv",
        "documentation": {}
    },
    {
        "label": "StatisticsError",
        "importPath": "statistics",
        "description": "statistics",
        "isExtraImport": true,
        "detail": "statistics",
        "documentation": {}
    },
    {
        "label": "mean",
        "importPath": "statistics",
        "description": "statistics",
        "isExtraImport": true,
        "detail": "statistics",
        "documentation": {}
    },
    {
        "label": "median",
        "importPath": "statistics",
        "description": "statistics",
        "isExtraImport": true,
        "detail": "statistics",
        "documentation": {}
    },
    {
        "label": "mode",
        "importPath": "statistics",
        "description": "statistics",
        "isExtraImport": true,
        "detail": "statistics",
        "documentation": {}
    },
    {
        "label": "stdev",
        "importPath": "statistics",
        "description": "statistics",
        "isExtraImport": true,
        "detail": "statistics",
        "documentation": {}
    },
    {
        "label": "CMUDict",
        "importPath": "text.cmudict",
        "description": "text.cmudict",
        "isExtraImport": true,
        "detail": "text.cmudict",
        "documentation": {}
    },
    {
        "label": "Tacotron",
        "importPath": "TTS.tts.models.tacotron",
        "description": "TTS.tts.models.tacotron",
        "isExtraImport": true,
        "detail": "TTS.tts.models.tacotron",
        "documentation": {}
    },
    {
        "label": "Tacotron",
        "importPath": "TTS.tts.models.tacotron",
        "description": "TTS.tts.models.tacotron",
        "isExtraImport": true,
        "detail": "TTS.tts.models.tacotron",
        "documentation": {}
    },
    {
        "label": "Tacotron",
        "importPath": "TTS.tts.models.tacotron",
        "description": "TTS.tts.models.tacotron",
        "isExtraImport": true,
        "detail": "TTS.tts.models.tacotron",
        "documentation": {}
    },
    {
        "label": "Tacotron2Config",
        "importPath": "TTS.tts.configs.tacotron2_config",
        "description": "TTS.tts.configs.tacotron2_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.tacotron2_config",
        "documentation": {}
    },
    {
        "label": "Tacotron2Config",
        "importPath": "TTS.tts.configs.tacotron2_config",
        "description": "TTS.tts.configs.tacotron2_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.tacotron2_config",
        "documentation": {}
    },
    {
        "label": "Tacotron2Config",
        "importPath": "TTS.tts.configs.tacotron2_config",
        "description": "TTS.tts.configs.tacotron2_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.tacotron2_config",
        "documentation": {}
    },
    {
        "label": "Tacotron2Config",
        "importPath": "TTS.tts.configs.tacotron2_config",
        "description": "TTS.tts.configs.tacotron2_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.tacotron2_config",
        "documentation": {}
    },
    {
        "label": "Tacotron2Config",
        "importPath": "TTS.tts.configs.tacotron2_config",
        "description": "TTS.tts.configs.tacotron2_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.tacotron2_config",
        "documentation": {}
    },
    {
        "label": "Tacotron2Config",
        "importPath": "TTS.tts.configs.tacotron2_config",
        "description": "TTS.tts.configs.tacotron2_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.tacotron2_config",
        "documentation": {}
    },
    {
        "label": "Tacotron2Config",
        "importPath": "TTS.tts.configs.tacotron2_config",
        "description": "TTS.tts.configs.tacotron2_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.tacotron2_config",
        "documentation": {}
    },
    {
        "label": "Tacotron2Config",
        "importPath": "TTS.tts.configs.tacotron2_config",
        "description": "TTS.tts.configs.tacotron2_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.tacotron2_config",
        "documentation": {}
    },
    {
        "label": "Tacotron2Config",
        "importPath": "TTS.tts.configs.tacotron2_config",
        "description": "TTS.tts.configs.tacotron2_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.tacotron2_config",
        "documentation": {}
    },
    {
        "label": "Tacotron2Config",
        "importPath": "TTS.tts.configs.tacotron2_config",
        "description": "TTS.tts.configs.tacotron2_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.tacotron2_config",
        "documentation": {}
    },
    {
        "label": "Tacotron2Config",
        "importPath": "TTS.tts.configs.tacotron2_config",
        "description": "TTS.tts.configs.tacotron2_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.tacotron2_config",
        "documentation": {}
    },
    {
        "label": "Tacotron2",
        "importPath": "TTS.tts.models.tacotron2",
        "description": "TTS.tts.models.tacotron2",
        "isExtraImport": true,
        "detail": "TTS.tts.models.tacotron2",
        "documentation": {}
    },
    {
        "label": "Tacotron2",
        "importPath": "TTS.tts.models.tacotron2",
        "description": "TTS.tts.models.tacotron2",
        "isExtraImport": true,
        "detail": "TTS.tts.models.tacotron2",
        "documentation": {}
    },
    {
        "label": "Tacotron2",
        "importPath": "TTS.tts.models.tacotron2",
        "description": "TTS.tts.models.tacotron2",
        "isExtraImport": true,
        "detail": "TTS.tts.models.tacotron2",
        "documentation": {}
    },
    {
        "label": "Tacotron2",
        "importPath": "TTS.tts.models.tacotron2",
        "description": "TTS.tts.models.tacotron2",
        "isExtraImport": true,
        "detail": "TTS.tts.models.tacotron2",
        "documentation": {}
    },
    {
        "label": "Tacotron2",
        "importPath": "TTS.tts.models.tacotron2",
        "description": "TTS.tts.models.tacotron2",
        "isExtraImport": true,
        "detail": "TTS.tts.models.tacotron2",
        "documentation": {}
    },
    {
        "label": "Tacotron2",
        "importPath": "TTS.tts.models.tacotron2",
        "description": "TTS.tts.models.tacotron2",
        "isExtraImport": true,
        "detail": "TTS.tts.models.tacotron2",
        "documentation": {}
    },
    {
        "label": "Tacotron2",
        "importPath": "TTS.tts.models.tacotron2",
        "description": "TTS.tts.models.tacotron2",
        "isExtraImport": true,
        "detail": "TTS.tts.models.tacotron2",
        "documentation": {}
    },
    {
        "label": "Tacotron2",
        "importPath": "TTS.tts.models.tacotron2",
        "description": "TTS.tts.models.tacotron2",
        "isExtraImport": true,
        "detail": "TTS.tts.models.tacotron2",
        "documentation": {}
    },
    {
        "label": "AlignTTSConfig",
        "importPath": "TTS.tts.configs.align_tts_config",
        "description": "TTS.tts.configs.align_tts_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.align_tts_config",
        "documentation": {}
    },
    {
        "label": "AlignTTSConfig",
        "importPath": "TTS.tts.configs.align_tts_config",
        "description": "TTS.tts.configs.align_tts_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.align_tts_config",
        "documentation": {}
    },
    {
        "label": "AlignTTSConfig",
        "importPath": "TTS.tts.configs.align_tts_config",
        "description": "TTS.tts.configs.align_tts_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.align_tts_config",
        "documentation": {}
    },
    {
        "label": "FastPitchConfig",
        "importPath": "TTS.tts.configs.fast_pitch_config",
        "description": "TTS.tts.configs.fast_pitch_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.fast_pitch_config",
        "documentation": {}
    },
    {
        "label": "FastPitchConfig",
        "importPath": "TTS.tts.configs.fast_pitch_config",
        "description": "TTS.tts.configs.fast_pitch_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.fast_pitch_config",
        "documentation": {}
    },
    {
        "label": "FastPitchConfig",
        "importPath": "TTS.tts.configs.fast_pitch_config",
        "description": "TTS.tts.configs.fast_pitch_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.fast_pitch_config",
        "documentation": {}
    },
    {
        "label": "FastPitchConfig",
        "importPath": "TTS.tts.configs.fast_pitch_config",
        "description": "TTS.tts.configs.fast_pitch_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.fast_pitch_config",
        "documentation": {}
    },
    {
        "label": "FastSpeechConfig",
        "importPath": "TTS.tts.configs.fast_speech_config",
        "description": "TTS.tts.configs.fast_speech_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.fast_speech_config",
        "documentation": {}
    },
    {
        "label": "FastSpeechConfig",
        "importPath": "TTS.tts.configs.fast_speech_config",
        "description": "TTS.tts.configs.fast_speech_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.fast_speech_config",
        "documentation": {}
    },
    {
        "label": "Fastspeech2Config",
        "importPath": "TTS.tts.configs.fastspeech2_config",
        "description": "TTS.tts.configs.fastspeech2_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.fastspeech2_config",
        "documentation": {}
    },
    {
        "label": "Fastspeech2Config",
        "importPath": "TTS.tts.configs.fastspeech2_config",
        "description": "TTS.tts.configs.fastspeech2_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.fastspeech2_config",
        "documentation": {}
    },
    {
        "label": "Fastspeech2Config",
        "importPath": "TTS.tts.configs.fastspeech2_config",
        "description": "TTS.tts.configs.fastspeech2_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.fastspeech2_config",
        "documentation": {}
    },
    {
        "label": "GlowTTS",
        "importPath": "TTS.tts.models.glow_tts",
        "description": "TTS.tts.models.glow_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.glow_tts",
        "documentation": {}
    },
    {
        "label": "GlowTTS",
        "importPath": "TTS.tts.models.glow_tts",
        "description": "TTS.tts.models.glow_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.glow_tts",
        "documentation": {}
    },
    {
        "label": "GlowTTS",
        "importPath": "TTS.tts.models.glow_tts",
        "description": "TTS.tts.models.glow_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.glow_tts",
        "documentation": {}
    },
    {
        "label": "GlowTTS",
        "importPath": "TTS.tts.models.glow_tts",
        "description": "TTS.tts.models.glow_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.glow_tts",
        "documentation": {}
    },
    {
        "label": "HifiganConfig",
        "importPath": "TTS.vocoder.configs",
        "description": "TTS.vocoder.configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs",
        "documentation": {}
    },
    {
        "label": "MultibandMelganConfig",
        "importPath": "TTS.vocoder.configs",
        "description": "TTS.vocoder.configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs",
        "documentation": {}
    },
    {
        "label": "UnivnetConfig",
        "importPath": "TTS.vocoder.configs",
        "description": "TTS.vocoder.configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs",
        "documentation": {}
    },
    {
        "label": "WavegradConfig",
        "importPath": "TTS.vocoder.configs",
        "description": "TTS.vocoder.configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs",
        "documentation": {}
    },
    {
        "label": "WavernnConfig",
        "importPath": "TTS.vocoder.configs",
        "description": "TTS.vocoder.configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs",
        "documentation": {}
    },
    {
        "label": "HifiganConfig",
        "importPath": "TTS.vocoder.configs",
        "description": "TTS.vocoder.configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs",
        "documentation": {}
    },
    {
        "label": "MultibandMelganConfig",
        "importPath": "TTS.vocoder.configs",
        "description": "TTS.vocoder.configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs",
        "documentation": {}
    },
    {
        "label": "UnivnetConfig",
        "importPath": "TTS.vocoder.configs",
        "description": "TTS.vocoder.configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs",
        "documentation": {}
    },
    {
        "label": "WavegradConfig",
        "importPath": "TTS.vocoder.configs",
        "description": "TTS.vocoder.configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs",
        "documentation": {}
    },
    {
        "label": "WavernnConfig",
        "importPath": "TTS.vocoder.configs",
        "description": "TTS.vocoder.configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs",
        "documentation": {}
    },
    {
        "label": "FullbandMelganConfig",
        "importPath": "TTS.vocoder.configs",
        "description": "TTS.vocoder.configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs",
        "documentation": {}
    },
    {
        "label": "HifiganConfig",
        "importPath": "TTS.vocoder.configs",
        "description": "TTS.vocoder.configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs",
        "documentation": {}
    },
    {
        "label": "MelganConfig",
        "importPath": "TTS.vocoder.configs",
        "description": "TTS.vocoder.configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs",
        "documentation": {}
    },
    {
        "label": "MultibandMelganConfig",
        "importPath": "TTS.vocoder.configs",
        "description": "TTS.vocoder.configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs",
        "documentation": {}
    },
    {
        "label": "ParallelWaveganConfig",
        "importPath": "TTS.vocoder.configs",
        "description": "TTS.vocoder.configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs",
        "documentation": {}
    },
    {
        "label": "BaseGANVocoderConfig",
        "importPath": "TTS.vocoder.configs",
        "description": "TTS.vocoder.configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs",
        "documentation": {}
    },
    {
        "label": "WavernnConfig",
        "importPath": "TTS.vocoder.configs",
        "description": "TTS.vocoder.configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs",
        "documentation": {}
    },
    {
        "label": "WavernnConfig",
        "importPath": "TTS.vocoder.configs",
        "description": "TTS.vocoder.configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs",
        "documentation": {}
    },
    {
        "label": "WavegradConfig",
        "importPath": "TTS.vocoder.configs",
        "description": "TTS.vocoder.configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs",
        "documentation": {}
    },
    {
        "label": "WavegradConfig",
        "importPath": "TTS.vocoder.configs",
        "description": "TTS.vocoder.configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs",
        "documentation": {}
    },
    {
        "label": "WavegradConfig",
        "importPath": "TTS.vocoder.configs",
        "description": "TTS.vocoder.configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs",
        "documentation": {}
    },
    {
        "label": "WavernnConfig",
        "importPath": "TTS.vocoder.configs",
        "description": "TTS.vocoder.configs",
        "isExtraImport": true,
        "detail": "TTS.vocoder.configs",
        "documentation": {}
    },
    {
        "label": "GAN",
        "importPath": "TTS.vocoder.models.gan",
        "description": "TTS.vocoder.models.gan",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.gan",
        "documentation": {}
    },
    {
        "label": "GAN",
        "importPath": "TTS.vocoder.models.gan",
        "description": "TTS.vocoder.models.gan",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.gan",
        "documentation": {}
    },
    {
        "label": "GAN",
        "importPath": "TTS.vocoder.models.gan",
        "description": "TTS.vocoder.models.gan",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.gan",
        "documentation": {}
    },
    {
        "label": "GAN",
        "importPath": "TTS.vocoder.models.gan",
        "description": "TTS.vocoder.models.gan",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.gan",
        "documentation": {}
    },
    {
        "label": "GAN",
        "importPath": "TTS.vocoder.models.gan",
        "description": "TTS.vocoder.models.gan",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.gan",
        "documentation": {}
    },
    {
        "label": "GAN",
        "importPath": "TTS.vocoder.models.gan",
        "description": "TTS.vocoder.models.gan",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.gan",
        "documentation": {}
    },
    {
        "label": "NeuralhmmTTSConfig",
        "importPath": "TTS.tts.configs.neuralhmm_tts_config",
        "description": "TTS.tts.configs.neuralhmm_tts_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.neuralhmm_tts_config",
        "documentation": {}
    },
    {
        "label": "NeuralhmmTTSConfig",
        "importPath": "TTS.tts.configs.neuralhmm_tts_config",
        "description": "TTS.tts.configs.neuralhmm_tts_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.neuralhmm_tts_config",
        "documentation": {}
    },
    {
        "label": "NeuralhmmTTS",
        "importPath": "TTS.tts.models.neuralhmm_tts",
        "description": "TTS.tts.models.neuralhmm_tts",
        "isExtraImport": true,
        "detail": "TTS.tts.models.neuralhmm_tts",
        "documentation": {}
    },
    {
        "label": "OverflowConfig",
        "importPath": "TTS.tts.configs.overflow_config",
        "description": "TTS.tts.configs.overflow_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.overflow_config",
        "documentation": {}
    },
    {
        "label": "OverflowConfig",
        "importPath": "TTS.tts.configs.overflow_config",
        "description": "TTS.tts.configs.overflow_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.overflow_config",
        "documentation": {}
    },
    {
        "label": "OverflowConfig",
        "importPath": "TTS.tts.configs.overflow_config",
        "description": "TTS.tts.configs.overflow_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.overflow_config",
        "documentation": {}
    },
    {
        "label": "Overflow",
        "importPath": "TTS.tts.models.overflow",
        "description": "TTS.tts.models.overflow",
        "isExtraImport": true,
        "detail": "TTS.tts.models.overflow",
        "documentation": {}
    },
    {
        "label": "Overflow",
        "importPath": "TTS.tts.models.overflow",
        "description": "TTS.tts.models.overflow",
        "isExtraImport": true,
        "detail": "TTS.tts.models.overflow",
        "documentation": {}
    },
    {
        "label": "SpeedySpeechConfig",
        "importPath": "TTS.tts.configs.speedy_speech_config",
        "description": "TTS.tts.configs.speedy_speech_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.speedy_speech_config",
        "documentation": {}
    },
    {
        "label": "SpeedySpeechConfig",
        "importPath": "TTS.tts.configs.speedy_speech_config",
        "description": "TTS.tts.configs.speedy_speech_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.speedy_speech_config",
        "documentation": {}
    },
    {
        "label": "SpeedySpeechConfig",
        "importPath": "TTS.tts.configs.speedy_speech_config",
        "description": "TTS.tts.configs.speedy_speech_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.speedy_speech_config",
        "documentation": {}
    },
    {
        "label": "SpeedySpeechConfig",
        "importPath": "TTS.tts.configs.speedy_speech_config",
        "description": "TTS.tts.configs.speedy_speech_config",
        "isExtraImport": true,
        "detail": "TTS.tts.configs.speedy_speech_config",
        "documentation": {}
    },
    {
        "label": "download_thorsten_de",
        "importPath": "TTS.utils.downloaders",
        "description": "TTS.utils.downloaders",
        "isExtraImport": true,
        "detail": "TTS.utils.downloaders",
        "documentation": {}
    },
    {
        "label": "download_thorsten_de",
        "importPath": "TTS.utils.downloaders",
        "description": "TTS.utils.downloaders",
        "isExtraImport": true,
        "detail": "TTS.utils.downloaders",
        "documentation": {}
    },
    {
        "label": "download_thorsten_de",
        "importPath": "TTS.utils.downloaders",
        "description": "TTS.utils.downloaders",
        "isExtraImport": true,
        "detail": "TTS.utils.downloaders",
        "documentation": {}
    },
    {
        "label": "download_thorsten_de",
        "importPath": "TTS.utils.downloaders",
        "description": "TTS.utils.downloaders",
        "isExtraImport": true,
        "detail": "TTS.utils.downloaders",
        "documentation": {}
    },
    {
        "label": "download_thorsten_de",
        "importPath": "TTS.utils.downloaders",
        "description": "TTS.utils.downloaders",
        "isExtraImport": true,
        "detail": "TTS.utils.downloaders",
        "documentation": {}
    },
    {
        "label": "download_thorsten_de",
        "importPath": "TTS.utils.downloaders",
        "description": "TTS.utils.downloaders",
        "isExtraImport": true,
        "detail": "TTS.utils.downloaders",
        "documentation": {}
    },
    {
        "label": "download_thorsten_de",
        "importPath": "TTS.utils.downloaders",
        "description": "TTS.utils.downloaders",
        "isExtraImport": true,
        "detail": "TTS.utils.downloaders",
        "documentation": {}
    },
    {
        "label": "download_thorsten_de",
        "importPath": "TTS.utils.downloaders",
        "description": "TTS.utils.downloaders",
        "isExtraImport": true,
        "detail": "TTS.utils.downloaders",
        "documentation": {}
    },
    {
        "label": "download_thorsten_de",
        "importPath": "TTS.utils.downloaders",
        "description": "TTS.utils.downloaders",
        "isExtraImport": true,
        "detail": "TTS.utils.downloaders",
        "documentation": {}
    },
    {
        "label": "download_thorsten_de",
        "importPath": "TTS.utils.downloaders",
        "description": "TTS.utils.downloaders",
        "isExtraImport": true,
        "detail": "TTS.utils.downloaders",
        "documentation": {}
    },
    {
        "label": "download_vctk",
        "importPath": "TTS.utils.downloaders",
        "description": "TTS.utils.downloaders",
        "isExtraImport": true,
        "detail": "TTS.utils.downloaders",
        "documentation": {}
    },
    {
        "label": "SpeakerEncoderConfig",
        "importPath": "TTS.encoder.configs.speaker_encoder_config",
        "description": "TTS.encoder.configs.speaker_encoder_config",
        "isExtraImport": true,
        "detail": "TTS.encoder.configs.speaker_encoder_config",
        "documentation": {}
    },
    {
        "label": "SpeakerEncoderConfig",
        "importPath": "TTS.encoder.configs.speaker_encoder_config",
        "description": "TTS.encoder.configs.speaker_encoder_config",
        "isExtraImport": true,
        "detail": "TTS.encoder.configs.speaker_encoder_config",
        "documentation": {}
    },
    {
        "label": "compute_embeddings",
        "importPath": "TTS.bin.compute_embeddings",
        "description": "TTS.bin.compute_embeddings",
        "isExtraImport": true,
        "detail": "TTS.bin.compute_embeddings",
        "documentation": {}
    },
    {
        "label": "resample_files",
        "importPath": "TTS.bin.resample",
        "description": "TTS.bin.resample",
        "isExtraImport": true,
        "detail": "TTS.bin.resample",
        "documentation": {}
    },
    {
        "label": "unittest",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "unittest",
        "description": "unittest",
        "detail": "unittest",
        "documentation": {}
    },
    {
        "label": "get_tests_input_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_input_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_input_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_input_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_input_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_input_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_input_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_data_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_data_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_input_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_data_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_input_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_input_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_input_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "assertHasAttr",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "assertHasNotAttr",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_data_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_input_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_input_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_input_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_input_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_device_id",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_data_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "get_tests_output_path",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "run_cli",
        "importPath": "tests",
        "description": "tests",
        "isExtraImport": true,
        "detail": "tests",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "importPath": "TTS.utils.audio.processor",
        "description": "TTS.utils.audio.processor",
        "isExtraImport": true,
        "detail": "TTS.utils.audio.processor",
        "documentation": {}
    },
    {
        "label": "save_checkpoint",
        "importPath": "TTS.encoder.utils.io",
        "description": "TTS.encoder.utils.io",
        "isExtraImport": true,
        "detail": "TTS.encoder.utils.io",
        "documentation": {}
    },
    {
        "label": "save_checkpoint",
        "importPath": "TTS.encoder.utils.io",
        "description": "TTS.encoder.utils.io",
        "isExtraImport": true,
        "detail": "TTS.encoder.utils.io",
        "documentation": {}
    },
    {
        "label": "common_voice",
        "importPath": "TTS.tts.datasets.formatters",
        "description": "TTS.tts.datasets.formatters",
        "isExtraImport": true,
        "detail": "TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "BN_Phonemizer",
        "importPath": "TTS.tts.utils.text.phonemizers.bangla_phonemizer",
        "description": "TTS.tts.utils.text.phonemizers.bangla_phonemizer",
        "isExtraImport": true,
        "detail": "TTS.tts.utils.text.phonemizers.bangla_phonemizer",
        "documentation": {}
    },
    {
        "label": "FreeVCConfig",
        "importPath": "TTS.vc.configs.freevc_config",
        "description": "TTS.vc.configs.freevc_config",
        "isExtraImport": true,
        "detail": "TTS.vc.configs.freevc_config",
        "documentation": {}
    },
    {
        "label": "MelganMultiscaleDiscriminator",
        "importPath": "TTS.vocoder.models.melgan_multiscale_discriminator",
        "description": "TTS.vocoder.models.melgan_multiscale_discriminator",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.melgan_multiscale_discriminator",
        "documentation": {}
    },
    {
        "label": "ParallelWaveganDiscriminator",
        "importPath": "TTS.vocoder.models.parallel_wavegan_discriminator",
        "description": "TTS.vocoder.models.parallel_wavegan_discriminator",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.parallel_wavegan_discriminator",
        "documentation": {}
    },
    {
        "label": "ResidualParallelWaveganDiscriminator",
        "importPath": "TTS.vocoder.models.parallel_wavegan_discriminator",
        "description": "TTS.vocoder.models.parallel_wavegan_discriminator",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.parallel_wavegan_discriminator",
        "documentation": {}
    },
    {
        "label": "ParallelWaveganGenerator",
        "importPath": "TTS.vocoder.models.parallel_wavegan_generator",
        "description": "TTS.vocoder.models.parallel_wavegan_generator",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.parallel_wavegan_generator",
        "documentation": {}
    },
    {
        "label": "load",
        "importPath": "librosa.core",
        "description": "librosa.core",
        "isExtraImport": true,
        "detail": "librosa.core",
        "documentation": {}
    },
    {
        "label": "RandomWindowDiscriminator",
        "importPath": "TTS.vocoder.models.random_window_discriminator",
        "description": "TTS.vocoder.models.random_window_discriminator",
        "isExtraImport": true,
        "detail": "TTS.vocoder.models.random_window_discriminator",
        "documentation": {}
    },
    {
        "label": "setuptools.command.build_py",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "setuptools.command.build_py",
        "description": "setuptools.command.build_py",
        "detail": "setuptools.command.build_py",
        "documentation": {}
    },
    {
        "label": "setuptools.command.develop",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "setuptools.command.develop",
        "description": "setuptools.command.develop",
        "detail": "setuptools.command.develop",
        "documentation": {}
    },
    {
        "label": "cythonize",
        "importPath": "Cython.Build",
        "description": "Cython.Build",
        "isExtraImport": true,
        "detail": "Cython.Build",
        "documentation": {}
    },
    {
        "label": "Extension",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "find_packages",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "setup",
        "importPath": "setuptools",
        "description": "setuptools",
        "isExtraImport": true,
        "detail": "setuptools",
        "documentation": {}
    },
    {
        "label": "TIMESTAMP",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "Boolean",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "Column",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "Float",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "ForeignKey",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "Integer",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "Enum",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "DateTime",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "String",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "text",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "event",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "create_engine",
        "importPath": "sqlalchemy",
        "description": "sqlalchemy",
        "isExtraImport": true,
        "detail": "sqlalchemy",
        "documentation": {}
    },
    {
        "label": "relationship",
        "importPath": "sqlalchemy.orm",
        "description": "sqlalchemy.orm",
        "isExtraImport": true,
        "detail": "sqlalchemy.orm",
        "documentation": {}
    },
    {
        "label": "Session",
        "importPath": "sqlalchemy.orm",
        "description": "sqlalchemy.orm",
        "isExtraImport": true,
        "detail": "sqlalchemy.orm",
        "documentation": {}
    },
    {
        "label": "sessionmaker",
        "importPath": "sqlalchemy.orm",
        "description": "sqlalchemy.orm",
        "isExtraImport": true,
        "detail": "sqlalchemy.orm",
        "documentation": {}
    },
    {
        "label": "load_dotenv",
        "importPath": "dotenv",
        "description": "dotenv",
        "isExtraImport": true,
        "detail": "dotenv",
        "documentation": {}
    },
    {
        "label": "BaseSettings",
        "importPath": "pydantic_settings",
        "description": "pydantic_settings",
        "isExtraImport": true,
        "detail": "pydantic_settings",
        "documentation": {}
    },
    {
        "label": "declarative_base",
        "importPath": "sqlalchemy.ext.declarative",
        "description": "sqlalchemy.ext.declarative",
        "isExtraImport": true,
        "detail": "sqlalchemy.ext.declarative",
        "documentation": {}
    },
    {
        "label": "RequestValidationError",
        "importPath": "fastapi.exceptions",
        "description": "fastapi.exceptions",
        "isExtraImport": true,
        "detail": "fastapi.exceptions",
        "documentation": {}
    },
    {
        "label": "HTTPException",
        "importPath": "starlette.exceptions",
        "description": "starlette.exceptions",
        "isExtraImport": true,
        "detail": "starlette.exceptions",
        "documentation": {}
    },
    {
        "label": "ExceptionMiddleware",
        "importPath": "starlette.middleware.exceptions",
        "description": "starlette.middleware.exceptions",
        "isExtraImport": true,
        "detail": "starlette.middleware.exceptions",
        "documentation": {}
    },
    {
        "label": "uvicorn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "uvicorn",
        "description": "uvicorn",
        "detail": "uvicorn",
        "documentation": {}
    },
    {
        "label": "requires",
        "importPath": "starlette.authentication",
        "description": "starlette.authentication",
        "isExtraImport": true,
        "detail": "starlette.authentication",
        "documentation": {}
    },
    {
        "label": "Middleware",
        "importPath": "fastapi.middleware",
        "description": "fastapi.middleware",
        "isExtraImport": true,
        "detail": "fastapi.middleware",
        "documentation": {}
    },
    {
        "label": "AdminAuth",
        "importPath": "admin.auth_backend",
        "description": "admin.auth_backend",
        "isExtraImport": true,
        "detail": "admin.auth_backend",
        "documentation": {}
    },
    {
        "label": "AdminBackend",
        "importPath": "admin.admin_backend",
        "description": "admin.admin_backend",
        "isExtraImport": true,
        "detail": "admin.admin_backend",
        "documentation": {}
    },
    {
        "label": "TagAdmin",
        "importPath": "admin.models.tag_admin_model",
        "description": "admin.models.tag_admin_model",
        "isExtraImport": true,
        "detail": "admin.models.tag_admin_model",
        "documentation": {}
    },
    {
        "label": "TagAdmin",
        "kind": 6,
        "importPath": "admin.models.tag_admin_model",
        "description": "admin.models.tag_admin_model",
        "peekOfCode": "class TagAdmin(ModelView, model=Tag):\n    name = \"Tag\"\n    icon = \"fa-solid fa-language\"\n    page_size_options = [25, 50, 100, 200]\n    category = \"PRODUCT\"\n    can_create = True\n    can_edit = True\n    can_delete = True\n    can_view_details = True\n    can_export = False",
        "detail": "admin.models.tag_admin_model",
        "documentation": {}
    },
    {
        "label": "AdminBackend",
        "kind": 6,
        "importPath": "admin.admin_backend",
        "description": "admin.admin_backend",
        "peekOfCode": "class AdminBackend(Admin):\n    async def login(self, request: Request) -> Response:\n        assert self.authentication_backend is not None\n        context = {\"request\": request, \"error\": \"\"}\n        cookies = request.cookies\n        refresh_token = cookies.get(\"refresh_token\")\n        if refresh_token is not None:\n            return RedirectResponse(request.url_for(\"admin:index\"), status_code=302)\n        if request.method == \"GET\":\n            return self.templates.TemplateResponse(\"login.html\", context)",
        "detail": "admin.admin_backend",
        "documentation": {}
    },
    {
        "label": "AdminAuth",
        "kind": 6,
        "importPath": "admin.auth_backend",
        "description": "admin.auth_backend",
        "peekOfCode": "class AdminAuth(AuthenticationBackend):\n    async def login(self, request: Request) -> bool:\n        form = await request.form()\n        username, password = form[\"username\"], form[\"password\"]\n        if not username or not password:\n            return False\n        if username == \"admin\" and password == \"admin\":\n            return True\n        return False\n    async def authenticate(self, request: Request) -> Optional[RedirectResponse]:",
        "detail": "admin.auth_backend",
        "documentation": {}
    },
    {
        "label": "AuthMiddleware",
        "kind": 6,
        "importPath": "auth.middlewares.auth_middleware",
        "description": "auth.middlewares.auth_middleware",
        "peekOfCode": "class AuthMiddleware:\n    def __init__(self, app):\n        self.app = app\n    async def __call__(self, scope, receive, send):\n        request = Request(scope, receive=receive)\n        try:\n            authorization = request.headers.get(\"Authorization\")\n            if authorization:\n                if authorization.lower().startswith(\"bearer\"):\n                    token = authorization.split(\" \")[1]",
        "detail": "auth.middlewares.auth_middleware",
        "documentation": {}
    },
    {
        "label": "JWTBearer",
        "kind": 6,
        "importPath": "auth.middlewares.jwt_bearer",
        "description": "auth.middlewares.jwt_bearer",
        "peekOfCode": "class JWTBearer(HTTPBearer):\n    def __init__(self, auto_error: bool = True):\n        super().__init__(auto_error=auto_error)\n    async def __call__(self, request: Request):\n        credentials: HTTPAuthorizationCredentials = await super().__call__(request)\n        if credentials:\n            if not credentials.scheme == \"Bearer\":\n                raise HTTPException(\n                    status_code=403, detail=\"Invalid authentication scheme\"\n                )",
        "detail": "auth.middlewares.jwt_bearer",
        "documentation": {}
    },
    {
        "label": "create_access_token",
        "kind": 2,
        "importPath": "auth.utils",
        "description": "auth.utils",
        "peekOfCode": "def create_access_token(\n    username: str = None,\n    user_id: int = None,\n    expires_delta: timedelta = None,\n) -> str:\n    if expires_delta is not None:\n        expires_delta = datetime.utcnow() + expires_delta\n    else:\n        expires_delta = datetime.utcnow() + timedelta(\n            minutes=ACCESS_TOKEN_EXPIRE_MINUTES",
        "detail": "auth.utils",
        "documentation": {}
    },
    {
        "label": "create_refresh_token",
        "kind": 2,
        "importPath": "auth.utils",
        "description": "auth.utils",
        "peekOfCode": "def create_refresh_token(\n    username: str = None,\n    user_id: int = None,\n    expires_delta: timedelta = None,\n) -> str:\n    if expires_delta is not None:\n        expires_delta = datetime.utcnow() + expires_delta\n    else:\n        expires_delta = datetime.utcnow() + timedelta(\n            minutes=REFRESH_TOKEN_EXPIRE_MINUTES",
        "detail": "auth.utils",
        "documentation": {}
    },
    {
        "label": "decode_and_verify_access_token",
        "kind": 2,
        "importPath": "auth.utils",
        "description": "auth.utils",
        "peekOfCode": "def decode_and_verify_access_token(token):\n    try:\n        decoded_token = jwt.decode(\n            token,\n            settings.JWT_SECRET_KEY,\n            algorithms=[settings.JWT_ALGORITHM],\n        )\n        return decoded_token\n    except ExpiredSignatureError:\n        return None",
        "detail": "auth.utils",
        "documentation": {}
    },
    {
        "label": "decode_and_verify_refresh_token",
        "kind": 2,
        "importPath": "auth.utils",
        "description": "auth.utils",
        "peekOfCode": "def decode_and_verify_refresh_token(token):\n    try:\n        decoded_token = jwt.decode(\n            token,\n            settings.JWT_REFRESH_SECRET_KEY,\n            algorithms=[settings.JWT_ALGORITHM],\n        )\n        return decoded_token\n    except ExpiredSignatureError:\n        return None",
        "detail": "auth.utils",
        "documentation": {}
    },
    {
        "label": "get_hashed_password",
        "kind": 2,
        "importPath": "auth.utils",
        "description": "auth.utils",
        "peekOfCode": "def get_hashed_password(password: str):\n    return password_context.hash(password)\ndef verify_password(password: str, hashed_pass: str):\n    return password_context.verify(password, hashed_pass)\ndef generate_password(length):\n    characters = string.ascii_letters + string.digits\n    password = ''.join(random.choice(characters) for _ in range(length))\n    return password",
        "detail": "auth.utils",
        "documentation": {}
    },
    {
        "label": "verify_password",
        "kind": 2,
        "importPath": "auth.utils",
        "description": "auth.utils",
        "peekOfCode": "def verify_password(password: str, hashed_pass: str):\n    return password_context.verify(password, hashed_pass)\ndef generate_password(length):\n    characters = string.ascii_letters + string.digits\n    password = ''.join(random.choice(characters) for _ in range(length))\n    return password",
        "detail": "auth.utils",
        "documentation": {}
    },
    {
        "label": "generate_password",
        "kind": 2,
        "importPath": "auth.utils",
        "description": "auth.utils",
        "peekOfCode": "def generate_password(length):\n    characters = string.ascii_letters + string.digits\n    password = ''.join(random.choice(characters) for _ in range(length))\n    return password",
        "detail": "auth.utils",
        "documentation": {}
    },
    {
        "label": "api_key",
        "kind": 5,
        "importPath": "auth.utils",
        "description": "auth.utils",
        "peekOfCode": "api_key = settings.MJ_APIKEY_PUBLIC\napi_secret = settings.MJ_APIKEY_PRIVATE\nmailjet = Client(auth=(api_key, api_secret), version=\"v3.1\")\nACCESS_TOKEN_EXPIRE_MINUTES = 30\nREFRESH_TOKEN_EXPIRE_MINUTES = 60 * 24 * 7\npassword_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\ndef create_access_token(\n    username: str = None,\n    user_id: int = None,\n    expires_delta: timedelta = None,",
        "detail": "auth.utils",
        "documentation": {}
    },
    {
        "label": "api_secret",
        "kind": 5,
        "importPath": "auth.utils",
        "description": "auth.utils",
        "peekOfCode": "api_secret = settings.MJ_APIKEY_PRIVATE\nmailjet = Client(auth=(api_key, api_secret), version=\"v3.1\")\nACCESS_TOKEN_EXPIRE_MINUTES = 30\nREFRESH_TOKEN_EXPIRE_MINUTES = 60 * 24 * 7\npassword_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\ndef create_access_token(\n    username: str = None,\n    user_id: int = None,\n    expires_delta: timedelta = None,\n) -> str:",
        "detail": "auth.utils",
        "documentation": {}
    },
    {
        "label": "mailjet",
        "kind": 5,
        "importPath": "auth.utils",
        "description": "auth.utils",
        "peekOfCode": "mailjet = Client(auth=(api_key, api_secret), version=\"v3.1\")\nACCESS_TOKEN_EXPIRE_MINUTES = 30\nREFRESH_TOKEN_EXPIRE_MINUTES = 60 * 24 * 7\npassword_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\ndef create_access_token(\n    username: str = None,\n    user_id: int = None,\n    expires_delta: timedelta = None,\n) -> str:\n    if expires_delta is not None:",
        "detail": "auth.utils",
        "documentation": {}
    },
    {
        "label": "ACCESS_TOKEN_EXPIRE_MINUTES",
        "kind": 5,
        "importPath": "auth.utils",
        "description": "auth.utils",
        "peekOfCode": "ACCESS_TOKEN_EXPIRE_MINUTES = 30\nREFRESH_TOKEN_EXPIRE_MINUTES = 60 * 24 * 7\npassword_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\ndef create_access_token(\n    username: str = None,\n    user_id: int = None,\n    expires_delta: timedelta = None,\n) -> str:\n    if expires_delta is not None:\n        expires_delta = datetime.utcnow() + expires_delta",
        "detail": "auth.utils",
        "documentation": {}
    },
    {
        "label": "REFRESH_TOKEN_EXPIRE_MINUTES",
        "kind": 5,
        "importPath": "auth.utils",
        "description": "auth.utils",
        "peekOfCode": "REFRESH_TOKEN_EXPIRE_MINUTES = 60 * 24 * 7\npassword_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\ndef create_access_token(\n    username: str = None,\n    user_id: int = None,\n    expires_delta: timedelta = None,\n) -> str:\n    if expires_delta is not None:\n        expires_delta = datetime.utcnow() + expires_delta\n    else:",
        "detail": "auth.utils",
        "documentation": {}
    },
    {
        "label": "password_context",
        "kind": 5,
        "importPath": "auth.utils",
        "description": "auth.utils",
        "peekOfCode": "password_context = CryptContext(schemes=[\"bcrypt\"], deprecated=\"auto\")\ndef create_access_token(\n    username: str = None,\n    user_id: int = None,\n    expires_delta: timedelta = None,\n) -> str:\n    if expires_delta is not None:\n        expires_delta = datetime.utcnow() + expires_delta\n    else:\n        expires_delta = datetime.utcnow() + timedelta(",
        "detail": "auth.utils",
        "documentation": {}
    },
    {
        "label": "H",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.3d_drawing.mayavi2_spring",
        "description": "env.share.doc.networkx-2.8.8.examples.3d_drawing.mayavi2_spring",
        "peekOfCode": "H = nx.cycle_graph(20)\n# reorder nodes from 0,len(G)-1\nG = nx.convert_node_labels_to_integers(H)\n# 3d spring layout\npos = nx.spring_layout(G, dim=3, seed=1001)\n# numpy array of x,y,z positions in sorted node order\nxyz = np.array([pos[v] for v in sorted(G)])\n# scalar colors\nscalars = np.array(list(G.nodes())) + 5\nmlab.figure()",
        "detail": "env.share.doc.networkx-2.8.8.examples.3d_drawing.mayavi2_spring",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.3d_drawing.mayavi2_spring",
        "description": "env.share.doc.networkx-2.8.8.examples.3d_drawing.mayavi2_spring",
        "peekOfCode": "G = nx.convert_node_labels_to_integers(H)\n# 3d spring layout\npos = nx.spring_layout(G, dim=3, seed=1001)\n# numpy array of x,y,z positions in sorted node order\nxyz = np.array([pos[v] for v in sorted(G)])\n# scalar colors\nscalars = np.array(list(G.nodes())) + 5\nmlab.figure()\npts = mlab.points3d(\n    xyz[:, 0],",
        "detail": "env.share.doc.networkx-2.8.8.examples.3d_drawing.mayavi2_spring",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.3d_drawing.mayavi2_spring",
        "description": "env.share.doc.networkx-2.8.8.examples.3d_drawing.mayavi2_spring",
        "peekOfCode": "pos = nx.spring_layout(G, dim=3, seed=1001)\n# numpy array of x,y,z positions in sorted node order\nxyz = np.array([pos[v] for v in sorted(G)])\n# scalar colors\nscalars = np.array(list(G.nodes())) + 5\nmlab.figure()\npts = mlab.points3d(\n    xyz[:, 0],\n    xyz[:, 1],\n    xyz[:, 2],",
        "detail": "env.share.doc.networkx-2.8.8.examples.3d_drawing.mayavi2_spring",
        "documentation": {}
    },
    {
        "label": "xyz",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.3d_drawing.mayavi2_spring",
        "description": "env.share.doc.networkx-2.8.8.examples.3d_drawing.mayavi2_spring",
        "peekOfCode": "xyz = np.array([pos[v] for v in sorted(G)])\n# scalar colors\nscalars = np.array(list(G.nodes())) + 5\nmlab.figure()\npts = mlab.points3d(\n    xyz[:, 0],\n    xyz[:, 1],\n    xyz[:, 2],\n    scalars,\n    scale_factor=0.1,",
        "detail": "env.share.doc.networkx-2.8.8.examples.3d_drawing.mayavi2_spring",
        "documentation": {}
    },
    {
        "label": "scalars",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.3d_drawing.mayavi2_spring",
        "description": "env.share.doc.networkx-2.8.8.examples.3d_drawing.mayavi2_spring",
        "peekOfCode": "scalars = np.array(list(G.nodes())) + 5\nmlab.figure()\npts = mlab.points3d(\n    xyz[:, 0],\n    xyz[:, 1],\n    xyz[:, 2],\n    scalars,\n    scale_factor=0.1,\n    scale_mode=\"none\",\n    colormap=\"Blues\",",
        "detail": "env.share.doc.networkx-2.8.8.examples.3d_drawing.mayavi2_spring",
        "documentation": {}
    },
    {
        "label": "pts",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.3d_drawing.mayavi2_spring",
        "description": "env.share.doc.networkx-2.8.8.examples.3d_drawing.mayavi2_spring",
        "peekOfCode": "pts = mlab.points3d(\n    xyz[:, 0],\n    xyz[:, 1],\n    xyz[:, 2],\n    scalars,\n    scale_factor=0.1,\n    scale_mode=\"none\",\n    colormap=\"Blues\",\n    resolution=20,\n)",
        "detail": "env.share.doc.networkx-2.8.8.examples.3d_drawing.mayavi2_spring",
        "documentation": {}
    },
    {
        "label": "pts.mlab_source.dataset.lines",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.3d_drawing.mayavi2_spring",
        "description": "env.share.doc.networkx-2.8.8.examples.3d_drawing.mayavi2_spring",
        "peekOfCode": "pts.mlab_source.dataset.lines = np.array(list(G.edges()))\ntube = mlab.pipeline.tube(pts, tube_radius=0.01)\nmlab.pipeline.surface(tube, color=(0.8, 0.8, 0.8))\nmlab.orientation_axes()",
        "detail": "env.share.doc.networkx-2.8.8.examples.3d_drawing.mayavi2_spring",
        "documentation": {}
    },
    {
        "label": "tube",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.3d_drawing.mayavi2_spring",
        "description": "env.share.doc.networkx-2.8.8.examples.3d_drawing.mayavi2_spring",
        "peekOfCode": "tube = mlab.pipeline.tube(pts, tube_radius=0.01)\nmlab.pipeline.surface(tube, color=(0.8, 0.8, 0.8))\nmlab.orientation_axes()",
        "detail": "env.share.doc.networkx-2.8.8.examples.3d_drawing.mayavi2_spring",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.3d_drawing.plot_basic",
        "description": "env.share.doc.networkx-2.8.8.examples.3d_drawing.plot_basic",
        "peekOfCode": "G = nx.cycle_graph(20)\n# 3d spring layout\npos = nx.spring_layout(G, dim=3, seed=779)\n# Extract node and edge positions from the layout\nnode_xyz = np.array([pos[v] for v in sorted(G)])\nedge_xyz = np.array([(pos[u], pos[v]) for u, v in G.edges()])\n# Create the 3D figure\nfig = plt.figure()\nax = fig.add_subplot(111, projection=\"3d\")\n# Plot the nodes - alpha is scaled by \"depth\" automatically",
        "detail": "env.share.doc.networkx-2.8.8.examples.3d_drawing.plot_basic",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.3d_drawing.plot_basic",
        "description": "env.share.doc.networkx-2.8.8.examples.3d_drawing.plot_basic",
        "peekOfCode": "pos = nx.spring_layout(G, dim=3, seed=779)\n# Extract node and edge positions from the layout\nnode_xyz = np.array([pos[v] for v in sorted(G)])\nedge_xyz = np.array([(pos[u], pos[v]) for u, v in G.edges()])\n# Create the 3D figure\nfig = plt.figure()\nax = fig.add_subplot(111, projection=\"3d\")\n# Plot the nodes - alpha is scaled by \"depth\" automatically\nax.scatter(*node_xyz.T, s=100, ec=\"w\")\n# Plot the edges",
        "detail": "env.share.doc.networkx-2.8.8.examples.3d_drawing.plot_basic",
        "documentation": {}
    },
    {
        "label": "node_xyz",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.3d_drawing.plot_basic",
        "description": "env.share.doc.networkx-2.8.8.examples.3d_drawing.plot_basic",
        "peekOfCode": "node_xyz = np.array([pos[v] for v in sorted(G)])\nedge_xyz = np.array([(pos[u], pos[v]) for u, v in G.edges()])\n# Create the 3D figure\nfig = plt.figure()\nax = fig.add_subplot(111, projection=\"3d\")\n# Plot the nodes - alpha is scaled by \"depth\" automatically\nax.scatter(*node_xyz.T, s=100, ec=\"w\")\n# Plot the edges\nfor vizedge in edge_xyz:\n    ax.plot(*vizedge.T, color=\"tab:gray\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.3d_drawing.plot_basic",
        "documentation": {}
    },
    {
        "label": "edge_xyz",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.3d_drawing.plot_basic",
        "description": "env.share.doc.networkx-2.8.8.examples.3d_drawing.plot_basic",
        "peekOfCode": "edge_xyz = np.array([(pos[u], pos[v]) for u, v in G.edges()])\n# Create the 3D figure\nfig = plt.figure()\nax = fig.add_subplot(111, projection=\"3d\")\n# Plot the nodes - alpha is scaled by \"depth\" automatically\nax.scatter(*node_xyz.T, s=100, ec=\"w\")\n# Plot the edges\nfor vizedge in edge_xyz:\n    ax.plot(*vizedge.T, color=\"tab:gray\")\ndef _format_axes(ax):",
        "detail": "env.share.doc.networkx-2.8.8.examples.3d_drawing.plot_basic",
        "documentation": {}
    },
    {
        "label": "fig",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.3d_drawing.plot_basic",
        "description": "env.share.doc.networkx-2.8.8.examples.3d_drawing.plot_basic",
        "peekOfCode": "fig = plt.figure()\nax = fig.add_subplot(111, projection=\"3d\")\n# Plot the nodes - alpha is scaled by \"depth\" automatically\nax.scatter(*node_xyz.T, s=100, ec=\"w\")\n# Plot the edges\nfor vizedge in edge_xyz:\n    ax.plot(*vizedge.T, color=\"tab:gray\")\ndef _format_axes(ax):\n    \"\"\"Visualization options for the 3D axes.\"\"\"\n    # Turn gridlines off",
        "detail": "env.share.doc.networkx-2.8.8.examples.3d_drawing.plot_basic",
        "documentation": {}
    },
    {
        "label": "ax",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.3d_drawing.plot_basic",
        "description": "env.share.doc.networkx-2.8.8.examples.3d_drawing.plot_basic",
        "peekOfCode": "ax = fig.add_subplot(111, projection=\"3d\")\n# Plot the nodes - alpha is scaled by \"depth\" automatically\nax.scatter(*node_xyz.T, s=100, ec=\"w\")\n# Plot the edges\nfor vizedge in edge_xyz:\n    ax.plot(*vizedge.T, color=\"tab:gray\")\ndef _format_axes(ax):\n    \"\"\"Visualization options for the 3D axes.\"\"\"\n    # Turn gridlines off\n    ax.grid(False)",
        "detail": "env.share.doc.networkx-2.8.8.examples.3d_drawing.plot_basic",
        "documentation": {}
    },
    {
        "label": "progressive_widening_search",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "peekOfCode": "def progressive_widening_search(G, source, value, condition, initial_width=1):\n    \"\"\"Progressive widening beam search to find a node.\n    The progressive widening beam search involves a repeated beam\n    search, starting with a small beam width then extending to\n    progressively larger beam widths if the target node is not\n    found. This implementation simply returns the first node found that\n    matches the termination condition.\n    `G` is a NetworkX graph.\n    `source` is a node in the graph. The search for the node of interest\n    begins here and extends only to those nodes in the (weakly)",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "documentation": {}
    },
    {
        "label": "has_high_centrality",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "peekOfCode": "def has_high_centrality(v):\n    return centrality[v] >= avg_centrality\nsource = 0\nvalue = centrality.get\ncondition = has_high_centrality\nfound_node = progressive_widening_search(G, source, value, condition)\nc = centrality[found_node]\nprint(f\"found node {found_node} with centrality {c}\")\n# Draw graph\npos = nx.spring_layout(G, seed=seed)",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "peekOfCode": "seed = 89\nG = nx.gnp_random_graph(100, 0.5, seed=seed)\ncentrality = nx.eigenvector_centrality(G)\navg_centrality = sum(centrality.values()) / len(G)\ndef has_high_centrality(v):\n    return centrality[v] >= avg_centrality\nsource = 0\nvalue = centrality.get\ncondition = has_high_centrality\nfound_node = progressive_widening_search(G, source, value, condition)",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "peekOfCode": "G = nx.gnp_random_graph(100, 0.5, seed=seed)\ncentrality = nx.eigenvector_centrality(G)\navg_centrality = sum(centrality.values()) / len(G)\ndef has_high_centrality(v):\n    return centrality[v] >= avg_centrality\nsource = 0\nvalue = centrality.get\ncondition = has_high_centrality\nfound_node = progressive_widening_search(G, source, value, condition)\nc = centrality[found_node]",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "documentation": {}
    },
    {
        "label": "centrality",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "peekOfCode": "centrality = nx.eigenvector_centrality(G)\navg_centrality = sum(centrality.values()) / len(G)\ndef has_high_centrality(v):\n    return centrality[v] >= avg_centrality\nsource = 0\nvalue = centrality.get\ncondition = has_high_centrality\nfound_node = progressive_widening_search(G, source, value, condition)\nc = centrality[found_node]\nprint(f\"found node {found_node} with centrality {c}\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "documentation": {}
    },
    {
        "label": "avg_centrality",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "peekOfCode": "avg_centrality = sum(centrality.values()) / len(G)\ndef has_high_centrality(v):\n    return centrality[v] >= avg_centrality\nsource = 0\nvalue = centrality.get\ncondition = has_high_centrality\nfound_node = progressive_widening_search(G, source, value, condition)\nc = centrality[found_node]\nprint(f\"found node {found_node} with centrality {c}\")\n# Draw graph",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "documentation": {}
    },
    {
        "label": "source",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "peekOfCode": "source = 0\nvalue = centrality.get\ncondition = has_high_centrality\nfound_node = progressive_widening_search(G, source, value, condition)\nc = centrality[found_node]\nprint(f\"found node {found_node} with centrality {c}\")\n# Draw graph\npos = nx.spring_layout(G, seed=seed)\noptions = {\n    \"node_color\": \"blue\",",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "documentation": {}
    },
    {
        "label": "value",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "peekOfCode": "value = centrality.get\ncondition = has_high_centrality\nfound_node = progressive_widening_search(G, source, value, condition)\nc = centrality[found_node]\nprint(f\"found node {found_node} with centrality {c}\")\n# Draw graph\npos = nx.spring_layout(G, seed=seed)\noptions = {\n    \"node_color\": \"blue\",\n    \"node_size\": 20,",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "documentation": {}
    },
    {
        "label": "condition",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "peekOfCode": "condition = has_high_centrality\nfound_node = progressive_widening_search(G, source, value, condition)\nc = centrality[found_node]\nprint(f\"found node {found_node} with centrality {c}\")\n# Draw graph\npos = nx.spring_layout(G, seed=seed)\noptions = {\n    \"node_color\": \"blue\",\n    \"node_size\": 20,\n    \"edge_color\": \"grey\",",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "documentation": {}
    },
    {
        "label": "found_node",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "peekOfCode": "found_node = progressive_widening_search(G, source, value, condition)\nc = centrality[found_node]\nprint(f\"found node {found_node} with centrality {c}\")\n# Draw graph\npos = nx.spring_layout(G, seed=seed)\noptions = {\n    \"node_color\": \"blue\",\n    \"node_size\": 20,\n    \"edge_color\": \"grey\",\n    \"linewidths\": 0,",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "documentation": {}
    },
    {
        "label": "c",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "peekOfCode": "c = centrality[found_node]\nprint(f\"found node {found_node} with centrality {c}\")\n# Draw graph\npos = nx.spring_layout(G, seed=seed)\noptions = {\n    \"node_color\": \"blue\",\n    \"node_size\": 20,\n    \"edge_color\": \"grey\",\n    \"linewidths\": 0,\n    \"width\": 0.1,",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "peekOfCode": "pos = nx.spring_layout(G, seed=seed)\noptions = {\n    \"node_color\": \"blue\",\n    \"node_size\": 20,\n    \"edge_color\": \"grey\",\n    \"linewidths\": 0,\n    \"width\": 0.1,\n}\nnx.draw(G, pos, **options)\n# Draw node with high centrality as large and red",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "documentation": {}
    },
    {
        "label": "options",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "peekOfCode": "options = {\n    \"node_color\": \"blue\",\n    \"node_size\": 20,\n    \"edge_color\": \"grey\",\n    \"linewidths\": 0,\n    \"width\": 0.1,\n}\nnx.draw(G, pos, **options)\n# Draw node with high centrality as large and red\nnx.draw_networkx_nodes(G, pos, nodelist=[found_node], node_size=100, node_color=\"r\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_beam_search",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "peekOfCode": "G = nx.read_edgelist(\"WormNet.v3.benchmark.txt\")\n# remove randomly selected nodes (to make example fast)\nnum_to_remove = int(len(G) / 1.5)\nnodes = sample(list(G.nodes), num_to_remove)\nG.remove_nodes_from(nodes)\n# remove low-degree nodes\nlow_degree = [n for n, d in G.degree() if d < 10]\nG.remove_nodes_from(low_degree)\n# largest connected component\ncomponents = nx.connected_components(G)",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "documentation": {}
    },
    {
        "label": "num_to_remove",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "peekOfCode": "num_to_remove = int(len(G) / 1.5)\nnodes = sample(list(G.nodes), num_to_remove)\nG.remove_nodes_from(nodes)\n# remove low-degree nodes\nlow_degree = [n for n, d in G.degree() if d < 10]\nG.remove_nodes_from(low_degree)\n# largest connected component\ncomponents = nx.connected_components(G)\nlargest_component = max(components, key=len)\nH = G.subgraph(largest_component)",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "documentation": {}
    },
    {
        "label": "nodes",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "peekOfCode": "nodes = sample(list(G.nodes), num_to_remove)\nG.remove_nodes_from(nodes)\n# remove low-degree nodes\nlow_degree = [n for n, d in G.degree() if d < 10]\nG.remove_nodes_from(low_degree)\n# largest connected component\ncomponents = nx.connected_components(G)\nlargest_component = max(components, key=len)\nH = G.subgraph(largest_component)\n# compute centrality",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "documentation": {}
    },
    {
        "label": "low_degree",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "peekOfCode": "low_degree = [n for n, d in G.degree() if d < 10]\nG.remove_nodes_from(low_degree)\n# largest connected component\ncomponents = nx.connected_components(G)\nlargest_component = max(components, key=len)\nH = G.subgraph(largest_component)\n# compute centrality\ncentrality = nx.betweenness_centrality(H, k=10, endpoints=True)\n# compute community structure\nlpc = nx.community.label_propagation_communities(H)",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "documentation": {}
    },
    {
        "label": "components",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "peekOfCode": "components = nx.connected_components(G)\nlargest_component = max(components, key=len)\nH = G.subgraph(largest_component)\n# compute centrality\ncentrality = nx.betweenness_centrality(H, k=10, endpoints=True)\n# compute community structure\nlpc = nx.community.label_propagation_communities(H)\ncommunity_index = {n: i for i, com in enumerate(lpc) for n in com}\n#### draw graph ####\nfig, ax = plt.subplots(figsize=(20, 15))",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "documentation": {}
    },
    {
        "label": "largest_component",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "peekOfCode": "largest_component = max(components, key=len)\nH = G.subgraph(largest_component)\n# compute centrality\ncentrality = nx.betweenness_centrality(H, k=10, endpoints=True)\n# compute community structure\nlpc = nx.community.label_propagation_communities(H)\ncommunity_index = {n: i for i, com in enumerate(lpc) for n in com}\n#### draw graph ####\nfig, ax = plt.subplots(figsize=(20, 15))\npos = nx.spring_layout(H, k=0.15, seed=4572321)",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "documentation": {}
    },
    {
        "label": "H",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "peekOfCode": "H = G.subgraph(largest_component)\n# compute centrality\ncentrality = nx.betweenness_centrality(H, k=10, endpoints=True)\n# compute community structure\nlpc = nx.community.label_propagation_communities(H)\ncommunity_index = {n: i for i, com in enumerate(lpc) for n in com}\n#### draw graph ####\nfig, ax = plt.subplots(figsize=(20, 15))\npos = nx.spring_layout(H, k=0.15, seed=4572321)\nnode_color = [community_index[n] for n in H]",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "documentation": {}
    },
    {
        "label": "centrality",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "peekOfCode": "centrality = nx.betweenness_centrality(H, k=10, endpoints=True)\n# compute community structure\nlpc = nx.community.label_propagation_communities(H)\ncommunity_index = {n: i for i, com in enumerate(lpc) for n in com}\n#### draw graph ####\nfig, ax = plt.subplots(figsize=(20, 15))\npos = nx.spring_layout(H, k=0.15, seed=4572321)\nnode_color = [community_index[n] for n in H]\nnode_size = [v * 20000 for v in centrality.values()]\nnx.draw_networkx(",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "documentation": {}
    },
    {
        "label": "lpc",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "peekOfCode": "lpc = nx.community.label_propagation_communities(H)\ncommunity_index = {n: i for i, com in enumerate(lpc) for n in com}\n#### draw graph ####\nfig, ax = plt.subplots(figsize=(20, 15))\npos = nx.spring_layout(H, k=0.15, seed=4572321)\nnode_color = [community_index[n] for n in H]\nnode_size = [v * 20000 for v in centrality.values()]\nnx.draw_networkx(\n    H,\n    pos=pos,",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "documentation": {}
    },
    {
        "label": "community_index",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "peekOfCode": "community_index = {n: i for i, com in enumerate(lpc) for n in com}\n#### draw graph ####\nfig, ax = plt.subplots(figsize=(20, 15))\npos = nx.spring_layout(H, k=0.15, seed=4572321)\nnode_color = [community_index[n] for n in H]\nnode_size = [v * 20000 for v in centrality.values()]\nnx.draw_networkx(\n    H,\n    pos=pos,\n    with_labels=False,",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "peekOfCode": "pos = nx.spring_layout(H, k=0.15, seed=4572321)\nnode_color = [community_index[n] for n in H]\nnode_size = [v * 20000 for v in centrality.values()]\nnx.draw_networkx(\n    H,\n    pos=pos,\n    with_labels=False,\n    node_color=node_color,\n    node_size=node_size,\n    edge_color=\"gainsboro\",",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "documentation": {}
    },
    {
        "label": "node_color",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "peekOfCode": "node_color = [community_index[n] for n in H]\nnode_size = [v * 20000 for v in centrality.values()]\nnx.draw_networkx(\n    H,\n    pos=pos,\n    with_labels=False,\n    node_color=node_color,\n    node_size=node_size,\n    edge_color=\"gainsboro\",\n    alpha=0.4,",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "documentation": {}
    },
    {
        "label": "node_size",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "peekOfCode": "node_size = [v * 20000 for v in centrality.values()]\nnx.draw_networkx(\n    H,\n    pos=pos,\n    with_labels=False,\n    node_color=node_color,\n    node_size=node_size,\n    edge_color=\"gainsboro\",\n    alpha=0.4,\n)",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "documentation": {}
    },
    {
        "label": "font",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "peekOfCode": "font = {\"color\": \"k\", \"fontweight\": \"bold\", \"fontsize\": 20}\nax.set_title(\"Gene functional association network (C. elegans)\", font)\n# Change font color for legend\nfont[\"color\"] = \"r\"\nax.text(\n    0.80,\n    0.10,\n    \"node color = community structure\",\n    horizontalalignment=\"center\",\n    transform=ax.transAxes,",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "documentation": {}
    },
    {
        "label": "font[\"color\"]",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "peekOfCode": "font[\"color\"] = \"r\"\nax.text(\n    0.80,\n    0.10,\n    \"node color = community structure\",\n    horizontalalignment=\"center\",\n    transform=ax.transAxes,\n    fontdict=font,\n)\nax.text(",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_betweenness_centrality",
        "documentation": {}
    },
    {
        "label": "create_hc",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "peekOfCode": "def create_hc(G):\n    \"\"\"Creates hierarchical cluster of graph G from distance matrix\"\"\"\n    path_length = nx.all_pairs_shortest_path_length(G)\n    distances = np.zeros((len(G), len(G)))\n    for u, p in path_length:\n        for v, d in p.items():\n            distances[u][v] = d\n    # Create hierarchical cluster\n    Y = distance.squareform(distances)\n    Z = hierarchy.complete(Y)  # Creates HC using farthest point linkage",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "peekOfCode": "G = nx.read_edgelist(\"hartford_drug.edgelist\")\n# Extract largest connected component into graph H\nH = G.subgraph(next(nx.connected_components(G)))\n# Makes life easier to have consecutively labeled integer nodes\nH = nx.convert_node_labels_to_integers(H)\n# Create parititions with hierarchical clustering\npartitions = create_hc(H)\n# Build blockmodel graph\nBM = nx.quotient_graph(H, partitions, relabel=True)\n# Draw original graph",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "documentation": {}
    },
    {
        "label": "H",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "peekOfCode": "H = G.subgraph(next(nx.connected_components(G)))\n# Makes life easier to have consecutively labeled integer nodes\nH = nx.convert_node_labels_to_integers(H)\n# Create parititions with hierarchical clustering\npartitions = create_hc(H)\n# Build blockmodel graph\nBM = nx.quotient_graph(H, partitions, relabel=True)\n# Draw original graph\npos = nx.spring_layout(H, iterations=100, seed=83)  # Seed for reproducibility\nplt.subplot(211)",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "documentation": {}
    },
    {
        "label": "H",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "peekOfCode": "H = nx.convert_node_labels_to_integers(H)\n# Create parititions with hierarchical clustering\npartitions = create_hc(H)\n# Build blockmodel graph\nBM = nx.quotient_graph(H, partitions, relabel=True)\n# Draw original graph\npos = nx.spring_layout(H, iterations=100, seed=83)  # Seed for reproducibility\nplt.subplot(211)\nnx.draw(H, pos, with_labels=False, node_size=10)\n# Draw block model with weighted edges and nodes sized by number of internal nodes",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "documentation": {}
    },
    {
        "label": "partitions",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "peekOfCode": "partitions = create_hc(H)\n# Build blockmodel graph\nBM = nx.quotient_graph(H, partitions, relabel=True)\n# Draw original graph\npos = nx.spring_layout(H, iterations=100, seed=83)  # Seed for reproducibility\nplt.subplot(211)\nnx.draw(H, pos, with_labels=False, node_size=10)\n# Draw block model with weighted edges and nodes sized by number of internal nodes\nnode_size = [BM.nodes[x][\"nnodes\"] * 10 for x in BM.nodes()]\nedge_width = [(2 * d[\"weight\"]) for (u, v, d) in BM.edges(data=True)]",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "documentation": {}
    },
    {
        "label": "BM",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "peekOfCode": "BM = nx.quotient_graph(H, partitions, relabel=True)\n# Draw original graph\npos = nx.spring_layout(H, iterations=100, seed=83)  # Seed for reproducibility\nplt.subplot(211)\nnx.draw(H, pos, with_labels=False, node_size=10)\n# Draw block model with weighted edges and nodes sized by number of internal nodes\nnode_size = [BM.nodes[x][\"nnodes\"] * 10 for x in BM.nodes()]\nedge_width = [(2 * d[\"weight\"]) for (u, v, d) in BM.edges(data=True)]\n# Set positions to mean of positions of internal nodes from original graph\nposBM = {}",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "peekOfCode": "pos = nx.spring_layout(H, iterations=100, seed=83)  # Seed for reproducibility\nplt.subplot(211)\nnx.draw(H, pos, with_labels=False, node_size=10)\n# Draw block model with weighted edges and nodes sized by number of internal nodes\nnode_size = [BM.nodes[x][\"nnodes\"] * 10 for x in BM.nodes()]\nedge_width = [(2 * d[\"weight\"]) for (u, v, d) in BM.edges(data=True)]\n# Set positions to mean of positions of internal nodes from original graph\nposBM = {}\nfor n in BM:\n    xy = np.array([pos[u] for u in BM.nodes[n][\"graph\"]])",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "documentation": {}
    },
    {
        "label": "node_size",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "peekOfCode": "node_size = [BM.nodes[x][\"nnodes\"] * 10 for x in BM.nodes()]\nedge_width = [(2 * d[\"weight\"]) for (u, v, d) in BM.edges(data=True)]\n# Set positions to mean of positions of internal nodes from original graph\nposBM = {}\nfor n in BM:\n    xy = np.array([pos[u] for u in BM.nodes[n][\"graph\"]])\n    posBM[n] = xy.mean(axis=0)\nplt.subplot(212)\nnx.draw(BM, posBM, node_size=node_size, width=edge_width, with_labels=False)\nplt.axis(\"off\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "documentation": {}
    },
    {
        "label": "edge_width",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "peekOfCode": "edge_width = [(2 * d[\"weight\"]) for (u, v, d) in BM.edges(data=True)]\n# Set positions to mean of positions of internal nodes from original graph\nposBM = {}\nfor n in BM:\n    xy = np.array([pos[u] for u in BM.nodes[n][\"graph\"]])\n    posBM[n] = xy.mean(axis=0)\nplt.subplot(212)\nnx.draw(BM, posBM, node_size=node_size, width=edge_width, with_labels=False)\nplt.axis(\"off\")\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "documentation": {}
    },
    {
        "label": "posBM",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "peekOfCode": "posBM = {}\nfor n in BM:\n    xy = np.array([pos[u] for u in BM.nodes[n][\"graph\"]])\n    posBM[n] = xy.mean(axis=0)\nplt.subplot(212)\nnx.draw(BM, posBM, node_size=node_size, width=edge_width, with_labels=False)\nplt.axis(\"off\")\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_blockmodel",
        "documentation": {}
    },
    {
        "label": "circuit_to_formula",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_circuits",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_circuits",
        "peekOfCode": "def circuit_to_formula(circuit):\n    # Convert the circuit to an equivalent formula.\n    formula = nx.dag_to_branching(circuit)\n    # Transfer the operator or variable labels for each node from the\n    # circuit to the formula.\n    for v in formula:\n        source = formula.nodes[v][\"source\"]\n        formula.nodes[v][\"label\"] = circuit.nodes[source][\"label\"]\n    return formula\ndef formula_to_string(formula):",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_circuits",
        "documentation": {}
    },
    {
        "label": "formula_to_string",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_circuits",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_circuits",
        "peekOfCode": "def formula_to_string(formula):\n    def _to_string(formula, root):\n        # If there are no children, this is a variable node.\n        label = formula.nodes[root][\"label\"]\n        if not formula[root]:\n            return label\n        # Otherwise, this is an operator.\n        children = formula[root]\n        # If one child, the label must be a NOT operator.\n        if len(children) == 1:",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_circuits",
        "documentation": {}
    },
    {
        "label": "circuit",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_circuits",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_circuits",
        "peekOfCode": "circuit = nx.DiGraph()\n# Layer 0\ncircuit.add_node(0, label=\"\", layer=0)\n# Layer 1\ncircuit.add_node(1, label=\"\", layer=1)\ncircuit.add_node(2, label=\"\", layer=1)\ncircuit.add_edge(0, 1)\ncircuit.add_edge(0, 2)\n# Layer 2\ncircuit.add_node(3, label=\"x\", layer=2)",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_circuits",
        "documentation": {}
    },
    {
        "label": "formula",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_circuits",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_circuits",
        "peekOfCode": "formula = circuit_to_formula(circuit)\nprint(formula_to_string(formula))\nlabels = nx.get_node_attributes(circuit, \"label\")\noptions = {\n    \"node_size\": 600,\n    \"alpha\": 0.5,\n    \"node_color\": \"blue\",\n    \"labels\": labels,\n    \"font_size\": 22,\n}",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_circuits",
        "documentation": {}
    },
    {
        "label": "labels",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_circuits",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_circuits",
        "peekOfCode": "labels = nx.get_node_attributes(circuit, \"label\")\noptions = {\n    \"node_size\": 600,\n    \"alpha\": 0.5,\n    \"node_color\": \"blue\",\n    \"labels\": labels,\n    \"font_size\": 22,\n}\nplt.figure(figsize=(8, 8))\npos = nx.multipartite_layout(circuit, subset_key=\"layer\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_circuits",
        "documentation": {}
    },
    {
        "label": "options",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_circuits",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_circuits",
        "peekOfCode": "options = {\n    \"node_size\": 600,\n    \"alpha\": 0.5,\n    \"node_color\": \"blue\",\n    \"labels\": labels,\n    \"font_size\": 22,\n}\nplt.figure(figsize=(8, 8))\npos = nx.multipartite_layout(circuit, subset_key=\"layer\")\nnx.draw_networkx(circuit, pos, **options)",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_circuits",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_circuits",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_circuits",
        "peekOfCode": "pos = nx.multipartite_layout(circuit, subset_key=\"layer\")\nnx.draw_networkx(circuit, pos, **options)\nplt.title(formula_to_string(formula))\nplt.axis(\"equal\")\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_circuits",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_davis_club",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_davis_club",
        "peekOfCode": "G = nx.davis_southern_women_graph()\nwomen = G.graph[\"top\"]\nclubs = G.graph[\"bottom\"]\nprint(\"Biadjacency matrix\")\nprint(bipartite.biadjacency_matrix(G, women, clubs))\n# project bipartite graph onto women nodes\nW = bipartite.projected_graph(G, women)\nprint()\nprint(\"#Friends, Member\")\nfor w in women:",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_davis_club",
        "documentation": {}
    },
    {
        "label": "women",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_davis_club",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_davis_club",
        "peekOfCode": "women = G.graph[\"top\"]\nclubs = G.graph[\"bottom\"]\nprint(\"Biadjacency matrix\")\nprint(bipartite.biadjacency_matrix(G, women, clubs))\n# project bipartite graph onto women nodes\nW = bipartite.projected_graph(G, women)\nprint()\nprint(\"#Friends, Member\")\nfor w in women:\n    print(f\"{W.degree(w)} {w}\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_davis_club",
        "documentation": {}
    },
    {
        "label": "clubs",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_davis_club",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_davis_club",
        "peekOfCode": "clubs = G.graph[\"bottom\"]\nprint(\"Biadjacency matrix\")\nprint(bipartite.biadjacency_matrix(G, women, clubs))\n# project bipartite graph onto women nodes\nW = bipartite.projected_graph(G, women)\nprint()\nprint(\"#Friends, Member\")\nfor w in women:\n    print(f\"{W.degree(w)} {w}\")\n# project bipartite graph onto women nodes keeping number of co-occurence",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_davis_club",
        "documentation": {}
    },
    {
        "label": "W",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_davis_club",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_davis_club",
        "peekOfCode": "W = bipartite.projected_graph(G, women)\nprint()\nprint(\"#Friends, Member\")\nfor w in women:\n    print(f\"{W.degree(w)} {w}\")\n# project bipartite graph onto women nodes keeping number of co-occurence\n# the degree computed is weighted and counts the total number of shared contacts\nW = bipartite.weighted_projected_graph(G, women)\nprint()\nprint(\"#Friend meetings, Member\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_davis_club",
        "documentation": {}
    },
    {
        "label": "W",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_davis_club",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_davis_club",
        "peekOfCode": "W = bipartite.weighted_projected_graph(G, women)\nprint()\nprint(\"#Friend meetings, Member\")\nfor w in women:\n    print(f\"{W.degree(w, weight='weight')} {w}\")\npos = nx.spring_layout(G, seed=648)  # Seed layout for reproducible node positions\nnx.draw(G, pos)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_davis_club",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_davis_club",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_davis_club",
        "peekOfCode": "pos = nx.spring_layout(G, seed=648)  # Seed layout for reproducible node positions\nnx.draw(G, pos)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_davis_club",
        "documentation": {}
    },
    {
        "label": "original_graph",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "peekOfCode": "original_graph = nx.DiGraph()\nwhite_nodes = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\nred_nodes = [\"A\", \"B\", \"C\"]\nnode_sizes = [250 for node in white_nodes + red_nodes]\nnode_colors = [\"white\" for n in white_nodes] + [\"red\" for n in red_nodes]\noriginal_graph.add_nodes_from(white_nodes + red_nodes)\noriginal_graph.add_edges_from(\n    [\n        (\"1\", \"C\"),\n        (\"1\", \"B\"),",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "documentation": {}
    },
    {
        "label": "white_nodes",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "peekOfCode": "white_nodes = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]\nred_nodes = [\"A\", \"B\", \"C\"]\nnode_sizes = [250 for node in white_nodes + red_nodes]\nnode_colors = [\"white\" for n in white_nodes] + [\"red\" for n in red_nodes]\noriginal_graph.add_nodes_from(white_nodes + red_nodes)\noriginal_graph.add_edges_from(\n    [\n        (\"1\", \"C\"),\n        (\"1\", \"B\"),\n        (\"2\", \"C\"),",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "documentation": {}
    },
    {
        "label": "red_nodes",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "peekOfCode": "red_nodes = [\"A\", \"B\", \"C\"]\nnode_sizes = [250 for node in white_nodes + red_nodes]\nnode_colors = [\"white\" for n in white_nodes] + [\"red\" for n in red_nodes]\noriginal_graph.add_nodes_from(white_nodes + red_nodes)\noriginal_graph.add_edges_from(\n    [\n        (\"1\", \"C\"),\n        (\"1\", \"B\"),\n        (\"2\", \"C\"),\n        (\"2\", \"B\"),",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "documentation": {}
    },
    {
        "label": "node_sizes",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "peekOfCode": "node_sizes = [250 for node in white_nodes + red_nodes]\nnode_colors = [\"white\" for n in white_nodes] + [\"red\" for n in red_nodes]\noriginal_graph.add_nodes_from(white_nodes + red_nodes)\noriginal_graph.add_edges_from(\n    [\n        (\"1\", \"C\"),\n        (\"1\", \"B\"),\n        (\"2\", \"C\"),\n        (\"2\", \"B\"),\n        (\"2\", \"A\"),",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "documentation": {}
    },
    {
        "label": "node_colors",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "peekOfCode": "node_colors = [\"white\" for n in white_nodes] + [\"red\" for n in red_nodes]\noriginal_graph.add_nodes_from(white_nodes + red_nodes)\noriginal_graph.add_edges_from(\n    [\n        (\"1\", \"C\"),\n        (\"1\", \"B\"),\n        (\"2\", \"C\"),\n        (\"2\", \"B\"),\n        (\"2\", \"A\"),\n        (\"3\", \"B\"),",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "documentation": {}
    },
    {
        "label": "base_options",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "peekOfCode": "base_options = dict(with_labels=True, edgecolors=\"black\")\npos = {\n    \"3\": (0, 1),\n    \"2\": (0, 2),\n    \"1\": (0, 3),\n    \"6\": (1, 0),\n    \"A\": (1, 1),\n    \"B\": (1, 2),\n    \"C\": (1, 3),\n    \"4\": (2, 3),",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "peekOfCode": "pos = {\n    \"3\": (0, 1),\n    \"2\": (0, 2),\n    \"1\": (0, 3),\n    \"6\": (1, 0),\n    \"A\": (1, 1),\n    \"B\": (1, 2),\n    \"C\": (1, 3),\n    \"4\": (2, 3),\n    \"5\": (2, 1),",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "documentation": {}
    },
    {
        "label": "ax1",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "peekOfCode": "ax1 = plt.subplot(1, 2, 1)\nplt.title(\"Original (%s edges)\" % original_graph.number_of_edges())\nnx.draw_networkx(original_graph, pos=pos, node_color=node_colors, **base_options)\nnonexp_graph, compression_nodes = nx.summarization.dedensify(\n    original_graph, threshold=2, copy=False\n)\nnonexp_node_colors = list(node_colors)\nnonexp_node_sizes = list(node_sizes)\nfor node in compression_nodes:\n    nonexp_node_colors.append(\"yellow\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "documentation": {}
    },
    {
        "label": "nonexp_node_colors",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "peekOfCode": "nonexp_node_colors = list(node_colors)\nnonexp_node_sizes = list(node_sizes)\nfor node in compression_nodes:\n    nonexp_node_colors.append(\"yellow\")\n    nonexp_node_sizes.append(600)\nplt.subplot(1, 2, 2)\nplt.title(\"Dedensified (%s edges)\" % nonexp_graph.number_of_edges())\nnonexp_pos = {\n    \"5\": (0, 0),\n    \"B\": (0, 2),",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "documentation": {}
    },
    {
        "label": "nonexp_node_sizes",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "peekOfCode": "nonexp_node_sizes = list(node_sizes)\nfor node in compression_nodes:\n    nonexp_node_colors.append(\"yellow\")\n    nonexp_node_sizes.append(600)\nplt.subplot(1, 2, 2)\nplt.title(\"Dedensified (%s edges)\" % nonexp_graph.number_of_edges())\nnonexp_pos = {\n    \"5\": (0, 0),\n    \"B\": (0, 2),\n    \"1\": (0, 3),",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "documentation": {}
    },
    {
        "label": "nonexp_pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "peekOfCode": "nonexp_pos = {\n    \"5\": (0, 0),\n    \"B\": (0, 2),\n    \"1\": (0, 3),\n    \"6\": (1, 0.75),\n    \"3\": (1.5, 1.5),\n    \"A\": (2, 0),\n    \"C\": (2, 3),\n    \"4\": (3, 1.5),\n    \"2\": (3, 2.5),",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "documentation": {}
    },
    {
        "label": "c_nodes",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "peekOfCode": "c_nodes = list(compression_nodes)\nc_nodes.sort()\nfor spot, node in enumerate(c_nodes):\n    nonexp_pos[node] = (2, spot + 2)\nnx.draw_networkx(\n    nonexp_graph,\n    pos=nonexp_pos,\n    node_color=nonexp_node_colors,\n    node_size=nonexp_node_sizes,\n    **base_options",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_dedensification",
        "documentation": {}
    },
    {
        "label": "digitsrep",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "peekOfCode": "def digitsrep(n, b=10):\n    \"\"\"Return list of digits comprising n represented in base b.\n    n must be a nonnegative integer\"\"\"\n    if n <= 0:\n        return [0]\n    dlist = []\n    while n > 0:\n        # Prepend next least-significant digit\n        dlist = [n % b] + dlist\n        # Floor-division",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "documentation": {}
    },
    {
        "label": "powersum",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "peekOfCode": "def powersum(n, p, b=10):\n    \"\"\"Return sum of digits of n (in base b) raised to the power p.\"\"\"\n    dlist = digitsrep(n, b)\n    sum = 0\n    for k in dlist:\n        sum += k**p\n    return sum\ndef attractor153_graph(n, p, multiple=3, b=10):\n    \"\"\"Return digraph of iterations of powersum(n,3,10).\"\"\"\n    G = nx.DiGraph()",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "documentation": {}
    },
    {
        "label": "attractor153_graph",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "peekOfCode": "def attractor153_graph(n, p, multiple=3, b=10):\n    \"\"\"Return digraph of iterations of powersum(n,3,10).\"\"\"\n    G = nx.DiGraph()\n    for k in range(1, n + 1):\n        if k % multiple == 0 and k not in G:\n            k1 = k\n            knext = powersum(k1, p, b)\n            while k1 != knext:\n                G.add_edge(k1, knext)\n                k1 = knext",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "documentation": {}
    },
    {
        "label": "squaring_cycle_graph_old",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "peekOfCode": "def squaring_cycle_graph_old(n, b=10):\n    \"\"\"Return digraph of iterations of powersum(n,2,10).\"\"\"\n    G = nx.DiGraph()\n    for k in range(1, n + 1):\n        k1 = k\n        G.add_node(k1)  # case k1==knext, at least add node\n        knext = powersum(k1, 2, b)\n        G.add_edge(k1, knext)\n        while k1 != knext:  # stop if fixed point\n            k1 = knext",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "documentation": {}
    },
    {
        "label": "sum_of_digits_graph",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "peekOfCode": "def sum_of_digits_graph(nmax, b=10):\n    def f(n):\n        return powersum(n, 1, b)\n    return discrete_dynamics_digraph(nmax, f)\ndef squaring_cycle_digraph(nmax, b=10):\n    def f(n):\n        return powersum(n, 2, b)\n    return discrete_dynamics_digraph(nmax, f)\ndef cubing_153_digraph(nmax):\n    def f(n):",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "documentation": {}
    },
    {
        "label": "squaring_cycle_digraph",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "peekOfCode": "def squaring_cycle_digraph(nmax, b=10):\n    def f(n):\n        return powersum(n, 2, b)\n    return discrete_dynamics_digraph(nmax, f)\ndef cubing_153_digraph(nmax):\n    def f(n):\n        return powersum(n, 3, 10)\n    return discrete_dynamics_digraph(nmax, f)\ndef discrete_dynamics_digraph(nmax, f, itermax=50000):\n    G = nx.DiGraph()",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "documentation": {}
    },
    {
        "label": "cubing_153_digraph",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "peekOfCode": "def cubing_153_digraph(nmax):\n    def f(n):\n        return powersum(n, 3, 10)\n    return discrete_dynamics_digraph(nmax, f)\ndef discrete_dynamics_digraph(nmax, f, itermax=50000):\n    G = nx.DiGraph()\n    for k in range(1, nmax + 1):\n        kold = k\n        G.add_node(kold)\n        knew = f(kold)",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "documentation": {}
    },
    {
        "label": "discrete_dynamics_digraph",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "peekOfCode": "def discrete_dynamics_digraph(nmax, f, itermax=50000):\n    G = nx.DiGraph()\n    for k in range(1, nmax + 1):\n        kold = k\n        G.add_node(kold)\n        knew = f(kold)\n        G.add_edge(kold, knew)\n        while kold != knew and kold << itermax:\n            # iterate until fixed point reached or itermax is exceeded\n            kold = knew",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "documentation": {}
    },
    {
        "label": "collatz_problem_digraph",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "peekOfCode": "def collatz_problem_digraph(nmax):\n    def f(n):\n        if n % 2 == 0:\n            return n // 2\n        else:\n            return 3 * n + 1\n    return discrete_dynamics_digraph(nmax, f)\ndef fixed_points(G):\n    \"\"\"Return a list of fixed points for the discrete dynamical\n    system represented by the digraph G.",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "documentation": {}
    },
    {
        "label": "fixed_points",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "peekOfCode": "def fixed_points(G):\n    \"\"\"Return a list of fixed points for the discrete dynamical\n    system represented by the digraph G.\n    \"\"\"\n    return [n for n in G if G.out_degree(n) == 0]\nnmax = 10000\nprint(f\"Building cubing_153_digraph({nmax})\")\nG = cubing_153_digraph(nmax)\nprint(\"Resulting digraph has\", len(G), \"nodes and\", G.size(), \" edges\")\nprint(\"Shortest path from 177 to 153 is:\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "documentation": {}
    },
    {
        "label": "f(108)",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "peekOfCode": "f(108) = 1**3 + 0**3 + 8**3 = 513\nand\nf(513) = 5**3 + 1**3 + 3**3 = 153\nSo, starting at 108 we reach 153 in two iterations,\nrepresented as:\n108->513->153\nComputing all orbits of 3N up to 10**5 reveals that the attractor\n153 is reached in a maximum of 14 iterations. In this code we\nshow that 13 cycles is the maximum required for all integers (in 3N)\nless than 10,000.",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "documentation": {}
    },
    {
        "label": "f(513)",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "peekOfCode": "f(513) = 5**3 + 1**3 + 3**3 = 153\nSo, starting at 108 we reach 153 in two iterations,\nrepresented as:\n108->513->153\nComputing all orbits of 3N up to 10**5 reveals that the attractor\n153 is reached in a maximum of 14 iterations. In this code we\nshow that 13 cycles is the maximum required for all integers (in 3N)\nless than 10,000.\nThe smallest number that requires 13 iterations to reach 153, is 177, i.e.,\n177->687->1071->345->216->225->141->66->432->99->1458->702->351->153",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "documentation": {}
    },
    {
        "label": "nmax",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "peekOfCode": "nmax = 10000\np = 3\ndef digitsrep(n, b=10):\n    \"\"\"Return list of digits comprising n represented in base b.\n    n must be a nonnegative integer\"\"\"\n    if n <= 0:\n        return [0]\n    dlist = []\n    while n > 0:\n        # Prepend next least-significant digit",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "documentation": {}
    },
    {
        "label": "p",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "peekOfCode": "p = 3\ndef digitsrep(n, b=10):\n    \"\"\"Return list of digits comprising n represented in base b.\n    n must be a nonnegative integer\"\"\"\n    if n <= 0:\n        return [0]\n    dlist = []\n    while n > 0:\n        # Prepend next least-significant digit\n        dlist = [n % b] + dlist",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "documentation": {}
    },
    {
        "label": "nmax",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "peekOfCode": "nmax = 10000\nprint(f\"Building cubing_153_digraph({nmax})\")\nG = cubing_153_digraph(nmax)\nprint(\"Resulting digraph has\", len(G), \"nodes and\", G.size(), \" edges\")\nprint(\"Shortest path from 177 to 153 is:\")\nprint(nx.shortest_path(G, 177, 153))\nprint(f\"fixed points are {fixed_points(G)}\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "peekOfCode": "G = cubing_153_digraph(nmax)\nprint(\"Resulting digraph has\", len(G), \"nodes and\", G.size(), \" edges\")\nprint(\"Shortest path from 177 to 153 is:\")\nprint(nx.shortest_path(G, 177, 153))\nprint(f\"fixed points are {fixed_points(G)}\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_iterated_dynamical_systems",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_krackhardt_centrality",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_krackhardt_centrality",
        "peekOfCode": "G = nx.krackhardt_kite_graph()\nprint(\"Betweenness\")\nb = nx.betweenness_centrality(G)\nfor v in G.nodes():\n    print(f\"{v:2} {b[v]:.3f}\")\nprint(\"Degree centrality\")\nd = nx.degree_centrality(G)\nfor v in G.nodes():\n    print(f\"{v:2} {d[v]:.3f}\")\nprint(\"Closeness centrality\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_krackhardt_centrality",
        "documentation": {}
    },
    {
        "label": "b",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_krackhardt_centrality",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_krackhardt_centrality",
        "peekOfCode": "b = nx.betweenness_centrality(G)\nfor v in G.nodes():\n    print(f\"{v:2} {b[v]:.3f}\")\nprint(\"Degree centrality\")\nd = nx.degree_centrality(G)\nfor v in G.nodes():\n    print(f\"{v:2} {d[v]:.3f}\")\nprint(\"Closeness centrality\")\nc = nx.closeness_centrality(G)\nfor v in G.nodes():",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_krackhardt_centrality",
        "documentation": {}
    },
    {
        "label": "d",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_krackhardt_centrality",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_krackhardt_centrality",
        "peekOfCode": "d = nx.degree_centrality(G)\nfor v in G.nodes():\n    print(f\"{v:2} {d[v]:.3f}\")\nprint(\"Closeness centrality\")\nc = nx.closeness_centrality(G)\nfor v in G.nodes():\n    print(f\"{v:2} {c[v]:.3f}\")\npos = nx.spring_layout(G, seed=367)  # Seed layout for reproducibility\nnx.draw(G, pos)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_krackhardt_centrality",
        "documentation": {}
    },
    {
        "label": "c",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_krackhardt_centrality",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_krackhardt_centrality",
        "peekOfCode": "c = nx.closeness_centrality(G)\nfor v in G.nodes():\n    print(f\"{v:2} {c[v]:.3f}\")\npos = nx.spring_layout(G, seed=367)  # Seed layout for reproducibility\nnx.draw(G, pos)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_krackhardt_centrality",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_krackhardt_centrality",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_krackhardt_centrality",
        "peekOfCode": "pos = nx.spring_layout(G, seed=367)  # Seed layout for reproducibility\nnx.draw(G, pos)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_krackhardt_centrality",
        "documentation": {}
    },
    {
        "label": "chunks",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_parallel_betweenness",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_parallel_betweenness",
        "peekOfCode": "def chunks(l, n):\n    \"\"\"Divide a list of nodes `l` in `n` chunks\"\"\"\n    l_c = iter(l)\n    while 1:\n        x = tuple(itertools.islice(l_c, n))\n        if not x:\n            return\n        yield x\ndef betweenness_centrality_parallel(G, processes=None):\n    \"\"\"Parallel betweenness centrality  function\"\"\"",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_parallel_betweenness",
        "documentation": {}
    },
    {
        "label": "betweenness_centrality_parallel",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_parallel_betweenness",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_parallel_betweenness",
        "peekOfCode": "def betweenness_centrality_parallel(G, processes=None):\n    \"\"\"Parallel betweenness centrality  function\"\"\"\n    p = Pool(processes=processes)\n    node_divisor = len(p._pool) * 4\n    node_chunks = list(chunks(G.nodes(), G.order() // node_divisor))\n    num_chunks = len(node_chunks)\n    bt_sc = p.starmap(\n        nx.betweenness_centrality_subset,\n        zip(\n            [G] * num_chunks,",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_parallel_betweenness",
        "documentation": {}
    },
    {
        "label": "G_ba",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_parallel_betweenness",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_parallel_betweenness",
        "peekOfCode": "G_ba = nx.barabasi_albert_graph(1000, 3)\nG_er = nx.gnp_random_graph(1000, 0.01)\nG_ws = nx.connected_watts_strogatz_graph(1000, 4, 0.1)\nfor G in [G_ba, G_er, G_ws]:\n    print(\"\")\n    print(\"Computing betweenness centrality for:\")\n    print(nx.info(G))\n    print(\"\\tParallel version\")\n    start = time.time()\n    bt = betweenness_centrality_parallel(G)",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_parallel_betweenness",
        "documentation": {}
    },
    {
        "label": "G_er",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_parallel_betweenness",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_parallel_betweenness",
        "peekOfCode": "G_er = nx.gnp_random_graph(1000, 0.01)\nG_ws = nx.connected_watts_strogatz_graph(1000, 4, 0.1)\nfor G in [G_ba, G_er, G_ws]:\n    print(\"\")\n    print(\"Computing betweenness centrality for:\")\n    print(nx.info(G))\n    print(\"\\tParallel version\")\n    start = time.time()\n    bt = betweenness_centrality_parallel(G)\n    print(f\"\\t\\tTime: {(time.time() - start):.4F} seconds\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_parallel_betweenness",
        "documentation": {}
    },
    {
        "label": "G_ws",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_parallel_betweenness",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_parallel_betweenness",
        "peekOfCode": "G_ws = nx.connected_watts_strogatz_graph(1000, 4, 0.1)\nfor G in [G_ba, G_er, G_ws]:\n    print(\"\")\n    print(\"Computing betweenness centrality for:\")\n    print(nx.info(G))\n    print(\"\\tParallel version\")\n    start = time.time()\n    bt = betweenness_centrality_parallel(G)\n    print(f\"\\t\\tTime: {(time.time() - start):.4F} seconds\")\n    print(f\"\\t\\tBetweenness centrality for node 0: {bt[0]:.5f}\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_parallel_betweenness",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_rcm",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_rcm",
        "peekOfCode": "G = nx.grid_2d_graph(3, 3)\nrcm = list(nx.utils.reverse_cuthill_mckee_ordering(G))\nprint(\"ordering\", rcm)\nprint(\"unordered Laplacian matrix\")\nA = nx.laplacian_matrix(G)\nx, y = np.nonzero(A)\n# print(f\"lower bandwidth: {(y - x).max()}\")\n# print(f\"upper bandwidth: {(x - y).max()}\")\nprint(f\"bandwidth: {(y - x).max() + (x - y).max() + 1}\")\nprint(A)",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_rcm",
        "documentation": {}
    },
    {
        "label": "rcm",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_rcm",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_rcm",
        "peekOfCode": "rcm = list(nx.utils.reverse_cuthill_mckee_ordering(G))\nprint(\"ordering\", rcm)\nprint(\"unordered Laplacian matrix\")\nA = nx.laplacian_matrix(G)\nx, y = np.nonzero(A)\n# print(f\"lower bandwidth: {(y - x).max()}\")\n# print(f\"upper bandwidth: {(x - y).max()}\")\nprint(f\"bandwidth: {(y - x).max() + (x - y).max() + 1}\")\nprint(A)\nB = nx.laplacian_matrix(G, nodelist=rcm)",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_rcm",
        "documentation": {}
    },
    {
        "label": "A",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_rcm",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_rcm",
        "peekOfCode": "A = nx.laplacian_matrix(G)\nx, y = np.nonzero(A)\n# print(f\"lower bandwidth: {(y - x).max()}\")\n# print(f\"upper bandwidth: {(x - y).max()}\")\nprint(f\"bandwidth: {(y - x).max() + (x - y).max() + 1}\")\nprint(A)\nB = nx.laplacian_matrix(G, nodelist=rcm)\nprint(\"low-bandwidth Laplacian matrix\")\nx, y = np.nonzero(B)\n# print(f\"lower bandwidth: {(y - x).max()}\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_rcm",
        "documentation": {}
    },
    {
        "label": "B",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_rcm",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_rcm",
        "peekOfCode": "B = nx.laplacian_matrix(G, nodelist=rcm)\nprint(\"low-bandwidth Laplacian matrix\")\nx, y = np.nonzero(B)\n# print(f\"lower bandwidth: {(y - x).max()}\")\n# print(f\"upper bandwidth: {(x - y).max()}\")\nprint(f\"bandwidth: {(y - x).max() + (x - y).max() + 1}\")\nprint(B)\nsns.heatmap(B.todense(), cbar=False, square=True, linewidths=0.5, annot=True)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_rcm",
        "documentation": {}
    },
    {
        "label": "nodes",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "peekOfCode": "nodes = {\n    \"A\": dict(color=\"Red\"),\n    \"B\": dict(color=\"Red\"),\n    \"C\": dict(color=\"Red\"),\n    \"D\": dict(color=\"Red\"),\n    \"E\": dict(color=\"Blue\"),\n    \"F\": dict(color=\"Blue\"),\n    \"G\": dict(color=\"Blue\"),\n    \"H\": dict(color=\"Blue\"),\n    \"I\": dict(color=\"Yellow\"),",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "documentation": {}
    },
    {
        "label": "edges",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "peekOfCode": "edges = [\n    (\"A\", \"B\", \"Strong\"),\n    (\"A\", \"C\", \"Weak\"),\n    (\"A\", \"E\", \"Strong\"),\n    (\"A\", \"I\", \"Weak\"),\n    (\"B\", \"D\", \"Weak\"),\n    (\"B\", \"J\", \"Weak\"),\n    (\"B\", \"F\", \"Strong\"),\n    (\"C\", \"G\", \"Weak\"),\n    (\"D\", \"H\", \"Weak\"),",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "documentation": {}
    },
    {
        "label": "original_graph",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "peekOfCode": "original_graph = nx.Graph()\noriginal_graph.add_nodes_from(n for n in nodes.items())\noriginal_graph.add_edges_from((u, v, {\"type\": label}) for u, v, label in edges)\nplt.suptitle(\"SNAP Summarization\")\nbase_options = dict(with_labels=True, edgecolors=\"black\", node_size=500)\nax1 = plt.subplot(1, 2, 1)\nplt.title(\n    \"Original (%s nodes, %s edges)\"\n    % (original_graph.number_of_nodes(), original_graph.number_of_edges())\n)",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "documentation": {}
    },
    {
        "label": "base_options",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "peekOfCode": "base_options = dict(with_labels=True, edgecolors=\"black\", node_size=500)\nax1 = plt.subplot(1, 2, 1)\nplt.title(\n    \"Original (%s nodes, %s edges)\"\n    % (original_graph.number_of_nodes(), original_graph.number_of_edges())\n)\npos = nx.spring_layout(original_graph, seed=7482934)\nnode_colors = [d[\"color\"] for _, d in original_graph.nodes(data=True)]\nedge_type_visual_weight_lookup = {\"Weak\": 1.0, \"Strong\": 3.0}\nedge_weights = [",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "documentation": {}
    },
    {
        "label": "ax1",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "peekOfCode": "ax1 = plt.subplot(1, 2, 1)\nplt.title(\n    \"Original (%s nodes, %s edges)\"\n    % (original_graph.number_of_nodes(), original_graph.number_of_edges())\n)\npos = nx.spring_layout(original_graph, seed=7482934)\nnode_colors = [d[\"color\"] for _, d in original_graph.nodes(data=True)]\nedge_type_visual_weight_lookup = {\"Weak\": 1.0, \"Strong\": 3.0}\nedge_weights = [\n    edge_type_visual_weight_lookup[d[\"type\"]]",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "peekOfCode": "pos = nx.spring_layout(original_graph, seed=7482934)\nnode_colors = [d[\"color\"] for _, d in original_graph.nodes(data=True)]\nedge_type_visual_weight_lookup = {\"Weak\": 1.0, \"Strong\": 3.0}\nedge_weights = [\n    edge_type_visual_weight_lookup[d[\"type\"]]\n    for _, _, d in original_graph.edges(data=True)\n]\nnx.draw_networkx(\n    original_graph, pos=pos, node_color=node_colors, width=edge_weights, **base_options\n)",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "documentation": {}
    },
    {
        "label": "node_colors",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "peekOfCode": "node_colors = [d[\"color\"] for _, d in original_graph.nodes(data=True)]\nedge_type_visual_weight_lookup = {\"Weak\": 1.0, \"Strong\": 3.0}\nedge_weights = [\n    edge_type_visual_weight_lookup[d[\"type\"]]\n    for _, _, d in original_graph.edges(data=True)\n]\nnx.draw_networkx(\n    original_graph, pos=pos, node_color=node_colors, width=edge_weights, **base_options\n)\nnode_attributes = (\"color\",)",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "documentation": {}
    },
    {
        "label": "edge_type_visual_weight_lookup",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "peekOfCode": "edge_type_visual_weight_lookup = {\"Weak\": 1.0, \"Strong\": 3.0}\nedge_weights = [\n    edge_type_visual_weight_lookup[d[\"type\"]]\n    for _, _, d in original_graph.edges(data=True)\n]\nnx.draw_networkx(\n    original_graph, pos=pos, node_color=node_colors, width=edge_weights, **base_options\n)\nnode_attributes = (\"color\",)\nedge_attributes = (\"type\",)",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "documentation": {}
    },
    {
        "label": "edge_weights",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "peekOfCode": "edge_weights = [\n    edge_type_visual_weight_lookup[d[\"type\"]]\n    for _, _, d in original_graph.edges(data=True)\n]\nnx.draw_networkx(\n    original_graph, pos=pos, node_color=node_colors, width=edge_weights, **base_options\n)\nnode_attributes = (\"color\",)\nedge_attributes = (\"type\",)\nsummary_graph = nx.snap_aggregation(",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "documentation": {}
    },
    {
        "label": "node_attributes",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "peekOfCode": "node_attributes = (\"color\",)\nedge_attributes = (\"type\",)\nsummary_graph = nx.snap_aggregation(\n    original_graph, node_attributes, edge_attributes, prefix=\"S-\"\n)\nplt.subplot(1, 2, 2)\nplt.title(\n    \"SNAP Aggregation (%s nodes, %s edges)\"\n    % (summary_graph.number_of_nodes(), summary_graph.number_of_edges())\n)",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "documentation": {}
    },
    {
        "label": "edge_attributes",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "peekOfCode": "edge_attributes = (\"type\",)\nsummary_graph = nx.snap_aggregation(\n    original_graph, node_attributes, edge_attributes, prefix=\"S-\"\n)\nplt.subplot(1, 2, 2)\nplt.title(\n    \"SNAP Aggregation (%s nodes, %s edges)\"\n    % (summary_graph.number_of_nodes(), summary_graph.number_of_edges())\n)\nsummary_pos = nx.spring_layout(summary_graph, seed=8375428)",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "documentation": {}
    },
    {
        "label": "summary_graph",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "peekOfCode": "summary_graph = nx.snap_aggregation(\n    original_graph, node_attributes, edge_attributes, prefix=\"S-\"\n)\nplt.subplot(1, 2, 2)\nplt.title(\n    \"SNAP Aggregation (%s nodes, %s edges)\"\n    % (summary_graph.number_of_nodes(), summary_graph.number_of_edges())\n)\nsummary_pos = nx.spring_layout(summary_graph, seed=8375428)\nnode_colors = []",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "documentation": {}
    },
    {
        "label": "summary_pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "peekOfCode": "summary_pos = nx.spring_layout(summary_graph, seed=8375428)\nnode_colors = []\nfor node in summary_graph:\n    color = summary_graph.nodes[node][\"color\"]\n    node_colors.append(color)\nedge_weights = []\nfor edge in summary_graph.edges():\n    edge_types = summary_graph.get_edge_data(*edge)[\"types\"]\n    edge_weight = 0.0\n    for edge_type in edge_types:",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "documentation": {}
    },
    {
        "label": "node_colors",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "peekOfCode": "node_colors = []\nfor node in summary_graph:\n    color = summary_graph.nodes[node][\"color\"]\n    node_colors.append(color)\nedge_weights = []\nfor edge in summary_graph.edges():\n    edge_types = summary_graph.get_edge_data(*edge)[\"types\"]\n    edge_weight = 0.0\n    for edge_type in edge_types:\n        edge_weight += edge_type_visual_weight_lookup[edge_type[\"type\"]]",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "documentation": {}
    },
    {
        "label": "edge_weights",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "peekOfCode": "edge_weights = []\nfor edge in summary_graph.edges():\n    edge_types = summary_graph.get_edge_data(*edge)[\"types\"]\n    edge_weight = 0.0\n    for edge_type in edge_types:\n        edge_weight += edge_type_visual_weight_lookup[edge_type[\"type\"]]\n    edge_weights.append(edge_weight)\nnx.draw_networkx(\n    summary_graph,\n    pos=summary_pos,",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_snap",
        "documentation": {}
    },
    {
        "label": "graph_partitioning",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_subgraphs",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_subgraphs",
        "peekOfCode": "def graph_partitioning(G, plotting=True):\n    \"\"\"Partition a directed graph into a list of subgraphs that contain\n    only entirely supported or entirely unsupported nodes.\n    \"\"\"\n    # Categorize nodes by their node_type attribute\n    supported_nodes = {n for n, d in G.nodes(data=\"node_type\") if d == \"supported\"}\n    unsupported_nodes = {n for n, d in G.nodes(data=\"node_type\") if d == \"unsupported\"}\n    # Make a copy of the graph.\n    H = G.copy()\n    # Remove all edges connecting supported and unsupported nodes.",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_subgraphs",
        "documentation": {}
    },
    {
        "label": "G_ex",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_subgraphs",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_subgraphs",
        "peekOfCode": "G_ex = nx.DiGraph()\nG_ex.add_nodes_from([\"In\"], node_type=\"input\", node_color=\"b\")\nG_ex.add_nodes_from([\"A\", \"C\", \"E\", \"F\"], node_type=\"supported\", node_color=\"g\")\nG_ex.add_nodes_from([\"B\", \"D\"], node_type=\"unsupported\", node_color=\"r\")\nG_ex.add_nodes_from([\"Out\"], node_type=\"output\", node_color=\"m\")\nG_ex.add_edges_from(\n    [\n        (\"In\", \"A\"),\n        (\"A\", \"B\"),\n        (\"B\", \"C\"),",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_subgraphs",
        "documentation": {}
    },
    {
        "label": "node_color_list",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_subgraphs",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_subgraphs",
        "peekOfCode": "node_color_list = [nc for _, nc in G_ex.nodes(data=\"node_color\")]\npos = nx.spectral_layout(G_ex)\nplt.figure(figsize=(8, 8))\nnx.draw_networkx_edges(G_ex, pos, alpha=0.3, edge_color=\"k\")\nnx.draw_networkx_nodes(G_ex, pos, alpha=0.8, node_color=node_color_list)\nnx.draw_networkx_labels(G_ex, pos, font_size=14)\nplt.axis(\"off\")\nplt.title(\"The original graph.\")\nplt.show()\n###############################################################################",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_subgraphs",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_subgraphs",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_subgraphs",
        "peekOfCode": "pos = nx.spectral_layout(G_ex)\nplt.figure(figsize=(8, 8))\nnx.draw_networkx_edges(G_ex, pos, alpha=0.3, edge_color=\"k\")\nnx.draw_networkx_nodes(G_ex, pos, alpha=0.8, node_color=node_color_list)\nnx.draw_networkx_labels(G_ex, pos, font_size=14)\nplt.axis(\"off\")\nplt.title(\"The original graph.\")\nplt.show()\n###############################################################################\n# Calculate the subgraphs with plotting all results of intemediate steps.",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_subgraphs",
        "documentation": {}
    },
    {
        "label": "G_ex_r",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_subgraphs",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_subgraphs",
        "peekOfCode": "G_ex_r = nx.DiGraph()\n# Composing all subgraphs.\nfor subgraph in subgraphs_of_G_ex:\n    G_ex_r = nx.compose(G_ex_r, subgraph)\n# Adding the previously stored edges.\nG_ex_r.add_edges_from(removed_edges.edges())\n###############################################################################\n# Check that the original graph and the reconstructed graphs are isomorphic.\n# --------------------------------------------------------------------------\n#",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_subgraphs",
        "documentation": {}
    },
    {
        "label": "node_color_list",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_subgraphs",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_subgraphs",
        "peekOfCode": "node_color_list = [nc for _, nc in G_ex_r.nodes(data=\"node_color\")]\npos = nx.spectral_layout(G_ex_r)\nplt.figure(figsize=(8, 8))\nnx.draw_networkx_edges(G_ex_r, pos, alpha=0.3, edge_color=\"k\")\nnx.draw_networkx_nodes(G_ex_r, pos, alpha=0.8, node_color=node_color_list)\nnx.draw_networkx_labels(G_ex_r, pos, font_size=14)\nplt.axis(\"off\")\nplt.title(\"The reconstructed graph.\")\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_subgraphs",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_subgraphs",
        "description": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_subgraphs",
        "peekOfCode": "pos = nx.spectral_layout(G_ex_r)\nplt.figure(figsize=(8, 8))\nnx.draw_networkx_edges(G_ex_r, pos, alpha=0.3, edge_color=\"k\")\nnx.draw_networkx_nodes(G_ex_r, pos, alpha=0.8, node_color=node_color_list)\nnx.draw_networkx_labels(G_ex_r, pos, font_size=14)\nplt.axis(\"off\")\nplt.title(\"The reconstructed graph.\")\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.algorithms.plot_subgraphs",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.basic.plot_properties",
        "description": "env.share.doc.networkx-2.8.8.examples.basic.plot_properties",
        "peekOfCode": "G = nx.lollipop_graph(4, 6)\npathlengths = []\nprint(\"source vertex {target:length, }\")\nfor v in G.nodes():\n    spl = dict(nx.single_source_shortest_path_length(G, v))\n    print(f\"{v} {spl} \")\n    for p in spl:\n        pathlengths.append(spl[p])\nprint()\nprint(f\"average shortest path length {sum(pathlengths) / len(pathlengths)}\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.basic.plot_properties",
        "documentation": {}
    },
    {
        "label": "pathlengths",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.basic.plot_properties",
        "description": "env.share.doc.networkx-2.8.8.examples.basic.plot_properties",
        "peekOfCode": "pathlengths = []\nprint(\"source vertex {target:length, }\")\nfor v in G.nodes():\n    spl = dict(nx.single_source_shortest_path_length(G, v))\n    print(f\"{v} {spl} \")\n    for p in spl:\n        pathlengths.append(spl[p])\nprint()\nprint(f\"average shortest path length {sum(pathlengths) / len(pathlengths)}\")\n# histogram of path lengths",
        "detail": "env.share.doc.networkx-2.8.8.examples.basic.plot_properties",
        "documentation": {}
    },
    {
        "label": "dist",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.basic.plot_properties",
        "description": "env.share.doc.networkx-2.8.8.examples.basic.plot_properties",
        "peekOfCode": "dist = {}\nfor p in pathlengths:\n    if p in dist:\n        dist[p] += 1\n    else:\n        dist[p] = 1\nprint()\nprint(\"length #paths\")\nverts = dist.keys()\nfor d in sorted(verts):",
        "detail": "env.share.doc.networkx-2.8.8.examples.basic.plot_properties",
        "documentation": {}
    },
    {
        "label": "verts",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.basic.plot_properties",
        "description": "env.share.doc.networkx-2.8.8.examples.basic.plot_properties",
        "peekOfCode": "verts = dist.keys()\nfor d in sorted(verts):\n    print(f\"{d} {dist[d]}\")\nprint(f\"radius: {nx.radius(G)}\")\nprint(f\"diameter: {nx.diameter(G)}\")\nprint(f\"eccentricity: {nx.eccentricity(G)}\")\nprint(f\"center: {nx.center(G)}\")\nprint(f\"periphery: {nx.periphery(G)}\")\nprint(f\"density: {nx.density(G)}\")\npos = nx.spring_layout(G, seed=3068)  # Seed layout for reproducibility",
        "detail": "env.share.doc.networkx-2.8.8.examples.basic.plot_properties",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.basic.plot_properties",
        "description": "env.share.doc.networkx-2.8.8.examples.basic.plot_properties",
        "peekOfCode": "pos = nx.spring_layout(G, seed=3068)  # Seed layout for reproducibility\nnx.draw(G, pos=pos, with_labels=True)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.basic.plot_properties",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.basic.plot_read_write",
        "description": "env.share.doc.networkx-2.8.8.examples.basic.plot_read_write",
        "peekOfCode": "G = nx.grid_2d_graph(5, 5)  # 5x5 grid\n# print the adjacency list\nfor line in nx.generate_adjlist(G):\n    print(line)\n# write edgelist to grid.edgelist\nnx.write_edgelist(G, path=\"grid.edgelist\", delimiter=\":\")\n# read edgelist from grid.edgelist\nH = nx.read_edgelist(path=\"grid.edgelist\", delimiter=\":\")\npos = nx.spring_layout(H, seed=200)\nnx.draw(H, pos)",
        "detail": "env.share.doc.networkx-2.8.8.examples.basic.plot_read_write",
        "documentation": {}
    },
    {
        "label": "H",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.basic.plot_read_write",
        "description": "env.share.doc.networkx-2.8.8.examples.basic.plot_read_write",
        "peekOfCode": "H = nx.read_edgelist(path=\"grid.edgelist\", delimiter=\":\")\npos = nx.spring_layout(H, seed=200)\nnx.draw(H, pos)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.basic.plot_read_write",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.basic.plot_read_write",
        "description": "env.share.doc.networkx-2.8.8.examples.basic.plot_read_write",
        "peekOfCode": "pos = nx.spring_layout(H, seed=200)\nnx.draw(H, pos)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.basic.plot_read_write",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "peekOfCode": "G = nx.Graph()\nG.add_edge(1, 2)\nG.add_edge(1, 3)\nG.add_edge(1, 5)\nG.add_edge(2, 3)\nG.add_edge(3, 4)\nG.add_edge(4, 5)\n# explicitly set positions\npos = {1: (0, 0), 2: (-1, 0.3), 3: (2, 0.17), 4: (4, 0.255), 5: (5, 0.03)}\noptions = {",
        "detail": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "peekOfCode": "pos = {1: (0, 0), 2: (-1, 0.3), 3: (2, 0.17), 4: (4, 0.255), 5: (5, 0.03)}\noptions = {\n    \"font_size\": 36,\n    \"node_size\": 3000,\n    \"node_color\": \"white\",\n    \"edgecolors\": \"black\",\n    \"linewidths\": 5,\n    \"width\": 5,\n}\nnx.draw_networkx(G, pos, **options)",
        "detail": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "documentation": {}
    },
    {
        "label": "options",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "peekOfCode": "options = {\n    \"font_size\": 36,\n    \"node_size\": 3000,\n    \"node_color\": \"white\",\n    \"edgecolors\": \"black\",\n    \"linewidths\": 5,\n    \"width\": 5,\n}\nnx.draw_networkx(G, pos, **options)\n# Set margins for the axes so that nodes aren't clipped",
        "detail": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "documentation": {}
    },
    {
        "label": "ax",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "peekOfCode": "ax = plt.gca()\nax.margins(0.20)\nplt.axis(\"off\")\nplt.show()\n# %%\n# A directed graph\nG = nx.DiGraph([(0, 3), (1, 3), (2, 4), (3, 5), (3, 6), (4, 6), (5, 6)])\n# group nodes by column\nleft_nodes = [0, 1, 2]\nmiddle_nodes = [3, 4]",
        "detail": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "peekOfCode": "G = nx.DiGraph([(0, 3), (1, 3), (2, 4), (3, 5), (3, 6), (4, 6), (5, 6)])\n# group nodes by column\nleft_nodes = [0, 1, 2]\nmiddle_nodes = [3, 4]\nright_nodes = [5, 6]\n# set the position according to column (x-coord)\npos = {n: (0, i) for i, n in enumerate(left_nodes)}\npos.update({n: (1, i + 0.5) for i, n in enumerate(middle_nodes)})\npos.update({n: (2, i + 0.5) for i, n in enumerate(right_nodes)})\nnx.draw_networkx(G, pos, **options)",
        "detail": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "documentation": {}
    },
    {
        "label": "left_nodes",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "peekOfCode": "left_nodes = [0, 1, 2]\nmiddle_nodes = [3, 4]\nright_nodes = [5, 6]\n# set the position according to column (x-coord)\npos = {n: (0, i) for i, n in enumerate(left_nodes)}\npos.update({n: (1, i + 0.5) for i, n in enumerate(middle_nodes)})\npos.update({n: (2, i + 0.5) for i, n in enumerate(right_nodes)})\nnx.draw_networkx(G, pos, **options)\n# Set margins for the axes so that nodes aren't clipped\nax = plt.gca()",
        "detail": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "documentation": {}
    },
    {
        "label": "middle_nodes",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "peekOfCode": "middle_nodes = [3, 4]\nright_nodes = [5, 6]\n# set the position according to column (x-coord)\npos = {n: (0, i) for i, n in enumerate(left_nodes)}\npos.update({n: (1, i + 0.5) for i, n in enumerate(middle_nodes)})\npos.update({n: (2, i + 0.5) for i, n in enumerate(right_nodes)})\nnx.draw_networkx(G, pos, **options)\n# Set margins for the axes so that nodes aren't clipped\nax = plt.gca()\nax.margins(0.20)",
        "detail": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "documentation": {}
    },
    {
        "label": "right_nodes",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "peekOfCode": "right_nodes = [5, 6]\n# set the position according to column (x-coord)\npos = {n: (0, i) for i, n in enumerate(left_nodes)}\npos.update({n: (1, i + 0.5) for i, n in enumerate(middle_nodes)})\npos.update({n: (2, i + 0.5) for i, n in enumerate(right_nodes)})\nnx.draw_networkx(G, pos, **options)\n# Set margins for the axes so that nodes aren't clipped\nax = plt.gca()\nax.margins(0.20)\nplt.axis(\"off\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "peekOfCode": "pos = {n: (0, i) for i, n in enumerate(left_nodes)}\npos.update({n: (1, i + 0.5) for i, n in enumerate(middle_nodes)})\npos.update({n: (2, i + 0.5) for i, n in enumerate(right_nodes)})\nnx.draw_networkx(G, pos, **options)\n# Set margins for the axes so that nodes aren't clipped\nax = plt.gca()\nax.margins(0.20)\nplt.axis(\"off\")\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "documentation": {}
    },
    {
        "label": "ax",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "peekOfCode": "ax = plt.gca()\nax.margins(0.20)\nplt.axis(\"off\")\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.basic.plot_simple_graph",
        "documentation": {}
    },
    {
        "label": "chess_pgn_graph",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "peekOfCode": "def chess_pgn_graph(pgn_file=\"chess_masters_WCC.pgn.bz2\"):\n    \"\"\"Read chess games in pgn format in pgn_file.\n    Filenames ending in .bz2 will be uncompressed.\n    Return the MultiDiGraph of players connected by a chess game.\n    Edges contain game data in a dict.\n    \"\"\"\n    import bz2\n    G = nx.MultiDiGraph()\n    game = {}\n    with bz2.BZ2File(pgn_file) as datafile:",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "documentation": {}
    },
    {
        "label": "game_details",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "peekOfCode": "game_details = [\"Event\", \"Date\", \"Result\", \"ECO\", \"Site\"]\ndef chess_pgn_graph(pgn_file=\"chess_masters_WCC.pgn.bz2\"):\n    \"\"\"Read chess games in pgn format in pgn_file.\n    Filenames ending in .bz2 will be uncompressed.\n    Return the MultiDiGraph of players connected by a chess game.\n    Edges contain game data in a dict.\n    \"\"\"\n    import bz2\n    G = nx.MultiDiGraph()\n    game = {}",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "peekOfCode": "G = chess_pgn_graph()\nprint(\n    f\"Loaded {G.number_of_edges()} chess games between {G.number_of_nodes()} players\\n\"\n)\n# identify connected components of the undirected version\nH = G.to_undirected()\nGcc = [H.subgraph(c) for c in nx.connected_components(H)]\nif len(Gcc) > 1:\n    print(f\"Note the disconnected component consisting of:\\n{Gcc[1].nodes()}\")\n# find all games with B97 opening (as described in ECO)",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "documentation": {}
    },
    {
        "label": "H",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "peekOfCode": "H = G.to_undirected()\nGcc = [H.subgraph(c) for c in nx.connected_components(H)]\nif len(Gcc) > 1:\n    print(f\"Note the disconnected component consisting of:\\n{Gcc[1].nodes()}\")\n# find all games with B97 opening (as described in ECO)\nopenings = {game_info[\"ECO\"] for (white, black, game_info) in G.edges(data=True)}\nprint(f\"\\nFrom a total of {len(openings)} different openings,\")\nprint(\"the following games used the Sicilian opening\")\nprint('with the Najdorff 7...Qb6 \"Poisoned Pawn\" variation.\\n')\nfor (white, black, game_info) in G.edges(data=True):",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "documentation": {}
    },
    {
        "label": "Gcc",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "peekOfCode": "Gcc = [H.subgraph(c) for c in nx.connected_components(H)]\nif len(Gcc) > 1:\n    print(f\"Note the disconnected component consisting of:\\n{Gcc[1].nodes()}\")\n# find all games with B97 opening (as described in ECO)\nopenings = {game_info[\"ECO\"] for (white, black, game_info) in G.edges(data=True)}\nprint(f\"\\nFrom a total of {len(openings)} different openings,\")\nprint(\"the following games used the Sicilian opening\")\nprint('with the Najdorff 7...Qb6 \"Poisoned Pawn\" variation.\\n')\nfor (white, black, game_info) in G.edges(data=True):\n    if game_info[\"ECO\"] == \"B97\":",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "documentation": {}
    },
    {
        "label": "openings",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "peekOfCode": "openings = {game_info[\"ECO\"] for (white, black, game_info) in G.edges(data=True)}\nprint(f\"\\nFrom a total of {len(openings)} different openings,\")\nprint(\"the following games used the Sicilian opening\")\nprint('with the Najdorff 7...Qb6 \"Poisoned Pawn\" variation.\\n')\nfor (white, black, game_info) in G.edges(data=True):\n    if game_info[\"ECO\"] == \"B97\":\n        summary = f\"{white} vs {black}\\n\"\n        for k, v in game_info.items():\n            summary += f\"   {k}: {v}\\n\"\n        summary += \"\\n\"",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "documentation": {}
    },
    {
        "label": "H",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "peekOfCode": "H = nx.Graph(G)\n# edge width is proportional number of games played\nedgewidth = [len(G.get_edge_data(u, v)) for u, v in H.edges()]\n# node size is proportional to number of games won\nwins = dict.fromkeys(G.nodes(), 0.0)\nfor (u, v, d) in G.edges(data=True):\n    r = d[\"Result\"].split(\"-\")\n    if r[0] == \"1\":\n        wins[u] += 1.0\n    elif r[0] == \"1/2\":",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "documentation": {}
    },
    {
        "label": "edgewidth",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "peekOfCode": "edgewidth = [len(G.get_edge_data(u, v)) for u, v in H.edges()]\n# node size is proportional to number of games won\nwins = dict.fromkeys(G.nodes(), 0.0)\nfor (u, v, d) in G.edges(data=True):\n    r = d[\"Result\"].split(\"-\")\n    if r[0] == \"1\":\n        wins[u] += 1.0\n    elif r[0] == \"1/2\":\n        wins[u] += 0.5\n        wins[v] += 0.5",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "documentation": {}
    },
    {
        "label": "wins",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "peekOfCode": "wins = dict.fromkeys(G.nodes(), 0.0)\nfor (u, v, d) in G.edges(data=True):\n    r = d[\"Result\"].split(\"-\")\n    if r[0] == \"1\":\n        wins[u] += 1.0\n    elif r[0] == \"1/2\":\n        wins[u] += 0.5\n        wins[v] += 0.5\n    else:\n        wins[v] += 1.0",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "documentation": {}
    },
    {
        "label": "nodesize",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "peekOfCode": "nodesize = [wins[v] * 50 for v in H]\n# Generate layout for visualization\npos = nx.kamada_kawai_layout(H)\n# Manual tweaking to limit node label overlap in the visualization\npos[\"Reshevsky, Samuel H\"] += (0.05, -0.10)\npos[\"Botvinnik, Mikhail M\"] += (0.03, -0.06)\npos[\"Smyslov, Vassily V\"] += (0.05, -0.03)\nfig, ax = plt.subplots(figsize=(12, 12))\n# Visualize graph components\nnx.draw_networkx_edges(H, pos, alpha=0.3, width=edgewidth, edge_color=\"m\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "peekOfCode": "pos = nx.kamada_kawai_layout(H)\n# Manual tweaking to limit node label overlap in the visualization\npos[\"Reshevsky, Samuel H\"] += (0.05, -0.10)\npos[\"Botvinnik, Mikhail M\"] += (0.03, -0.06)\npos[\"Smyslov, Vassily V\"] += (0.05, -0.03)\nfig, ax = plt.subplots(figsize=(12, 12))\n# Visualize graph components\nnx.draw_networkx_edges(H, pos, alpha=0.3, width=edgewidth, edge_color=\"m\")\nnx.draw_networkx_nodes(H, pos, node_size=nodesize, node_color=\"#210070\", alpha=0.9)\nlabel_options = {\"ec\": \"k\", \"fc\": \"white\", \"alpha\": 0.7}",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "documentation": {}
    },
    {
        "label": "label_options",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "peekOfCode": "label_options = {\"ec\": \"k\", \"fc\": \"white\", \"alpha\": 0.7}\nnx.draw_networkx_labels(H, pos, font_size=14, bbox=label_options)\n# Title/legend\nfont = {\"fontname\": \"Helvetica\", \"color\": \"k\", \"fontweight\": \"bold\", \"fontsize\": 14}\nax.set_title(\"World Chess Championship Games: 1886 - 1985\", font)\n# Change font color for legend\nfont[\"color\"] = \"r\"\nax.text(\n    0.80,\n    0.10,",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "documentation": {}
    },
    {
        "label": "font",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "peekOfCode": "font = {\"fontname\": \"Helvetica\", \"color\": \"k\", \"fontweight\": \"bold\", \"fontsize\": 14}\nax.set_title(\"World Chess Championship Games: 1886 - 1985\", font)\n# Change font color for legend\nfont[\"color\"] = \"r\"\nax.text(\n    0.80,\n    0.10,\n    \"edge width = # games played\",\n    horizontalalignment=\"center\",\n    transform=ax.transAxes,",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "documentation": {}
    },
    {
        "label": "font[\"color\"]",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "peekOfCode": "font[\"color\"] = \"r\"\nax.text(\n    0.80,\n    0.10,\n    \"edge width = # games played\",\n    horizontalalignment=\"center\",\n    transform=ax.transAxes,\n    fontdict=font,\n)\nax.text(",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_chess_masters",
        "documentation": {}
    },
    {
        "label": "icons",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_custom_node_icons",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_custom_node_icons",
        "peekOfCode": "icons = {\n    \"router\": \"icons/router_black_144x144.png\",\n    \"switch\": \"icons/switch_black_144x144.png\",\n    \"PC\": \"icons/computer_black_144x144.png\",\n}\n# Load images\nimages = {k: PIL.Image.open(fname) for k, fname in icons.items()}\n# Generate the computer network graph\nG = nx.Graph()\nG.add_node(\"router\", image=images[\"router\"])",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_custom_node_icons",
        "documentation": {}
    },
    {
        "label": "images",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_custom_node_icons",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_custom_node_icons",
        "peekOfCode": "images = {k: PIL.Image.open(fname) for k, fname in icons.items()}\n# Generate the computer network graph\nG = nx.Graph()\nG.add_node(\"router\", image=images[\"router\"])\nfor i in range(1, 4):\n    G.add_node(f\"switch_{i}\", image=images[\"switch\"])\n    for j in range(1, 4):\n        G.add_node(\"PC_\" + str(i) + \"_\" + str(j), image=images[\"PC\"])\nG.add_edge(\"router\", \"switch_1\")\nG.add_edge(\"router\", \"switch_2\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_custom_node_icons",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_custom_node_icons",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_custom_node_icons",
        "peekOfCode": "G = nx.Graph()\nG.add_node(\"router\", image=images[\"router\"])\nfor i in range(1, 4):\n    G.add_node(f\"switch_{i}\", image=images[\"switch\"])\n    for j in range(1, 4):\n        G.add_node(\"PC_\" + str(i) + \"_\" + str(j), image=images[\"PC\"])\nG.add_edge(\"router\", \"switch_1\")\nG.add_edge(\"router\", \"switch_2\")\nG.add_edge(\"router\", \"switch_3\")\nfor u in range(1, 4):",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_custom_node_icons",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_custom_node_icons",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_custom_node_icons",
        "peekOfCode": "pos = nx.spring_layout(G, seed=1734289230)\nfig, ax = plt.subplots()\n# Note: the min_source/target_margin kwargs only work with FancyArrowPatch objects.\n# Force the use of FancyArrowPatch for edge drawing by setting `arrows=True`,\n# but suppress arrowheads with `arrowstyle=\"-\"`\nnx.draw_networkx_edges(\n    G,\n    pos=pos,\n    ax=ax,\n    arrows=True,",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_custom_node_icons",
        "documentation": {}
    },
    {
        "label": "tr_figure",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_custom_node_icons",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_custom_node_icons",
        "peekOfCode": "tr_figure = ax.transData.transform\n# Transform from display to figure coordinates\ntr_axes = fig.transFigure.inverted().transform\n# Select the size of the image (relative to the X axis)\nicon_size = (ax.get_xlim()[1] - ax.get_xlim()[0]) * 0.025\nicon_center = icon_size / 2.0\n# Add the respective image to each node\nfor n in G.nodes:\n    xf, yf = tr_figure(pos[n])\n    xa, ya = tr_axes((xf, yf))",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_custom_node_icons",
        "documentation": {}
    },
    {
        "label": "tr_axes",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_custom_node_icons",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_custom_node_icons",
        "peekOfCode": "tr_axes = fig.transFigure.inverted().transform\n# Select the size of the image (relative to the X axis)\nicon_size = (ax.get_xlim()[1] - ax.get_xlim()[0]) * 0.025\nicon_center = icon_size / 2.0\n# Add the respective image to each node\nfor n in G.nodes:\n    xf, yf = tr_figure(pos[n])\n    xa, ya = tr_axes((xf, yf))\n    # get overlapped axes and plot icon\n    a = plt.axes([xa - icon_center, ya - icon_center, icon_size, icon_size])",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_custom_node_icons",
        "documentation": {}
    },
    {
        "label": "icon_size",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_custom_node_icons",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_custom_node_icons",
        "peekOfCode": "icon_size = (ax.get_xlim()[1] - ax.get_xlim()[0]) * 0.025\nicon_center = icon_size / 2.0\n# Add the respective image to each node\nfor n in G.nodes:\n    xf, yf = tr_figure(pos[n])\n    xa, ya = tr_axes((xf, yf))\n    # get overlapped axes and plot icon\n    a = plt.axes([xa - icon_center, ya - icon_center, icon_size, icon_size])\n    a.imshow(G.nodes[n][\"image\"])\n    a.axis(\"off\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_custom_node_icons",
        "documentation": {}
    },
    {
        "label": "icon_center",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_custom_node_icons",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_custom_node_icons",
        "peekOfCode": "icon_center = icon_size / 2.0\n# Add the respective image to each node\nfor n in G.nodes:\n    xf, yf = tr_figure(pos[n])\n    xa, ya = tr_axes((xf, yf))\n    # get overlapped axes and plot icon\n    a = plt.axes([xa - icon_center, ya - icon_center, icon_size, icon_size])\n    a.imshow(G.nodes[n][\"image\"])\n    a.axis(\"off\")\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_custom_node_icons",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "peekOfCode": "G = nx.gnp_random_graph(100, 0.02, seed=10374196)\ndegree_sequence = sorted((d for n, d in G.degree()), reverse=True)\ndmax = max(degree_sequence)\nfig = plt.figure(\"Degree of a random graph\", figsize=(8, 8))\n# Create a gridspec for adding subplots of different sizes\naxgrid = fig.add_gridspec(5, 4)\nax0 = fig.add_subplot(axgrid[0:3, :])\nGcc = G.subgraph(sorted(nx.connected_components(G), key=len, reverse=True)[0])\npos = nx.spring_layout(Gcc, seed=10396953)\nnx.draw_networkx_nodes(Gcc, pos, ax=ax0, node_size=20)",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "documentation": {}
    },
    {
        "label": "degree_sequence",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "peekOfCode": "degree_sequence = sorted((d for n, d in G.degree()), reverse=True)\ndmax = max(degree_sequence)\nfig = plt.figure(\"Degree of a random graph\", figsize=(8, 8))\n# Create a gridspec for adding subplots of different sizes\naxgrid = fig.add_gridspec(5, 4)\nax0 = fig.add_subplot(axgrid[0:3, :])\nGcc = G.subgraph(sorted(nx.connected_components(G), key=len, reverse=True)[0])\npos = nx.spring_layout(Gcc, seed=10396953)\nnx.draw_networkx_nodes(Gcc, pos, ax=ax0, node_size=20)\nnx.draw_networkx_edges(Gcc, pos, ax=ax0, alpha=0.4)",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "documentation": {}
    },
    {
        "label": "dmax",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "peekOfCode": "dmax = max(degree_sequence)\nfig = plt.figure(\"Degree of a random graph\", figsize=(8, 8))\n# Create a gridspec for adding subplots of different sizes\naxgrid = fig.add_gridspec(5, 4)\nax0 = fig.add_subplot(axgrid[0:3, :])\nGcc = G.subgraph(sorted(nx.connected_components(G), key=len, reverse=True)[0])\npos = nx.spring_layout(Gcc, seed=10396953)\nnx.draw_networkx_nodes(Gcc, pos, ax=ax0, node_size=20)\nnx.draw_networkx_edges(Gcc, pos, ax=ax0, alpha=0.4)\nax0.set_title(\"Connected components of G\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "documentation": {}
    },
    {
        "label": "fig",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "peekOfCode": "fig = plt.figure(\"Degree of a random graph\", figsize=(8, 8))\n# Create a gridspec for adding subplots of different sizes\naxgrid = fig.add_gridspec(5, 4)\nax0 = fig.add_subplot(axgrid[0:3, :])\nGcc = G.subgraph(sorted(nx.connected_components(G), key=len, reverse=True)[0])\npos = nx.spring_layout(Gcc, seed=10396953)\nnx.draw_networkx_nodes(Gcc, pos, ax=ax0, node_size=20)\nnx.draw_networkx_edges(Gcc, pos, ax=ax0, alpha=0.4)\nax0.set_title(\"Connected components of G\")\nax0.set_axis_off()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "documentation": {}
    },
    {
        "label": "axgrid",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "peekOfCode": "axgrid = fig.add_gridspec(5, 4)\nax0 = fig.add_subplot(axgrid[0:3, :])\nGcc = G.subgraph(sorted(nx.connected_components(G), key=len, reverse=True)[0])\npos = nx.spring_layout(Gcc, seed=10396953)\nnx.draw_networkx_nodes(Gcc, pos, ax=ax0, node_size=20)\nnx.draw_networkx_edges(Gcc, pos, ax=ax0, alpha=0.4)\nax0.set_title(\"Connected components of G\")\nax0.set_axis_off()\nax1 = fig.add_subplot(axgrid[3:, :2])\nax1.plot(degree_sequence, \"b-\", marker=\"o\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "documentation": {}
    },
    {
        "label": "ax0",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "peekOfCode": "ax0 = fig.add_subplot(axgrid[0:3, :])\nGcc = G.subgraph(sorted(nx.connected_components(G), key=len, reverse=True)[0])\npos = nx.spring_layout(Gcc, seed=10396953)\nnx.draw_networkx_nodes(Gcc, pos, ax=ax0, node_size=20)\nnx.draw_networkx_edges(Gcc, pos, ax=ax0, alpha=0.4)\nax0.set_title(\"Connected components of G\")\nax0.set_axis_off()\nax1 = fig.add_subplot(axgrid[3:, :2])\nax1.plot(degree_sequence, \"b-\", marker=\"o\")\nax1.set_title(\"Degree Rank Plot\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "documentation": {}
    },
    {
        "label": "Gcc",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "peekOfCode": "Gcc = G.subgraph(sorted(nx.connected_components(G), key=len, reverse=True)[0])\npos = nx.spring_layout(Gcc, seed=10396953)\nnx.draw_networkx_nodes(Gcc, pos, ax=ax0, node_size=20)\nnx.draw_networkx_edges(Gcc, pos, ax=ax0, alpha=0.4)\nax0.set_title(\"Connected components of G\")\nax0.set_axis_off()\nax1 = fig.add_subplot(axgrid[3:, :2])\nax1.plot(degree_sequence, \"b-\", marker=\"o\")\nax1.set_title(\"Degree Rank Plot\")\nax1.set_ylabel(\"Degree\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "peekOfCode": "pos = nx.spring_layout(Gcc, seed=10396953)\nnx.draw_networkx_nodes(Gcc, pos, ax=ax0, node_size=20)\nnx.draw_networkx_edges(Gcc, pos, ax=ax0, alpha=0.4)\nax0.set_title(\"Connected components of G\")\nax0.set_axis_off()\nax1 = fig.add_subplot(axgrid[3:, :2])\nax1.plot(degree_sequence, \"b-\", marker=\"o\")\nax1.set_title(\"Degree Rank Plot\")\nax1.set_ylabel(\"Degree\")\nax1.set_xlabel(\"Rank\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "documentation": {}
    },
    {
        "label": "ax1",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "peekOfCode": "ax1 = fig.add_subplot(axgrid[3:, :2])\nax1.plot(degree_sequence, \"b-\", marker=\"o\")\nax1.set_title(\"Degree Rank Plot\")\nax1.set_ylabel(\"Degree\")\nax1.set_xlabel(\"Rank\")\nax2 = fig.add_subplot(axgrid[3:, 2:])\nax2.bar(*np.unique(degree_sequence, return_counts=True))\nax2.set_title(\"Degree histogram\")\nax2.set_xlabel(\"Degree\")\nax2.set_ylabel(\"# of Nodes\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "documentation": {}
    },
    {
        "label": "ax2",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "peekOfCode": "ax2 = fig.add_subplot(axgrid[3:, 2:])\nax2.bar(*np.unique(degree_sequence, return_counts=True))\nax2.set_title(\"Degree histogram\")\nax2.set_xlabel(\"Degree\")\nax2.set_ylabel(\"# of Nodes\")\nfig.tight_layout()\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_degree",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "peekOfCode": "seed = 13648  # Seed random number generators for reproducibility\nG = nx.random_k_out_graph(10, 3, 0.5, seed=seed)\npos = nx.spring_layout(G, seed=seed)\nnode_sizes = [3 + 10 * i for i in range(len(G))]\nM = G.number_of_edges()\nedge_colors = range(2, M + 2)\nedge_alphas = [(5 + i) / (M + 4) for i in range(M)]\ncmap = plt.cm.plasma\nnodes = nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=\"indigo\")\nedges = nx.draw_networkx_edges(",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "peekOfCode": "G = nx.random_k_out_graph(10, 3, 0.5, seed=seed)\npos = nx.spring_layout(G, seed=seed)\nnode_sizes = [3 + 10 * i for i in range(len(G))]\nM = G.number_of_edges()\nedge_colors = range(2, M + 2)\nedge_alphas = [(5 + i) / (M + 4) for i in range(M)]\ncmap = plt.cm.plasma\nnodes = nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=\"indigo\")\nedges = nx.draw_networkx_edges(\n    G,",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "peekOfCode": "pos = nx.spring_layout(G, seed=seed)\nnode_sizes = [3 + 10 * i for i in range(len(G))]\nM = G.number_of_edges()\nedge_colors = range(2, M + 2)\nedge_alphas = [(5 + i) / (M + 4) for i in range(M)]\ncmap = plt.cm.plasma\nnodes = nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=\"indigo\")\nedges = nx.draw_networkx_edges(\n    G,\n    pos,",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "documentation": {}
    },
    {
        "label": "node_sizes",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "peekOfCode": "node_sizes = [3 + 10 * i for i in range(len(G))]\nM = G.number_of_edges()\nedge_colors = range(2, M + 2)\nedge_alphas = [(5 + i) / (M + 4) for i in range(M)]\ncmap = plt.cm.plasma\nnodes = nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=\"indigo\")\nedges = nx.draw_networkx_edges(\n    G,\n    pos,\n    node_size=node_sizes,",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "documentation": {}
    },
    {
        "label": "M",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "peekOfCode": "M = G.number_of_edges()\nedge_colors = range(2, M + 2)\nedge_alphas = [(5 + i) / (M + 4) for i in range(M)]\ncmap = plt.cm.plasma\nnodes = nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=\"indigo\")\nedges = nx.draw_networkx_edges(\n    G,\n    pos,\n    node_size=node_sizes,\n    arrowstyle=\"->\",",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "documentation": {}
    },
    {
        "label": "edge_colors",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "peekOfCode": "edge_colors = range(2, M + 2)\nedge_alphas = [(5 + i) / (M + 4) for i in range(M)]\ncmap = plt.cm.plasma\nnodes = nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=\"indigo\")\nedges = nx.draw_networkx_edges(\n    G,\n    pos,\n    node_size=node_sizes,\n    arrowstyle=\"->\",\n    arrowsize=10,",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "documentation": {}
    },
    {
        "label": "edge_alphas",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "peekOfCode": "edge_alphas = [(5 + i) / (M + 4) for i in range(M)]\ncmap = plt.cm.plasma\nnodes = nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=\"indigo\")\nedges = nx.draw_networkx_edges(\n    G,\n    pos,\n    node_size=node_sizes,\n    arrowstyle=\"->\",\n    arrowsize=10,\n    edge_color=edge_colors,",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "documentation": {}
    },
    {
        "label": "cmap",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "peekOfCode": "cmap = plt.cm.plasma\nnodes = nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=\"indigo\")\nedges = nx.draw_networkx_edges(\n    G,\n    pos,\n    node_size=node_sizes,\n    arrowstyle=\"->\",\n    arrowsize=10,\n    edge_color=edge_colors,\n    edge_cmap=cmap,",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "documentation": {}
    },
    {
        "label": "nodes",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "peekOfCode": "nodes = nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=\"indigo\")\nedges = nx.draw_networkx_edges(\n    G,\n    pos,\n    node_size=node_sizes,\n    arrowstyle=\"->\",\n    arrowsize=10,\n    edge_color=edge_colors,\n    edge_cmap=cmap,\n    width=2,",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "documentation": {}
    },
    {
        "label": "edges",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "peekOfCode": "edges = nx.draw_networkx_edges(\n    G,\n    pos,\n    node_size=node_sizes,\n    arrowstyle=\"->\",\n    arrowsize=10,\n    edge_color=edge_colors,\n    edge_cmap=cmap,\n    width=2,\n)",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "documentation": {}
    },
    {
        "label": "pc",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "peekOfCode": "pc = mpl.collections.PatchCollection(edges, cmap=cmap)\npc.set_array(edge_colors)\nax = plt.gca()\nax.set_axis_off()\nplt.colorbar(pc, ax=ax)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "documentation": {}
    },
    {
        "label": "ax",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "peekOfCode": "ax = plt.gca()\nax.set_axis_off()\nplt.colorbar(pc, ax=ax)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_directed",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_edge_colormap",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_edge_colormap",
        "peekOfCode": "G = nx.star_graph(20)\npos = nx.spring_layout(G, seed=63)  # Seed layout for reproducibility\ncolors = range(20)\noptions = {\n    \"node_color\": \"#A0CBE2\",\n    \"edge_color\": colors,\n    \"width\": 4,\n    \"edge_cmap\": plt.cm.Blues,\n    \"with_labels\": False,\n}",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_edge_colormap",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_edge_colormap",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_edge_colormap",
        "peekOfCode": "pos = nx.spring_layout(G, seed=63)  # Seed layout for reproducibility\ncolors = range(20)\noptions = {\n    \"node_color\": \"#A0CBE2\",\n    \"edge_color\": colors,\n    \"width\": 4,\n    \"edge_cmap\": plt.cm.Blues,\n    \"with_labels\": False,\n}\nnx.draw(G, pos, **options)",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_edge_colormap",
        "documentation": {}
    },
    {
        "label": "colors",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_edge_colormap",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_edge_colormap",
        "peekOfCode": "colors = range(20)\noptions = {\n    \"node_color\": \"#A0CBE2\",\n    \"edge_color\": colors,\n    \"width\": 4,\n    \"edge_cmap\": plt.cm.Blues,\n    \"with_labels\": False,\n}\nnx.draw(G, pos, **options)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_edge_colormap",
        "documentation": {}
    },
    {
        "label": "options",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_edge_colormap",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_edge_colormap",
        "peekOfCode": "options = {\n    \"node_color\": \"#A0CBE2\",\n    \"edge_color\": colors,\n    \"width\": 4,\n    \"edge_cmap\": plt.cm.Blues,\n    \"with_labels\": False,\n}\nnx.draw(G, pos, **options)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_edge_colormap",
        "documentation": {}
    },
    {
        "label": "n",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_ego_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_ego_graph",
        "peekOfCode": "n = 1000\nm = 2\nseed = 20532\nG = nx.barabasi_albert_graph(n, m, seed=seed)\n# find node with largest degree\nnode_and_degree = G.degree()\n(largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n# Create ego graph of main hub\nhub_ego = nx.ego_graph(G, largest_hub)\n# Draw graph",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_ego_graph",
        "documentation": {}
    },
    {
        "label": "m",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_ego_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_ego_graph",
        "peekOfCode": "m = 2\nseed = 20532\nG = nx.barabasi_albert_graph(n, m, seed=seed)\n# find node with largest degree\nnode_and_degree = G.degree()\n(largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n# Create ego graph of main hub\nhub_ego = nx.ego_graph(G, largest_hub)\n# Draw graph\npos = nx.spring_layout(hub_ego, seed=seed)  # Seed layout for reproducibility",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_ego_graph",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_ego_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_ego_graph",
        "peekOfCode": "seed = 20532\nG = nx.barabasi_albert_graph(n, m, seed=seed)\n# find node with largest degree\nnode_and_degree = G.degree()\n(largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n# Create ego graph of main hub\nhub_ego = nx.ego_graph(G, largest_hub)\n# Draw graph\npos = nx.spring_layout(hub_ego, seed=seed)  # Seed layout for reproducibility\nnx.draw(hub_ego, pos, node_color=\"b\", node_size=50, with_labels=False)",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_ego_graph",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_ego_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_ego_graph",
        "peekOfCode": "G = nx.barabasi_albert_graph(n, m, seed=seed)\n# find node with largest degree\nnode_and_degree = G.degree()\n(largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n# Create ego graph of main hub\nhub_ego = nx.ego_graph(G, largest_hub)\n# Draw graph\npos = nx.spring_layout(hub_ego, seed=seed)  # Seed layout for reproducibility\nnx.draw(hub_ego, pos, node_color=\"b\", node_size=50, with_labels=False)\n# Draw ego as large and red",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_ego_graph",
        "documentation": {}
    },
    {
        "label": "node_and_degree",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_ego_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_ego_graph",
        "peekOfCode": "node_and_degree = G.degree()\n(largest_hub, degree) = sorted(node_and_degree, key=itemgetter(1))[-1]\n# Create ego graph of main hub\nhub_ego = nx.ego_graph(G, largest_hub)\n# Draw graph\npos = nx.spring_layout(hub_ego, seed=seed)  # Seed layout for reproducibility\nnx.draw(hub_ego, pos, node_color=\"b\", node_size=50, with_labels=False)\n# Draw ego as large and red\noptions = {\"node_size\": 300, \"node_color\": \"r\"}\nnx.draw_networkx_nodes(hub_ego, pos, nodelist=[largest_hub], **options)",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_ego_graph",
        "documentation": {}
    },
    {
        "label": "hub_ego",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_ego_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_ego_graph",
        "peekOfCode": "hub_ego = nx.ego_graph(G, largest_hub)\n# Draw graph\npos = nx.spring_layout(hub_ego, seed=seed)  # Seed layout for reproducibility\nnx.draw(hub_ego, pos, node_color=\"b\", node_size=50, with_labels=False)\n# Draw ego as large and red\noptions = {\"node_size\": 300, \"node_color\": \"r\"}\nnx.draw_networkx_nodes(hub_ego, pos, nodelist=[largest_hub], **options)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_ego_graph",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_ego_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_ego_graph",
        "peekOfCode": "pos = nx.spring_layout(hub_ego, seed=seed)  # Seed layout for reproducibility\nnx.draw(hub_ego, pos, node_color=\"b\", node_size=50, with_labels=False)\n# Draw ego as large and red\noptions = {\"node_size\": 300, \"node_color\": \"r\"}\nnx.draw_networkx_nodes(hub_ego, pos, nodelist=[largest_hub], **options)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_ego_graph",
        "documentation": {}
    },
    {
        "label": "options",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_ego_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_ego_graph",
        "peekOfCode": "options = {\"node_size\": 300, \"node_color\": \"r\"}\nnx.draw_networkx_nodes(hub_ego, pos, nodelist=[largest_hub], **options)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_ego_graph",
        "documentation": {}
    },
    {
        "label": "n",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_eigenvalues",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_eigenvalues",
        "peekOfCode": "n = 1000  # 1000 nodes\nm = 5000  # 5000 edges\nG = nx.gnm_random_graph(n, m, seed=5040)  # Seed for reproducibility\nL = nx.normalized_laplacian_matrix(G)\ne = numpy.linalg.eigvals(L.toarray())\nprint(\"Largest eigenvalue:\", max(e))\nprint(\"Smallest eigenvalue:\", min(e))\nplt.hist(e, bins=100)  # histogram with 100 bins\nplt.xlim(0, 2)  # eigenvalues between 0 and 2\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_eigenvalues",
        "documentation": {}
    },
    {
        "label": "m",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_eigenvalues",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_eigenvalues",
        "peekOfCode": "m = 5000  # 5000 edges\nG = nx.gnm_random_graph(n, m, seed=5040)  # Seed for reproducibility\nL = nx.normalized_laplacian_matrix(G)\ne = numpy.linalg.eigvals(L.toarray())\nprint(\"Largest eigenvalue:\", max(e))\nprint(\"Smallest eigenvalue:\", min(e))\nplt.hist(e, bins=100)  # histogram with 100 bins\nplt.xlim(0, 2)  # eigenvalues between 0 and 2\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_eigenvalues",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_eigenvalues",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_eigenvalues",
        "peekOfCode": "G = nx.gnm_random_graph(n, m, seed=5040)  # Seed for reproducibility\nL = nx.normalized_laplacian_matrix(G)\ne = numpy.linalg.eigvals(L.toarray())\nprint(\"Largest eigenvalue:\", max(e))\nprint(\"Smallest eigenvalue:\", min(e))\nplt.hist(e, bins=100)  # histogram with 100 bins\nplt.xlim(0, 2)  # eigenvalues between 0 and 2\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_eigenvalues",
        "documentation": {}
    },
    {
        "label": "L",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_eigenvalues",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_eigenvalues",
        "peekOfCode": "L = nx.normalized_laplacian_matrix(G)\ne = numpy.linalg.eigvals(L.toarray())\nprint(\"Largest eigenvalue:\", max(e))\nprint(\"Smallest eigenvalue:\", min(e))\nplt.hist(e, bins=100)  # histogram with 100 bins\nplt.xlim(0, 2)  # eigenvalues between 0 and 2\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_eigenvalues",
        "documentation": {}
    },
    {
        "label": "e",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_eigenvalues",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_eigenvalues",
        "peekOfCode": "e = numpy.linalg.eigvals(L.toarray())\nprint(\"Largest eigenvalue:\", max(e))\nprint(\"Smallest eigenvalue:\", min(e))\nplt.hist(e, bins=100)  # histogram with 100 bins\nplt.xlim(0, 2)  # eigenvalues between 0 and 2\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_eigenvalues",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_four_grids",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_four_grids",
        "peekOfCode": "G = nx.grid_2d_graph(4, 4)  # 4x4 grid\npos = nx.spring_layout(G, iterations=100, seed=39775)\n# Create a 2x2 subplot\nfig, all_axes = plt.subplots(2, 2)\nax = all_axes.flat\nnx.draw(G, pos, ax=ax[0], font_size=8)\nnx.draw(G, pos, ax=ax[1], node_size=0, with_labels=False)\nnx.draw(\n    G,\n    pos,",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_four_grids",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_four_grids",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_four_grids",
        "peekOfCode": "pos = nx.spring_layout(G, iterations=100, seed=39775)\n# Create a 2x2 subplot\nfig, all_axes = plt.subplots(2, 2)\nax = all_axes.flat\nnx.draw(G, pos, ax=ax[0], font_size=8)\nnx.draw(G, pos, ax=ax[1], node_size=0, with_labels=False)\nnx.draw(\n    G,\n    pos,\n    ax=ax[2],",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_four_grids",
        "documentation": {}
    },
    {
        "label": "ax",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_four_grids",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_four_grids",
        "peekOfCode": "ax = all_axes.flat\nnx.draw(G, pos, ax=ax[0], font_size=8)\nnx.draw(G, pos, ax=ax[1], node_size=0, with_labels=False)\nnx.draw(\n    G,\n    pos,\n    ax=ax[2],\n    node_color=\"tab:green\",\n    edgecolors=\"tab:gray\",  # Node surface color\n    edge_color=\"tab:gray\",  # Color of graph edges",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_four_grids",
        "documentation": {}
    },
    {
        "label": "H",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_four_grids",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_four_grids",
        "peekOfCode": "H = G.to_directed()\nnx.draw(\n    H,\n    pos,\n    ax=ax[3],\n    node_color=\"tab:orange\",\n    node_size=20,\n    with_labels=False,\n    arrowsize=10,\n    width=2,",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_four_grids",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_house_with_colors",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_house_with_colors",
        "peekOfCode": "G = nx.house_graph()\n# explicitly set positions\npos = {0: (0, 0), 1: (1, 0), 2: (0, 1), 3: (1, 1), 4: (0.5, 2.0)}\n# Plot nodes with different properties for the \"wall\" and \"roof\" nodes\nnx.draw_networkx_nodes(\n    G, pos, node_size=3000, nodelist=[0, 1, 2, 3], node_color=\"tab:blue\"\n)\nnx.draw_networkx_nodes(G, pos, node_size=2000, nodelist=[4], node_color=\"tab:orange\")\nnx.draw_networkx_edges(G, pos, alpha=0.5, width=6)\n# Customize axes",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_house_with_colors",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_house_with_colors",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_house_with_colors",
        "peekOfCode": "pos = {0: (0, 0), 1: (1, 0), 2: (0, 1), 3: (1, 1), 4: (0.5, 2.0)}\n# Plot nodes with different properties for the \"wall\" and \"roof\" nodes\nnx.draw_networkx_nodes(\n    G, pos, node_size=3000, nodelist=[0, 1, 2, 3], node_color=\"tab:blue\"\n)\nnx.draw_networkx_nodes(G, pos, node_size=2000, nodelist=[4], node_color=\"tab:orange\")\nnx.draw_networkx_edges(G, pos, alpha=0.5, width=6)\n# Customize axes\nax = plt.gca()\nax.margins(0.11)",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_house_with_colors",
        "documentation": {}
    },
    {
        "label": "ax",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_house_with_colors",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_house_with_colors",
        "peekOfCode": "ax = plt.gca()\nax.margins(0.11)\nplt.tight_layout()\nplt.axis(\"off\")\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_house_with_colors",
        "documentation": {}
    },
    {
        "label": "miles_graph",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_knuth_miles",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_knuth_miles",
        "peekOfCode": "def miles_graph():\n    \"\"\"Return the cites example graph in miles_dat.txt\n    from the Stanford GraphBase.\n    \"\"\"\n    # open file miles_dat.txt.gz (or miles_dat.txt)\n    fh = gzip.open(\"knuth_miles.txt.gz\", \"r\")\n    G = nx.Graph()\n    G.position = {}\n    G.population = {}\n    cities = []",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_knuth_miles",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_knuth_miles",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_knuth_miles",
        "peekOfCode": "G = miles_graph()\nprint(\"Loaded miles_dat.txt containing 128 cities.\")\nprint(G)\n# make new graph of cites, edge if less then 300 miles between them\nH = nx.Graph()\nfor v in G:\n    H.add_node(v)\nfor (u, v, d) in G.edges(data=True):\n    if d[\"weight\"] < 300:\n        H.add_edge(u, v)",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_knuth_miles",
        "documentation": {}
    },
    {
        "label": "H",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_knuth_miles",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_knuth_miles",
        "peekOfCode": "H = nx.Graph()\nfor v in G:\n    H.add_node(v)\nfor (u, v, d) in G.edges(data=True):\n    if d[\"weight\"] < 300:\n        H.add_edge(u, v)\n# draw with matplotlib/pylab\nfig = plt.figure(figsize=(8, 6))\n# nodes colored by degree sized by population\nnode_color = [float(H.degree(v)) for v in H]",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_knuth_miles",
        "documentation": {}
    },
    {
        "label": "fig",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_knuth_miles",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_knuth_miles",
        "peekOfCode": "fig = plt.figure(figsize=(8, 6))\n# nodes colored by degree sized by population\nnode_color = [float(H.degree(v)) for v in H]\n# Use cartopy to provide a backdrop for the visualization\ntry:\n    import cartopy.crs as ccrs\n    import cartopy.io.shapereader as shpreader\n    ax = fig.add_axes([0, 0, 1, 1], projection=ccrs.LambertConformal(), frameon=False)\n    ax.set_extent([-125, -66.5, 20, 50], ccrs.Geodetic())\n    # Add map of countries & US states as a backdrop",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_knuth_miles",
        "documentation": {}
    },
    {
        "label": "node_color",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_knuth_miles",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_knuth_miles",
        "peekOfCode": "node_color = [float(H.degree(v)) for v in H]\n# Use cartopy to provide a backdrop for the visualization\ntry:\n    import cartopy.crs as ccrs\n    import cartopy.io.shapereader as shpreader\n    ax = fig.add_axes([0, 0, 1, 1], projection=ccrs.LambertConformal(), frameon=False)\n    ax.set_extent([-125, -66.5, 20, 50], ccrs.Geodetic())\n    # Add map of countries & US states as a backdrop\n    for shapename in (\"admin_1_states_provinces_lakes_shp\", \"admin_0_countries\"):\n        shp = shpreader.natural_earth(",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_knuth_miles",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "peekOfCode": "G = nx.cubical_graph()\npos = nx.spring_layout(G, seed=3113794652)  # positions for all nodes\n# nodes\noptions = {\"edgecolors\": \"tab:gray\", \"node_size\": 800, \"alpha\": 0.9}\nnx.draw_networkx_nodes(G, pos, nodelist=[0, 1, 2, 3], node_color=\"tab:red\", **options)\nnx.draw_networkx_nodes(G, pos, nodelist=[4, 5, 6, 7], node_color=\"tab:blue\", **options)\n# edges\nnx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5)\nnx.draw_networkx_edges(\n    G,",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "peekOfCode": "pos = nx.spring_layout(G, seed=3113794652)  # positions for all nodes\n# nodes\noptions = {\"edgecolors\": \"tab:gray\", \"node_size\": 800, \"alpha\": 0.9}\nnx.draw_networkx_nodes(G, pos, nodelist=[0, 1, 2, 3], node_color=\"tab:red\", **options)\nnx.draw_networkx_nodes(G, pos, nodelist=[4, 5, 6, 7], node_color=\"tab:blue\", **options)\n# edges\nnx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5)\nnx.draw_networkx_edges(\n    G,\n    pos,",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "documentation": {}
    },
    {
        "label": "options",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "peekOfCode": "options = {\"edgecolors\": \"tab:gray\", \"node_size\": 800, \"alpha\": 0.9}\nnx.draw_networkx_nodes(G, pos, nodelist=[0, 1, 2, 3], node_color=\"tab:red\", **options)\nnx.draw_networkx_nodes(G, pos, nodelist=[4, 5, 6, 7], node_color=\"tab:blue\", **options)\n# edges\nnx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5)\nnx.draw_networkx_edges(\n    G,\n    pos,\n    edgelist=[(0, 1), (1, 2), (2, 3), (3, 0)],\n    width=8,",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "documentation": {}
    },
    {
        "label": "labels",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "peekOfCode": "labels = {}\nlabels[0] = r\"$a$\"\nlabels[1] = r\"$b$\"\nlabels[2] = r\"$c$\"\nlabels[3] = r\"$d$\"\nlabels[4] = r\"$\\alpha$\"\nlabels[5] = r\"$\\beta$\"\nlabels[6] = r\"$\\gamma$\"\nlabels[7] = r\"$\\delta$\"\nnx.draw_networkx_labels(G, pos, labels, font_size=22, font_color=\"whitesmoke\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "documentation": {}
    },
    {
        "label": "labels[0]",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "peekOfCode": "labels[0] = r\"$a$\"\nlabels[1] = r\"$b$\"\nlabels[2] = r\"$c$\"\nlabels[3] = r\"$d$\"\nlabels[4] = r\"$\\alpha$\"\nlabels[5] = r\"$\\beta$\"\nlabels[6] = r\"$\\gamma$\"\nlabels[7] = r\"$\\delta$\"\nnx.draw_networkx_labels(G, pos, labels, font_size=22, font_color=\"whitesmoke\")\nplt.tight_layout()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "documentation": {}
    },
    {
        "label": "labels[1]",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "peekOfCode": "labels[1] = r\"$b$\"\nlabels[2] = r\"$c$\"\nlabels[3] = r\"$d$\"\nlabels[4] = r\"$\\alpha$\"\nlabels[5] = r\"$\\beta$\"\nlabels[6] = r\"$\\gamma$\"\nlabels[7] = r\"$\\delta$\"\nnx.draw_networkx_labels(G, pos, labels, font_size=22, font_color=\"whitesmoke\")\nplt.tight_layout()\nplt.axis(\"off\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "documentation": {}
    },
    {
        "label": "labels[2]",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "peekOfCode": "labels[2] = r\"$c$\"\nlabels[3] = r\"$d$\"\nlabels[4] = r\"$\\alpha$\"\nlabels[5] = r\"$\\beta$\"\nlabels[6] = r\"$\\gamma$\"\nlabels[7] = r\"$\\delta$\"\nnx.draw_networkx_labels(G, pos, labels, font_size=22, font_color=\"whitesmoke\")\nplt.tight_layout()\nplt.axis(\"off\")\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "documentation": {}
    },
    {
        "label": "labels[3]",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "peekOfCode": "labels[3] = r\"$d$\"\nlabels[4] = r\"$\\alpha$\"\nlabels[5] = r\"$\\beta$\"\nlabels[6] = r\"$\\gamma$\"\nlabels[7] = r\"$\\delta$\"\nnx.draw_networkx_labels(G, pos, labels, font_size=22, font_color=\"whitesmoke\")\nplt.tight_layout()\nplt.axis(\"off\")\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "documentation": {}
    },
    {
        "label": "labels[4]",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "peekOfCode": "labels[4] = r\"$\\alpha$\"\nlabels[5] = r\"$\\beta$\"\nlabels[6] = r\"$\\gamma$\"\nlabels[7] = r\"$\\delta$\"\nnx.draw_networkx_labels(G, pos, labels, font_size=22, font_color=\"whitesmoke\")\nplt.tight_layout()\nplt.axis(\"off\")\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "documentation": {}
    },
    {
        "label": "labels[5]",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "peekOfCode": "labels[5] = r\"$\\beta$\"\nlabels[6] = r\"$\\gamma$\"\nlabels[7] = r\"$\\delta$\"\nnx.draw_networkx_labels(G, pos, labels, font_size=22, font_color=\"whitesmoke\")\nplt.tight_layout()\nplt.axis(\"off\")\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "documentation": {}
    },
    {
        "label": "labels[6]",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "peekOfCode": "labels[6] = r\"$\\gamma$\"\nlabels[7] = r\"$\\delta$\"\nnx.draw_networkx_labels(G, pos, labels, font_size=22, font_color=\"whitesmoke\")\nplt.tight_layout()\nplt.axis(\"off\")\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "documentation": {}
    },
    {
        "label": "labels[7]",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "peekOfCode": "labels[7] = r\"$\\delta$\"\nnx.draw_networkx_labels(G, pos, labels, font_size=22, font_color=\"whitesmoke\")\nplt.tight_layout()\nplt.axis(\"off\")\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_labels_and_colors",
        "documentation": {}
    },
    {
        "label": "multilayered_graph",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_multipartite_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_multipartite_graph",
        "peekOfCode": "def multilayered_graph(*subset_sizes):\n    extents = nx.utils.pairwise(itertools.accumulate((0,) + subset_sizes))\n    layers = [range(start, end) for start, end in extents]\n    G = nx.Graph()\n    for (i, layer) in enumerate(layers):\n        G.add_nodes_from(layer, layer=i)\n    for layer1, layer2 in nx.utils.pairwise(layers):\n        G.add_edges_from(itertools.product(layer1, layer2))\n    return G\nG = multilayered_graph(*subset_sizes)",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_multipartite_graph",
        "documentation": {}
    },
    {
        "label": "subset_sizes",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_multipartite_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_multipartite_graph",
        "peekOfCode": "subset_sizes = [5, 5, 4, 3, 2, 4, 4, 3]\nsubset_color = [\n    \"gold\",\n    \"violet\",\n    \"violet\",\n    \"violet\",\n    \"violet\",\n    \"limegreen\",\n    \"limegreen\",\n    \"darkorange\",",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_multipartite_graph",
        "documentation": {}
    },
    {
        "label": "subset_color",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_multipartite_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_multipartite_graph",
        "peekOfCode": "subset_color = [\n    \"gold\",\n    \"violet\",\n    \"violet\",\n    \"violet\",\n    \"violet\",\n    \"limegreen\",\n    \"limegreen\",\n    \"darkorange\",\n]",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_multipartite_graph",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_multipartite_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_multipartite_graph",
        "peekOfCode": "G = multilayered_graph(*subset_sizes)\ncolor = [subset_color[data[\"layer\"]] for v, data in G.nodes(data=True)]\npos = nx.multipartite_layout(G, subset_key=\"layer\")\nplt.figure(figsize=(8, 8))\nnx.draw(G, pos, node_color=color, with_labels=False)\nplt.axis(\"equal\")\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_multipartite_graph",
        "documentation": {}
    },
    {
        "label": "color",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_multipartite_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_multipartite_graph",
        "peekOfCode": "color = [subset_color[data[\"layer\"]] for v, data in G.nodes(data=True)]\npos = nx.multipartite_layout(G, subset_key=\"layer\")\nplt.figure(figsize=(8, 8))\nnx.draw(G, pos, node_color=color, with_labels=False)\nplt.axis(\"equal\")\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_multipartite_graph",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_multipartite_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_multipartite_graph",
        "peekOfCode": "pos = nx.multipartite_layout(G, subset_key=\"layer\")\nplt.figure(figsize=(8, 8))\nnx.draw(G, pos, node_color=color, with_labels=False)\nplt.axis(\"equal\")\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_multipartite_graph",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_node_colormap",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_node_colormap",
        "peekOfCode": "G = nx.cycle_graph(24)\npos = nx.circular_layout(G)\nnx.draw(G, pos, node_color=range(24), node_size=800, cmap=plt.cm.Blues)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_node_colormap",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_node_colormap",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_node_colormap",
        "peekOfCode": "pos = nx.circular_layout(G)\nnx.draw(G, pos, node_color=range(24), node_size=800, cmap=plt.cm.Blues)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_node_colormap",
        "documentation": {}
    },
    {
        "label": "cycle",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "peekOfCode": "def cycle(nlist, n):\n    return nlist[-n:] + nlist[:-n]\n# Rotate nodes around the circle and assign colors for each edge based on\n# node distance\nnodes = list(G.nodes())\nfor i, nd in enumerate(ndist_iter):\n    for u, v in zip(nodes, cycle(nodes, i + 1)):\n        G[u][v][\"color\"] = node_dist_to_color[nd]\npos = nx.circular_layout(G)\n# Create a figure with 1:1 aspect ratio to preserve the circle.",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "documentation": {}
    },
    {
        "label": "node_dist_to_color",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "peekOfCode": "node_dist_to_color = {\n    1: \"tab:red\",\n    2: \"tab:orange\",\n    3: \"tab:olive\",\n    4: \"tab:green\",\n    5: \"tab:blue\",\n    6: \"tab:purple\",\n}\n# Create a complete graph with an odd number of nodes\nnnodes = 13",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "documentation": {}
    },
    {
        "label": "nnodes",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "peekOfCode": "nnodes = 13\nG = nx.complete_graph(nnodes)\n# A graph with (2n + 1) nodes requires n colors for the edges\nn = (nnodes - 1) // 2\nndist_iter = list(range(1, n + 1))\n# Take advantage of circular symmetry in determining node distances\nndist_iter += ndist_iter[::-1]\ndef cycle(nlist, n):\n    return nlist[-n:] + nlist[:-n]\n# Rotate nodes around the circle and assign colors for each edge based on",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "peekOfCode": "G = nx.complete_graph(nnodes)\n# A graph with (2n + 1) nodes requires n colors for the edges\nn = (nnodes - 1) // 2\nndist_iter = list(range(1, n + 1))\n# Take advantage of circular symmetry in determining node distances\nndist_iter += ndist_iter[::-1]\ndef cycle(nlist, n):\n    return nlist[-n:] + nlist[:-n]\n# Rotate nodes around the circle and assign colors for each edge based on\n# node distance",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "documentation": {}
    },
    {
        "label": "n",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "peekOfCode": "n = (nnodes - 1) // 2\nndist_iter = list(range(1, n + 1))\n# Take advantage of circular symmetry in determining node distances\nndist_iter += ndist_iter[::-1]\ndef cycle(nlist, n):\n    return nlist[-n:] + nlist[:-n]\n# Rotate nodes around the circle and assign colors for each edge based on\n# node distance\nnodes = list(G.nodes())\nfor i, nd in enumerate(ndist_iter):",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "documentation": {}
    },
    {
        "label": "ndist_iter",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "peekOfCode": "ndist_iter = list(range(1, n + 1))\n# Take advantage of circular symmetry in determining node distances\nndist_iter += ndist_iter[::-1]\ndef cycle(nlist, n):\n    return nlist[-n:] + nlist[:-n]\n# Rotate nodes around the circle and assign colors for each edge based on\n# node distance\nnodes = list(G.nodes())\nfor i, nd in enumerate(ndist_iter):\n    for u, v in zip(nodes, cycle(nodes, i + 1)):",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "documentation": {}
    },
    {
        "label": "nodes",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "peekOfCode": "nodes = list(G.nodes())\nfor i, nd in enumerate(ndist_iter):\n    for u, v in zip(nodes, cycle(nodes, i + 1)):\n        G[u][v][\"color\"] = node_dist_to_color[nd]\npos = nx.circular_layout(G)\n# Create a figure with 1:1 aspect ratio to preserve the circle.\nfig, ax = plt.subplots(figsize=(8, 8))\nnode_opts = {\"node_size\": 500, \"node_color\": \"w\", \"edgecolors\": \"k\", \"linewidths\": 2.0}\nnx.draw_networkx_nodes(G, pos, **node_opts)\nnx.draw_networkx_labels(G, pos, font_size=14)",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "peekOfCode": "pos = nx.circular_layout(G)\n# Create a figure with 1:1 aspect ratio to preserve the circle.\nfig, ax = plt.subplots(figsize=(8, 8))\nnode_opts = {\"node_size\": 500, \"node_color\": \"w\", \"edgecolors\": \"k\", \"linewidths\": 2.0}\nnx.draw_networkx_nodes(G, pos, **node_opts)\nnx.draw_networkx_labels(G, pos, font_size=14)\n# Extract color from edge data\nedge_colors = [edgedata[\"color\"] for _, _, edgedata in G.edges(data=True)]\nnx.draw_networkx_edges(G, pos, width=2.0, edge_color=edge_colors)\nax.set_axis_off()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "documentation": {}
    },
    {
        "label": "node_opts",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "peekOfCode": "node_opts = {\"node_size\": 500, \"node_color\": \"w\", \"edgecolors\": \"k\", \"linewidths\": 2.0}\nnx.draw_networkx_nodes(G, pos, **node_opts)\nnx.draw_networkx_labels(G, pos, font_size=14)\n# Extract color from edge data\nedge_colors = [edgedata[\"color\"] for _, _, edgedata in G.edges(data=True)]\nnx.draw_networkx_edges(G, pos, width=2.0, edge_color=edge_colors)\nax.set_axis_off()\nfig.tight_layout()\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "documentation": {}
    },
    {
        "label": "edge_colors",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "peekOfCode": "edge_colors = [edgedata[\"color\"] for _, _, edgedata in G.edges(data=True)]\nnx.draw_networkx_edges(G, pos, width=2.0, edge_color=edge_colors)\nax.set_axis_off()\nfig.tight_layout()\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_rainbow_coloring",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_random_geometric_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_random_geometric_graph",
        "peekOfCode": "G = nx.random_geometric_graph(200, 0.125, seed=896803)\n# position is stored as node attribute data for random_geometric_graph\npos = nx.get_node_attributes(G, \"pos\")\n# find node near center (0.5,0.5)\ndmin = 1\nncenter = 0\nfor n in pos:\n    x, y = pos[n]\n    d = (x - 0.5) ** 2 + (y - 0.5) ** 2\n    if d < dmin:",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_random_geometric_graph",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_random_geometric_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_random_geometric_graph",
        "peekOfCode": "pos = nx.get_node_attributes(G, \"pos\")\n# find node near center (0.5,0.5)\ndmin = 1\nncenter = 0\nfor n in pos:\n    x, y = pos[n]\n    d = (x - 0.5) ** 2 + (y - 0.5) ** 2\n    if d < dmin:\n        ncenter = n\n        dmin = d",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_random_geometric_graph",
        "documentation": {}
    },
    {
        "label": "dmin",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_random_geometric_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_random_geometric_graph",
        "peekOfCode": "dmin = 1\nncenter = 0\nfor n in pos:\n    x, y = pos[n]\n    d = (x - 0.5) ** 2 + (y - 0.5) ** 2\n    if d < dmin:\n        ncenter = n\n        dmin = d\n# color by path length from node near center\np = dict(nx.single_source_shortest_path_length(G, ncenter))",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_random_geometric_graph",
        "documentation": {}
    },
    {
        "label": "ncenter",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_random_geometric_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_random_geometric_graph",
        "peekOfCode": "ncenter = 0\nfor n in pos:\n    x, y = pos[n]\n    d = (x - 0.5) ** 2 + (y - 0.5) ** 2\n    if d < dmin:\n        ncenter = n\n        dmin = d\n# color by path length from node near center\np = dict(nx.single_source_shortest_path_length(G, ncenter))\nplt.figure(figsize=(8, 8))",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_random_geometric_graph",
        "documentation": {}
    },
    {
        "label": "p",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_random_geometric_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_random_geometric_graph",
        "peekOfCode": "p = dict(nx.single_source_shortest_path_length(G, ncenter))\nplt.figure(figsize=(8, 8))\nnx.draw_networkx_edges(G, pos, alpha=0.4)\nnx.draw_networkx_nodes(\n    G,\n    pos,\n    nodelist=list(p.keys()),\n    node_size=80,\n    node_color=list(p.values()),\n    cmap=plt.cm.Reds_r,",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_random_geometric_graph",
        "documentation": {}
    },
    {
        "label": "G1",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_sampson",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_sampson",
        "peekOfCode": "G1 = nx.read_edgelist(e1, delimiter=\"\\t\")\nG2 = nx.read_edgelist(e2, delimiter=\"\\t\")\nG3 = nx.read_edgelist(e3, delimiter=\"\\t\")\npos = nx.spring_layout(G3, iterations=100, seed=173)\nplt.clf()\nplt.subplot(221)\nplt.title(\"samplike1\")\nnx.draw(G1, pos, node_size=50, with_labels=False)\nplt.subplot(222)\nplt.title(\"samplike2\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_sampson",
        "documentation": {}
    },
    {
        "label": "G2",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_sampson",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_sampson",
        "peekOfCode": "G2 = nx.read_edgelist(e2, delimiter=\"\\t\")\nG3 = nx.read_edgelist(e3, delimiter=\"\\t\")\npos = nx.spring_layout(G3, iterations=100, seed=173)\nplt.clf()\nplt.subplot(221)\nplt.title(\"samplike1\")\nnx.draw(G1, pos, node_size=50, with_labels=False)\nplt.subplot(222)\nplt.title(\"samplike2\")\nnx.draw(G2, pos, node_size=50, with_labels=False)",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_sampson",
        "documentation": {}
    },
    {
        "label": "G3",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_sampson",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_sampson",
        "peekOfCode": "G3 = nx.read_edgelist(e3, delimiter=\"\\t\")\npos = nx.spring_layout(G3, iterations=100, seed=173)\nplt.clf()\nplt.subplot(221)\nplt.title(\"samplike1\")\nnx.draw(G1, pos, node_size=50, with_labels=False)\nplt.subplot(222)\nplt.title(\"samplike2\")\nnx.draw(G2, pos, node_size=50, with_labels=False)\nplt.subplot(223)",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_sampson",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_sampson",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_sampson",
        "peekOfCode": "pos = nx.spring_layout(G3, iterations=100, seed=173)\nplt.clf()\nplt.subplot(221)\nplt.title(\"samplike1\")\nnx.draw(G1, pos, node_size=50, with_labels=False)\nplt.subplot(222)\nplt.title(\"samplike2\")\nnx.draw(G2, pos, node_size=50, with_labels=False)\nplt.subplot(223)\nplt.title(\"samplike3\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_sampson",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_selfloops",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_selfloops",
        "peekOfCode": "G = nx.complete_graph(3, create_using=nx.DiGraph)\nG.add_edge(0, 0)\npos = nx.circular_layout(G)\n# As of version 2.6, self-loops are drawn by default with the same styling as\n# other edges\nnx.draw(G, pos, with_labels=True)\n# Add self-loops to the remaining nodes\nedgelist = [(1, 1), (2, 2)]\nG.add_edges_from(edgelist)\n# Draw the newly added self-loops with different formatting",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_selfloops",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_selfloops",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_selfloops",
        "peekOfCode": "pos = nx.circular_layout(G)\n# As of version 2.6, self-loops are drawn by default with the same styling as\n# other edges\nnx.draw(G, pos, with_labels=True)\n# Add self-loops to the remaining nodes\nedgelist = [(1, 1), (2, 2)]\nG.add_edges_from(edgelist)\n# Draw the newly added self-loops with different formatting\nnx.draw_networkx_edges(G, pos, edgelist=edgelist, arrowstyle=\"<|-\", style=\"dashed\")\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_selfloops",
        "documentation": {}
    },
    {
        "label": "edgelist",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_selfloops",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_selfloops",
        "peekOfCode": "edgelist = [(1, 1), (2, 2)]\nG.add_edges_from(edgelist)\n# Draw the newly added self-loops with different formatting\nnx.draw_networkx_edges(G, pos, edgelist=edgelist, arrowstyle=\"<|-\", style=\"dashed\")\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_selfloops",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_simple_path",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_simple_path",
        "peekOfCode": "G = nx.path_graph(8)\npos = nx.spring_layout(G, seed=47)  # Seed layout for reproducibility\nnx.draw(G, pos=pos)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_simple_path",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_simple_path",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_simple_path",
        "peekOfCode": "pos = nx.spring_layout(G, seed=47)  # Seed layout for reproducibility\nnx.draw(G, pos=pos)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_simple_path",
        "documentation": {}
    },
    {
        "label": "options",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_spectral_grid",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_spectral_grid",
        "peekOfCode": "options = {\"node_color\": \"C0\", \"node_size\": 100}\nG = nx.grid_2d_graph(6, 6)\nplt.subplot(332)\nnx.draw_spectral(G, **options)\nG.remove_edge((2, 2), (2, 3))\nplt.subplot(334)\nnx.draw_spectral(G, **options)\nG.remove_edge((3, 2), (3, 3))\nplt.subplot(335)\nnx.draw_spectral(G, **options)",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_spectral_grid",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_spectral_grid",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_spectral_grid",
        "peekOfCode": "G = nx.grid_2d_graph(6, 6)\nplt.subplot(332)\nnx.draw_spectral(G, **options)\nG.remove_edge((2, 2), (2, 3))\nplt.subplot(334)\nnx.draw_spectral(G, **options)\nG.remove_edge((3, 2), (3, 3))\nplt.subplot(335)\nnx.draw_spectral(G, **options)\nG.remove_edge((2, 2), (3, 2))",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_spectral_grid",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_tsp",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_tsp",
        "peekOfCode": "G = nx.random_geometric_graph(20, radius=0.4, seed=3)\npos = nx.get_node_attributes(G, \"pos\")\n# Depot should be at (0,0)\npos[0] = (0.5, 0.5)\nH = G.copy()\n# Calculating the distances between the nodes as edge's weight.\nfor i in range(len(pos)):\n    for j in range(i + 1, len(pos)):\n        dist = math.hypot(pos[i][0] - pos[j][0], pos[i][1] - pos[j][1])\n        dist = dist",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_tsp",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_tsp",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_tsp",
        "peekOfCode": "pos = nx.get_node_attributes(G, \"pos\")\n# Depot should be at (0,0)\npos[0] = (0.5, 0.5)\nH = G.copy()\n# Calculating the distances between the nodes as edge's weight.\nfor i in range(len(pos)):\n    for j in range(i + 1, len(pos)):\n        dist = math.hypot(pos[i][0] - pos[j][0], pos[i][1] - pos[j][1])\n        dist = dist\n        G.add_edge(i, j, weight=dist)",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_tsp",
        "documentation": {}
    },
    {
        "label": "pos[0]",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_tsp",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_tsp",
        "peekOfCode": "pos[0] = (0.5, 0.5)\nH = G.copy()\n# Calculating the distances between the nodes as edge's weight.\nfor i in range(len(pos)):\n    for j in range(i + 1, len(pos)):\n        dist = math.hypot(pos[i][0] - pos[j][0], pos[i][1] - pos[j][1])\n        dist = dist\n        G.add_edge(i, j, weight=dist)\ncycle = nx_app.christofides(G, weight=\"weight\")\nedge_list = list(nx.utils.pairwise(cycle))",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_tsp",
        "documentation": {}
    },
    {
        "label": "H",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_tsp",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_tsp",
        "peekOfCode": "H = G.copy()\n# Calculating the distances between the nodes as edge's weight.\nfor i in range(len(pos)):\n    for j in range(i + 1, len(pos)):\n        dist = math.hypot(pos[i][0] - pos[j][0], pos[i][1] - pos[j][1])\n        dist = dist\n        G.add_edge(i, j, weight=dist)\ncycle = nx_app.christofides(G, weight=\"weight\")\nedge_list = list(nx.utils.pairwise(cycle))\n# Draw closest edges on each node only",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_tsp",
        "documentation": {}
    },
    {
        "label": "cycle",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_tsp",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_tsp",
        "peekOfCode": "cycle = nx_app.christofides(G, weight=\"weight\")\nedge_list = list(nx.utils.pairwise(cycle))\n# Draw closest edges on each node only\nnx.draw_networkx_edges(H, pos, edge_color=\"blue\", width=0.5)\n# Draw the route\nnx.draw_networkx(\n    G,\n    pos,\n    with_labels=True,\n    edgelist=edge_list,",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_tsp",
        "documentation": {}
    },
    {
        "label": "edge_list",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_tsp",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_tsp",
        "peekOfCode": "edge_list = list(nx.utils.pairwise(cycle))\n# Draw closest edges on each node only\nnx.draw_networkx_edges(H, pos, edge_color=\"blue\", width=0.5)\n# Draw the route\nnx.draw_networkx(\n    G,\n    pos,\n    with_labels=True,\n    edgelist=edge_list,\n    edge_color=\"red\",",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_tsp",
        "documentation": {}
    },
    {
        "label": "mbox_graph",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_unix_email",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_unix_email",
        "peekOfCode": "def mbox_graph():\n    mbox = mailbox.mbox(\"unix_email.mbox\")  # parse unix mailbox\n    G = nx.MultiDiGraph()  # create empty graph\n    # parse each messages and build graph\n    for msg in mbox:  # msg is python email.Message.Message object\n        (source_name, source_addr) = parseaddr(msg[\"From\"])  # sender\n        # get all recipients\n        # see https://docs.python.org/3/library/email.html\n        tos = msg.get_all(\"to\", [])\n        ccs = msg.get_all(\"cc\", [])",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_unix_email",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_unix_email",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_unix_email",
        "peekOfCode": "G = mbox_graph()\n# print edges with message subject\nfor (u, v, d) in G.edges(data=True):\n    print(f\"From: {u} To: {v} Subject: {d['message']['Subject']}\")\npos = nx.spring_layout(G, iterations=10, seed=227)\nnx.draw(G, pos, node_size=0, alpha=0.4, edge_color=\"r\", font_size=16, with_labels=True)\nax = plt.gca()\nax.margins(0.08)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_unix_email",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_unix_email",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_unix_email",
        "peekOfCode": "pos = nx.spring_layout(G, iterations=10, seed=227)\nnx.draw(G, pos, node_size=0, alpha=0.4, edge_color=\"r\", font_size=16, with_labels=True)\nax = plt.gca()\nax.margins(0.08)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_unix_email",
        "documentation": {}
    },
    {
        "label": "ax",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_unix_email",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_unix_email",
        "peekOfCode": "ax = plt.gca()\nax.margins(0.08)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_unix_email",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_weighted_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_weighted_graph",
        "peekOfCode": "G = nx.Graph()\nG.add_edge(\"a\", \"b\", weight=0.6)\nG.add_edge(\"a\", \"c\", weight=0.2)\nG.add_edge(\"c\", \"d\", weight=0.1)\nG.add_edge(\"c\", \"e\", weight=0.7)\nG.add_edge(\"c\", \"f\", weight=0.9)\nG.add_edge(\"a\", \"d\", weight=0.3)\nelarge = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] > 0.5]\nesmall = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] <= 0.5]\npos = nx.spring_layout(G, seed=7)  # positions for all nodes - seed for reproducibility",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_weighted_graph",
        "documentation": {}
    },
    {
        "label": "elarge",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_weighted_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_weighted_graph",
        "peekOfCode": "elarge = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] > 0.5]\nesmall = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] <= 0.5]\npos = nx.spring_layout(G, seed=7)  # positions for all nodes - seed for reproducibility\n# nodes\nnx.draw_networkx_nodes(G, pos, node_size=700)\n# edges\nnx.draw_networkx_edges(G, pos, edgelist=elarge, width=6)\nnx.draw_networkx_edges(\n    G, pos, edgelist=esmall, width=6, alpha=0.5, edge_color=\"b\", style=\"dashed\"\n)",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_weighted_graph",
        "documentation": {}
    },
    {
        "label": "esmall",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_weighted_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_weighted_graph",
        "peekOfCode": "esmall = [(u, v) for (u, v, d) in G.edges(data=True) if d[\"weight\"] <= 0.5]\npos = nx.spring_layout(G, seed=7)  # positions for all nodes - seed for reproducibility\n# nodes\nnx.draw_networkx_nodes(G, pos, node_size=700)\n# edges\nnx.draw_networkx_edges(G, pos, edgelist=elarge, width=6)\nnx.draw_networkx_edges(\n    G, pos, edgelist=esmall, width=6, alpha=0.5, edge_color=\"b\", style=\"dashed\"\n)\n# node labels",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_weighted_graph",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_weighted_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_weighted_graph",
        "peekOfCode": "pos = nx.spring_layout(G, seed=7)  # positions for all nodes - seed for reproducibility\n# nodes\nnx.draw_networkx_nodes(G, pos, node_size=700)\n# edges\nnx.draw_networkx_edges(G, pos, edgelist=elarge, width=6)\nnx.draw_networkx_edges(\n    G, pos, edgelist=esmall, width=6, alpha=0.5, edge_color=\"b\", style=\"dashed\"\n)\n# node labels\nnx.draw_networkx_labels(G, pos, font_size=20, font_family=\"sans-serif\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_weighted_graph",
        "documentation": {}
    },
    {
        "label": "edge_labels",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_weighted_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_weighted_graph",
        "peekOfCode": "edge_labels = nx.get_edge_attributes(G, \"weight\")\nnx.draw_networkx_edge_labels(G, pos, edge_labels)\nax = plt.gca()\nax.margins(0.08)\nplt.axis(\"off\")\nplt.tight_layout()\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_weighted_graph",
        "documentation": {}
    },
    {
        "label": "ax",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.drawing.plot_weighted_graph",
        "description": "env.share.doc.networkx-2.8.8.examples.drawing.plot_weighted_graph",
        "peekOfCode": "ax = plt.gca()\nax.margins(0.08)\nplt.axis(\"off\")\nplt.tight_layout()\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.drawing.plot_weighted_graph",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_degree_sequence",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_degree_sequence",
        "peekOfCode": "seed = 668273\nz = [5, 3, 3, 3, 3, 2, 2, 2, 1, 1, 1]\nprint(nx.is_graphical(z))\nprint(\"Configuration model\")\nG = nx.configuration_model(z, seed=seed)  # configuration model, seed for reproduciblity\ndegree_sequence = [d for n, d in G.degree()]  # degree sequence\nprint(f\"Degree sequence {degree_sequence}\")\nprint(\"Degree histogram\")\nhist = {}\nfor d in degree_sequence:",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_degree_sequence",
        "documentation": {}
    },
    {
        "label": "z",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_degree_sequence",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_degree_sequence",
        "peekOfCode": "z = [5, 3, 3, 3, 3, 2, 2, 2, 1, 1, 1]\nprint(nx.is_graphical(z))\nprint(\"Configuration model\")\nG = nx.configuration_model(z, seed=seed)  # configuration model, seed for reproduciblity\ndegree_sequence = [d for n, d in G.degree()]  # degree sequence\nprint(f\"Degree sequence {degree_sequence}\")\nprint(\"Degree histogram\")\nhist = {}\nfor d in degree_sequence:\n    if d in hist:",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_degree_sequence",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_degree_sequence",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_degree_sequence",
        "peekOfCode": "G = nx.configuration_model(z, seed=seed)  # configuration model, seed for reproduciblity\ndegree_sequence = [d for n, d in G.degree()]  # degree sequence\nprint(f\"Degree sequence {degree_sequence}\")\nprint(\"Degree histogram\")\nhist = {}\nfor d in degree_sequence:\n    if d in hist:\n        hist[d] += 1\n    else:\n        hist[d] = 1",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_degree_sequence",
        "documentation": {}
    },
    {
        "label": "degree_sequence",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_degree_sequence",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_degree_sequence",
        "peekOfCode": "degree_sequence = [d for n, d in G.degree()]  # degree sequence\nprint(f\"Degree sequence {degree_sequence}\")\nprint(\"Degree histogram\")\nhist = {}\nfor d in degree_sequence:\n    if d in hist:\n        hist[d] += 1\n    else:\n        hist[d] = 1\nprint(\"degree #nodes\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_degree_sequence",
        "documentation": {}
    },
    {
        "label": "hist",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_degree_sequence",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_degree_sequence",
        "peekOfCode": "hist = {}\nfor d in degree_sequence:\n    if d in hist:\n        hist[d] += 1\n    else:\n        hist[d] = 1\nprint(\"degree #nodes\")\nfor d in hist:\n    print(f\"{d:4} {hist[d]:6}\")\npos = nx.spring_layout(G, seed=seed)  # Seed layout for reproducibility",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_degree_sequence",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_degree_sequence",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_degree_sequence",
        "peekOfCode": "pos = nx.spring_layout(G, seed=seed)  # Seed layout for reproducibility\nnx.draw(G, pos=pos)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_degree_sequence",
        "documentation": {}
    },
    {
        "label": "n",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_erdos_renyi",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_erdos_renyi",
        "peekOfCode": "n = 10  # 10 nodes\nm = 20  # 20 edges\nseed = 20160  # seed random number generators for reproducibility\n# Use seed for reproducibility\nG = nx.gnm_random_graph(n, m, seed=seed)\n# some properties\nprint(\"node degree clustering\")\nfor v in nx.nodes(G):\n    print(f\"{v} {nx.degree(G, v)} {nx.clustering(G, v)}\")\nprint()",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_erdos_renyi",
        "documentation": {}
    },
    {
        "label": "m",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_erdos_renyi",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_erdos_renyi",
        "peekOfCode": "m = 20  # 20 edges\nseed = 20160  # seed random number generators for reproducibility\n# Use seed for reproducibility\nG = nx.gnm_random_graph(n, m, seed=seed)\n# some properties\nprint(\"node degree clustering\")\nfor v in nx.nodes(G):\n    print(f\"{v} {nx.degree(G, v)} {nx.clustering(G, v)}\")\nprint()\nprint(\"the adjacency list\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_erdos_renyi",
        "documentation": {}
    },
    {
        "label": "seed",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_erdos_renyi",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_erdos_renyi",
        "peekOfCode": "seed = 20160  # seed random number generators for reproducibility\n# Use seed for reproducibility\nG = nx.gnm_random_graph(n, m, seed=seed)\n# some properties\nprint(\"node degree clustering\")\nfor v in nx.nodes(G):\n    print(f\"{v} {nx.degree(G, v)} {nx.clustering(G, v)}\")\nprint()\nprint(\"the adjacency list\")\nfor line in nx.generate_adjlist(G):",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_erdos_renyi",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_erdos_renyi",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_erdos_renyi",
        "peekOfCode": "G = nx.gnm_random_graph(n, m, seed=seed)\n# some properties\nprint(\"node degree clustering\")\nfor v in nx.nodes(G):\n    print(f\"{v} {nx.degree(G, v)} {nx.clustering(G, v)}\")\nprint()\nprint(\"the adjacency list\")\nfor line in nx.generate_adjlist(G):\n    print(line)\npos = nx.spring_layout(G, seed=seed)  # Seed for reproducible layout",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_erdos_renyi",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_erdos_renyi",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_erdos_renyi",
        "peekOfCode": "pos = nx.spring_layout(G, seed=seed)  # Seed for reproducible layout\nnx.draw(G, pos=pos)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_erdos_renyi",
        "documentation": {}
    },
    {
        "label": "n",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_expected_degree_sequence",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_expected_degree_sequence",
        "peekOfCode": "n = 500  # n nodes\np = 0.1\nw = [p * n for i in range(n)]  # w = p*n for all nodes\nG = nx.expected_degree_graph(w)  # configuration model\nprint(\"Degree histogram\")\nprint(\"degree (#nodes) ****\")\ndh = nx.degree_histogram(G)\nfor i, d in enumerate(dh):\n    print(f\"{i:2} ({d:2}) {'*'*d}\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_expected_degree_sequence",
        "documentation": {}
    },
    {
        "label": "p",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_expected_degree_sequence",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_expected_degree_sequence",
        "peekOfCode": "p = 0.1\nw = [p * n for i in range(n)]  # w = p*n for all nodes\nG = nx.expected_degree_graph(w)  # configuration model\nprint(\"Degree histogram\")\nprint(\"degree (#nodes) ****\")\ndh = nx.degree_histogram(G)\nfor i, d in enumerate(dh):\n    print(f\"{i:2} ({d:2}) {'*'*d}\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_expected_degree_sequence",
        "documentation": {}
    },
    {
        "label": "w",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_expected_degree_sequence",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_expected_degree_sequence",
        "peekOfCode": "w = [p * n for i in range(n)]  # w = p*n for all nodes\nG = nx.expected_degree_graph(w)  # configuration model\nprint(\"Degree histogram\")\nprint(\"degree (#nodes) ****\")\ndh = nx.degree_histogram(G)\nfor i, d in enumerate(dh):\n    print(f\"{i:2} ({d:2}) {'*'*d}\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_expected_degree_sequence",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_expected_degree_sequence",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_expected_degree_sequence",
        "peekOfCode": "G = nx.expected_degree_graph(w)  # configuration model\nprint(\"Degree histogram\")\nprint(\"degree (#nodes) ****\")\ndh = nx.degree_histogram(G)\nfor i, d in enumerate(dh):\n    print(f\"{i:2} ({d:2}) {'*'*d}\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_expected_degree_sequence",
        "documentation": {}
    },
    {
        "label": "dh",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_expected_degree_sequence",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_expected_degree_sequence",
        "peekOfCode": "dh = nx.degree_histogram(G)\nfor i, d in enumerate(dh):\n    print(f\"{i:2} ({d:2}) {'*'*d}\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_expected_degree_sequence",
        "documentation": {}
    },
    {
        "label": "url",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "peekOfCode": "url = \"http://www-personal.umich.edu/~mejn/netdata/football.zip\"\nsock = urllib.request.urlopen(url)  # open URL\ns = io.BytesIO(sock.read())  # read into BytesIO \"file\"\nsock.close()\nzf = zipfile.ZipFile(s)  # zipfile object\ntxt = zf.read(\"football.txt\").decode()  # read info file\ngml = zf.read(\"football.gml\").decode()  # read gml data\n# throw away bogus first line with # from mejn files\ngml = gml.split(\"\\n\")[1:]\nG = nx.parse_gml(gml)  # parse gml data",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "documentation": {}
    },
    {
        "label": "sock",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "peekOfCode": "sock = urllib.request.urlopen(url)  # open URL\ns = io.BytesIO(sock.read())  # read into BytesIO \"file\"\nsock.close()\nzf = zipfile.ZipFile(s)  # zipfile object\ntxt = zf.read(\"football.txt\").decode()  # read info file\ngml = zf.read(\"football.gml\").decode()  # read gml data\n# throw away bogus first line with # from mejn files\ngml = gml.split(\"\\n\")[1:]\nG = nx.parse_gml(gml)  # parse gml data\nprint(txt)",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "documentation": {}
    },
    {
        "label": "s",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "peekOfCode": "s = io.BytesIO(sock.read())  # read into BytesIO \"file\"\nsock.close()\nzf = zipfile.ZipFile(s)  # zipfile object\ntxt = zf.read(\"football.txt\").decode()  # read info file\ngml = zf.read(\"football.gml\").decode()  # read gml data\n# throw away bogus first line with # from mejn files\ngml = gml.split(\"\\n\")[1:]\nG = nx.parse_gml(gml)  # parse gml data\nprint(txt)\n# print degree for each team - number of games",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "documentation": {}
    },
    {
        "label": "zf",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "peekOfCode": "zf = zipfile.ZipFile(s)  # zipfile object\ntxt = zf.read(\"football.txt\").decode()  # read info file\ngml = zf.read(\"football.gml\").decode()  # read gml data\n# throw away bogus first line with # from mejn files\ngml = gml.split(\"\\n\")[1:]\nG = nx.parse_gml(gml)  # parse gml data\nprint(txt)\n# print degree for each team - number of games\nfor n, d in G.degree():\n    print(f\"{n:20} {d:2}\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "documentation": {}
    },
    {
        "label": "txt",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "peekOfCode": "txt = zf.read(\"football.txt\").decode()  # read info file\ngml = zf.read(\"football.gml\").decode()  # read gml data\n# throw away bogus first line with # from mejn files\ngml = gml.split(\"\\n\")[1:]\nG = nx.parse_gml(gml)  # parse gml data\nprint(txt)\n# print degree for each team - number of games\nfor n, d in G.degree():\n    print(f\"{n:20} {d:2}\")\noptions = {\"node_color\": \"black\", \"node_size\": 50, \"linewidths\": 0, \"width\": 0.1}",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "documentation": {}
    },
    {
        "label": "gml",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "peekOfCode": "gml = zf.read(\"football.gml\").decode()  # read gml data\n# throw away bogus first line with # from mejn files\ngml = gml.split(\"\\n\")[1:]\nG = nx.parse_gml(gml)  # parse gml data\nprint(txt)\n# print degree for each team - number of games\nfor n, d in G.degree():\n    print(f\"{n:20} {d:2}\")\noptions = {\"node_color\": \"black\", \"node_size\": 50, \"linewidths\": 0, \"width\": 0.1}\npos = nx.spring_layout(G, seed=1969)  # Seed for reproducible layout",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "documentation": {}
    },
    {
        "label": "gml",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "peekOfCode": "gml = gml.split(\"\\n\")[1:]\nG = nx.parse_gml(gml)  # parse gml data\nprint(txt)\n# print degree for each team - number of games\nfor n, d in G.degree():\n    print(f\"{n:20} {d:2}\")\noptions = {\"node_color\": \"black\", \"node_size\": 50, \"linewidths\": 0, \"width\": 0.1}\npos = nx.spring_layout(G, seed=1969)  # Seed for reproducible layout\nnx.draw(G, pos, **options)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "peekOfCode": "G = nx.parse_gml(gml)  # parse gml data\nprint(txt)\n# print degree for each team - number of games\nfor n, d in G.degree():\n    print(f\"{n:20} {d:2}\")\noptions = {\"node_color\": \"black\", \"node_size\": 50, \"linewidths\": 0, \"width\": 0.1}\npos = nx.spring_layout(G, seed=1969)  # Seed for reproducible layout\nnx.draw(G, pos, **options)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "documentation": {}
    },
    {
        "label": "options",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "peekOfCode": "options = {\"node_color\": \"black\", \"node_size\": 50, \"linewidths\": 0, \"width\": 0.1}\npos = nx.spring_layout(G, seed=1969)  # Seed for reproducible layout\nnx.draw(G, pos, **options)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "peekOfCode": "pos = nx.spring_layout(G, seed=1969)  # Seed for reproducible layout\nnx.draw(G, pos, **options)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_football",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_karate_club",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_karate_club",
        "peekOfCode": "G = nx.karate_club_graph()\nprint(\"Node Degree\")\nfor v in G:\n    print(f\"{v:4} {G.degree(v):6}\")\nnx.draw_circular(G, with_labels=True)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_karate_club",
        "documentation": {}
    },
    {
        "label": "morse_encode",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "peekOfCode": "def morse_encode(letter):\n    pred = next(G.predecessors(letter))  # Each letter has only 1 predecessor\n    symbol = G[pred][letter][\"char\"]\n    if pred != \"\":\n        return morse_encode(pred) + symbol  # Traversing the trie in reverse\n    return symbol\n# Verify that the trie encoding is correct\nimport string\nfor letter in string.ascii_lowercase:\n    assert morse_encode(letter) == morse_direct_mapping[letter]",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "documentation": {}
    },
    {
        "label": "dot",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "peekOfCode": "dot = \"\"\ndash = \"\"\n# Start with the direct mapping of letter -> code\nmorse_direct_mapping = {\n    \"a\": dot + dash,\n    \"b\": dash + dot * 3,\n    \"c\": dash + dot + dash + dot,\n    \"d\": dash + dot * 2,\n    \"e\": dot,\n    \"f\": dot * 2 + dash + dot,",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "documentation": {}
    },
    {
        "label": "dash",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "peekOfCode": "dash = \"\"\n# Start with the direct mapping of letter -> code\nmorse_direct_mapping = {\n    \"a\": dot + dash,\n    \"b\": dash + dot * 3,\n    \"c\": dash + dot + dash + dot,\n    \"d\": dash + dot * 2,\n    \"e\": dot,\n    \"f\": dot * 2 + dash + dot,\n    \"g\": dash * 2 + dot,",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "documentation": {}
    },
    {
        "label": "morse_direct_mapping",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "peekOfCode": "morse_direct_mapping = {\n    \"a\": dot + dash,\n    \"b\": dash + dot * 3,\n    \"c\": dash + dot + dash + dot,\n    \"d\": dash + dot * 2,\n    \"e\": dot,\n    \"f\": dot * 2 + dash + dot,\n    \"g\": dash * 2 + dot,\n    \"h\": dot * 4,\n    \"i\": dot * 2,",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "documentation": {}
    },
    {
        "label": "morse_mapping_sorted",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "peekOfCode": "morse_mapping_sorted = dict(\n    sorted(morse_direct_mapping.items(), key=lambda item: (len(item[1]), item[1]))\n)\n# More preprocessing: create the reverse mapping to simplify lookup\nreverse_mapping = {v: k for k, v in morse_direct_mapping.items()}\nreverse_mapping[\"\"] = \"\"  # Represent the \"root\" node with an empty string\n# Construct the prefix tree from the sorted mapping\nG = nx.DiGraph()\nfor node, char in morse_mapping_sorted.items():\n    pred = char[:-1]",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "documentation": {}
    },
    {
        "label": "reverse_mapping",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "peekOfCode": "reverse_mapping = {v: k for k, v in morse_direct_mapping.items()}\nreverse_mapping[\"\"] = \"\"  # Represent the \"root\" node with an empty string\n# Construct the prefix tree from the sorted mapping\nG = nx.DiGraph()\nfor node, char in morse_mapping_sorted.items():\n    pred = char[:-1]\n    # Store the dot/dash relating the two letters as an edge attribute \"char\"\n    G.add_edge(reverse_mapping[pred], node, char=char[-1])\n# For visualization purposes, layout the nodes in topological order\nfor i, layer in enumerate(nx.topological_generations(G)):",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "documentation": {}
    },
    {
        "label": "reverse_mapping[\"\"]",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "peekOfCode": "reverse_mapping[\"\"] = \"\"  # Represent the \"root\" node with an empty string\n# Construct the prefix tree from the sorted mapping\nG = nx.DiGraph()\nfor node, char in morse_mapping_sorted.items():\n    pred = char[:-1]\n    # Store the dot/dash relating the two letters as an edge attribute \"char\"\n    G.add_edge(reverse_mapping[pred], node, char=char[-1])\n# For visualization purposes, layout the nodes in topological order\nfor i, layer in enumerate(nx.topological_generations(G)):\n    for n in layer:",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "peekOfCode": "G = nx.DiGraph()\nfor node, char in morse_mapping_sorted.items():\n    pred = char[:-1]\n    # Store the dot/dash relating the two letters as an edge attribute \"char\"\n    G.add_edge(reverse_mapping[pred], node, char=char[-1])\n# For visualization purposes, layout the nodes in topological order\nfor i, layer in enumerate(nx.topological_generations(G)):\n    for n in layer:\n        G.nodes[n][\"layer\"] = i\npos = nx.multipartite_layout(G, subset_key=\"layer\", align=\"horizontal\")",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "peekOfCode": "pos = nx.multipartite_layout(G, subset_key=\"layer\", align=\"horizontal\")\n# Flip the layout so the root node is on top\nfor k in pos:\n    pos[k][-1] *= -1\n# Visualize the trie\nnx.draw(G, pos=pos, with_labels=True)\nelabels = {(u, v): l for u, v, l in G.edges(data=\"char\")}\nnx.draw_networkx_edge_labels(G, pos, edge_labels=elabels)\n# A letter can be encoded by following the path from the given letter (node) to\n# the root node",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "documentation": {}
    },
    {
        "label": "elabels",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "peekOfCode": "elabels = {(u, v): l for u, v, l in G.edges(data=\"char\")}\nnx.draw_networkx_edge_labels(G, pos, edge_labels=elabels)\n# A letter can be encoded by following the path from the given letter (node) to\n# the root node\ndef morse_encode(letter):\n    pred = next(G.predecessors(letter))  # Each letter has only 1 predecessor\n    symbol = G[pred][letter][\"char\"]\n    if pred != \"\":\n        return morse_encode(pred) + symbol  # Traversing the trie in reverse\n    return symbol",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_morse_trie",
        "documentation": {}
    },
    {
        "label": "minard_graph",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_napoleon_russian_campaign",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_napoleon_russian_campaign",
        "peekOfCode": "def minard_graph():\n    data1 = \"\"\"\\\n24.0,54.9,340000,A,1\n24.5,55.0,340000,A,1\n25.5,54.5,340000,A,1\n26.0,54.7,320000,A,1\n27.0,54.8,300000,A,1\n28.0,54.9,280000,A,1\n28.5,55.0,240000,A,1\n29.0,55.1,210000,A,1",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_napoleon_russian_campaign",
        "documentation": {}
    },
    {
        "label": "colors",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_napoleon_russian_campaign",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_napoleon_russian_campaign",
        "peekOfCode": "colors = [\"b\", \"g\", \"r\"]\nfor G in g:\n    c = colors.pop(0)\n    node_size = [G.pop[n] // 300 for n in G]\n    nx.draw_networkx_edges(G, G.pos, edge_color=c, width=4, alpha=0.5)\n    nx.draw_networkx_nodes(G, G.pos, node_size=node_size, node_color=c, alpha=0.5)\n    nx.draw_networkx_nodes(G, G.pos, node_size=5, node_color=\"k\")\nfor c in city:\n    x, y = city[c]\n    plt.text(x, y + 0.1, c)",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_napoleon_russian_campaign",
        "documentation": {}
    },
    {
        "label": "roget_graph",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_roget",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_roget",
        "peekOfCode": "def roget_graph():\n    \"\"\"Return the thesaurus graph from the roget.dat example in\n    the Stanford Graph Base.\n    \"\"\"\n    # open file roget_dat.txt.gz\n    fh = gzip.open(\"roget_dat.txt.gz\", \"r\")\n    G = nx.DiGraph()\n    for line in fh.readlines():\n        line = line.decode()\n        if line.startswith(\"*\"):  # skip comments",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_roget",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_roget",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_roget",
        "peekOfCode": "G = roget_graph()\nprint(\"Loaded roget_dat.txt containing 1022 categories.\")\nprint(G)\nUG = G.to_undirected()\nprint(nx.number_connected_components(UG), \"connected components\")\noptions = {\n    \"node_color\": \"black\",\n    \"node_size\": 1,\n    \"edge_color\": \"gray\",\n    \"linewidths\": 0,",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_roget",
        "documentation": {}
    },
    {
        "label": "UG",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_roget",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_roget",
        "peekOfCode": "UG = G.to_undirected()\nprint(nx.number_connected_components(UG), \"connected components\")\noptions = {\n    \"node_color\": \"black\",\n    \"node_size\": 1,\n    \"edge_color\": \"gray\",\n    \"linewidths\": 0,\n    \"width\": 0.1,\n}\nnx.draw_circular(UG, **options)",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_roget",
        "documentation": {}
    },
    {
        "label": "options",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_roget",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_roget",
        "peekOfCode": "options = {\n    \"node_color\": \"black\",\n    \"node_size\": 1,\n    \"edge_color\": \"gray\",\n    \"linewidths\": 0,\n    \"width\": 0.1,\n}\nnx.draw_circular(UG, **options)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_roget",
        "documentation": {}
    },
    {
        "label": "generate_graph",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_words",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_words",
        "peekOfCode": "def generate_graph(words):\n    G = nx.Graph(name=\"words\")\n    lookup = {c: lowercase.index(c) for c in lowercase}\n    def edit_distance_one(word):\n        for i in range(len(word)):\n            left, c, right = word[0:i], word[i], word[i + 1 :]\n            j = lookup[c]  # lowercase.index(c)\n            for cc in lowercase[j + 1 :]:\n                yield left + cc + right\n    candgen = (",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_words",
        "documentation": {}
    },
    {
        "label": "words_graph",
        "kind": 2,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_words",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_words",
        "peekOfCode": "def words_graph():\n    \"\"\"Return the words example graph from the Stanford GraphBase\"\"\"\n    fh = gzip.open(\"words_dat.txt.gz\", \"r\")\n    words = set()\n    for line in fh.readlines():\n        line = line.decode()\n        if line.startswith(\"*\"):\n            continue\n        w = str(line[0:5])\n        words.add(w)",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_words",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_words",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_words",
        "peekOfCode": "G = words_graph()\nprint(\"Loaded words_dat.txt containing 5757 five-letter English words.\")\nprint(\"Two words are connected if they differ in one letter.\")\nprint(G)\nprint(f\"{nx.number_connected_components(G)} connected components\")\nfor (source, target) in [(\"chaos\", \"order\"), (\"nodes\", \"graph\"), (\"pound\", \"marks\")]:\n    print(f\"Shortest path between {source} and {target} is\")\n    try:\n        shortest_path = nx.shortest_path(G, source, target)\n        for n in shortest_path:",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_words",
        "documentation": {}
    },
    {
        "label": "boundary",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_words",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_words",
        "peekOfCode": "boundary = list(nx.node_boundary(G, shortest_path))\nG.add_nodes_from(shortest_path, color=\"red\")\nG.add_nodes_from(boundary, color=\"blue\")\nH = G.subgraph(shortest_path + boundary)\ncolors = nx.get_node_attributes(H, \"color\")\noptions = {\"node_size\": 1500, \"alpha\": 0.3, \"node_color\": colors.values()}\npos = nx.kamada_kawai_layout(H)\nnx.draw(H, pos, **options)\nnx.draw_networkx_labels(H, pos, font_weight=\"bold\")\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_words",
        "documentation": {}
    },
    {
        "label": "H",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_words",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_words",
        "peekOfCode": "H = G.subgraph(shortest_path + boundary)\ncolors = nx.get_node_attributes(H, \"color\")\noptions = {\"node_size\": 1500, \"alpha\": 0.3, \"node_color\": colors.values()}\npos = nx.kamada_kawai_layout(H)\nnx.draw(H, pos, **options)\nnx.draw_networkx_labels(H, pos, font_weight=\"bold\")\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_words",
        "documentation": {}
    },
    {
        "label": "colors",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_words",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_words",
        "peekOfCode": "colors = nx.get_node_attributes(H, \"color\")\noptions = {\"node_size\": 1500, \"alpha\": 0.3, \"node_color\": colors.values()}\npos = nx.kamada_kawai_layout(H)\nnx.draw(H, pos, **options)\nnx.draw_networkx_labels(H, pos, font_weight=\"bold\")\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_words",
        "documentation": {}
    },
    {
        "label": "options",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_words",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_words",
        "peekOfCode": "options = {\"node_size\": 1500, \"alpha\": 0.3, \"node_color\": colors.values()}\npos = nx.kamada_kawai_layout(H)\nnx.draw(H, pos, **options)\nnx.draw_networkx_labels(H, pos, font_weight=\"bold\")\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_words",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.graph.plot_words",
        "description": "env.share.doc.networkx-2.8.8.examples.graph.plot_words",
        "peekOfCode": "pos = nx.kamada_kawai_layout(H)\nnx.draw(H, pos, **options)\nnx.draw_networkx_labels(H, pos, font_weight=\"bold\")\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.graph.plot_words",
        "documentation": {}
    },
    {
        "label": "AntiGraph",
        "kind": 6,
        "importPath": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "description": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "peekOfCode": "class AntiGraph(Graph):\n    \"\"\"\n    Class for complement graphs.\n    The main goal is to be able to work with big and dense graphs with\n    a low memory footprint.\n    In this class you add the edges that *do not exist* in the dense graph,\n    the report methods of the class return the neighbors, the edges and\n    the degree as if it was the dense graph. Thus it's possible to use\n    an instance of this class with some of NetworkX functions.\n    \"\"\"",
        "detail": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "documentation": {}
    },
    {
        "label": "Gnp",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "description": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "peekOfCode": "Gnp = nx.gnp_random_graph(20, 0.8, seed=42)\nAnp = AntiGraph(nx.complement(Gnp))\nGd = nx.davis_southern_women_graph()\nAd = AntiGraph(nx.complement(Gd))\nGk = nx.karate_club_graph()\nAk = AntiGraph(nx.complement(Gk))\npairs = [(Gnp, Anp), (Gd, Ad), (Gk, Ak)]\n# test connected components\nfor G, A in pairs:\n    gc = [set(c) for c in nx.connected_components(G)]",
        "detail": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "documentation": {}
    },
    {
        "label": "Anp",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "description": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "peekOfCode": "Anp = AntiGraph(nx.complement(Gnp))\nGd = nx.davis_southern_women_graph()\nAd = AntiGraph(nx.complement(Gd))\nGk = nx.karate_club_graph()\nAk = AntiGraph(nx.complement(Gk))\npairs = [(Gnp, Anp), (Gd, Ad), (Gk, Ak)]\n# test connected components\nfor G, A in pairs:\n    gc = [set(c) for c in nx.connected_components(G)]\n    ac = [set(c) for c in nx.connected_components(A)]",
        "detail": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "documentation": {}
    },
    {
        "label": "Gd",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "description": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "peekOfCode": "Gd = nx.davis_southern_women_graph()\nAd = AntiGraph(nx.complement(Gd))\nGk = nx.karate_club_graph()\nAk = AntiGraph(nx.complement(Gk))\npairs = [(Gnp, Anp), (Gd, Ad), (Gk, Ak)]\n# test connected components\nfor G, A in pairs:\n    gc = [set(c) for c in nx.connected_components(G)]\n    ac = [set(c) for c in nx.connected_components(A)]\n    for comp in ac:",
        "detail": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "documentation": {}
    },
    {
        "label": "Ad",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "description": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "peekOfCode": "Ad = AntiGraph(nx.complement(Gd))\nGk = nx.karate_club_graph()\nAk = AntiGraph(nx.complement(Gk))\npairs = [(Gnp, Anp), (Gd, Ad), (Gk, Ak)]\n# test connected components\nfor G, A in pairs:\n    gc = [set(c) for c in nx.connected_components(G)]\n    ac = [set(c) for c in nx.connected_components(A)]\n    for comp in ac:\n        assert comp in gc",
        "detail": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "documentation": {}
    },
    {
        "label": "Gk",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "description": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "peekOfCode": "Gk = nx.karate_club_graph()\nAk = AntiGraph(nx.complement(Gk))\npairs = [(Gnp, Anp), (Gd, Ad), (Gk, Ak)]\n# test connected components\nfor G, A in pairs:\n    gc = [set(c) for c in nx.connected_components(G)]\n    ac = [set(c) for c in nx.connected_components(A)]\n    for comp in ac:\n        assert comp in gc\n# test biconnected components",
        "detail": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "documentation": {}
    },
    {
        "label": "Ak",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "description": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "peekOfCode": "Ak = AntiGraph(nx.complement(Gk))\npairs = [(Gnp, Anp), (Gd, Ad), (Gk, Ak)]\n# test connected components\nfor G, A in pairs:\n    gc = [set(c) for c in nx.connected_components(G)]\n    ac = [set(c) for c in nx.connected_components(A)]\n    for comp in ac:\n        assert comp in gc\n# test biconnected components\nfor G, A in pairs:",
        "detail": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "documentation": {}
    },
    {
        "label": "pairs",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "description": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "peekOfCode": "pairs = [(Gnp, Anp), (Gd, Ad), (Gk, Ak)]\n# test connected components\nfor G, A in pairs:\n    gc = [set(c) for c in nx.connected_components(G)]\n    ac = [set(c) for c in nx.connected_components(A)]\n    for comp in ac:\n        assert comp in gc\n# test biconnected components\nfor G, A in pairs:\n    gc = [set(c) for c in nx.biconnected_components(G)]",
        "detail": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "description": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "peekOfCode": "pos = nx.spring_layout(G, seed=268)  # Seed for reproducible layout\nnx.draw(Gnp, pos=pos)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.subclass.plot_antigraph",
        "documentation": {}
    },
    {
        "label": "PrintGraph",
        "kind": 6,
        "importPath": "env.share.doc.networkx-2.8.8.examples.subclass.plot_printgraph",
        "description": "env.share.doc.networkx-2.8.8.examples.subclass.plot_printgraph",
        "peekOfCode": "class PrintGraph(Graph):\n    \"\"\"\n    Example subclass of the Graph class.\n    Prints activity log to file or standard output.\n    \"\"\"\n    def __init__(self, data=None, name=\"\", file=None, **attr):\n        super().__init__(data=data, name=name, **attr)\n        if file is None:\n            import sys\n            self.fh = sys.stdout",
        "detail": "env.share.doc.networkx-2.8.8.examples.subclass.plot_printgraph",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.subclass.plot_printgraph",
        "description": "env.share.doc.networkx-2.8.8.examples.subclass.plot_printgraph",
        "peekOfCode": "G = PrintGraph()\nG.add_node(\"foo\")\nG.add_nodes_from(\"bar\", weight=8)\nG.remove_node(\"b\")\nG.remove_nodes_from(\"ar\")\nprint(\"Nodes in G: \", G.nodes(data=True))\nG.add_edge(0, 1, weight=10)\nprint(\"Edges in G: \", G.edges(data=True))\nG.remove_edge(0, 1)\nG.add_edges_from(zip(range(0, 3), range(1, 4)), weight=10)",
        "detail": "env.share.doc.networkx-2.8.8.examples.subclass.plot_printgraph",
        "documentation": {}
    },
    {
        "label": "G",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.subclass.plot_printgraph",
        "description": "env.share.doc.networkx-2.8.8.examples.subclass.plot_printgraph",
        "peekOfCode": "G = PrintGraph()\nnx.add_path(G, range(10))\nnx.add_star(G, range(9, 13))\npos = nx.spring_layout(G, seed=225)  # Seed for reproducible layout\nnx.draw(G, pos)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.subclass.plot_printgraph",
        "documentation": {}
    },
    {
        "label": "pos",
        "kind": 5,
        "importPath": "env.share.doc.networkx-2.8.8.examples.subclass.plot_printgraph",
        "description": "env.share.doc.networkx-2.8.8.examples.subclass.plot_printgraph",
        "peekOfCode": "pos = nx.spring_layout(G, seed=225)  # Seed for reproducible layout\nnx.draw(G, pos)\nplt.show()",
        "detail": "env.share.doc.networkx-2.8.8.examples.subclass.plot_printgraph",
        "documentation": {}
    },
    {
        "label": "system_info",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.collect_env_info",
        "description": "env.src.tts.TTS.bin.collect_env_info",
        "peekOfCode": "def system_info():\n    return {\n        \"OS\": platform.system(),\n        \"architecture\": platform.architecture(),\n        \"version\": platform.version(),\n        \"processor\": platform.processor(),\n        \"python\": platform.python_version(),\n    }\ndef cuda_info():\n    return {",
        "detail": "env.src.tts.TTS.bin.collect_env_info",
        "documentation": {}
    },
    {
        "label": "cuda_info",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.collect_env_info",
        "description": "env.src.tts.TTS.bin.collect_env_info",
        "peekOfCode": "def cuda_info():\n    return {\n        \"GPU\": [torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())],\n        \"available\": torch.cuda.is_available(),\n        \"version\": torch.version.cuda,\n    }\ndef package_info():\n    return {\n        \"numpy\": numpy.__version__,\n        \"PyTorch_version\": torch.__version__,",
        "detail": "env.src.tts.TTS.bin.collect_env_info",
        "documentation": {}
    },
    {
        "label": "package_info",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.collect_env_info",
        "description": "env.src.tts.TTS.bin.collect_env_info",
        "peekOfCode": "def package_info():\n    return {\n        \"numpy\": numpy.__version__,\n        \"PyTorch_version\": torch.__version__,\n        \"PyTorch_debug\": torch.version.debug,\n        \"TTS\": TTS.__version__,\n    }\ndef main():\n    details = {\"System\": system_info(), \"CUDA\": cuda_info(), \"Packages\": package_info()}\n    print(json.dumps(details, indent=4, sort_keys=True))",
        "detail": "env.src.tts.TTS.bin.collect_env_info",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.collect_env_info",
        "description": "env.src.tts.TTS.bin.collect_env_info",
        "peekOfCode": "def main():\n    details = {\"System\": system_info(), \"CUDA\": cuda_info(), \"Packages\": package_info()}\n    print(json.dumps(details, indent=4, sort_keys=True))\nif __name__ == \"__main__\":\n    main()",
        "detail": "env.src.tts.TTS.bin.collect_env_info",
        "documentation": {}
    },
    {
        "label": "compute_embeddings",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.compute_embeddings",
        "description": "env.src.tts.TTS.bin.compute_embeddings",
        "peekOfCode": "def compute_embeddings(\n    model_path,\n    config_path,\n    output_path,\n    old_spakers_file=None,\n    config_dataset_path=None,\n    formatter_name=None,\n    dataset_name=None,\n    dataset_path=None,\n    meta_file_train=None,",
        "detail": "env.src.tts.TTS.bin.compute_embeddings",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.compute_statistics",
        "description": "env.src.tts.TTS.bin.compute_statistics",
        "peekOfCode": "def main():\n    \"\"\"Run preprocessing process.\"\"\"\n    parser = argparse.ArgumentParser(description=\"Compute mean and variance of spectrogtram features.\")\n    parser.add_argument(\"config_path\", type=str, help=\"TTS config file path to define audio processin parameters.\")\n    parser.add_argument(\"out_path\", type=str, help=\"save path (directory and filename).\")\n    parser.add_argument(\n        \"--data_path\",\n        type=str,\n        required=False,\n        help=\"folder including the target set of wavs overriding dataset config.\",",
        "detail": "env.src.tts.TTS.bin.compute_statistics",
        "documentation": {}
    },
    {
        "label": "compute_encoder_accuracy",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.eval_encoder",
        "description": "env.src.tts.TTS.bin.eval_encoder",
        "peekOfCode": "def compute_encoder_accuracy(dataset_items, encoder_manager):\n    class_name_key = encoder_manager.encoder_config.class_name_key\n    map_classid_to_classname = getattr(encoder_manager.encoder_config, \"map_classid_to_classname\", None)\n    class_acc_dict = {}\n    # compute embeddings for all wav_files\n    for item in tqdm(dataset_items):\n        class_name = item[class_name_key]\n        wav_file = item[\"audio_file\"]\n        # extract the embedding\n        embedd = encoder_manager.compute_embedding_from_clip(wav_file)",
        "detail": "env.src.tts.TTS.bin.eval_encoder",
        "documentation": {}
    },
    {
        "label": "setup_loader",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.extract_tts_spectrograms",
        "description": "env.src.tts.TTS.bin.extract_tts_spectrograms",
        "peekOfCode": "def setup_loader(ap, r, verbose=False):\n    tokenizer, _ = TTSTokenizer.init_from_config(c)\n    dataset = TTSDataset(\n        outputs_per_step=r,\n        compute_linear_spec=False,\n        samples=meta_data,\n        tokenizer=tokenizer,\n        ap=ap,\n        batch_group_size=0,\n        min_text_len=c.min_text_len,",
        "detail": "env.src.tts.TTS.bin.extract_tts_spectrograms",
        "documentation": {}
    },
    {
        "label": "set_filename",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.extract_tts_spectrograms",
        "description": "env.src.tts.TTS.bin.extract_tts_spectrograms",
        "peekOfCode": "def set_filename(wav_path, out_path):\n    wav_file = os.path.basename(wav_path)\n    file_name = wav_file.split(\".\")[0]\n    os.makedirs(os.path.join(out_path, \"quant\"), exist_ok=True)\n    os.makedirs(os.path.join(out_path, \"mel\"), exist_ok=True)\n    os.makedirs(os.path.join(out_path, \"wav_gl\"), exist_ok=True)\n    os.makedirs(os.path.join(out_path, \"wav\"), exist_ok=True)\n    wavq_path = os.path.join(out_path, \"quant\", file_name)\n    mel_path = os.path.join(out_path, \"mel\", file_name)\n    wav_gl_path = os.path.join(out_path, \"wav_gl\", file_name + \".wav\")",
        "detail": "env.src.tts.TTS.bin.extract_tts_spectrograms",
        "documentation": {}
    },
    {
        "label": "format_data",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.extract_tts_spectrograms",
        "description": "env.src.tts.TTS.bin.extract_tts_spectrograms",
        "peekOfCode": "def format_data(data):\n    # setup input data\n    text_input = data[\"token_id\"]\n    text_lengths = data[\"token_id_lengths\"]\n    mel_input = data[\"mel\"]\n    mel_lengths = data[\"mel_lengths\"]\n    item_idx = data[\"item_idxs\"]\n    d_vectors = data[\"d_vectors\"]\n    speaker_ids = data[\"speaker_ids\"]\n    attn_mask = data[\"attns\"]",
        "detail": "env.src.tts.TTS.bin.extract_tts_spectrograms",
        "documentation": {}
    },
    {
        "label": "inference",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.extract_tts_spectrograms",
        "description": "env.src.tts.TTS.bin.extract_tts_spectrograms",
        "peekOfCode": "def inference(\n    model_name,\n    model,\n    ap,\n    text_input,\n    text_lengths,\n    mel_input,\n    mel_lengths,\n    speaker_ids=None,\n    d_vectors=None,",
        "detail": "env.src.tts.TTS.bin.extract_tts_spectrograms",
        "documentation": {}
    },
    {
        "label": "extract_spectrograms",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.extract_tts_spectrograms",
        "description": "env.src.tts.TTS.bin.extract_tts_spectrograms",
        "peekOfCode": "def extract_spectrograms(\n    data_loader, model, ap, output_path, quantized_wav=False, save_audio=False, debug=False, metada_name=\"metada.txt\"\n):\n    model.eval()\n    export_metadata = []\n    for _, data in tqdm(enumerate(data_loader), total=len(data_loader)):\n        # format data\n        (\n            text_input,\n            text_lengths,",
        "detail": "env.src.tts.TTS.bin.extract_tts_spectrograms",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.extract_tts_spectrograms",
        "description": "env.src.tts.TTS.bin.extract_tts_spectrograms",
        "peekOfCode": "def main(args):  # pylint: disable=redefined-outer-name\n    # pylint: disable=global-variable-undefined\n    global meta_data, speaker_manager\n    # Audio processor\n    ap = AudioProcessor(**c.audio)\n    # load data instances\n    meta_data_train, meta_data_eval = load_tts_samples(\n        c.datasets, eval_split=args.eval, eval_split_max_size=c.eval_split_max_size, eval_split_size=c.eval_split_size\n    )\n    # use eval and training partitions",
        "detail": "env.src.tts.TTS.bin.extract_tts_spectrograms",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "env.src.tts.TTS.bin.extract_tts_spectrograms",
        "description": "env.src.tts.TTS.bin.extract_tts_spectrograms",
        "peekOfCode": "use_cuda = torch.cuda.is_available()\ndef setup_loader(ap, r, verbose=False):\n    tokenizer, _ = TTSTokenizer.init_from_config(c)\n    dataset = TTSDataset(\n        outputs_per_step=r,\n        compute_linear_spec=False,\n        samples=meta_data,\n        tokenizer=tokenizer,\n        ap=ap,\n        batch_group_size=0,",
        "detail": "env.src.tts.TTS.bin.extract_tts_spectrograms",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.find_unique_chars",
        "description": "env.src.tts.TTS.bin.find_unique_chars",
        "peekOfCode": "def main():\n    # pylint: disable=bad-option-value\n    parser = argparse.ArgumentParser(\n        description=\"\"\"Find all the unique characters or phonemes in a dataset.\\n\\n\"\"\"\n        \"\"\"\n    Example runs:\n    python TTS/bin/find_unique_chars.py --config_path config.json\n    \"\"\",\n        formatter_class=RawTextHelpFormatter,\n    )",
        "detail": "env.src.tts.TTS.bin.find_unique_chars",
        "documentation": {}
    },
    {
        "label": "compute_phonemes",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.find_unique_phonemes",
        "description": "env.src.tts.TTS.bin.find_unique_phonemes",
        "peekOfCode": "def compute_phonemes(item):\n    text = item[\"text\"]\n    ph = phonemizer.phonemize(text).replace(\"|\", \"\")\n    return set(list(ph))\ndef main():\n    # pylint: disable=W0601\n    global c, phonemizer\n    # pylint: disable=bad-option-value\n    parser = argparse.ArgumentParser(\n        description=\"\"\"Find all the unique characters or phonemes in a dataset.\\n\\n\"\"\"",
        "detail": "env.src.tts.TTS.bin.find_unique_phonemes",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.find_unique_phonemes",
        "description": "env.src.tts.TTS.bin.find_unique_phonemes",
        "peekOfCode": "def main():\n    # pylint: disable=W0601\n    global c, phonemizer\n    # pylint: disable=bad-option-value\n    parser = argparse.ArgumentParser(\n        description=\"\"\"Find all the unique characters or phonemes in a dataset.\\n\\n\"\"\"\n        \"\"\"\n    Example runs:\n    python TTS/bin/find_unique_phonemes.py --config_path config.json\n    \"\"\",",
        "detail": "env.src.tts.TTS.bin.find_unique_phonemes",
        "documentation": {}
    },
    {
        "label": "adjust_path_and_remove_silence",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.remove_silence_using_vad",
        "description": "env.src.tts.TTS.bin.remove_silence_using_vad",
        "peekOfCode": "def adjust_path_and_remove_silence(audio_path):\n    output_path = audio_path.replace(os.path.join(args.input_dir, \"\"), os.path.join(args.output_dir, \"\"))\n    # ignore if the file exists\n    if os.path.exists(output_path) and not args.force:\n        return output_path, False\n    # create all directory structure\n    pathlib.Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n    # remove the silence and save the audio\n    output_path, is_speech = remove_silence(\n        model_and_utils,",
        "detail": "env.src.tts.TTS.bin.remove_silence_using_vad",
        "documentation": {}
    },
    {
        "label": "preprocess_audios",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.remove_silence_using_vad",
        "description": "env.src.tts.TTS.bin.remove_silence_using_vad",
        "peekOfCode": "def preprocess_audios():\n    files = sorted(glob.glob(os.path.join(args.input_dir, args.glob), recursive=True))\n    print(\"> Number of files: \", len(files))\n    if not args.force:\n        print(\"> Ignoring files that already exist in the output idrectory.\")\n    if args.trim_just_beginning_and_end:\n        print(\"> Trimming just the beginning and the end with nonspeech parts.\")\n    else:\n        print(\"> Trimming all nonspeech parts.\")\n    filtered_files = []",
        "detail": "env.src.tts.TTS.bin.remove_silence_using_vad",
        "documentation": {}
    },
    {
        "label": "resample_file",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.resample",
        "description": "env.src.tts.TTS.bin.resample",
        "peekOfCode": "def resample_file(func_args):\n    filename, output_sr = func_args\n    y, sr = librosa.load(filename, sr=output_sr)\n    sf.write(filename, y, sr)\ndef resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10):\n    if output_dir:\n        print(\"Recursively copying the input folder...\")\n        copytree(input_dir, output_dir)\n        input_dir = output_dir\n    print(\"Resampling the audio files...\")",
        "detail": "env.src.tts.TTS.bin.resample",
        "documentation": {}
    },
    {
        "label": "resample_files",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.resample",
        "description": "env.src.tts.TTS.bin.resample",
        "peekOfCode": "def resample_files(input_dir, output_sr, output_dir=None, file_ext=\"wav\", n_jobs=10):\n    if output_dir:\n        print(\"Recursively copying the input folder...\")\n        copytree(input_dir, output_dir)\n        input_dir = output_dir\n    print(\"Resampling the audio files...\")\n    audio_files = glob.glob(os.path.join(input_dir, f\"**/*.{file_ext}\"), recursive=True)\n    print(f\"Found {len(audio_files)} files...\")\n    audio_files = list(zip(audio_files, len(audio_files) * [output_sr]))\n    with Pool(processes=n_jobs) as p:",
        "detail": "env.src.tts.TTS.bin.resample",
        "documentation": {}
    },
    {
        "label": "str2bool",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.synthesize",
        "description": "env.src.tts.TTS.bin.synthesize",
        "peekOfCode": "def str2bool(v):\n    if isinstance(v, bool):\n        return v\n    if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n        return True\n    if v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n        return False\n    raise argparse.ArgumentTypeError(\"Boolean value expected.\")\ndef main():\n    description = \"\"\"Synthesize speech on command line.",
        "detail": "env.src.tts.TTS.bin.synthesize",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.synthesize",
        "description": "env.src.tts.TTS.bin.synthesize",
        "peekOfCode": "def main():\n    description = \"\"\"Synthesize speech on command line.\nYou can either use your trained model or choose a model from the provided list.\nIf you don't specify any models, then it uses LJSpeech based English model.\n## Example Runs\n### Single Speaker Models\n- List provided models:\n    ```\n    $ tts --list_models\n    ```",
        "detail": "env.src.tts.TTS.bin.synthesize",
        "documentation": {}
    },
    {
        "label": "setup_loader",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.train_encoder",
        "description": "env.src.tts.TTS.bin.train_encoder",
        "peekOfCode": "def setup_loader(ap: AudioProcessor, is_val: bool = False, verbose: bool = False):\n    num_utter_per_class = c.num_utter_per_class if not is_val else c.eval_num_utter_per_class\n    num_classes_in_batch = c.num_classes_in_batch if not is_val else c.eval_num_classes_in_batch\n    dataset = EncoderDataset(\n        c,\n        ap,\n        meta_data_eval if is_val else meta_data_train,\n        voice_len=c.voice_len,\n        num_utter_per_class=num_utter_per_class,\n        num_classes_in_batch=num_classes_in_batch,",
        "detail": "env.src.tts.TTS.bin.train_encoder",
        "documentation": {}
    },
    {
        "label": "evaluation",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.train_encoder",
        "description": "env.src.tts.TTS.bin.train_encoder",
        "peekOfCode": "def evaluation(model, criterion, data_loader, global_step):\n    eval_loss = 0\n    for _, data in enumerate(data_loader):\n        with torch.no_grad():\n            # setup input data\n            inputs, labels = data\n            # agroup samples of each class in the batch. perfect sampler produces [3,2,1,3,2,1] we need [3,3,2,2,1,1]\n            labels = torch.transpose(\n                labels.view(c.eval_num_utter_per_class, c.eval_num_classes_in_batch), 0, 1\n            ).reshape(labels.shape)",
        "detail": "env.src.tts.TTS.bin.train_encoder",
        "documentation": {}
    },
    {
        "label": "train",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.train_encoder",
        "description": "env.src.tts.TTS.bin.train_encoder",
        "peekOfCode": "def train(model, optimizer, scheduler, criterion, data_loader, eval_data_loader, global_step):\n    model.train()\n    best_loss = float(\"inf\")\n    avg_loader_time = 0\n    end_time = time.time()\n    for epoch in range(c.epochs):\n        tot_loss = 0\n        epoch_time = 0\n        for _, data in enumerate(data_loader):\n            start_time = time.time()",
        "detail": "env.src.tts.TTS.bin.train_encoder",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.train_encoder",
        "description": "env.src.tts.TTS.bin.train_encoder",
        "peekOfCode": "def main(args):  # pylint: disable=redefined-outer-name\n    # pylint: disable=global-variable-undefined\n    global meta_data_train\n    global meta_data_eval\n    global train_classes\n    ap = AudioProcessor(**c.audio)\n    model = setup_encoder_model(c)\n    optimizer = get_optimizer(c.optimizer, c.optimizer_params, c.lr, model)\n    # pylint: disable=redefined-outer-name\n    meta_data_train, meta_data_eval = load_tts_samples(c.datasets, eval_split=True)",
        "detail": "env.src.tts.TTS.bin.train_encoder",
        "documentation": {}
    },
    {
        "label": "torch.backends.cudnn.enabled",
        "kind": 5,
        "importPath": "env.src.tts.TTS.bin.train_encoder",
        "description": "env.src.tts.TTS.bin.train_encoder",
        "peekOfCode": "torch.backends.cudnn.enabled = True\ntorch.backends.cudnn.benchmark = True\ntorch.manual_seed(54321)\nuse_cuda = torch.cuda.is_available()\nnum_gpus = torch.cuda.device_count()\nprint(\" > Using CUDA: \", use_cuda)\nprint(\" > Number of GPUs: \", num_gpus)\ndef setup_loader(ap: AudioProcessor, is_val: bool = False, verbose: bool = False):\n    num_utter_per_class = c.num_utter_per_class if not is_val else c.eval_num_utter_per_class\n    num_classes_in_batch = c.num_classes_in_batch if not is_val else c.eval_num_classes_in_batch",
        "detail": "env.src.tts.TTS.bin.train_encoder",
        "documentation": {}
    },
    {
        "label": "torch.backends.cudnn.benchmark",
        "kind": 5,
        "importPath": "env.src.tts.TTS.bin.train_encoder",
        "description": "env.src.tts.TTS.bin.train_encoder",
        "peekOfCode": "torch.backends.cudnn.benchmark = True\ntorch.manual_seed(54321)\nuse_cuda = torch.cuda.is_available()\nnum_gpus = torch.cuda.device_count()\nprint(\" > Using CUDA: \", use_cuda)\nprint(\" > Number of GPUs: \", num_gpus)\ndef setup_loader(ap: AudioProcessor, is_val: bool = False, verbose: bool = False):\n    num_utter_per_class = c.num_utter_per_class if not is_val else c.eval_num_utter_per_class\n    num_classes_in_batch = c.num_classes_in_batch if not is_val else c.eval_num_classes_in_batch\n    dataset = EncoderDataset(",
        "detail": "env.src.tts.TTS.bin.train_encoder",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "env.src.tts.TTS.bin.train_encoder",
        "description": "env.src.tts.TTS.bin.train_encoder",
        "peekOfCode": "use_cuda = torch.cuda.is_available()\nnum_gpus = torch.cuda.device_count()\nprint(\" > Using CUDA: \", use_cuda)\nprint(\" > Number of GPUs: \", num_gpus)\ndef setup_loader(ap: AudioProcessor, is_val: bool = False, verbose: bool = False):\n    num_utter_per_class = c.num_utter_per_class if not is_val else c.eval_num_utter_per_class\n    num_classes_in_batch = c.num_classes_in_batch if not is_val else c.eval_num_classes_in_batch\n    dataset = EncoderDataset(\n        c,\n        ap,",
        "detail": "env.src.tts.TTS.bin.train_encoder",
        "documentation": {}
    },
    {
        "label": "num_gpus",
        "kind": 5,
        "importPath": "env.src.tts.TTS.bin.train_encoder",
        "description": "env.src.tts.TTS.bin.train_encoder",
        "peekOfCode": "num_gpus = torch.cuda.device_count()\nprint(\" > Using CUDA: \", use_cuda)\nprint(\" > Number of GPUs: \", num_gpus)\ndef setup_loader(ap: AudioProcessor, is_val: bool = False, verbose: bool = False):\n    num_utter_per_class = c.num_utter_per_class if not is_val else c.eval_num_utter_per_class\n    num_classes_in_batch = c.num_classes_in_batch if not is_val else c.eval_num_classes_in_batch\n    dataset = EncoderDataset(\n        c,\n        ap,\n        meta_data_eval if is_val else meta_data_train,",
        "detail": "env.src.tts.TTS.bin.train_encoder",
        "documentation": {}
    },
    {
        "label": "TrainTTSArgs",
        "kind": 6,
        "importPath": "env.src.tts.TTS.bin.train_tts",
        "description": "env.src.tts.TTS.bin.train_tts",
        "peekOfCode": "class TrainTTSArgs(TrainerArgs):\n    config_path: str = field(default=None, metadata={\"help\": \"Path to the config file.\"})\ndef main():\n    \"\"\"Run `tts` model training directly by a `config.json` file.\"\"\"\n    # init trainer args\n    train_args = TrainTTSArgs()\n    parser = train_args.init_argparse(arg_prefix=\"\")\n    # override trainer args from comman-line args\n    args, config_overrides = parser.parse_known_args()\n    train_args.parse_args(args)",
        "detail": "env.src.tts.TTS.bin.train_tts",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.train_tts",
        "description": "env.src.tts.TTS.bin.train_tts",
        "peekOfCode": "def main():\n    \"\"\"Run `tts` model training directly by a `config.json` file.\"\"\"\n    # init trainer args\n    train_args = TrainTTSArgs()\n    parser = train_args.init_argparse(arg_prefix=\"\")\n    # override trainer args from comman-line args\n    args, config_overrides = parser.parse_known_args()\n    train_args.parse_args(args)\n    # load config.json and register\n    if args.config_path or args.continue_path:",
        "detail": "env.src.tts.TTS.bin.train_tts",
        "documentation": {}
    },
    {
        "label": "TrainVocoderArgs",
        "kind": 6,
        "importPath": "env.src.tts.TTS.bin.train_vocoder",
        "description": "env.src.tts.TTS.bin.train_vocoder",
        "peekOfCode": "class TrainVocoderArgs(TrainerArgs):\n    config_path: str = field(default=None, metadata={\"help\": \"Path to the config file.\"})\ndef main():\n    \"\"\"Run `tts` model training directly by a `config.json` file.\"\"\"\n    # init trainer args\n    train_args = TrainVocoderArgs()\n    parser = train_args.init_argparse(arg_prefix=\"\")\n    # override trainer args from comman-line args\n    args, config_overrides = parser.parse_known_args()\n    train_args.parse_args(args)",
        "detail": "env.src.tts.TTS.bin.train_vocoder",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "env.src.tts.TTS.bin.train_vocoder",
        "description": "env.src.tts.TTS.bin.train_vocoder",
        "peekOfCode": "def main():\n    \"\"\"Run `tts` model training directly by a `config.json` file.\"\"\"\n    # init trainer args\n    train_args = TrainVocoderArgs()\n    parser = train_args.init_argparse(arg_prefix=\"\")\n    # override trainer args from comman-line args\n    args, config_overrides = parser.parse_known_args()\n    train_args.parse_args(args)\n    # load config.json and register\n    if args.config_path or args.continue_path:",
        "detail": "env.src.tts.TTS.bin.train_vocoder",
        "documentation": {}
    },
    {
        "label": "BaseAudioConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.config.shared_configs",
        "description": "env.src.tts.TTS.config.shared_configs",
        "peekOfCode": "class BaseAudioConfig(Coqpit):\n    \"\"\"Base config to definge audio processing parameters. It is used to initialize\n    ```TTS.utils.audio.AudioProcessor.```\n    Args:\n        fft_size (int):\n            Number of STFT frequency levels aka.size of the linear spectogram frame. Defaults to 1024.\n        win_length (int):\n            Each frame of audio is windowed by window of length ```win_length``` and then padded with zeros to match\n            ```fft_size```. Defaults to 1024.\n        hop_length (int):",
        "detail": "env.src.tts.TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseDatasetConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.config.shared_configs",
        "description": "env.src.tts.TTS.config.shared_configs",
        "peekOfCode": "class BaseDatasetConfig(Coqpit):\n    \"\"\"Base config for TTS datasets.\n    Args:\n        formatter (str):\n            Formatter name that defines used formatter in ```TTS.tts.datasets.formatter```. Defaults to `\"\"`.\n        dataset_name (str):\n            Unique name for the dataset. Defaults to `\"\"`.\n        path (str):\n            Root path to the dataset files. Defaults to `\"\"`.\n        meta_file_train (str):",
        "detail": "env.src.tts.TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseTrainingConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.config.shared_configs",
        "description": "env.src.tts.TTS.config.shared_configs",
        "peekOfCode": "class BaseTrainingConfig(TrainerConfig):\n    \"\"\"Base config to define the basic TTS training parameters that are shared\n    among all the models. It is based on ```Trainer.TrainingConfig```.\n    Args:\n        model (str):\n            Name of the model that is used in the training.\n        num_loader_workers (int):\n            Number of workers for training time dataloader.\n        num_eval_loader_workers (int):\n            Number of workers for evaluation time dataloader.",
        "detail": "env.src.tts.TTS.config.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseEncoderConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.encoder.configs.base_encoder_config",
        "description": "env.src.tts.TTS.encoder.configs.base_encoder_config",
        "peekOfCode": "class BaseEncoderConfig(BaseTrainingConfig):\n    \"\"\"Defines parameters for a Generic Encoder model.\"\"\"\n    model: str = None\n    audio: BaseAudioConfig = field(default_factory=BaseAudioConfig)\n    datasets: List[BaseDatasetConfig] = field(default_factory=lambda: [BaseDatasetConfig()])\n    # model params\n    model_params: Dict = field(\n        default_factory=lambda: {\n            \"model_name\": \"lstm\",\n            \"input_dim\": 80,",
        "detail": "env.src.tts.TTS.encoder.configs.base_encoder_config",
        "documentation": {}
    },
    {
        "label": "EmotionEncoderConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.encoder.configs.emotion_encoder_config",
        "description": "env.src.tts.TTS.encoder.configs.emotion_encoder_config",
        "peekOfCode": "class EmotionEncoderConfig(BaseEncoderConfig):\n    \"\"\"Defines parameters for Emotion Encoder model.\"\"\"\n    model: str = \"emotion_encoder\"\n    map_classid_to_classname: dict = None\n    class_name_key: str = \"emotion_name\"",
        "detail": "env.src.tts.TTS.encoder.configs.emotion_encoder_config",
        "documentation": {}
    },
    {
        "label": "SpeakerEncoderConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.encoder.configs.speaker_encoder_config",
        "description": "env.src.tts.TTS.encoder.configs.speaker_encoder_config",
        "peekOfCode": "class SpeakerEncoderConfig(BaseEncoderConfig):\n    \"\"\"Defines parameters for Speaker Encoder model.\"\"\"\n    model: str = \"speaker_encoder\"\n    class_name_key: str = \"speaker_name\"",
        "detail": "env.src.tts.TTS.encoder.configs.speaker_encoder_config",
        "documentation": {}
    },
    {
        "label": "PreEmphasis",
        "kind": 6,
        "importPath": "env.src.tts.TTS.encoder.models.base_encoder",
        "description": "env.src.tts.TTS.encoder.models.base_encoder",
        "peekOfCode": "class PreEmphasis(nn.Module):\n    def __init__(self, coefficient=0.97):\n        super().__init__()\n        self.coefficient = coefficient\n        self.register_buffer(\"filter\", torch.FloatTensor([-self.coefficient, 1.0]).unsqueeze(0).unsqueeze(0))\n    def forward(self, x):\n        assert len(x.size()) == 2\n        x = torch.nn.functional.pad(x.unsqueeze(1), (1, 0), \"reflect\")\n        return torch.nn.functional.conv1d(x, self.filter).squeeze(1)\nclass BaseEncoder(nn.Module):",
        "detail": "env.src.tts.TTS.encoder.models.base_encoder",
        "documentation": {}
    },
    {
        "label": "BaseEncoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.encoder.models.base_encoder",
        "description": "env.src.tts.TTS.encoder.models.base_encoder",
        "peekOfCode": "class BaseEncoder(nn.Module):\n    \"\"\"Base `encoder` class. Every new `encoder` model must inherit this.\n    It defines common `encoder` specific functions.\n    \"\"\"\n    # pylint: disable=W0102\n    def __init__(self):\n        super(BaseEncoder, self).__init__()\n    def get_torch_mel_spectrogram_class(self, audio_config):\n        return torch.nn.Sequential(\n            PreEmphasis(audio_config[\"preemphasis\"]),",
        "detail": "env.src.tts.TTS.encoder.models.base_encoder",
        "documentation": {}
    },
    {
        "label": "LSTMWithProjection",
        "kind": 6,
        "importPath": "env.src.tts.TTS.encoder.models.lstm",
        "description": "env.src.tts.TTS.encoder.models.lstm",
        "peekOfCode": "class LSTMWithProjection(nn.Module):\n    def __init__(self, input_size, hidden_size, proj_size):\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.proj_size = proj_size\n        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n        self.linear = nn.Linear(hidden_size, proj_size, bias=False)\n    def forward(self, x):\n        self.lstm.flatten_parameters()",
        "detail": "env.src.tts.TTS.encoder.models.lstm",
        "documentation": {}
    },
    {
        "label": "LSTMWithoutProjection",
        "kind": 6,
        "importPath": "env.src.tts.TTS.encoder.models.lstm",
        "description": "env.src.tts.TTS.encoder.models.lstm",
        "peekOfCode": "class LSTMWithoutProjection(nn.Module):\n    def __init__(self, input_dim, lstm_dim, proj_dim, num_lstm_layers):\n        super().__init__()\n        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=lstm_dim, num_layers=num_lstm_layers, batch_first=True)\n        self.linear = nn.Linear(lstm_dim, proj_dim, bias=True)\n        self.relu = nn.ReLU()\n    def forward(self, x):\n        _, (hidden, _) = self.lstm(x)\n        return self.relu(self.linear(hidden[-1]))\nclass LSTMSpeakerEncoder(BaseEncoder):",
        "detail": "env.src.tts.TTS.encoder.models.lstm",
        "documentation": {}
    },
    {
        "label": "LSTMSpeakerEncoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.encoder.models.lstm",
        "description": "env.src.tts.TTS.encoder.models.lstm",
        "peekOfCode": "class LSTMSpeakerEncoder(BaseEncoder):\n    def __init__(\n        self,\n        input_dim,\n        proj_dim=256,\n        lstm_dim=768,\n        num_lstm_layers=3,\n        use_lstm_with_projection=True,\n        use_torch_spec=False,\n        audio_config=None,",
        "detail": "env.src.tts.TTS.encoder.models.lstm",
        "documentation": {}
    },
    {
        "label": "SELayer",
        "kind": 6,
        "importPath": "env.src.tts.TTS.encoder.models.resnet",
        "description": "env.src.tts.TTS.encoder.models.resnet",
        "peekOfCode": "class SELayer(nn.Module):\n    def __init__(self, channel, reduction=8):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction),\n            nn.ReLU(inplace=True),\n            nn.Linear(channel // reduction, channel),\n            nn.Sigmoid(),\n        )",
        "detail": "env.src.tts.TTS.encoder.models.resnet",
        "documentation": {}
    },
    {
        "label": "SEBasicBlock",
        "kind": 6,
        "importPath": "env.src.tts.TTS.encoder.models.resnet",
        "description": "env.src.tts.TTS.encoder.models.resnet",
        "peekOfCode": "class SEBasicBlock(nn.Module):\n    expansion = 1\n    def __init__(self, inplanes, planes, stride=1, downsample=None, reduction=8):\n        super(SEBasicBlock, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.se = SELayer(planes, reduction)",
        "detail": "env.src.tts.TTS.encoder.models.resnet",
        "documentation": {}
    },
    {
        "label": "ResNetSpeakerEncoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.encoder.models.resnet",
        "description": "env.src.tts.TTS.encoder.models.resnet",
        "peekOfCode": "class ResNetSpeakerEncoder(BaseEncoder):\n    \"\"\"Implementation of the model H/ASP without batch normalization in speaker embedding. This model was proposed in: https://arxiv.org/abs/2009.14153\n    Adapted from: https://github.com/clovaai/voxceleb_trainer\n    \"\"\"\n    # pylint: disable=W0102\n    def __init__(\n        self,\n        input_dim=64,\n        proj_dim=512,\n        layers=[3, 4, 6, 3],",
        "detail": "env.src.tts.TTS.encoder.models.resnet",
        "documentation": {}
    },
    {
        "label": "AugmentWAV",
        "kind": 6,
        "importPath": "env.src.tts.TTS.encoder.utils.generic_utils",
        "description": "env.src.tts.TTS.encoder.utils.generic_utils",
        "peekOfCode": "class AugmentWAV(object):\n    def __init__(self, ap, augmentation_config):\n        self.ap = ap\n        self.use_additive_noise = False\n        if \"additive\" in augmentation_config.keys():\n            self.additive_noise_config = augmentation_config[\"additive\"]\n            additive_path = self.additive_noise_config[\"sounds_path\"]\n            if additive_path:\n                self.use_additive_noise = True\n                # get noise types",
        "detail": "env.src.tts.TTS.encoder.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "to_camel",
        "kind": 2,
        "importPath": "env.src.tts.TTS.encoder.utils.generic_utils",
        "description": "env.src.tts.TTS.encoder.utils.generic_utils",
        "peekOfCode": "def to_camel(text):\n    text = text.capitalize()\n    return re.sub(r\"(?!^)_([a-zA-Z])\", lambda m: m.group(1).upper(), text)\ndef setup_encoder_model(config: \"Coqpit\"):\n    if config.model_params[\"model_name\"].lower() == \"lstm\":\n        model = LSTMSpeakerEncoder(\n            config.model_params[\"input_dim\"],\n            config.model_params[\"proj_dim\"],\n            config.model_params[\"lstm_dim\"],\n            config.model_params[\"num_lstm_layers\"],",
        "detail": "env.src.tts.TTS.encoder.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "setup_encoder_model",
        "kind": 2,
        "importPath": "env.src.tts.TTS.encoder.utils.generic_utils",
        "description": "env.src.tts.TTS.encoder.utils.generic_utils",
        "peekOfCode": "def setup_encoder_model(config: \"Coqpit\"):\n    if config.model_params[\"model_name\"].lower() == \"lstm\":\n        model = LSTMSpeakerEncoder(\n            config.model_params[\"input_dim\"],\n            config.model_params[\"proj_dim\"],\n            config.model_params[\"lstm_dim\"],\n            config.model_params[\"num_lstm_layers\"],\n            use_torch_spec=config.model_params.get(\"use_torch_spec\", False),\n            audio_config=config.audio,\n        )",
        "detail": "env.src.tts.TTS.encoder.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "save_checkpoint",
        "kind": 2,
        "importPath": "env.src.tts.TTS.encoder.utils.generic_utils",
        "description": "env.src.tts.TTS.encoder.utils.generic_utils",
        "peekOfCode": "def save_checkpoint(model, optimizer, criterion, model_loss, out_path, current_step, epoch):\n    checkpoint_path = \"checkpoint_{}.pth\".format(current_step)\n    checkpoint_path = os.path.join(out_path, checkpoint_path)\n    print(\" | | > Checkpoint saving : {}\".format(checkpoint_path))\n    new_state_dict = model.state_dict()\n    state = {\n        \"model\": new_state_dict,\n        \"optimizer\": optimizer.state_dict() if optimizer is not None else None,\n        \"criterion\": criterion.state_dict(),\n        \"step\": current_step,",
        "detail": "env.src.tts.TTS.encoder.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "save_best_model",
        "kind": 2,
        "importPath": "env.src.tts.TTS.encoder.utils.generic_utils",
        "description": "env.src.tts.TTS.encoder.utils.generic_utils",
        "peekOfCode": "def save_best_model(model, optimizer, criterion, model_loss, best_loss, out_path, current_step, epoch):\n    if model_loss < best_loss:\n        new_state_dict = model.state_dict()\n        state = {\n            \"model\": new_state_dict,\n            \"optimizer\": optimizer.state_dict(),\n            \"criterion\": criterion.state_dict(),\n            \"step\": current_step,\n            \"epoch\": epoch,\n            \"loss\": model_loss,",
        "detail": "env.src.tts.TTS.encoder.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "save_checkpoint",
        "kind": 2,
        "importPath": "env.src.tts.TTS.encoder.utils.io",
        "description": "env.src.tts.TTS.encoder.utils.io",
        "peekOfCode": "def save_checkpoint(model, optimizer, model_loss, out_path, current_step):\n    checkpoint_path = \"checkpoint_{}.pth\".format(current_step)\n    checkpoint_path = os.path.join(out_path, checkpoint_path)\n    print(\" | | > Checkpoint saving : {}\".format(checkpoint_path))\n    new_state_dict = model.state_dict()\n    state = {\n        \"model\": new_state_dict,\n        \"optimizer\": optimizer.state_dict() if optimizer is not None else None,\n        \"step\": current_step,\n        \"loss\": model_loss,",
        "detail": "env.src.tts.TTS.encoder.utils.io",
        "documentation": {}
    },
    {
        "label": "save_best_model",
        "kind": 2,
        "importPath": "env.src.tts.TTS.encoder.utils.io",
        "description": "env.src.tts.TTS.encoder.utils.io",
        "peekOfCode": "def save_best_model(model, optimizer, model_loss, best_loss, out_path, current_step):\n    if model_loss < best_loss:\n        new_state_dict = model.state_dict()\n        state = {\n            \"model\": new_state_dict,\n            \"optimizer\": optimizer.state_dict(),\n            \"step\": current_step,\n            \"loss\": model_loss,\n            \"date\": datetime.date.today().strftime(\"%B %d, %Y\"),\n        }",
        "detail": "env.src.tts.TTS.encoder.utils.io",
        "documentation": {}
    },
    {
        "label": "download_and_extract",
        "kind": 2,
        "importPath": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "description": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "peekOfCode": "def download_and_extract(directory, subset, urls):\n    \"\"\"Download and extract the given split of dataset.\n    Args:\n        directory: the directory where to put the downloaded data.\n        subset: subset name of the corpus.\n        urls: the list of urls to download the data file.\n    \"\"\"\n    os.makedirs(directory, exist_ok=True)\n    try:\n        for url in urls:",
        "detail": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "documentation": {}
    },
    {
        "label": "exec_cmd",
        "kind": 2,
        "importPath": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "description": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "peekOfCode": "def exec_cmd(cmd):\n    \"\"\"Run a command in a subprocess.\n    Args:\n        cmd: command line to be executed.\n    Return:\n        int, the return code.\n    \"\"\"\n    try:\n        retcode = subprocess.call(cmd, shell=True)\n        if retcode < 0:",
        "detail": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "documentation": {}
    },
    {
        "label": "decode_aac_with_ffmpeg",
        "kind": 2,
        "importPath": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "description": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "peekOfCode": "def decode_aac_with_ffmpeg(aac_file, wav_file):\n    \"\"\"Decode a given AAC file into WAV using ffmpeg.\n    Args:\n        aac_file: file path to input AAC file.\n        wav_file: file path to output WAV file.\n    Return:\n        bool, True if success.\n    \"\"\"\n    cmd = f\"ffmpeg -i {aac_file} {wav_file}\"\n    logging.info(f\"Decoding aac file using command line: {cmd}\")",
        "detail": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "documentation": {}
    },
    {
        "label": "convert_audio_and_make_label",
        "kind": 2,
        "importPath": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "description": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "peekOfCode": "def convert_audio_and_make_label(input_dir, subset, output_dir, output_file):\n    \"\"\"Optionally convert AAC to WAV and make speaker labels.\n    Args:\n        input_dir: the directory which holds the input dataset.\n        subset: the name of the specified subset. e.g. vox1_dev_wav\n        output_dir: the directory to place the newly generated csv files.\n        output_file: the name of the newly generated csv file. e.g. vox1_dev_wav.csv\n    \"\"\"\n    logging.info(\"Preprocessing audio and label for subset %s\" % subset)\n    source_dir = os.path.join(input_dir, subset)",
        "detail": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "documentation": {}
    },
    {
        "label": "processor",
        "kind": 2,
        "importPath": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "description": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "peekOfCode": "def processor(directory, subset, force_process):\n    \"\"\"download and process\"\"\"\n    urls = SUBSETS\n    if subset not in urls:\n        raise ValueError(subset, \"is not in voxceleb\")\n    subset_csv = os.path.join(directory, subset + \".csv\")\n    if not force_process and os.path.exists(subset_csv):\n        return subset_csv\n    logging.info(\"Downloading and process the voxceleb in %s\", directory)\n    logging.info(\"Preparing subset %s\", subset)",
        "detail": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "documentation": {}
    },
    {
        "label": "SUBSETS",
        "kind": 5,
        "importPath": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "description": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "peekOfCode": "SUBSETS = {\n    \"vox1_dev_wav\": [\n        \"https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partaa\",\n        \"https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partab\",\n        \"https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partac\",\n        \"https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_dev_wav_partad\",\n    ],\n    \"vox1_test_wav\": [\"https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox1_test_wav.zip\"],\n    \"vox2_dev_aac\": [\n        \"https://thor.robots.ox.ac.uk/~vgg/data/voxceleb/vox1a/vox2_dev_aac_partaa\",",
        "detail": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "documentation": {}
    },
    {
        "label": "MD5SUM",
        "kind": 5,
        "importPath": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "description": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "peekOfCode": "MD5SUM = {\n    \"vox1_dev_wav\": \"ae63e55b951748cc486645f532ba230b\",\n    \"vox2_dev_aac\": \"bbc063c46078a602ca71605645c2a402\",\n    \"vox1_test_wav\": \"185fdc63c3c739954633d50379a3d102\",\n    \"vox2_test_aac\": \"0d2b3ea430a821c33263b5ea37ede312\",\n}\nUSER = {\"user\": \"\", \"password\": \"\"}\nspeaker_id_dict = {}\ndef download_and_extract(directory, subset, urls):\n    \"\"\"Download and extract the given split of dataset.",
        "detail": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "documentation": {}
    },
    {
        "label": "USER",
        "kind": 5,
        "importPath": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "description": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "peekOfCode": "USER = {\"user\": \"\", \"password\": \"\"}\nspeaker_id_dict = {}\ndef download_and_extract(directory, subset, urls):\n    \"\"\"Download and extract the given split of dataset.\n    Args:\n        directory: the directory where to put the downloaded data.\n        subset: subset name of the corpus.\n        urls: the list of urls to download the data file.\n    \"\"\"\n    os.makedirs(directory, exist_ok=True)",
        "detail": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "documentation": {}
    },
    {
        "label": "speaker_id_dict",
        "kind": 5,
        "importPath": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "description": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "peekOfCode": "speaker_id_dict = {}\ndef download_and_extract(directory, subset, urls):\n    \"\"\"Download and extract the given split of dataset.\n    Args:\n        directory: the directory where to put the downloaded data.\n        subset: subset name of the corpus.\n        urls: the list of urls to download the data file.\n    \"\"\"\n    os.makedirs(directory, exist_ok=True)\n    try:",
        "detail": "env.src.tts.TTS.encoder.utils.prepare_voxceleb",
        "documentation": {}
    },
    {
        "label": "TrainArgs",
        "kind": 6,
        "importPath": "env.src.tts.TTS.encoder.utils.training",
        "description": "env.src.tts.TTS.encoder.utils.training",
        "peekOfCode": "class TrainArgs(TrainerArgs):\n    config_path: str = field(default=None, metadata={\"help\": \"Path to the config file.\"})\ndef getarguments():\n    train_config = TrainArgs()\n    parser = train_config.init_argparse(arg_prefix=\"\")\n    return parser\ndef process_args(args, config=None):\n    \"\"\"Process parsed comand line arguments and initialize the config if not provided.\n    Args:\n        args (argparse.Namespace or dict like): Parsed input arguments.",
        "detail": "env.src.tts.TTS.encoder.utils.training",
        "documentation": {}
    },
    {
        "label": "getarguments",
        "kind": 2,
        "importPath": "env.src.tts.TTS.encoder.utils.training",
        "description": "env.src.tts.TTS.encoder.utils.training",
        "peekOfCode": "def getarguments():\n    train_config = TrainArgs()\n    parser = train_config.init_argparse(arg_prefix=\"\")\n    return parser\ndef process_args(args, config=None):\n    \"\"\"Process parsed comand line arguments and initialize the config if not provided.\n    Args:\n        args (argparse.Namespace or dict like): Parsed input arguments.\n        config (Coqpit): Model config. If none, it is generated from `args`. Defaults to None.\n    Returns:",
        "detail": "env.src.tts.TTS.encoder.utils.training",
        "documentation": {}
    },
    {
        "label": "process_args",
        "kind": 2,
        "importPath": "env.src.tts.TTS.encoder.utils.training",
        "description": "env.src.tts.TTS.encoder.utils.training",
        "peekOfCode": "def process_args(args, config=None):\n    \"\"\"Process parsed comand line arguments and initialize the config if not provided.\n    Args:\n        args (argparse.Namespace or dict like): Parsed input arguments.\n        config (Coqpit): Model config. If none, it is generated from `args`. Defaults to None.\n    Returns:\n        c (TTS.utils.io.AttrDict): Config paramaters.\n        out_path (str): Path to save models and logging.\n        audio_path (str): Path to save generated test audios.\n        c_logger (TTS.utils.console_logger.ConsoleLogger): Class that does",
        "detail": "env.src.tts.TTS.encoder.utils.training",
        "documentation": {}
    },
    {
        "label": "init_arguments",
        "kind": 2,
        "importPath": "env.src.tts.TTS.encoder.utils.training",
        "description": "env.src.tts.TTS.encoder.utils.training",
        "peekOfCode": "def init_arguments():\n    train_config = TrainArgs()\n    parser = train_config.init_argparse(arg_prefix=\"\")\n    return parser\ndef init_training(config: Coqpit = None):\n    \"\"\"Initialization of a training run.\"\"\"\n    parser = init_arguments()\n    args = parser.parse_known_args()\n    config, OUT_PATH, AUDIO_PATH, c_logger, dashboard_logger = process_args(args, config)\n    return args[0], config, OUT_PATH, AUDIO_PATH, c_logger, dashboard_logger",
        "detail": "env.src.tts.TTS.encoder.utils.training",
        "documentation": {}
    },
    {
        "label": "init_training",
        "kind": 2,
        "importPath": "env.src.tts.TTS.encoder.utils.training",
        "description": "env.src.tts.TTS.encoder.utils.training",
        "peekOfCode": "def init_training(config: Coqpit = None):\n    \"\"\"Initialization of a training run.\"\"\"\n    parser = init_arguments()\n    args = parser.parse_known_args()\n    config, OUT_PATH, AUDIO_PATH, c_logger, dashboard_logger = process_args(args, config)\n    return args[0], config, OUT_PATH, AUDIO_PATH, c_logger, dashboard_logger",
        "detail": "env.src.tts.TTS.encoder.utils.training",
        "documentation": {}
    },
    {
        "label": "plot_embeddings",
        "kind": 2,
        "importPath": "env.src.tts.TTS.encoder.utils.visual",
        "description": "env.src.tts.TTS.encoder.utils.visual",
        "peekOfCode": "def plot_embeddings(embeddings, num_classes_in_batch):\n    num_utter_per_class = embeddings.shape[0] // num_classes_in_batch\n    # if necessary get just the first 10 classes\n    if num_classes_in_batch > 10:\n        num_classes_in_batch = 10\n        embeddings = embeddings[: num_classes_in_batch * num_utter_per_class]\n    model = umap.UMAP()\n    projection = model.fit_transform(embeddings)\n    ground_truth = np.repeat(np.arange(num_classes_in_batch), num_utter_per_class)\n    colors = [colormap[i] for i in ground_truth]",
        "detail": "env.src.tts.TTS.encoder.utils.visual",
        "documentation": {}
    },
    {
        "label": "colormap",
        "kind": 5,
        "importPath": "env.src.tts.TTS.encoder.utils.visual",
        "description": "env.src.tts.TTS.encoder.utils.visual",
        "peekOfCode": "colormap = (\n    np.array(\n        [\n            [76, 255, 0],\n            [0, 127, 70],\n            [255, 0, 0],\n            [255, 217, 38],\n            [0, 135, 255],\n            [165, 0, 165],\n            [255, 167, 255],",
        "detail": "env.src.tts.TTS.encoder.utils.visual",
        "documentation": {}
    },
    {
        "label": "EncoderDataset",
        "kind": 6,
        "importPath": "env.src.tts.TTS.encoder.dataset",
        "description": "env.src.tts.TTS.encoder.dataset",
        "peekOfCode": "class EncoderDataset(Dataset):\n    def __init__(\n        self,\n        config,\n        ap,\n        meta_data,\n        voice_len=1.6,\n        num_classes_in_batch=64,\n        num_utter_per_class=10,\n        verbose=False,",
        "detail": "env.src.tts.TTS.encoder.dataset",
        "documentation": {}
    },
    {
        "label": "GE2ELoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.encoder.losses",
        "description": "env.src.tts.TTS.encoder.losses",
        "peekOfCode": "class GE2ELoss(nn.Module):\n    def __init__(self, init_w=10.0, init_b=-5.0, loss_method=\"softmax\"):\n        \"\"\"\n        Implementation of the Generalized End-to-End loss defined in https://arxiv.org/abs/1710.10467 [1]\n        Accepts an input of size (N, M, D)\n            where N is the number of speakers in the batch,\n            M is the number of utterances per speaker,\n            and D is the dimensionality of the embedding vector (e.g. d-vector)\n        Args:\n            - init_w (float): defines the initial value of w in Equation (5) of [1]",
        "detail": "env.src.tts.TTS.encoder.losses",
        "documentation": {}
    },
    {
        "label": "AngleProtoLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.encoder.losses",
        "description": "env.src.tts.TTS.encoder.losses",
        "peekOfCode": "class AngleProtoLoss(nn.Module):\n    \"\"\"\n    Implementation of the Angular Prototypical loss defined in https://arxiv.org/abs/2003.11982\n        Accepts an input of size (N, M, D)\n            where N is the number of speakers in the batch,\n            M is the number of utterances per speaker,\n            and D is the dimensionality of the embedding vector\n        Args:\n            - init_w (float): defines the initial value of w\n            - init_b (float): definies the initial value of b",
        "detail": "env.src.tts.TTS.encoder.losses",
        "documentation": {}
    },
    {
        "label": "SoftmaxLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.encoder.losses",
        "description": "env.src.tts.TTS.encoder.losses",
        "peekOfCode": "class SoftmaxLoss(nn.Module):\n    \"\"\"\n    Implementation of the Softmax loss as defined in https://arxiv.org/abs/2003.11982\n        Args:\n            - embedding_dim (float): speaker embedding dim\n            - n_speakers (float): number of speakers\n    \"\"\"\n    def __init__(self, embedding_dim, n_speakers):\n        super().__init__()\n        self.criterion = torch.nn.CrossEntropyLoss()",
        "detail": "env.src.tts.TTS.encoder.losses",
        "documentation": {}
    },
    {
        "label": "SoftmaxAngleProtoLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.encoder.losses",
        "description": "env.src.tts.TTS.encoder.losses",
        "peekOfCode": "class SoftmaxAngleProtoLoss(nn.Module):\n    \"\"\"\n    Implementation of the Softmax AnglePrototypical loss as defined in https://arxiv.org/abs/2009.14153\n        Args:\n            - embedding_dim (float): speaker embedding dim\n            - n_speakers (float): number of speakers\n            - init_w (float): defines the initial value of w\n            - init_b (float): definies the initial value of b\n    \"\"\"\n    def __init__(self, embedding_dim, n_speakers, init_w=10.0, init_b=-5.0):",
        "detail": "env.src.tts.TTS.encoder.losses",
        "documentation": {}
    },
    {
        "label": "create_argparser",
        "kind": 2,
        "importPath": "env.src.tts.TTS.server.server",
        "description": "env.src.tts.TTS.server.server",
        "peekOfCode": "def create_argparser():\n    def convert_boolean(x):\n        return x.lower() in [\"true\", \"1\", \"yes\"]\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--list_models\",\n        type=convert_boolean,\n        nargs=\"?\",\n        const=True,\n        default=False,",
        "detail": "env.src.tts.TTS.server.server",
        "documentation": {}
    },
    {
        "label": "style_wav_uri_to_dict",
        "kind": 2,
        "importPath": "env.src.tts.TTS.server.server",
        "description": "env.src.tts.TTS.server.server",
        "peekOfCode": "def style_wav_uri_to_dict(style_wav: str) -> Union[str, dict]:\n    \"\"\"Transform an uri style_wav, in either a string (path to wav file to be use for style transfer)\n    or a dict (gst tokens/values to be use for styling)\n    Args:\n        style_wav (str): uri\n    Returns:\n        Union[str, dict]: path to file (str) or gst style (dict)\n    \"\"\"\n    if style_wav:\n        if os.path.isfile(style_wav) and style_wav.endswith(\".wav\"):",
        "detail": "env.src.tts.TTS.server.server",
        "documentation": {}
    },
    {
        "label": "index",
        "kind": 2,
        "importPath": "env.src.tts.TTS.server.server",
        "description": "env.src.tts.TTS.server.server",
        "peekOfCode": "def index():\n    return render_template(\n        \"index.html\",\n        show_details=args.show_details,\n        use_multi_speaker=use_multi_speaker,\n        use_multi_language=use_multi_language,\n        speaker_ids=speaker_manager.name_to_id if speaker_manager is not None else None,\n        language_ids=language_manager.name_to_id if language_manager is not None else None,\n        use_gst=use_gst,\n    )",
        "detail": "env.src.tts.TTS.server.server",
        "documentation": {}
    },
    {
        "label": "details",
        "kind": 2,
        "importPath": "env.src.tts.TTS.server.server",
        "description": "env.src.tts.TTS.server.server",
        "peekOfCode": "def details():\n    model_config = load_config(args.tts_config)\n    if args.vocoder_config is not None and os.path.isfile(args.vocoder_config):\n        vocoder_config = load_config(args.vocoder_config)\n    else:\n        vocoder_config = None\n    return render_template(\n        \"details.html\",\n        show_details=args.show_details,\n        model_config=model_config,",
        "detail": "env.src.tts.TTS.server.server",
        "documentation": {}
    },
    {
        "label": "tts",
        "kind": 2,
        "importPath": "env.src.tts.TTS.server.server",
        "description": "env.src.tts.TTS.server.server",
        "peekOfCode": "def tts():\n    with lock:\n        text = request.args.get(\"text\")\n        speaker_idx = request.args.get(\"speaker_id\", \"\")\n        language_idx = request.args.get(\"language_id\", \"\")\n        style_wav = request.args.get(\"style_wav\", \"\")\n        style_wav = style_wav_uri_to_dict(style_wav)\n        print(f\" > Model input: {text}\")\n        print(f\" > Speaker Idx: {speaker_idx}\")\n        print(f\" > Language Idx: {language_idx}\")",
        "detail": "env.src.tts.TTS.server.server",
        "documentation": {}
    },
    {
        "label": "mary_tts_api_locales",
        "kind": 2,
        "importPath": "env.src.tts.TTS.server.server",
        "description": "env.src.tts.TTS.server.server",
        "peekOfCode": "def mary_tts_api_locales():\n    \"\"\"MaryTTS-compatible /locales endpoint\"\"\"\n    # NOTE: We currently assume there is only one model active at the same time\n    if args.model_name is not None:\n        model_details = args.model_name.split(\"/\")\n    else:\n        model_details = [\"\", \"en\", \"\", \"default\"]\n    return render_template_string(\"{{ locale }}\\n\", locale=model_details[1])\n@app.route(\"/voices\", methods=[\"GET\"])\ndef mary_tts_api_voices():",
        "detail": "env.src.tts.TTS.server.server",
        "documentation": {}
    },
    {
        "label": "mary_tts_api_voices",
        "kind": 2,
        "importPath": "env.src.tts.TTS.server.server",
        "description": "env.src.tts.TTS.server.server",
        "peekOfCode": "def mary_tts_api_voices():\n    \"\"\"MaryTTS-compatible /voices endpoint\"\"\"\n    # NOTE: We currently assume there is only one model active at the same time\n    if args.model_name is not None:\n        model_details = args.model_name.split(\"/\")\n    else:\n        model_details = [\"\", \"en\", \"\", \"default\"]\n    return render_template_string(\n        \"{{ name }} {{ locale }} {{ gender }}\\n\", name=model_details[3], locale=model_details[1], gender=\"u\"\n    )",
        "detail": "env.src.tts.TTS.server.server",
        "documentation": {}
    },
    {
        "label": "mary_tts_api_process",
        "kind": 2,
        "importPath": "env.src.tts.TTS.server.server",
        "description": "env.src.tts.TTS.server.server",
        "peekOfCode": "def mary_tts_api_process():\n    \"\"\"MaryTTS-compatible /process endpoint\"\"\"\n    with lock:\n        if request.method == \"POST\":\n            data = parse_qs(request.get_data(as_text=True))\n            # NOTE: we ignore param. LOCALE and VOICE for now since we have only one active model\n            text = data.get(\"INPUT_TEXT\", [\"\"])[0]\n        else:\n            text = request.args.get(\"INPUT_TEXT\", \"\")\n        print(f\" > Model input: {text}\")",
        "detail": "env.src.tts.TTS.server.server",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "env.src.tts.TTS.server.server",
        "description": "env.src.tts.TTS.server.server",
        "peekOfCode": "def main():\n    app.run(debug=args.debug, host=\"::\", port=args.port)\nif __name__ == \"__main__\":\n    main()",
        "detail": "env.src.tts.TTS.server.server",
        "documentation": {}
    },
    {
        "label": "args",
        "kind": 5,
        "importPath": "env.src.tts.TTS.server.server",
        "description": "env.src.tts.TTS.server.server",
        "peekOfCode": "args = create_argparser().parse_args()\npath = Path(__file__).parent / \"../.models.json\"\nmanager = ModelManager(path)\nif args.list_models:\n    manager.list_models()\n    sys.exit()\n# update in-use models to the specified released models.\nmodel_path = None\nconfig_path = None\nspeakers_file_path = None",
        "detail": "env.src.tts.TTS.server.server",
        "documentation": {}
    },
    {
        "label": "path",
        "kind": 5,
        "importPath": "env.src.tts.TTS.server.server",
        "description": "env.src.tts.TTS.server.server",
        "peekOfCode": "path = Path(__file__).parent / \"../.models.json\"\nmanager = ModelManager(path)\nif args.list_models:\n    manager.list_models()\n    sys.exit()\n# update in-use models to the specified released models.\nmodel_path = None\nconfig_path = None\nspeakers_file_path = None\nvocoder_path = None",
        "detail": "env.src.tts.TTS.server.server",
        "documentation": {}
    },
    {
        "label": "manager",
        "kind": 5,
        "importPath": "env.src.tts.TTS.server.server",
        "description": "env.src.tts.TTS.server.server",
        "peekOfCode": "manager = ModelManager(path)\nif args.list_models:\n    manager.list_models()\n    sys.exit()\n# update in-use models to the specified released models.\nmodel_path = None\nconfig_path = None\nspeakers_file_path = None\nvocoder_path = None\nvocoder_config_path = None",
        "detail": "env.src.tts.TTS.server.server",
        "documentation": {}
    },
    {
        "label": "model_path",
        "kind": 5,
        "importPath": "env.src.tts.TTS.server.server",
        "description": "env.src.tts.TTS.server.server",
        "peekOfCode": "model_path = None\nconfig_path = None\nspeakers_file_path = None\nvocoder_path = None\nvocoder_config_path = None\n# CASE1: list pre-trained TTS models\nif args.list_models:\n    manager.list_models()\n    sys.exit()\n# CASE2: load pre-trained model paths",
        "detail": "env.src.tts.TTS.server.server",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.TTS.server.server",
        "description": "env.src.tts.TTS.server.server",
        "peekOfCode": "config_path = None\nspeakers_file_path = None\nvocoder_path = None\nvocoder_config_path = None\n# CASE1: list pre-trained TTS models\nif args.list_models:\n    manager.list_models()\n    sys.exit()\n# CASE2: load pre-trained model paths\nif args.model_name is not None and not args.model_path:",
        "detail": "env.src.tts.TTS.server.server",
        "documentation": {}
    },
    {
        "label": "speakers_file_path",
        "kind": 5,
        "importPath": "env.src.tts.TTS.server.server",
        "description": "env.src.tts.TTS.server.server",
        "peekOfCode": "speakers_file_path = None\nvocoder_path = None\nvocoder_config_path = None\n# CASE1: list pre-trained TTS models\nif args.list_models:\n    manager.list_models()\n    sys.exit()\n# CASE2: load pre-trained model paths\nif args.model_name is not None and not args.model_path:\n    model_path, config_path, model_item = manager.download_model(args.model_name)",
        "detail": "env.src.tts.TTS.server.server",
        "documentation": {}
    },
    {
        "label": "vocoder_path",
        "kind": 5,
        "importPath": "env.src.tts.TTS.server.server",
        "description": "env.src.tts.TTS.server.server",
        "peekOfCode": "vocoder_path = None\nvocoder_config_path = None\n# CASE1: list pre-trained TTS models\nif args.list_models:\n    manager.list_models()\n    sys.exit()\n# CASE2: load pre-trained model paths\nif args.model_name is not None and not args.model_path:\n    model_path, config_path, model_item = manager.download_model(args.model_name)\n    args.vocoder_name = model_item[\"default_vocoder\"] if args.vocoder_name is None else args.vocoder_name",
        "detail": "env.src.tts.TTS.server.server",
        "documentation": {}
    },
    {
        "label": "vocoder_config_path",
        "kind": 5,
        "importPath": "env.src.tts.TTS.server.server",
        "description": "env.src.tts.TTS.server.server",
        "peekOfCode": "vocoder_config_path = None\n# CASE1: list pre-trained TTS models\nif args.list_models:\n    manager.list_models()\n    sys.exit()\n# CASE2: load pre-trained model paths\nif args.model_name is not None and not args.model_path:\n    model_path, config_path, model_item = manager.download_model(args.model_name)\n    args.vocoder_name = model_item[\"default_vocoder\"] if args.vocoder_name is None else args.vocoder_name\nif args.vocoder_name is not None and not args.vocoder_path:",
        "detail": "env.src.tts.TTS.server.server",
        "documentation": {}
    },
    {
        "label": "synthesizer",
        "kind": 5,
        "importPath": "env.src.tts.TTS.server.server",
        "description": "env.src.tts.TTS.server.server",
        "peekOfCode": "synthesizer = Synthesizer(\n    tts_checkpoint=model_path,\n    tts_config_path=config_path,\n    tts_speakers_file=speakers_file_path,\n    tts_languages_file=None,\n    vocoder_checkpoint=vocoder_path,\n    vocoder_config=vocoder_config_path,\n    encoder_checkpoint=\"\",\n    encoder_config=\"\",\n    use_cuda=args.use_cuda,",
        "detail": "env.src.tts.TTS.server.server",
        "documentation": {}
    },
    {
        "label": "use_multi_speaker",
        "kind": 5,
        "importPath": "env.src.tts.TTS.server.server",
        "description": "env.src.tts.TTS.server.server",
        "peekOfCode": "use_multi_speaker = hasattr(synthesizer.tts_model, \"num_speakers\") and (\n    synthesizer.tts_model.num_speakers > 1 or synthesizer.tts_speakers_file is not None\n)\nspeaker_manager = getattr(synthesizer.tts_model, \"speaker_manager\", None)\nuse_multi_language = hasattr(synthesizer.tts_model, \"num_languages\") and (\n    synthesizer.tts_model.num_languages > 1 or synthesizer.tts_languages_file is not None\n)\nlanguage_manager = getattr(synthesizer.tts_model, \"language_manager\", None)\n# TODO: set this from SpeakerManager\nuse_gst = synthesizer.tts_config.get(\"use_gst\", False)",
        "detail": "env.src.tts.TTS.server.server",
        "documentation": {}
    },
    {
        "label": "speaker_manager",
        "kind": 5,
        "importPath": "env.src.tts.TTS.server.server",
        "description": "env.src.tts.TTS.server.server",
        "peekOfCode": "speaker_manager = getattr(synthesizer.tts_model, \"speaker_manager\", None)\nuse_multi_language = hasattr(synthesizer.tts_model, \"num_languages\") and (\n    synthesizer.tts_model.num_languages > 1 or synthesizer.tts_languages_file is not None\n)\nlanguage_manager = getattr(synthesizer.tts_model, \"language_manager\", None)\n# TODO: set this from SpeakerManager\nuse_gst = synthesizer.tts_config.get(\"use_gst\", False)\napp = Flask(__name__)\ndef style_wav_uri_to_dict(style_wav: str) -> Union[str, dict]:\n    \"\"\"Transform an uri style_wav, in either a string (path to wav file to be use for style transfer)",
        "detail": "env.src.tts.TTS.server.server",
        "documentation": {}
    },
    {
        "label": "use_multi_language",
        "kind": 5,
        "importPath": "env.src.tts.TTS.server.server",
        "description": "env.src.tts.TTS.server.server",
        "peekOfCode": "use_multi_language = hasattr(synthesizer.tts_model, \"num_languages\") and (\n    synthesizer.tts_model.num_languages > 1 or synthesizer.tts_languages_file is not None\n)\nlanguage_manager = getattr(synthesizer.tts_model, \"language_manager\", None)\n# TODO: set this from SpeakerManager\nuse_gst = synthesizer.tts_config.get(\"use_gst\", False)\napp = Flask(__name__)\ndef style_wav_uri_to_dict(style_wav: str) -> Union[str, dict]:\n    \"\"\"Transform an uri style_wav, in either a string (path to wav file to be use for style transfer)\n    or a dict (gst tokens/values to be use for styling)",
        "detail": "env.src.tts.TTS.server.server",
        "documentation": {}
    },
    {
        "label": "language_manager",
        "kind": 5,
        "importPath": "env.src.tts.TTS.server.server",
        "description": "env.src.tts.TTS.server.server",
        "peekOfCode": "language_manager = getattr(synthesizer.tts_model, \"language_manager\", None)\n# TODO: set this from SpeakerManager\nuse_gst = synthesizer.tts_config.get(\"use_gst\", False)\napp = Flask(__name__)\ndef style_wav_uri_to_dict(style_wav: str) -> Union[str, dict]:\n    \"\"\"Transform an uri style_wav, in either a string (path to wav file to be use for style transfer)\n    or a dict (gst tokens/values to be use for styling)\n    Args:\n        style_wav (str): uri\n    Returns:",
        "detail": "env.src.tts.TTS.server.server",
        "documentation": {}
    },
    {
        "label": "use_gst",
        "kind": 5,
        "importPath": "env.src.tts.TTS.server.server",
        "description": "env.src.tts.TTS.server.server",
        "peekOfCode": "use_gst = synthesizer.tts_config.get(\"use_gst\", False)\napp = Flask(__name__)\ndef style_wav_uri_to_dict(style_wav: str) -> Union[str, dict]:\n    \"\"\"Transform an uri style_wav, in either a string (path to wav file to be use for style transfer)\n    or a dict (gst tokens/values to be use for styling)\n    Args:\n        style_wav (str): uri\n    Returns:\n        Union[str, dict]: path to file (str) or gst style (dict)\n    \"\"\"",
        "detail": "env.src.tts.TTS.server.server",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "env.src.tts.TTS.server.server",
        "description": "env.src.tts.TTS.server.server",
        "peekOfCode": "app = Flask(__name__)\ndef style_wav_uri_to_dict(style_wav: str) -> Union[str, dict]:\n    \"\"\"Transform an uri style_wav, in either a string (path to wav file to be use for style transfer)\n    or a dict (gst tokens/values to be use for styling)\n    Args:\n        style_wav (str): uri\n    Returns:\n        Union[str, dict]: path to file (str) or gst style (dict)\n    \"\"\"\n    if style_wav:",
        "detail": "env.src.tts.TTS.server.server",
        "documentation": {}
    },
    {
        "label": "lock",
        "kind": 5,
        "importPath": "env.src.tts.TTS.server.server",
        "description": "env.src.tts.TTS.server.server",
        "peekOfCode": "lock = Lock()\n@app.route(\"/api/tts\", methods=[\"GET\"])\ndef tts():\n    with lock:\n        text = request.args.get(\"text\")\n        speaker_idx = request.args.get(\"speaker_id\", \"\")\n        language_idx = request.args.get(\"language_id\", \"\")\n        style_wav = request.args.get(\"style_wav\", \"\")\n        style_wav = style_wav_uri_to_dict(style_wav)\n        print(f\" > Model input: {text}\")",
        "detail": "env.src.tts.TTS.server.server",
        "documentation": {}
    },
    {
        "label": "AlignTTSConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.configs.align_tts_config",
        "description": "env.src.tts.TTS.tts.configs.align_tts_config",
        "peekOfCode": "class AlignTTSConfig(BaseTTSConfig):\n    \"\"\"Defines parameters for AlignTTS model.\n    Example:\n        >>> from TTS.tts.configs.align_tts_config import AlignTTSConfig\n        >>> config = AlignTTSConfig()\n    Args:\n        model(str):\n            Model name used for selecting the right model at initialization. Defaults to `align_tts`.\n        positional_encoding (bool):\n            enable / disable positional encoding applied to the encoder output. Defaults to True.",
        "detail": "env.src.tts.TTS.tts.configs.align_tts_config",
        "documentation": {}
    },
    {
        "label": "FastPitchConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.configs.fast_pitch_config",
        "description": "env.src.tts.TTS.tts.configs.fast_pitch_config",
        "peekOfCode": "class FastPitchConfig(BaseTTSConfig):\n    \"\"\"Configure `ForwardTTS` as FastPitch model.\n    Example:\n        >>> from TTS.tts.configs.fast_pitch_config import FastPitchConfig\n        >>> config = FastPitchConfig()\n    Args:\n        model (str):\n            Model name used for selecting the right model at initialization. Defaults to `fast_pitch`.\n        base_model (str):\n            Name of the base model being configured as this model so that  TTS knows it needs to initiate",
        "detail": "env.src.tts.TTS.tts.configs.fast_pitch_config",
        "documentation": {}
    },
    {
        "label": "FastSpeechConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.configs.fast_speech_config",
        "description": "env.src.tts.TTS.tts.configs.fast_speech_config",
        "peekOfCode": "class FastSpeechConfig(BaseTTSConfig):\n    \"\"\"Configure `ForwardTTS` as FastSpeech model.\n    Example:\n        >>> from TTS.tts.configs.fast_speech_config import FastSpeechConfig\n        >>> config = FastSpeechConfig()\n    Args:\n        model (str):\n            Model name used for selecting the right model at initialization. Defaults to `fast_pitch`.\n        base_model (str):\n            Name of the base model being configured as this model so that  TTS knows it needs to initiate",
        "detail": "env.src.tts.TTS.tts.configs.fast_speech_config",
        "documentation": {}
    },
    {
        "label": "Fastspeech2Config",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.configs.fastspeech2_config",
        "description": "env.src.tts.TTS.tts.configs.fastspeech2_config",
        "peekOfCode": "class Fastspeech2Config(BaseTTSConfig):\n    \"\"\"Configure `ForwardTTS` as FastPitch model.\n    Example:\n        >>> from TTS.tts.configs.fastspeech2_config import FastSpeech2Config\n        >>> config = FastSpeech2Config()\n    Args:\n        model (str):\n            Model name used for selecting the right model at initialization. Defaults to `fast_pitch`.\n        base_model (str):\n            Name of the base model being configured as this model so that  TTS knows it needs to initiate",
        "detail": "env.src.tts.TTS.tts.configs.fastspeech2_config",
        "documentation": {}
    },
    {
        "label": "GlowTTSConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.configs.glow_tts_config",
        "description": "env.src.tts.TTS.tts.configs.glow_tts_config",
        "peekOfCode": "class GlowTTSConfig(BaseTTSConfig):\n    \"\"\"Defines parameters for GlowTTS model.\n    Example:\n        >>> from TTS.tts.configs.glow_tts_config import GlowTTSConfig\n        >>> config = GlowTTSConfig()\n    Args:\n        model(str):\n            Model name used for selecting the right model at initialization. Defaults to `glow_tts`.\n        encoder_type (str):\n            Type of the encoder used by the model. Look at `TTS.tts.layers.glow_tts.encoder` for more details.",
        "detail": "env.src.tts.TTS.tts.configs.glow_tts_config",
        "documentation": {}
    },
    {
        "label": "NeuralhmmTTSConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.configs.neuralhmm_tts_config",
        "description": "env.src.tts.TTS.tts.configs.neuralhmm_tts_config",
        "peekOfCode": "class NeuralhmmTTSConfig(BaseTTSConfig):\n    \"\"\"\n    Define parameters for Neural HMM TTS model.\n    Example:\n        >>> from TTS.tts.configs.overflow_config import OverflowConfig\n        >>> config = OverflowConfig()\n    Args:\n        model (str):\n            Model name used to select the right model class to initilize. Defaults to `Overflow`.\n        run_eval_steps (int):",
        "detail": "env.src.tts.TTS.tts.configs.neuralhmm_tts_config",
        "documentation": {}
    },
    {
        "label": "OverflowConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.configs.overflow_config",
        "description": "env.src.tts.TTS.tts.configs.overflow_config",
        "peekOfCode": "class OverflowConfig(BaseTTSConfig):  # The classname has to be camel case\n    \"\"\"\n    Define parameters for OverFlow model.\n    Example:\n        >>> from TTS.tts.configs.overflow_config import OverflowConfig\n        >>> config = OverflowConfig()\n    Args:\n        model (str):\n            Model name used to select the right model class to initilize. Defaults to `Overflow`.\n        run_eval_steps (int):",
        "detail": "env.src.tts.TTS.tts.configs.overflow_config",
        "documentation": {}
    },
    {
        "label": "GSTConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.configs.shared_configs",
        "description": "env.src.tts.TTS.tts.configs.shared_configs",
        "peekOfCode": "class GSTConfig(Coqpit):\n    \"\"\"Defines the Global Style Token Module\n    Args:\n        gst_style_input_wav (str):\n            Path to the wav file used to define the style of the output speech at inference. Defaults to None.\n        gst_style_input_weights (dict):\n            Defines the weights for each style token used at inference. Defaults to None.\n        gst_embedding_dim (int):\n            Defines the size of the GST embedding vector dimensions. Defaults to 256.\n        gst_num_heads (int):",
        "detail": "env.src.tts.TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "CapacitronVAEConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.configs.shared_configs",
        "description": "env.src.tts.TTS.tts.configs.shared_configs",
        "peekOfCode": "class CapacitronVAEConfig(Coqpit):\n    \"\"\"Defines the capacitron VAE Module\n    Args:\n        capacitron_capacity (int):\n            Defines the variational capacity limit of the prosody embeddings. Defaults to 150.\n        capacitron_VAE_embedding_dim (int):\n            Defines the size of the Capacitron embedding vector dimension. Defaults to 128.\n        capacitron_use_text_summary_embeddings (bool):\n            If True, use a text summary embedding in Capacitron. Defaults to True.\n        capacitron_text_summary_embedding_dim (int):",
        "detail": "env.src.tts.TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "CharactersConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.configs.shared_configs",
        "description": "env.src.tts.TTS.tts.configs.shared_configs",
        "peekOfCode": "class CharactersConfig(Coqpit):\n    \"\"\"Defines arguments for the `BaseCharacters` or `BaseVocabulary` and their subclasses.\n    Args:\n        characters_class (str):\n            Defines the class of the characters used. If None, we pick ```Phonemes``` or ```Graphemes``` based on\n            the configuration. Defaults to None.\n        vocab_dict (dict):\n            Defines the vocabulary dictionary used to encode the characters. Defaults to None.\n        pad (str):\n            characters in place of empty padding. Defaults to None.",
        "detail": "env.src.tts.TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseTTSConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.configs.shared_configs",
        "description": "env.src.tts.TTS.tts.configs.shared_configs",
        "peekOfCode": "class BaseTTSConfig(BaseTrainingConfig):\n    \"\"\"Shared parameters among all the tts models.\n    Args:\n        audio (BaseAudioConfig):\n            Audio processor config object instance.\n        use_phonemes (bool):\n            enable / disable phoneme use.\n        phonemizer (str):\n            Name of the phonemizer to use. If set None, the phonemizer will be selected by `phoneme_language`.\n            Defaults to None.",
        "detail": "env.src.tts.TTS.tts.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "SpeedySpeechConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.configs.speedy_speech_config",
        "description": "env.src.tts.TTS.tts.configs.speedy_speech_config",
        "peekOfCode": "class SpeedySpeechConfig(BaseTTSConfig):\n    \"\"\"Configure `ForwardTTS` as SpeedySpeech model.\n    Example:\n        >>> from TTS.tts.configs.speedy_speech_config import SpeedySpeechConfig\n        >>> config = SpeedySpeechConfig()\n     Args:\n        model (str):\n            Model name used for selecting the right model at initialization. Defaults to `speedy_speech`.\n        base_model (str):\n            Name of the base model being configured as this model so that  TTS knows it needs to initiate",
        "detail": "env.src.tts.TTS.tts.configs.speedy_speech_config",
        "documentation": {}
    },
    {
        "label": "Tacotron2Config",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.configs.tacotron2_config",
        "description": "env.src.tts.TTS.tts.configs.tacotron2_config",
        "peekOfCode": "class Tacotron2Config(TacotronConfig):\n    \"\"\"Defines parameters for Tacotron2 based models.\n    Example:\n        >>> from TTS.tts.configs.tacotron2_config import Tacotron2Config\n        >>> config = Tacotron2Config()\n    Check `TacotronConfig` for argument descriptions.\n    \"\"\"\n    model: str = \"tacotron2\"\n    out_channels: int = 80\n    encoder_in_features: int = 512",
        "detail": "env.src.tts.TTS.tts.configs.tacotron2_config",
        "documentation": {}
    },
    {
        "label": "TacotronConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.configs.tacotron_config",
        "description": "env.src.tts.TTS.tts.configs.tacotron_config",
        "peekOfCode": "class TacotronConfig(BaseTTSConfig):\n    \"\"\"Defines parameters for Tacotron based models.\n    Example:\n        >>> from TTS.tts.configs.tacotron_config import TacotronConfig\n        >>> config = TacotronConfig()\n    Args:\n        model (str):\n            Model name used to select the right model class to initilize. Defaults to `Tacotron`.\n        use_gst (bool):\n            enable / disable the use of Global Style Token modules. Defaults to False.",
        "detail": "env.src.tts.TTS.tts.configs.tacotron_config",
        "documentation": {}
    },
    {
        "label": "TortoiseConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.configs.tortoise_config",
        "description": "env.src.tts.TTS.tts.configs.tortoise_config",
        "peekOfCode": "class TortoiseConfig(BaseTTSConfig):\n    \"\"\"Defines parameters for Tortoise TTS model.\n    Args:\n        model (str):\n            Model name. Do not change unless you know what you are doing.\n        model_args (TortoiseArgs):\n            Model architecture arguments. Defaults to `TortoiseArgs()`.\n        audio (TortoiseAudioConfig):\n            Audio processing configuration. Defaults to `TortoiseAudioConfig()`.\n        model_dir (str):",
        "detail": "env.src.tts.TTS.tts.configs.tortoise_config",
        "documentation": {}
    },
    {
        "label": "VitsConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.configs.vits_config",
        "description": "env.src.tts.TTS.tts.configs.vits_config",
        "peekOfCode": "class VitsConfig(BaseTTSConfig):\n    \"\"\"Defines parameters for VITS End2End TTS model.\n    Args:\n        model (str):\n            Model name. Do not change unless you know what you are doing.\n        model_args (VitsArgs):\n            Model architecture arguments. Defaults to `VitsArgs()`.\n        audio (VitsAudioConfig):\n            Audio processing configuration. Defaults to `VitsAudioConfig()`.\n        grad_clip (List):",
        "detail": "env.src.tts.TTS.tts.configs.vits_config",
        "documentation": {}
    },
    {
        "label": "TTSDataset",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.datasets.dataset",
        "description": "env.src.tts.TTS.tts.datasets.dataset",
        "peekOfCode": "class TTSDataset(Dataset):\n    def __init__(\n        self,\n        outputs_per_step: int = 1,\n        compute_linear_spec: bool = False,\n        ap: AudioProcessor = None,\n        samples: List[Dict] = None,\n        tokenizer: \"TTSTokenizer\" = None,\n        compute_f0: bool = False,\n        compute_energy: bool = False,",
        "detail": "env.src.tts.TTS.tts.datasets.dataset",
        "documentation": {}
    },
    {
        "label": "PhonemeDataset",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.datasets.dataset",
        "description": "env.src.tts.TTS.tts.datasets.dataset",
        "peekOfCode": "class PhonemeDataset(Dataset):\n    \"\"\"Phoneme Dataset for converting input text to phonemes and then token IDs\n    At initialization, it pre-computes the phonemes under `cache_path` and loads them in training to reduce data\n    loading latency. If `cache_path` is already present, it skips the pre-computation.\n    Args:\n        samples (Union[List[List], List[Dict]]):\n            List of samples. Each sample is a list or a dict.\n        tokenizer (TTSTokenizer):\n            Tokenizer to convert input text to phonemes.\n        cache_path (str):",
        "detail": "env.src.tts.TTS.tts.datasets.dataset",
        "documentation": {}
    },
    {
        "label": "F0Dataset",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.datasets.dataset",
        "description": "env.src.tts.TTS.tts.datasets.dataset",
        "peekOfCode": "class F0Dataset:\n    \"\"\"F0 Dataset for computing F0 from wav files in CPU\n    Pre-compute F0 values for all the samples at initialization if `cache_path` is not None or already present. It\n    also computes the mean and std of F0 values if `normalize_f0` is True.\n    Args:\n        samples (Union[List[List], List[Dict]]):\n            List of samples. Each sample is a list or a dict.\n        ap (AudioProcessor):\n            AudioProcessor to compute F0 from wav files.\n        cache_path (str):",
        "detail": "env.src.tts.TTS.tts.datasets.dataset",
        "documentation": {}
    },
    {
        "label": "EnergyDataset",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.datasets.dataset",
        "description": "env.src.tts.TTS.tts.datasets.dataset",
        "peekOfCode": "class EnergyDataset:\n    \"\"\"Energy Dataset for computing Energy from wav files in CPU\n    Pre-compute Energy values for all the samples at initialization if `cache_path` is not None or already present. It\n    also computes the mean and std of Energy values if `normalize_Energy` is True.\n    Args:\n        samples (Union[List[List], List[Dict]]):\n            List of samples. Each sample is a list or a dict.\n        ap (AudioProcessor):\n            AudioProcessor to compute Energy from wav files.\n        cache_path (str):",
        "detail": "env.src.tts.TTS.tts.datasets.dataset",
        "documentation": {}
    },
    {
        "label": "noise_augment_audio",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.dataset",
        "description": "env.src.tts.TTS.tts.datasets.dataset",
        "peekOfCode": "def noise_augment_audio(wav):\n    return wav + (1.0 / 32768.0) * np.random.rand(*wav.shape)\ndef string2filename(string):\n    # generate a safe and reversible filename based on a string\n    filename = base64.urlsafe_b64encode(string.encode(\"utf-8\")).decode(\"utf-8\", \"ignore\")\n    return filename\nclass TTSDataset(Dataset):\n    def __init__(\n        self,\n        outputs_per_step: int = 1,",
        "detail": "env.src.tts.TTS.tts.datasets.dataset",
        "documentation": {}
    },
    {
        "label": "string2filename",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.dataset",
        "description": "env.src.tts.TTS.tts.datasets.dataset",
        "peekOfCode": "def string2filename(string):\n    # generate a safe and reversible filename based on a string\n    filename = base64.urlsafe_b64encode(string.encode(\"utf-8\")).decode(\"utf-8\", \"ignore\")\n    return filename\nclass TTSDataset(Dataset):\n    def __init__(\n        self,\n        outputs_per_step: int = 1,\n        compute_linear_spec: bool = False,\n        ap: AudioProcessor = None,",
        "detail": "env.src.tts.TTS.tts.datasets.dataset",
        "documentation": {}
    },
    {
        "label": "coqui",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def coqui(root_path, meta_file, ignored_speakers=None):\n    \"\"\"Interal dataset formatter.\"\"\"\n    filepath = os.path.join(root_path, meta_file)\n    # ensure there are 4 columns for every line\n    with open(filepath, \"r\", encoding=\"utf8\") as f:\n        lines = f.readlines()\n    num_cols = len(lines[0].split(\"|\"))  # take the first row as reference\n    for idx, line in enumerate(lines[1:]):\n        if len(line.split(\"|\")) != num_cols:\n            print(f\" > Missing column in line {idx + 1} -> {line.strip()}\")",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "tweb",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def tweb(root_path, meta_file, **kwargs):  # pylint: disable=unused-argument\n    \"\"\"Normalize TWEB dataset.\n    https://www.kaggle.com/bryanpark/the-world-english-bible-speech-dataset\n    \"\"\"\n    txt_file = os.path.join(root_path, meta_file)\n    items = []\n    speaker_name = \"tweb\"\n    with open(txt_file, \"r\", encoding=\"utf-8\") as ttf:\n        for line in ttf:\n            cols = line.split(\"\\t\")",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "mozilla",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def mozilla(root_path, meta_file, **kwargs):  # pylint: disable=unused-argument\n    \"\"\"Normalizes Mozilla meta data files to TTS format\"\"\"\n    txt_file = os.path.join(root_path, meta_file)\n    items = []\n    speaker_name = \"mozilla\"\n    with open(txt_file, \"r\", encoding=\"utf-8\") as ttf:\n        for line in ttf:\n            cols = line.split(\"|\")\n            wav_file = cols[1].strip()\n            text = cols[0].strip()",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "mozilla_de",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def mozilla_de(root_path, meta_file, **kwargs):  # pylint: disable=unused-argument\n    \"\"\"Normalizes Mozilla meta data files to TTS format\"\"\"\n    txt_file = os.path.join(root_path, meta_file)\n    items = []\n    speaker_name = \"mozilla\"\n    with open(txt_file, \"r\", encoding=\"ISO 8859-1\") as ttf:\n        for line in ttf:\n            cols = line.strip().split(\"|\")\n            wav_file = cols[0].strip()\n            text = cols[1].strip()",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "mailabs",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def mailabs(root_path, meta_files=None, ignored_speakers=None):\n    \"\"\"Normalizes M-AI-Labs meta data files to TTS format\n    Args:\n        root_path (str): root folder of the MAILAB language folder.\n        meta_files (str):  list of meta files to be used in the training. If None, finds all the csv files\n            recursively. Defaults to None\n    \"\"\"\n    speaker_regex = re.compile(f\"by_book{os.sep}(male|female){os.sep}(?P<speaker_name>[^{os.sep}]+){os.sep}\")\n    if not meta_files:\n        csv_files = glob(root_path + f\"{os.sep}**{os.sep}metadata.csv\", recursive=True)",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "ljspeech",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def ljspeech(root_path, meta_file, **kwargs):  # pylint: disable=unused-argument\n    \"\"\"Normalizes the LJSpeech meta data file to TTS format\n    https://keithito.com/LJ-Speech-Dataset/\"\"\"\n    txt_file = os.path.join(root_path, meta_file)\n    items = []\n    speaker_name = \"ljspeech\"\n    with open(txt_file, \"r\", encoding=\"utf-8\") as ttf:\n        for line in ttf:\n            cols = line.split(\"|\")\n            wav_file = os.path.join(root_path, \"wavs\", cols[0] + \".wav\")",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "ljspeech_test",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def ljspeech_test(root_path, meta_file, **kwargs):  # pylint: disable=unused-argument\n    \"\"\"Normalizes the LJSpeech meta data file for TTS testing\n    https://keithito.com/LJ-Speech-Dataset/\"\"\"\n    txt_file = os.path.join(root_path, meta_file)\n    items = []\n    with open(txt_file, \"r\", encoding=\"utf-8\") as ttf:\n        speaker_id = 0\n        for idx, line in enumerate(ttf):\n            # 2 samples per speaker to avoid eval split issues\n            if idx % 2 == 0:",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "thorsten",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def thorsten(root_path, meta_file, **kwargs):  # pylint: disable=unused-argument\n    \"\"\"Normalizes the thorsten meta data file to TTS format\n    https://github.com/thorstenMueller/deep-learning-german-tts/\"\"\"\n    txt_file = os.path.join(root_path, meta_file)\n    items = []\n    speaker_name = \"thorsten\"\n    with open(txt_file, \"r\", encoding=\"utf-8\") as ttf:\n        for line in ttf:\n            cols = line.split(\"|\")\n            wav_file = os.path.join(root_path, \"wavs\", cols[0] + \".wav\")",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "sam_accenture",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def sam_accenture(root_path, meta_file, **kwargs):  # pylint: disable=unused-argument\n    \"\"\"Normalizes the sam-accenture meta data file to TTS format\n    https://github.com/Sam-Accenture-Non-Binary-Voice/non-binary-voice-files\"\"\"\n    xml_file = os.path.join(root_path, \"voice_over_recordings\", meta_file)\n    xml_root = ET.parse(xml_file).getroot()\n    items = []\n    speaker_name = \"sam_accenture\"\n    for item in xml_root.findall(\"./fileid\"):\n        text = item.text\n        wav_file = os.path.join(root_path, \"vo_voice_quality_transformation\", item.get(\"id\") + \".wav\")",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "ruslan",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def ruslan(root_path, meta_file, **kwargs):  # pylint: disable=unused-argument\n    \"\"\"Normalizes the RUSLAN meta data file to TTS format\n    https://ruslan-corpus.github.io/\"\"\"\n    txt_file = os.path.join(root_path, meta_file)\n    items = []\n    speaker_name = \"ruslan\"\n    with open(txt_file, \"r\", encoding=\"utf-8\") as ttf:\n        for line in ttf:\n            cols = line.split(\"|\")\n            wav_file = os.path.join(root_path, \"RUSLAN\", cols[0] + \".wav\")",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "css10",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def css10(root_path, meta_file, **kwargs):  # pylint: disable=unused-argument\n    \"\"\"Normalizes the CSS10 dataset file to TTS format\"\"\"\n    txt_file = os.path.join(root_path, meta_file)\n    items = []\n    speaker_name = \"css10\"\n    with open(txt_file, \"r\", encoding=\"utf-8\") as ttf:\n        for line in ttf:\n            cols = line.split(\"|\")\n            wav_file = os.path.join(root_path, cols[0])\n            text = cols[1]",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "nancy",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def nancy(root_path, meta_file, **kwargs):  # pylint: disable=unused-argument\n    \"\"\"Normalizes the Nancy meta data file to TTS format\"\"\"\n    txt_file = os.path.join(root_path, meta_file)\n    items = []\n    speaker_name = \"nancy\"\n    with open(txt_file, \"r\", encoding=\"utf-8\") as ttf:\n        for line in ttf:\n            utt_id = line.split()[1]\n            text = line[line.find('\"') + 1 : line.rfind('\"') - 1]\n            wav_file = os.path.join(root_path, \"wavn\", utt_id + \".wav\")",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "common_voice",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def common_voice(root_path, meta_file, ignored_speakers=None):\n    \"\"\"Normalize the common voice meta data file to TTS format.\"\"\"\n    txt_file = os.path.join(root_path, meta_file)\n    items = []\n    with open(txt_file, \"r\", encoding=\"utf-8\") as ttf:\n        for line in ttf:\n            if line.startswith(\"client_id\"):\n                continue\n            cols = line.split(\"\\t\")\n            text = cols[2]",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "libri_tts",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def libri_tts(root_path, meta_files=None, ignored_speakers=None):\n    \"\"\"https://ai.google/tools/datasets/libri-tts/\"\"\"\n    items = []\n    if not meta_files:\n        meta_files = glob(f\"{root_path}/**/*trans.tsv\", recursive=True)\n    else:\n        if isinstance(meta_files, str):\n            meta_files = [os.path.join(root_path, meta_files)]\n    for meta_file in meta_files:\n        _meta_file = os.path.basename(meta_file).split(\".\")[0]",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "custom_turkish",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def custom_turkish(root_path, meta_file, **kwargs):  # pylint: disable=unused-argument\n    txt_file = os.path.join(root_path, meta_file)\n    items = []\n    speaker_name = \"turkish-female\"\n    skipped_files = []\n    with open(txt_file, \"r\", encoding=\"utf-8\") as ttf:\n        for line in ttf:\n            cols = line.split(\"|\")\n            wav_file = os.path.join(root_path, \"wavs\", cols[0].strip() + \".wav\")\n            if not os.path.exists(wav_file):",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "brspeech",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def brspeech(root_path, meta_file, ignored_speakers=None):\n    \"\"\"BRSpeech 3.0 beta\"\"\"\n    txt_file = os.path.join(root_path, meta_file)\n    items = []\n    with open(txt_file, \"r\", encoding=\"utf-8\") as ttf:\n        for line in ttf:\n            if line.startswith(\"wav_filename\"):\n                continue\n            cols = line.split(\"|\")\n            wav_file = os.path.join(root_path, cols[0])",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "vctk",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def vctk(root_path, meta_files=None, wavs_path=\"wav48_silence_trimmed\", mic=\"mic1\", ignored_speakers=None):\n    \"\"\"VCTK dataset v0.92.\n    URL:\n        https://datashare.ed.ac.uk/bitstream/handle/10283/3443/VCTK-Corpus-0.92.zip\n    This dataset has 2 recordings per speaker that are annotated with ```mic1``` and ```mic2```.\n    It is believed that ( ) ```mic1``` files are the same as the previous version of the dataset.\n    mic1:\n        Audio recorded using an omni-directional microphone (DPA 4035).\n        Contains very low frequency noises.\n        This is the same audio released in previous versions of VCTK:",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "vctk_old",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def vctk_old(root_path, meta_files=None, wavs_path=\"wav48\", ignored_speakers=None):\n    \"\"\"homepages.inf.ed.ac.uk/jyamagis/release/VCTK-Corpus.tar.gz\"\"\"\n    items = []\n    meta_files = glob(f\"{os.path.join(root_path,'txt')}/**/*.txt\", recursive=True)\n    for meta_file in meta_files:\n        _, speaker_id, txt_file = os.path.relpath(meta_file, root_path).split(os.sep)\n        file_id = txt_file.split(\".\")[0]\n        # ignore speakers\n        if isinstance(ignored_speakers, list):\n            if speaker_id in ignored_speakers:",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "synpaflex",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def synpaflex(root_path, metafiles=None, **kwargs):  # pylint: disable=unused-argument\n    items = []\n    speaker_name = \"synpaflex\"\n    root_path = os.path.join(root_path, \"\")\n    wav_files = glob(f\"{root_path}**/*.wav\", recursive=True)\n    for wav_file in wav_files:\n        if os.sep + \"wav\" + os.sep in wav_file:\n            txt_file = wav_file.replace(\"wav\", \"txt\")\n        else:\n            txt_file = os.path.join(",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "open_bible",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def open_bible(root_path, meta_files=\"train\", ignore_digits_sentences=True, ignored_speakers=None):\n    \"\"\"ToDo: Refer the paper when available\"\"\"\n    items = []\n    split_dir = meta_files\n    meta_files = glob(f\"{os.path.join(root_path, split_dir)}/**/*.txt\", recursive=True)\n    for meta_file in meta_files:\n        _, speaker_id, txt_file = os.path.relpath(meta_file, root_path).split(os.sep)\n        file_id = txt_file.split(\".\")[0]\n        # ignore speakers\n        if isinstance(ignored_speakers, list):",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "mls",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def mls(root_path, meta_files=None, ignored_speakers=None):\n    \"\"\"http://www.openslr.org/94/\"\"\"\n    items = []\n    with open(os.path.join(root_path, meta_files), \"r\", encoding=\"utf-8\") as meta:\n        for line in meta:\n            file, text = line.split(\"\\t\")\n            text = text[:-1]\n            speaker, book, *_ = file.split(\"_\")\n            wav_file = os.path.join(root_path, os.path.dirname(meta_files), \"audio\", speaker, book, file + \".wav\")\n            # ignore speakers",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "voxceleb2",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def voxceleb2(root_path, meta_file=None, **kwargs):  # pylint: disable=unused-argument\n    \"\"\"\n    :param meta_file   Used only for consistency with load_tts_samples api\n    \"\"\"\n    return _voxcel_x(root_path, meta_file, voxcel_idx=\"2\")\ndef voxceleb1(root_path, meta_file=None, **kwargs):  # pylint: disable=unused-argument\n    \"\"\"\n    :param meta_file   Used only for consistency with load_tts_samples api\n    \"\"\"\n    return _voxcel_x(root_path, meta_file, voxcel_idx=\"1\")",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "voxceleb1",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def voxceleb1(root_path, meta_file=None, **kwargs):  # pylint: disable=unused-argument\n    \"\"\"\n    :param meta_file   Used only for consistency with load_tts_samples api\n    \"\"\"\n    return _voxcel_x(root_path, meta_file, voxcel_idx=\"1\")\ndef _voxcel_x(root_path, meta_file, voxcel_idx):\n    assert voxcel_idx in [\"1\", \"2\"]\n    expected_count = 148_000 if voxcel_idx == \"1\" else 1_000_000\n    voxceleb_path = Path(root_path)\n    cache_to = voxceleb_path / f\"metafile_voxceleb{voxcel_idx}.csv\"",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "emotion",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def emotion(root_path, meta_file, ignored_speakers=None):\n    \"\"\"Generic emotion dataset\"\"\"\n    txt_file = os.path.join(root_path, meta_file)\n    items = []\n    with open(txt_file, \"r\", encoding=\"utf-8\") as ttf:\n        for line in ttf:\n            if line.startswith(\"file_path\"):\n                continue\n            cols = line.split(\",\")\n            wav_file = os.path.join(root_path, cols[0])",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "baker",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def baker(root_path: str, meta_file: str, **kwargs) -> List[List[str]]:  # pylint: disable=unused-argument\n    \"\"\"Normalizes the Baker meta data file to TTS format\n    Args:\n        root_path (str): path to the baker dataset\n        meta_file (str): name of the meta dataset containing names of wav to select and the transcript of the sentence\n    Returns:\n        List[List[str]]: List of (text, wav_path, speaker_name) associated with each sentences\n    \"\"\"\n    txt_file = os.path.join(root_path, meta_file)\n    items = []",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "kokoro",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def kokoro(root_path, meta_file, **kwargs):  # pylint: disable=unused-argument\n    \"\"\"Japanese single-speaker dataset from https://github.com/kaiidams/Kokoro-Speech-Dataset\"\"\"\n    txt_file = os.path.join(root_path, meta_file)\n    items = []\n    speaker_name = \"kokoro\"\n    with open(txt_file, \"r\", encoding=\"utf-8\") as ttf:\n        for line in ttf:\n            cols = line.split(\"|\")\n            wav_file = os.path.join(root_path, \"wavs\", cols[0] + \".wav\")\n            text = cols[2].replace(\" \", \"\")",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "kss",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.datasets.formatters",
        "description": "env.src.tts.TTS.tts.datasets.formatters",
        "peekOfCode": "def kss(root_path, meta_file, **kwargs):  # pylint: disable=unused-argument\n    \"\"\"Korean single-speaker dataset from https://www.kaggle.com/datasets/bryanpark/korean-single-speaker-speech-dataset\"\"\"\n    txt_file = os.path.join(root_path, meta_file)\n    items = []\n    speaker_name = \"kss\"\n    with open(txt_file, \"r\", encoding=\"utf-8\") as ttf:\n        for line in ttf:\n            cols = line.split(\"|\")\n            wav_file = os.path.join(root_path, cols[0])\n            text = cols[2]  # cols[1] => 6, cols[2] => ",
        "detail": "env.src.tts.TTS.tts.datasets.formatters",
        "documentation": {}
    },
    {
        "label": "DurationPredictor",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.align_tts.duration_predictor",
        "description": "env.src.tts.TTS.tts.layers.align_tts.duration_predictor",
        "peekOfCode": "class DurationPredictor(nn.Module):\n    def __init__(self, num_chars, hidden_channels, hidden_channels_ffn, num_heads):\n        super().__init__()\n        self.embed = nn.Embedding(num_chars, hidden_channels)\n        self.pos_enc = PositionalEncoding(hidden_channels, dropout_p=0.1)\n        self.FFT = FFTransformerBlock(hidden_channels, num_heads, hidden_channels_ffn, 2, 0.1)\n        self.out_layer = nn.Conv1d(hidden_channels, 1, 1)\n    def forward(self, text, text_lengths):\n        # B, L -> B, L\n        emb = self.embed(text)",
        "detail": "env.src.tts.TTS.tts.layers.align_tts.duration_predictor",
        "documentation": {}
    },
    {
        "label": "MDNBlock",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.align_tts.mdn",
        "description": "env.src.tts.TTS.tts.layers.align_tts.mdn",
        "peekOfCode": "class MDNBlock(nn.Module):\n    \"\"\"Mixture of Density Network implementation\n    https://arxiv.org/pdf/2003.01950.pdf\n    \"\"\"\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.out_channels = out_channels\n        self.conv1 = nn.Conv1d(in_channels, in_channels, 1)\n        self.norm = nn.LayerNorm(in_channels)\n        self.relu = nn.ReLU()",
        "detail": "env.src.tts.TTS.tts.layers.align_tts.mdn",
        "documentation": {}
    },
    {
        "label": "WaveNetDecoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.feed_forward.decoder",
        "description": "env.src.tts.TTS.tts.layers.feed_forward.decoder",
        "peekOfCode": "class WaveNetDecoder(nn.Module):\n    \"\"\"WaveNet based decoder with a prenet and a postnet.\n    prenet: conv1d_1x1\n    postnet: 3 x [conv1d_1x1 -> relu] -> conv1d_1x1\n    TODO: Integrate speaker conditioning vector.\n    Note:\n        default wavenet parameters;\n            params = {\n                \"num_blocks\": 12,\n                \"hidden_channels\":192,",
        "detail": "env.src.tts.TTS.tts.layers.feed_forward.decoder",
        "documentation": {}
    },
    {
        "label": "RelativePositionTransformerDecoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.feed_forward.decoder",
        "description": "env.src.tts.TTS.tts.layers.feed_forward.decoder",
        "peekOfCode": "class RelativePositionTransformerDecoder(nn.Module):\n    \"\"\"Decoder with Relative Positional Transformer.\n    Note:\n        Default params\n            params={\n                'hidden_channels_ffn': 128,\n                'num_heads': 2,\n                \"kernel_size\": 3,\n                \"dropout_p\": 0.1,\n                \"num_layers\": 8,",
        "detail": "env.src.tts.TTS.tts.layers.feed_forward.decoder",
        "documentation": {}
    },
    {
        "label": "FFTransformerDecoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.feed_forward.decoder",
        "description": "env.src.tts.TTS.tts.layers.feed_forward.decoder",
        "peekOfCode": "class FFTransformerDecoder(nn.Module):\n    \"\"\"Decoder with FeedForwardTransformer.\n    Default params\n            params={\n                'hidden_channels_ffn': 1024,\n                'num_heads': 2,\n                \"dropout_p\": 0.1,\n                \"num_layers\": 6,\n            }\n    Args:",
        "detail": "env.src.tts.TTS.tts.layers.feed_forward.decoder",
        "documentation": {}
    },
    {
        "label": "ResidualConv1dBNDecoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.feed_forward.decoder",
        "description": "env.src.tts.TTS.tts.layers.feed_forward.decoder",
        "peekOfCode": "class ResidualConv1dBNDecoder(nn.Module):\n    \"\"\"Residual Convolutional Decoder as in the original Speedy Speech paper\n    TODO: Integrate speaker conditioning vector.\n    Note:\n        Default params\n                params = {\n                    \"kernel_size\": 4,\n                    \"dilations\": 4 * [1, 2, 4, 8] + [1],\n                    \"num_conv_blocks\": 2,\n                    \"num_res_blocks\": 17",
        "detail": "env.src.tts.TTS.tts.layers.feed_forward.decoder",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.feed_forward.decoder",
        "description": "env.src.tts.TTS.tts.layers.feed_forward.decoder",
        "peekOfCode": "class Decoder(nn.Module):\n    \"\"\"Decodes the expanded phoneme encoding into spectrograms\n    Args:\n        out_channels (int): number of output channels.\n        in_hidden_channels (int): input and hidden channels. Model keeps the input channels for the intermediate layers.\n        decoder_type (str): decoder layer types. 'transformers' or 'residual_conv_bn'. Default 'residual_conv_bn'.\n        decoder_params (dict): model parameters for specified decoder type.\n        c_in_channels (int): number of channels for conditional input.\n    Shapes:\n        - input: (B, C, T)",
        "detail": "env.src.tts.TTS.tts.layers.feed_forward.decoder",
        "documentation": {}
    },
    {
        "label": "DurationPredictor",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.feed_forward.duration_predictor",
        "description": "env.src.tts.TTS.tts.layers.feed_forward.duration_predictor",
        "peekOfCode": "class DurationPredictor(nn.Module):\n    \"\"\"Speedy Speech duration predictor model.\n    Predicts phoneme durations from encoder outputs.\n    Note:\n        Outputs interpreted as log(durations)\n        To get actual durations, do exp transformation\n    conv_BN_4x1 -> conv_BN_3x1 -> conv_BN_1x1 -> conv_1x1\n    Args:\n        hidden_channels (int): number of channels in the inner layers.\n    \"\"\"",
        "detail": "env.src.tts.TTS.tts.layers.feed_forward.duration_predictor",
        "documentation": {}
    },
    {
        "label": "RelativePositionTransformerEncoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.feed_forward.encoder",
        "description": "env.src.tts.TTS.tts.layers.feed_forward.encoder",
        "peekOfCode": "class RelativePositionTransformerEncoder(nn.Module):\n    \"\"\"Speedy speech encoder built on Transformer with Relative Position encoding.\n    TODO: Integrate speaker conditioning vector.\n    Args:\n        in_channels (int): number of input channels.\n        out_channels (int): number of output channels.\n        hidden_channels (int): number of hidden channels\n        params (dict): dictionary for residual convolutional blocks.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, hidden_channels, params):",
        "detail": "env.src.tts.TTS.tts.layers.feed_forward.encoder",
        "documentation": {}
    },
    {
        "label": "ResidualConv1dBNEncoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.feed_forward.encoder",
        "description": "env.src.tts.TTS.tts.layers.feed_forward.encoder",
        "peekOfCode": "class ResidualConv1dBNEncoder(nn.Module):\n    \"\"\"Residual Convolutional Encoder as in the original Speedy Speech paper\n    TODO: Integrate speaker conditioning vector.\n    Args:\n        in_channels (int): number of input channels.\n        out_channels (int): number of output channels.\n        hidden_channels (int): number of hidden channels\n        params (dict): dictionary for residual convolutional blocks.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, hidden_channels, params):",
        "detail": "env.src.tts.TTS.tts.layers.feed_forward.encoder",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.feed_forward.encoder",
        "description": "env.src.tts.TTS.tts.layers.feed_forward.encoder",
        "peekOfCode": "class Encoder(nn.Module):\n    # pylint: disable=dangerous-default-value\n    \"\"\"Factory class for Speedy Speech encoder enables different encoder types internally.\n    Args:\n        num_chars (int): number of characters.\n        out_channels (int): number of output channels.\n        in_hidden_channels (int): input and hidden channels. Model keeps the input channels for the intermediate layers.\n        encoder_type (str): encoder layer types. 'transformers' or 'residual_conv_bn'. Default 'residual_conv_bn'.\n        encoder_params (dict): model parameters for specified encoder type.\n        c_in_channels (int): number of channels for conditional input.",
        "detail": "env.src.tts.TTS.tts.layers.feed_forward.encoder",
        "documentation": {}
    },
    {
        "label": "AlignmentNetwork",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.generic.aligner",
        "description": "env.src.tts.TTS.tts.layers.generic.aligner",
        "peekOfCode": "class AlignmentNetwork(torch.nn.Module):\n    \"\"\"Aligner Network for learning alignment between the input text and the model output with Gaussian Attention.\n    ::\n        query -> conv1d -> relu -> conv1d -> relu -> conv1d -> L2_dist -> softmax -> alignment\n        key   -> conv1d -> relu -> conv1d -----------------------^\n    Args:\n        in_query_channels (int): Number of channels in the query network. Defaults to 80.\n        in_key_channels (int): Number of channels in the key network. Defaults to 512.\n        attn_channels (int): Number of inner channels in the attention layers. Defaults to 80.\n        temperature (float): Temperature for the softmax. Defaults to 0.0005.",
        "detail": "env.src.tts.TTS.tts.layers.generic.aligner",
        "documentation": {}
    },
    {
        "label": "GatedConvBlock",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.generic.gated_conv",
        "description": "env.src.tts.TTS.tts.layers.generic.gated_conv",
        "peekOfCode": "class GatedConvBlock(nn.Module):\n    \"\"\"Gated convolutional block as in https://arxiv.org/pdf/1612.08083.pdf\n    Args:\n        in_out_channels (int): number of input/output channels.\n        kernel_size (int): convolution kernel size.\n        dropout_p (float): dropout rate.\n    \"\"\"\n    def __init__(self, in_out_channels, kernel_size, dropout_p, num_layers):\n        super().__init__()\n        # class arguments",
        "detail": "env.src.tts.TTS.tts.layers.generic.gated_conv",
        "documentation": {}
    },
    {
        "label": "LayerNorm",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.generic.normalization",
        "description": "env.src.tts.TTS.tts.layers.generic.normalization",
        "peekOfCode": "class LayerNorm(nn.Module):\n    def __init__(self, channels, eps=1e-4):\n        \"\"\"Layer norm for the 2nd dimension of the input.\n        Args:\n            channels (int): number of channels (2nd dimension) of the input.\n            eps (float): to prevent 0 division\n        Shapes:\n            - input: (B, C, T)\n            - output: (B, C, T)\n        \"\"\"",
        "detail": "env.src.tts.TTS.tts.layers.generic.normalization",
        "documentation": {}
    },
    {
        "label": "LayerNorm2",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.generic.normalization",
        "description": "env.src.tts.TTS.tts.layers.generic.normalization",
        "peekOfCode": "class LayerNorm2(nn.Module):\n    \"\"\"Layer norm for the 2nd dimension of the input using torch primitive.\n    Args:\n        channels (int): number of channels (2nd dimension) of the input.\n        eps (float): to prevent 0 division\n    Shapes:\n        - input: (B, C, T)\n        - output: (B, C, T)\n    \"\"\"\n    def __init__(self, channels, eps=1e-5):",
        "detail": "env.src.tts.TTS.tts.layers.generic.normalization",
        "documentation": {}
    },
    {
        "label": "TemporalBatchNorm1d",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.generic.normalization",
        "description": "env.src.tts.TTS.tts.layers.generic.normalization",
        "peekOfCode": "class TemporalBatchNorm1d(nn.BatchNorm1d):\n    \"\"\"Normalize each channel separately over time and batch.\"\"\"\n    def __init__(self, channels, affine=True, track_running_stats=True, momentum=0.1):\n        super().__init__(channels, affine=affine, track_running_stats=track_running_stats, momentum=momentum)\n    def forward(self, x):\n        return super().forward(x.transpose(2, 1)).transpose(2, 1)\nclass ActNorm(nn.Module):\n    \"\"\"Activation Normalization bijector as an alternative to Batch Norm. It computes\n    mean and std from a sample data in advance and it uses these values\n    for normalization at training.",
        "detail": "env.src.tts.TTS.tts.layers.generic.normalization",
        "documentation": {}
    },
    {
        "label": "ActNorm",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.generic.normalization",
        "description": "env.src.tts.TTS.tts.layers.generic.normalization",
        "peekOfCode": "class ActNorm(nn.Module):\n    \"\"\"Activation Normalization bijector as an alternative to Batch Norm. It computes\n    mean and std from a sample data in advance and it uses these values\n    for normalization at training.\n    Args:\n        channels (int): input channels.\n        ddi (False): data depended initialization flag.\n    Shapes:\n        - inputs: (B, C, T)\n        - outputs: (B, C, T)",
        "detail": "env.src.tts.TTS.tts.layers.generic.normalization",
        "documentation": {}
    },
    {
        "label": "PositionalEncoding",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.generic.pos_encoding",
        "description": "env.src.tts.TTS.tts.layers.generic.pos_encoding",
        "peekOfCode": "class PositionalEncoding(nn.Module):\n    \"\"\"Sinusoidal positional encoding for non-recurrent neural networks.\n    Implementation based on \"Attention Is All You Need\"\n    Args:\n       channels (int): embedding size\n       dropout_p (float): dropout rate applied to the output.\n       max_len (int): maximum sequence length.\n       use_scale (bool): whether to use a learnable scaling coefficient.\n    \"\"\"\n    def __init__(self, channels, dropout_p=0.0, max_len=5000, use_scale=False):",
        "detail": "env.src.tts.TTS.tts.layers.generic.pos_encoding",
        "documentation": {}
    },
    {
        "label": "ZeroTemporalPad",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.generic.res_conv_bn",
        "description": "env.src.tts.TTS.tts.layers.generic.res_conv_bn",
        "peekOfCode": "class ZeroTemporalPad(nn.Module):\n    \"\"\"Pad sequences to equal lentgh in the temporal dimension\"\"\"\n    def __init__(self, kernel_size, dilation):\n        super().__init__()\n        total_pad = dilation * (kernel_size - 1)\n        begin = total_pad // 2\n        end = total_pad - begin\n        self.pad_layer = nn.ZeroPad2d((0, 0, begin, end))\n    def forward(self, x):\n        return self.pad_layer(x)",
        "detail": "env.src.tts.TTS.tts.layers.generic.res_conv_bn",
        "documentation": {}
    },
    {
        "label": "Conv1dBN",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.generic.res_conv_bn",
        "description": "env.src.tts.TTS.tts.layers.generic.res_conv_bn",
        "peekOfCode": "class Conv1dBN(nn.Module):\n    \"\"\"1d convolutional with batch norm.\n    conv1d -> relu -> BN blocks.\n    Note:\n        Batch normalization is applied after ReLU regarding the original implementation.\n    Args:\n        in_channels (int): number of input channels.\n        out_channels (int): number of output channels.\n        kernel_size (int): kernel size for convolutional filters.\n        dilation (int): dilation for convolution layers.",
        "detail": "env.src.tts.TTS.tts.layers.generic.res_conv_bn",
        "documentation": {}
    },
    {
        "label": "Conv1dBNBlock",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.generic.res_conv_bn",
        "description": "env.src.tts.TTS.tts.layers.generic.res_conv_bn",
        "peekOfCode": "class Conv1dBNBlock(nn.Module):\n    \"\"\"1d convolutional block with batch norm. It is a set of conv1d -> relu -> BN blocks.\n    Args:\n        in_channels (int): number of input channels.\n        out_channels (int): number of output channels.\n        hidden_channels (int): number of inner convolution channels.\n        kernel_size (int): kernel size for convolutional filters.\n        dilation (int): dilation for convolution layers.\n        num_conv_blocks (int, optional): number of convolutional blocks. Defaults to 2.\n    \"\"\"",
        "detail": "env.src.tts.TTS.tts.layers.generic.res_conv_bn",
        "documentation": {}
    },
    {
        "label": "ResidualConv1dBNBlock",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.generic.res_conv_bn",
        "description": "env.src.tts.TTS.tts.layers.generic.res_conv_bn",
        "peekOfCode": "class ResidualConv1dBNBlock(nn.Module):\n    \"\"\"Residual Convolutional Blocks with BN\n    Each block has 'num_conv_block' conv layers and 'num_res_blocks' such blocks are connected\n    with residual connections.\n    conv_block = (conv1d -> relu -> bn) x 'num_conv_blocks'\n    residuak_conv_block =  (x -> conv_block ->  + ->) x 'num_res_blocks'\n                            ' - - - - - - - - - ^\n    Args:\n        in_channels (int): number of input channels.\n        out_channels (int): number of output channels.",
        "detail": "env.src.tts.TTS.tts.layers.generic.res_conv_bn",
        "documentation": {}
    },
    {
        "label": "TimeDepthSeparableConv",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.generic.time_depth_sep_conv",
        "description": "env.src.tts.TTS.tts.layers.generic.time_depth_sep_conv",
        "peekOfCode": "class TimeDepthSeparableConv(nn.Module):\n    \"\"\"Time depth separable convolution as in https://arxiv.org/pdf/1904.02619.pdf\n    It shows competative results with less computation and memory footprint.\"\"\"\n    def __init__(self, in_channels, hid_channels, out_channels, kernel_size, bias=True):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.hid_channels = hid_channels\n        self.kernel_size = kernel_size\n        self.time_conv = nn.Conv1d(",
        "detail": "env.src.tts.TTS.tts.layers.generic.time_depth_sep_conv",
        "documentation": {}
    },
    {
        "label": "TimeDepthSeparableConvBlock",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.generic.time_depth_sep_conv",
        "description": "env.src.tts.TTS.tts.layers.generic.time_depth_sep_conv",
        "peekOfCode": "class TimeDepthSeparableConvBlock(nn.Module):\n    def __init__(self, in_channels, hid_channels, out_channels, num_layers, kernel_size, bias=True):\n        super().__init__()\n        assert (kernel_size - 1) % 2 == 0\n        assert num_layers > 1\n        self.layers = nn.ModuleList()\n        layer = TimeDepthSeparableConv(\n            in_channels, hid_channels, out_channels if num_layers == 1 else hid_channels, kernel_size, bias\n        )\n        self.layers.append(layer)",
        "detail": "env.src.tts.TTS.tts.layers.generic.time_depth_sep_conv",
        "documentation": {}
    },
    {
        "label": "FFTransformer",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.generic.transformer",
        "description": "env.src.tts.TTS.tts.layers.generic.transformer",
        "peekOfCode": "class FFTransformer(nn.Module):\n    def __init__(self, in_out_channels, num_heads, hidden_channels_ffn=1024, kernel_size_fft=3, dropout_p=0.1):\n        super().__init__()\n        self.self_attn = nn.MultiheadAttention(in_out_channels, num_heads, dropout=dropout_p)\n        padding = (kernel_size_fft - 1) // 2\n        self.conv1 = nn.Conv1d(in_out_channels, hidden_channels_ffn, kernel_size=kernel_size_fft, padding=padding)\n        self.conv2 = nn.Conv1d(hidden_channels_ffn, in_out_channels, kernel_size=kernel_size_fft, padding=padding)\n        self.norm1 = nn.LayerNorm(in_out_channels)\n        self.norm2 = nn.LayerNorm(in_out_channels)\n        self.dropout1 = nn.Dropout(dropout_p)",
        "detail": "env.src.tts.TTS.tts.layers.generic.transformer",
        "documentation": {}
    },
    {
        "label": "FFTransformerBlock",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.generic.transformer",
        "description": "env.src.tts.TTS.tts.layers.generic.transformer",
        "peekOfCode": "class FFTransformerBlock(nn.Module):\n    def __init__(self, in_out_channels, num_heads, hidden_channels_ffn, num_layers, dropout_p):\n        super().__init__()\n        self.fft_layers = nn.ModuleList(\n            [\n                FFTransformer(\n                    in_out_channels=in_out_channels,\n                    num_heads=num_heads,\n                    hidden_channels_ffn=hidden_channels_ffn,\n                    dropout_p=dropout_p,",
        "detail": "env.src.tts.TTS.tts.layers.generic.transformer",
        "documentation": {}
    },
    {
        "label": "FFTDurationPredictor",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.generic.transformer",
        "description": "env.src.tts.TTS.tts.layers.generic.transformer",
        "peekOfCode": "class FFTDurationPredictor:\n    def __init__(\n        self, in_channels, hidden_channels, num_heads, num_layers, dropout_p=0.1, cond_channels=None\n    ):  # pylint: disable=unused-argument\n        self.fft = FFTransformerBlock(in_channels, num_heads, hidden_channels, num_layers, dropout_p)\n        self.proj = nn.Linear(in_channels, 1)\n    def forward(self, x, mask=None, g=None):  # pylint: disable=unused-argument\n        \"\"\"\n        Shapes:\n            - x: :math:`[B, C, T]`",
        "detail": "env.src.tts.TTS.tts.layers.generic.transformer",
        "documentation": {}
    },
    {
        "label": "WN",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.generic.wavenet",
        "description": "env.src.tts.TTS.tts.layers.generic.wavenet",
        "peekOfCode": "class WN(torch.nn.Module):\n    \"\"\"Wavenet layers with weight norm and no input conditioning.\n         |-----------------------------------------------------------------------------|\n         |                                    |-> tanh    -|                           |\n    res -|- conv1d(dilation) -> dropout -> + -|            * -> conv1d1x1 -> split -|- + -> res\n    g -------------------------------------|  |-> sigmoid -|                        |\n    o --------------------------------------------------------------------------- + --------- o\n    Args:\n        in_channels (int): number of input channels.\n        hidden_channes (int): number of hidden channels.",
        "detail": "env.src.tts.TTS.tts.layers.generic.wavenet",
        "documentation": {}
    },
    {
        "label": "WNBlocks",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.generic.wavenet",
        "description": "env.src.tts.TTS.tts.layers.generic.wavenet",
        "peekOfCode": "class WNBlocks(nn.Module):\n    \"\"\"Wavenet blocks.\n    Note: After each block dilation resets to 1 and it increases in each block\n        along the dilation rate.\n    Args:\n        in_channels (int): number of input channels.\n        hidden_channes (int): number of hidden channels.\n        kernel_size (int): filter kernel size for the first conv layer.\n        dilation_rate (int): dilations rate to increase dilation per layer.\n            If it is 2, dilations are 1, 2, 4, 8 for the next 4 layers.",
        "detail": "env.src.tts.TTS.tts.layers.generic.wavenet",
        "documentation": {}
    },
    {
        "label": "fused_add_tanh_sigmoid_multiply",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.generic.wavenet",
        "description": "env.src.tts.TTS.tts.layers.generic.wavenet",
        "peekOfCode": "def fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):\n    n_channels_int = n_channels[0]\n    in_act = input_a + input_b\n    t_act = torch.tanh(in_act[:, :n_channels_int, :])\n    s_act = torch.sigmoid(in_act[:, n_channels_int:, :])\n    acts = t_act * s_act\n    return acts\nclass WN(torch.nn.Module):\n    \"\"\"Wavenet layers with weight norm and no input conditioning.\n         |-----------------------------------------------------------------------------|",
        "detail": "env.src.tts.TTS.tts.layers.generic.wavenet",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.glow_tts.decoder",
        "description": "env.src.tts.TTS.tts.layers.glow_tts.decoder",
        "peekOfCode": "class Decoder(nn.Module):\n    \"\"\"Stack of Glow Decoder Modules.\n    ::\n        Squeeze -> ActNorm -> InvertibleConv1x1 -> AffineCoupling -> Unsqueeze\n    Args:\n        in_channels (int): channels of input tensor.\n        hidden_channels (int): hidden decoder channels.\n        kernel_size (int): Coupling block kernel size. (Wavenet filter kernel size.)\n        dilation_rate (int): rate to increase dilation by each layer in a decoder block.\n        num_flow_blocks (int): number of decoder blocks.",
        "detail": "env.src.tts.TTS.tts.layers.glow_tts.decoder",
        "documentation": {}
    },
    {
        "label": "squeeze",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.glow_tts.decoder",
        "description": "env.src.tts.TTS.tts.layers.glow_tts.decoder",
        "peekOfCode": "def squeeze(x, x_mask=None, num_sqz=2):\n    \"\"\"GlowTTS squeeze operation\n    Increase number of channels and reduce number of time steps\n    by the same factor.\n    Note:\n        each 's' is a n-dimensional vector.\n        ``[s1,s2,s3,s4,s5,s6] --> [[s1, s3, s5], [s2, s4, s6]]``\n    \"\"\"\n    b, c, t = x.size()\n    t = (t // num_sqz) * num_sqz",
        "detail": "env.src.tts.TTS.tts.layers.glow_tts.decoder",
        "documentation": {}
    },
    {
        "label": "unsqueeze",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.glow_tts.decoder",
        "description": "env.src.tts.TTS.tts.layers.glow_tts.decoder",
        "peekOfCode": "def unsqueeze(x, x_mask=None, num_sqz=2):\n    \"\"\"GlowTTS unsqueeze operation (revert the squeeze)\n    Note:\n        each 's' is a n-dimensional vector.\n        ``[[s1, s3, s5], [s2, s4, s6]] --> [[s1, s3, s5, s2, s4, s6]]``\n    \"\"\"\n    b, c, t = x.size()\n    x_unsqz = x.view(b, num_sqz, c // num_sqz, t)\n    x_unsqz = x_unsqz.permute(0, 2, 3, 1).contiguous().view(b, c // num_sqz, t * num_sqz)\n    if x_mask is not None:",
        "detail": "env.src.tts.TTS.tts.layers.glow_tts.decoder",
        "documentation": {}
    },
    {
        "label": "DurationPredictor",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.glow_tts.duration_predictor",
        "description": "env.src.tts.TTS.tts.layers.glow_tts.duration_predictor",
        "peekOfCode": "class DurationPredictor(nn.Module):\n    \"\"\"Glow-TTS duration prediction model.\n    ::\n        [2 x (conv1d_kxk -> relu -> layer_norm -> dropout)] -> conv1d_1x1 -> durs\n    Args:\n        in_channels (int): Number of channels of the input tensor.\n        hidden_channels (int): Number of hidden channels of the network.\n        kernel_size (int): Kernel size for the conv layers.\n        dropout_p (float): Dropout rate used after each conv layer.\n    \"\"\"",
        "detail": "env.src.tts.TTS.tts.layers.glow_tts.duration_predictor",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.glow_tts.encoder",
        "description": "env.src.tts.TTS.tts.layers.glow_tts.encoder",
        "peekOfCode": "class Encoder(nn.Module):\n    \"\"\"Glow-TTS encoder module.\n    ::\n        embedding -> <prenet> -> encoder_module -> <postnet> --> proj_mean\n                                                             |\n                                                             |-> proj_var\n                                                             |\n                                                             |-> concat -> duration_predictor\n                                                                    \n                                                              speaker_embed",
        "detail": "env.src.tts.TTS.tts.layers.glow_tts.encoder",
        "documentation": {}
    },
    {
        "label": "ResidualConv1dLayerNormBlock",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.glow_tts.glow",
        "description": "env.src.tts.TTS.tts.layers.glow_tts.glow",
        "peekOfCode": "class ResidualConv1dLayerNormBlock(nn.Module):\n    \"\"\"Conv1d with Layer Normalization and residual connection as in GlowTTS paper.\n    https://arxiv.org/pdf/1811.00002.pdf\n    ::\n        x |-> conv1d -> layer_norm -> relu -> dropout -> + -> o\n          |---------------> conv1d_1x1 ------------------|\n    Args:\n        in_channels (int): number of input tensor channels.\n        hidden_channels (int): number of inner layer channels.\n        out_channels (int): number of output tensor channels.",
        "detail": "env.src.tts.TTS.tts.layers.glow_tts.glow",
        "documentation": {}
    },
    {
        "label": "InvConvNear",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.glow_tts.glow",
        "description": "env.src.tts.TTS.tts.layers.glow_tts.glow",
        "peekOfCode": "class InvConvNear(nn.Module):\n    \"\"\"Invertible Convolution with input splitting as in GlowTTS paper.\n    https://arxiv.org/pdf/1811.00002.pdf\n    Args:\n        channels (int): input and output channels.\n        num_splits (int): number of splits, also H and W of conv layer.\n        no_jacobian (bool): enable/disable jacobian computations.\n    Note:\n        Split the input into groups of size self.num_splits and\n        perform 1x1 convolution separately. Cast 1x1 conv operation",
        "detail": "env.src.tts.TTS.tts.layers.glow_tts.glow",
        "documentation": {}
    },
    {
        "label": "CouplingBlock",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.glow_tts.glow",
        "description": "env.src.tts.TTS.tts.layers.glow_tts.glow",
        "peekOfCode": "class CouplingBlock(nn.Module):\n    \"\"\"Glow Affine Coupling block as in GlowTTS paper.\n    https://arxiv.org/pdf/1811.00002.pdf\n    ::\n        x --> x0 -> conv1d -> wavenet -> conv1d --> t, s -> concat(s*x1 + t, x0) -> o\n        '-> x1 - - - - - - - - - - - - - - - - - - - - - - - - - ^\n    Args:\n         in_channels (int): number of input tensor channels.\n         hidden_channels (int): number of hidden channels.\n         kernel_size (int): WaveNet filter kernel size.",
        "detail": "env.src.tts.TTS.tts.layers.glow_tts.glow",
        "documentation": {}
    },
    {
        "label": "RelativePositionMultiHeadAttention",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.glow_tts.transformer",
        "description": "env.src.tts.TTS.tts.layers.glow_tts.transformer",
        "peekOfCode": "class RelativePositionMultiHeadAttention(nn.Module):\n    \"\"\"Multi-head attention with Relative Positional embedding.\n    https://arxiv.org/pdf/1809.04281.pdf\n    It learns positional embeddings for a window of neighbours. For keys and values,\n    it learns different set of embeddings. Key embeddings are agregated with the attention\n    scores and value embeddings are aggregated with the output.\n    Note:\n        Example with relative attention window size 2\n        - input = [a, b, c, d, e]\n        - rel_attn_embeddings = [e(t-2), e(t-1), e(t+1), e(t+2)]",
        "detail": "env.src.tts.TTS.tts.layers.glow_tts.transformer",
        "documentation": {}
    },
    {
        "label": "FeedForwardNetwork",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.glow_tts.transformer",
        "description": "env.src.tts.TTS.tts.layers.glow_tts.transformer",
        "peekOfCode": "class FeedForwardNetwork(nn.Module):\n    \"\"\"Feed Forward Inner layers for Transformer.\n    Args:\n        in_channels (int): input tensor channels.\n        out_channels (int): output tensor channels.\n        hidden_channels (int): inner layers hidden channels.\n        kernel_size (int): conv1d filter kernel size.\n        dropout_p (float, optional): dropout rate. Defaults to 0.\n    \"\"\"\n    def __init__(self, in_channels, out_channels, hidden_channels, kernel_size, dropout_p=0.0, causal=False):",
        "detail": "env.src.tts.TTS.tts.layers.glow_tts.transformer",
        "documentation": {}
    },
    {
        "label": "RelativePositionTransformer",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.glow_tts.transformer",
        "description": "env.src.tts.TTS.tts.layers.glow_tts.transformer",
        "peekOfCode": "class RelativePositionTransformer(nn.Module):\n    \"\"\"Transformer with Relative Potional Encoding.\n    https://arxiv.org/abs/1803.02155\n    Args:\n        in_channels (int): number of channels of the input tensor.\n        out_chanels (int): number of channels of the output tensor.\n        hidden_channels (int): model hidden channels.\n        hidden_channels_ffn (int): hidden channels of FeedForwardNetwork.\n        num_heads (int): number of attention heads.\n        num_layers (int): number of transformer layers.",
        "detail": "env.src.tts.TTS.tts.layers.glow_tts.transformer",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.overflow.common_layers",
        "description": "env.src.tts.TTS.tts.layers.overflow.common_layers",
        "peekOfCode": "class Encoder(nn.Module):\n    r\"\"\"Neural HMM Encoder\n    Same as Tacotron 2 encoder but increases the input length by states per phone\n    Args:\n        num_chars (int): Number of characters in the input.\n        state_per_phone (int): Number of states per phone.\n        in_out_channels (int): number of input and output channels.\n        n_convolutions (int): number of convolutional layers.\n    \"\"\"\n    def __init__(self, num_chars, state_per_phone, in_out_channels=512, n_convolutions=3):",
        "detail": "env.src.tts.TTS.tts.layers.overflow.common_layers",
        "documentation": {}
    },
    {
        "label": "ParameterModel",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.overflow.common_layers",
        "description": "env.src.tts.TTS.tts.layers.overflow.common_layers",
        "peekOfCode": "class ParameterModel(nn.Module):\n    r\"\"\"Main neural network of the outputnet\n    Note: Do not put dropout layers here, the model will not converge.\n    Args:\n            outputnet_size (List[int]): the architecture of the parameter model\n            input_size (int): size of input for the first layer\n            output_size (int): size of output i.e size of the feature dim\n            frame_channels (int): feature dim to set the flat start bias\n            flat_start_params (dict): flat start parameters to set the bias\n    \"\"\"",
        "detail": "env.src.tts.TTS.tts.layers.overflow.common_layers",
        "documentation": {}
    },
    {
        "label": "Outputnet",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.overflow.common_layers",
        "description": "env.src.tts.TTS.tts.layers.overflow.common_layers",
        "peekOfCode": "class Outputnet(nn.Module):\n    r\"\"\"\n    This network takes current state and previous observed values as input\n    and returns its parameters, mean, standard deviation and probability\n    of transition to the next state\n    \"\"\"\n    def __init__(\n        self,\n        encoder_dim: int,\n        memory_rnn_dim: int,",
        "detail": "env.src.tts.TTS.tts.layers.overflow.common_layers",
        "documentation": {}
    },
    {
        "label": "OverflowUtils",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.overflow.common_layers",
        "description": "env.src.tts.TTS.tts.layers.overflow.common_layers",
        "peekOfCode": "class OverflowUtils:\n    @staticmethod\n    def get_data_parameters_for_flat_start(\n        data_loader: torch.utils.data.DataLoader, out_channels: int, states_per_phone: int\n    ):\n        \"\"\"Generates data parameters for flat starting the HMM.\n        Args:\n            data_loader (torch.utils.data.Dataloader): _description_\n            out_channels (int): mel spectrogram channels\n            states_per_phone (_type_): HMM states per phone",
        "detail": "env.src.tts.TTS.tts.layers.overflow.common_layers",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.overflow.decoder",
        "description": "env.src.tts.TTS.tts.layers.overflow.decoder",
        "peekOfCode": "class Decoder(nn.Module):\n    \"\"\"Uses glow decoder with some modifications.\n    ::\n        Squeeze -> ActNorm -> InvertibleConv1x1 -> AffineCoupling -> Unsqueeze\n    Args:\n        in_channels (int): channels of input tensor.\n        hidden_channels (int): hidden decoder channels.\n        kernel_size (int): Coupling block kernel size. (Wavenet filter kernel size.)\n        dilation_rate (int): rate to increase dilation by each layer in a decoder block.\n        num_flow_blocks (int): number of decoder blocks.",
        "detail": "env.src.tts.TTS.tts.layers.overflow.decoder",
        "documentation": {}
    },
    {
        "label": "NeuralHMM",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.overflow.neural_hmm",
        "description": "env.src.tts.TTS.tts.layers.overflow.neural_hmm",
        "peekOfCode": "class NeuralHMM(nn.Module):\n    \"\"\"Autoregressive left to right HMM model primarily used in \"Neural HMMs are all you need (for high-quality attention-free TTS)\"\n    Paper::\n        https://arxiv.org/abs/2108.13320\n    Paper abstract::\n        Neural sequence-to-sequence TTS has achieved significantly better output quality than statistical speech synthesis using\n        HMMs. However, neural TTS is generally not probabilistic and uses non-monotonic attention. Attention failures increase\n        training time and can make synthesis babble incoherently. This paper describes how the old and new paradigms can be\n        combined to obtain the advantages of both worlds, by replacing attention in neural TTS with an autoregressive left-right\n        no-skip hidden Markov model defined by a neural network. Based on this proposal, we modify Tacotron 2 to obtain an",
        "detail": "env.src.tts.TTS.tts.layers.overflow.neural_hmm",
        "documentation": {}
    },
    {
        "label": "TransitionModel",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.overflow.neural_hmm",
        "description": "env.src.tts.TTS.tts.layers.overflow.neural_hmm",
        "peekOfCode": "class TransitionModel(nn.Module):\n    \"\"\"Transition Model of the HMM, it represents the probability of transitioning\n    form current state to all other states\"\"\"\n    def forward(self, log_alpha_scaled, transition_vector, inputs_len):  # pylint: disable=no-self-use\n        r\"\"\"\n        product of the past state with transitional probabilities in log space\n        Args:\n            log_alpha_scaled (torch.Tensor): Multiply previous timestep's alphas by\n                        transition matrix (in log domain)\n                - shape: (batch size, N)",
        "detail": "env.src.tts.TTS.tts.layers.overflow.neural_hmm",
        "documentation": {}
    },
    {
        "label": "EmissionModel",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.overflow.neural_hmm",
        "description": "env.src.tts.TTS.tts.layers.overflow.neural_hmm",
        "peekOfCode": "class EmissionModel(nn.Module):\n    \"\"\"Emission Model of the HMM, it represents the probability of\n    emitting an observation based on the current state\"\"\"\n    def __init__(self) -> None:\n        super().__init__()\n        self.distribution_function: tdist.Distribution = tdist.normal.Normal\n    def sample(self, means, stds, sampling_temp):\n        return self.distribution_function(means, stds * sampling_temp).sample() if sampling_temp > 0 else means\n    def forward(self, x_t, means, stds, state_lengths):\n        r\"\"\"Calculates the log probability of the the given data (x_t)",
        "detail": "env.src.tts.TTS.tts.layers.overflow.neural_hmm",
        "documentation": {}
    },
    {
        "label": "validate_numpy_array",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.overflow.plotting_utils",
        "description": "env.src.tts.TTS.tts.layers.overflow.plotting_utils",
        "peekOfCode": "def validate_numpy_array(value: Any):\n    r\"\"\"\n    Validates the input and makes sure it returns a numpy array (i.e on CPU)\n    Args:\n        value (Any): the input value\n    Raises:\n        TypeError: if the value is not a numpy array or torch tensor\n    Returns:\n        np.ndarray: numpy array of the value\n    \"\"\"",
        "detail": "env.src.tts.TTS.tts.layers.overflow.plotting_utils",
        "documentation": {}
    },
    {
        "label": "get_spec_from_most_probable_state",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.overflow.plotting_utils",
        "description": "env.src.tts.TTS.tts.layers.overflow.plotting_utils",
        "peekOfCode": "def get_spec_from_most_probable_state(log_alpha_scaled, means, decoder=None):\n    \"\"\"Get the most probable state means from the log_alpha_scaled.\n    Args:\n        log_alpha_scaled (torch.Tensor): Log alpha scaled values.\n            - Shape: :math:`(T, N)`\n        means (torch.Tensor): Means of the states.\n            - Shape: :math:`(N, T, D_out)`\n        decoder (torch.nn.Module): Decoder module to decode the latent to melspectrogram. Defaults to None.\n    \"\"\"\n    max_state_numbers = torch.max(log_alpha_scaled, dim=1)[1]",
        "detail": "env.src.tts.TTS.tts.layers.overflow.plotting_utils",
        "documentation": {}
    },
    {
        "label": "plot_transition_probabilities_to_numpy",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.overflow.plotting_utils",
        "description": "env.src.tts.TTS.tts.layers.overflow.plotting_utils",
        "peekOfCode": "def plot_transition_probabilities_to_numpy(states, transition_probabilities, output_fig=False):\n    \"\"\"Generates trainsition probabilities plot for the states and the probability of transition.\n    Args:\n        states (torch.IntTensor): the states\n        transition_probabilities (torch.FloatTensor): the transition probabilities\n    \"\"\"\n    states = validate_numpy_array(states)\n    transition_probabilities = validate_numpy_array(transition_probabilities)\n    fig, ax = plt.subplots(figsize=(30, 3))\n    ax.plot(transition_probabilities, \"o\")",
        "detail": "env.src.tts.TTS.tts.layers.overflow.plotting_utils",
        "documentation": {}
    },
    {
        "label": "LocationLayer",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.attentions",
        "description": "env.src.tts.TTS.tts.layers.tacotron.attentions",
        "peekOfCode": "class LocationLayer(nn.Module):\n    \"\"\"Layers for Location Sensitive Attention\n    Args:\n        attention_dim (int): number of channels in the input tensor.\n        attention_n_filters (int, optional): number of filters in convolution. Defaults to 32.\n        attention_kernel_size (int, optional): kernel size of convolution filter. Defaults to 31.\n    \"\"\"\n    def __init__(self, attention_dim, attention_n_filters=32, attention_kernel_size=31):\n        super().__init__()\n        self.location_conv1d = nn.Conv1d(",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.attentions",
        "documentation": {}
    },
    {
        "label": "GravesAttention",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.attentions",
        "description": "env.src.tts.TTS.tts.layers.tacotron.attentions",
        "peekOfCode": "class GravesAttention(nn.Module):\n    \"\"\"Graves Attention as is ref1 with updates from ref2.\n    ref1: https://arxiv.org/abs/1910.10288\n    ref2: https://arxiv.org/pdf/1906.01083.pdf\n    Args:\n        query_dim (int): number of channels in query tensor.\n        K (int): number of Gaussian heads to be used for computing attention.\n    \"\"\"\n    COEF = 0.3989422917366028  # numpy.sqrt(1/(2*numpy.pi))\n    def __init__(self, query_dim, K):",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.attentions",
        "documentation": {}
    },
    {
        "label": "OriginalAttention",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.attentions",
        "description": "env.src.tts.TTS.tts.layers.tacotron.attentions",
        "peekOfCode": "class OriginalAttention(nn.Module):\n    \"\"\"Bahdanau Attention with various optional modifications.\n    - Location sensitive attnetion: https://arxiv.org/abs/1712.05884\n    - Forward Attention: https://arxiv.org/abs/1807.06736 + state masking at inference\n    - Using sigmoid instead of softmax normalization\n    - Attention windowing at inference time\n    Note:\n        Location Sensitive Attention extends the additive attention mechanism\n    to use cumulative attention weights from previous decoder time steps with the current time step features.\n        Forward attention computes most probable monotonic alignment. The modified attention probabilities at each",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.attentions",
        "documentation": {}
    },
    {
        "label": "MonotonicDynamicConvolutionAttention",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.attentions",
        "description": "env.src.tts.TTS.tts.layers.tacotron.attentions",
        "peekOfCode": "class MonotonicDynamicConvolutionAttention(nn.Module):\n    \"\"\"Dynamic convolution attention from\n    https://arxiv.org/pdf/1910.10288.pdf\n    query -> linear -> tanh -> linear ->|\n                                        |                                            mask values\n                                        v                                              |    |\n               atten_w(t-1) -|-> conv1d_dynamic -> linear -|-> tanh -> + -> softmax -> * -> * -> context\n                             |-> conv1d_static  -> linear -|           |\n                             |-> conv1d_prior   -> log ----------------|\n    query: attention rnn output.",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.attentions",
        "documentation": {}
    },
    {
        "label": "init_attn",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.attentions",
        "description": "env.src.tts.TTS.tts.layers.tacotron.attentions",
        "peekOfCode": "def init_attn(\n    attn_type,\n    query_dim,\n    embedding_dim,\n    attention_dim,\n    location_attention,\n    attention_location_n_filters,\n    attention_location_kernel_size,\n    windowing,\n    norm,",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.attentions",
        "documentation": {}
    },
    {
        "label": "CapacitronVAE",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.capacitron_layers",
        "description": "env.src.tts.TTS.tts.layers.tacotron.capacitron_layers",
        "peekOfCode": "class CapacitronVAE(nn.Module):\n    \"\"\"Effective Use of Variational Embedding Capacity for prosody transfer.\n    See https://arxiv.org/abs/1906.03402\"\"\"\n    def __init__(\n        self,\n        num_mel,\n        capacitron_VAE_embedding_dim,\n        encoder_output_dim=256,\n        reference_encoder_out_dim=128,\n        speaker_embedding_dim=None,",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.capacitron_layers",
        "documentation": {}
    },
    {
        "label": "ReferenceEncoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.capacitron_layers",
        "description": "env.src.tts.TTS.tts.layers.tacotron.capacitron_layers",
        "peekOfCode": "class ReferenceEncoder(nn.Module):\n    \"\"\"NN module creating a fixed size prosody embedding from a spectrogram.\n    inputs: mel spectrograms [batch_size, num_spec_frames, num_mel]\n    outputs: [batch_size, embedding_dim]\n    \"\"\"\n    def __init__(self, num_mel, out_dim):\n        super().__init__()\n        self.num_mel = num_mel\n        filters = [1] + [32, 32, 64, 64, 128, 128]\n        num_layers = len(filters) - 1",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.capacitron_layers",
        "documentation": {}
    },
    {
        "label": "TextSummary",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.capacitron_layers",
        "description": "env.src.tts.TTS.tts.layers.tacotron.capacitron_layers",
        "peekOfCode": "class TextSummary(nn.Module):\n    def __init__(self, embedding_dim, encoder_output_dim):\n        super().__init__()\n        self.lstm = nn.LSTM(\n            encoder_output_dim,  # text embedding dimension from the text encoder\n            embedding_dim,  # fixed length output summary the lstm creates from the input\n            batch_first=True,\n            bidirectional=False,\n        )\n    def forward(self, inputs, input_lengths):",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.capacitron_layers",
        "documentation": {}
    },
    {
        "label": "PostEncoderMLP",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.capacitron_layers",
        "description": "env.src.tts.TTS.tts.layers.tacotron.capacitron_layers",
        "peekOfCode": "class PostEncoderMLP(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super().__init__()\n        self.hidden_size = hidden_size\n        modules = [\n            nn.Linear(input_size, hidden_size),  # Hidden Layer\n            nn.Tanh(),\n            nn.Linear(hidden_size, hidden_size * 2),\n        ]  # Output layer twice the size for mean and variance\n        self.net = nn.Sequential(*modules)",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.capacitron_layers",
        "documentation": {}
    },
    {
        "label": "Linear",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.common_layers",
        "description": "env.src.tts.TTS.tts.layers.tacotron.common_layers",
        "peekOfCode": "class Linear(nn.Module):\n    \"\"\"Linear layer with a specific initialization.\n    Args:\n        in_features (int): number of channels in the input tensor.\n        out_features (int): number of channels in the output tensor.\n        bias (bool, optional): enable/disable bias in the layer. Defaults to True.\n        init_gain (str, optional): method to compute the gain in the weight initializtion based on the nonlinear activation used afterwards. Defaults to 'linear'.\n    \"\"\"\n    def __init__(self, in_features, out_features, bias=True, init_gain=\"linear\"):\n        super().__init__()",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.common_layers",
        "documentation": {}
    },
    {
        "label": "LinearBN",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.common_layers",
        "description": "env.src.tts.TTS.tts.layers.tacotron.common_layers",
        "peekOfCode": "class LinearBN(nn.Module):\n    \"\"\"Linear layer with Batch Normalization.\n    x -> linear -> BN -> o\n    Args:\n        in_features (int): number of channels in the input tensor.\n        out_features (int ): number of channels in the output tensor.\n        bias (bool, optional): enable/disable bias in the linear layer. Defaults to True.\n        init_gain (str, optional): method to set the gain for weight initialization. Defaults to 'linear'.\n    \"\"\"\n    def __init__(self, in_features, out_features, bias=True, init_gain=\"linear\"):",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.common_layers",
        "documentation": {}
    },
    {
        "label": "Prenet",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.common_layers",
        "description": "env.src.tts.TTS.tts.layers.tacotron.common_layers",
        "peekOfCode": "class Prenet(nn.Module):\n    \"\"\"Tacotron specific Prenet with an optional Batch Normalization.\n    Note:\n        Prenet with BN improves the model performance significantly especially\n    if it is enabled after learning a diagonal attention alignment with the original\n    prenet. However, if the target dataset is high quality then it also works from\n    the start. It is also suggested to disable dropout if BN is in use.\n        prenet_type == \"original\"\n            x -> [linear -> ReLU -> Dropout]xN -> o\n        prenet_type == \"bn\"",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.common_layers",
        "documentation": {}
    },
    {
        "label": "GST",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.gst_layers",
        "description": "env.src.tts.TTS.tts.layers.tacotron.gst_layers",
        "peekOfCode": "class GST(nn.Module):\n    \"\"\"Global Style Token Module for factorizing prosody in speech.\n    See https://arxiv.org/pdf/1803.09017\"\"\"\n    def __init__(self, num_mel, num_heads, num_style_tokens, gst_embedding_dim, embedded_speaker_dim=None):\n        super().__init__()\n        self.encoder = ReferenceEncoder(num_mel, gst_embedding_dim)\n        self.style_token_layer = StyleTokenLayer(num_heads, num_style_tokens, gst_embedding_dim, embedded_speaker_dim)\n    def forward(self, inputs, speaker_embedding=None):\n        enc_out = self.encoder(inputs)\n        # concat speaker_embedding",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.gst_layers",
        "documentation": {}
    },
    {
        "label": "ReferenceEncoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.gst_layers",
        "description": "env.src.tts.TTS.tts.layers.tacotron.gst_layers",
        "peekOfCode": "class ReferenceEncoder(nn.Module):\n    \"\"\"NN module creating a fixed size prosody embedding from a spectrogram.\n    inputs: mel spectrograms [batch_size, num_spec_frames, num_mel]\n    outputs: [batch_size, embedding_dim]\n    \"\"\"\n    def __init__(self, num_mel, embedding_dim):\n        super().__init__()\n        self.num_mel = num_mel\n        filters = [1] + [32, 32, 64, 64, 128, 128]\n        num_layers = len(filters) - 1",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.gst_layers",
        "documentation": {}
    },
    {
        "label": "StyleTokenLayer",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.gst_layers",
        "description": "env.src.tts.TTS.tts.layers.tacotron.gst_layers",
        "peekOfCode": "class StyleTokenLayer(nn.Module):\n    \"\"\"NN Module attending to style tokens based on prosody encodings.\"\"\"\n    def __init__(self, num_heads, num_style_tokens, gst_embedding_dim, d_vector_dim=None):\n        super().__init__()\n        self.query_dim = gst_embedding_dim // 2\n        if d_vector_dim:\n            self.query_dim += d_vector_dim\n        self.key_dim = gst_embedding_dim // num_heads\n        self.style_tokens = nn.Parameter(torch.FloatTensor(num_style_tokens, self.key_dim))\n        nn.init.normal_(self.style_tokens, mean=0, std=0.5)",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.gst_layers",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttention",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.gst_layers",
        "description": "env.src.tts.TTS.tts.layers.tacotron.gst_layers",
        "peekOfCode": "class MultiHeadAttention(nn.Module):\n    \"\"\"\n    input:\n        query --- [N, T_q, query_dim]\n        key --- [N, T_k, key_dim]\n    output:\n        out --- [N, T_q, num_units]\n    \"\"\"\n    def __init__(self, query_dim, key_dim, num_units, num_heads):\n        super().__init__()",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.gst_layers",
        "documentation": {}
    },
    {
        "label": "BatchNormConv1d",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.tacotron",
        "description": "env.src.tts.TTS.tts.layers.tacotron.tacotron",
        "peekOfCode": "class BatchNormConv1d(nn.Module):\n    r\"\"\"A wrapper for Conv1d with BatchNorm. It sets the activation\n    function between Conv and BatchNorm layers. BatchNorm layer\n    is initialized with the TF default values for momentum and eps.\n    Args:\n        in_channels: size of each input sample\n        out_channels: size of each output samples\n        kernel_size: kernel size of conv filters\n        stride: stride of conv filters\n        padding: padding of conv filters",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.tacotron",
        "documentation": {}
    },
    {
        "label": "Highway",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.tacotron",
        "description": "env.src.tts.TTS.tts.layers.tacotron.tacotron",
        "peekOfCode": "class Highway(nn.Module):\n    r\"\"\"Highway layers as explained in https://arxiv.org/abs/1505.00387\n    Args:\n        in_features (int): size of each input sample\n        out_feature (int): size of each output sample\n    Shapes:\n        - input: (B, *, H_in)\n        - output: (B, *, H_out)\n    \"\"\"\n    # TODO: Try GLU layer",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.tacotron",
        "documentation": {}
    },
    {
        "label": "CBHG",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.tacotron",
        "description": "env.src.tts.TTS.tts.layers.tacotron.tacotron",
        "peekOfCode": "class CBHG(nn.Module):\n    \"\"\"CBHG module: a recurrent neural network composed of:\n    - 1-d convolution banks\n    - Highway networks + residual connections\n    - Bidirectional gated recurrent units\n    Args:\n        in_features (int): sample size\n        K (int): max filter size in conv bank\n        projections (list): conv channel sizes for conv projections\n        num_highways (int): number of highways layers",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.tacotron",
        "documentation": {}
    },
    {
        "label": "EncoderCBHG",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.tacotron",
        "description": "env.src.tts.TTS.tts.layers.tacotron.tacotron",
        "peekOfCode": "class EncoderCBHG(nn.Module):\n    r\"\"\"CBHG module with Encoder specific arguments\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.cbhg = CBHG(\n            128,\n            K=16,\n            conv_bank_features=128,\n            conv_projections=[128, 128],\n            highway_features=128,",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.tacotron",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.tacotron",
        "description": "env.src.tts.TTS.tts.layers.tacotron.tacotron",
        "peekOfCode": "class Encoder(nn.Module):\n    r\"\"\"Stack Prenet and CBHG module for encoder\n    Args:\n        inputs (FloatTensor): embedding features\n    Shapes:\n        - inputs: (B, T, D_in)\n        - outputs: (B, T, 128 * 2)\n    \"\"\"\n    def __init__(self, in_features):\n        super().__init__()",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.tacotron",
        "documentation": {}
    },
    {
        "label": "PostCBHG",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.tacotron",
        "description": "env.src.tts.TTS.tts.layers.tacotron.tacotron",
        "peekOfCode": "class PostCBHG(nn.Module):\n    def __init__(self, mel_dim):\n        super().__init__()\n        self.cbhg = CBHG(\n            mel_dim,\n            K=8,\n            conv_bank_features=128,\n            conv_projections=[256, mel_dim],\n            highway_features=128,\n            gru_features=128,",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.tacotron",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.tacotron",
        "description": "env.src.tts.TTS.tts.layers.tacotron.tacotron",
        "peekOfCode": "class Decoder(nn.Module):\n    \"\"\"Tacotron decoder.\n    Args:\n        in_channels (int): number of input channels.\n        frame_channels (int): number of feature frame channels.\n        r (int): number of outputs per time step (reduction rate).\n        memory_size (int): size of the past window. if <= 0 memory_size = r\n        attn_type (string): type of attention used in decoder.\n        attn_windowing (bool): if true, define an attention window centered to maximum\n            attention response. It provides more robust attention alignment especially",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.tacotron",
        "documentation": {}
    },
    {
        "label": "StopNet",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.tacotron",
        "description": "env.src.tts.TTS.tts.layers.tacotron.tacotron",
        "peekOfCode": "class StopNet(nn.Module):\n    r\"\"\"Stopnet signalling decoder to stop inference.\n    Args:\n        in_features (int): feature dimension of input.\n    \"\"\"\n    def __init__(self, in_features):\n        super().__init__()\n        self.dropout = nn.Dropout(0.1)\n        self.linear = nn.Linear(in_features, 1)\n        torch.nn.init.xavier_uniform_(self.linear.weight, gain=torch.nn.init.calculate_gain(\"linear\"))",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.tacotron",
        "documentation": {}
    },
    {
        "label": "ConvBNBlock",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.tacotron2",
        "description": "env.src.tts.TTS.tts.layers.tacotron.tacotron2",
        "peekOfCode": "class ConvBNBlock(nn.Module):\n    r\"\"\"Convolutions with Batch Normalization and non-linear activation.\n    Args:\n        in_channels (int): number of input channels.\n        out_channels (int): number of output channels.\n        kernel_size (int): convolution kernel size.\n        activation (str): 'relu', 'tanh', None (linear).\n    Shapes:\n        - input: (B, C_in, T)\n        - output: (B, C_out, T)",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.tacotron2",
        "documentation": {}
    },
    {
        "label": "Postnet",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.tacotron2",
        "description": "env.src.tts.TTS.tts.layers.tacotron.tacotron2",
        "peekOfCode": "class Postnet(nn.Module):\n    r\"\"\"Tacotron2 Postnet\n    Args:\n        in_out_channels (int): number of output channels.\n    Shapes:\n        - input: (B, C_in, T)\n        - output: (B, C_in, T)\n    \"\"\"\n    def __init__(self, in_out_channels, num_convs=5):\n        super().__init__()",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.tacotron2",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.tacotron2",
        "description": "env.src.tts.TTS.tts.layers.tacotron.tacotron2",
        "peekOfCode": "class Encoder(nn.Module):\n    r\"\"\"Tacotron2 Encoder\n    Args:\n        in_out_channels (int): number of input and output channels.\n    Shapes:\n        - input: (B, C_in, T)\n        - output: (B, C_in, T)\n    \"\"\"\n    def __init__(self, in_out_channels=512):\n        super().__init__()",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.tacotron2",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tacotron.tacotron2",
        "description": "env.src.tts.TTS.tts.layers.tacotron.tacotron2",
        "peekOfCode": "class Decoder(nn.Module):\n    \"\"\"Tacotron2 decoder. We don't use Zoneout but Dropout between RNN layers.\n    Args:\n        in_channels (int): number of input channels.\n        frame_channels (int): number of feature frame channels.\n        r (int): number of outputs per time step (reduction rate).\n        memory_size (int): size of the past window. if <= 0 memory_size = r\n        attn_type (string): type of attention used in decoder.\n        attn_win (bool): if true, define an attention window centered to maximum\n            attention response. It provides more robust attention alignment especially",
        "detail": "env.src.tts.TTS.tts.layers.tacotron.tacotron2",
        "documentation": {}
    },
    {
        "label": "GroupNorm32",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "peekOfCode": "class GroupNorm32(nn.GroupNorm):\n    def forward(self, x):\n        return super().forward(x.float()).type(x.dtype)\ndef normalization(channels):\n    \"\"\"\n    Make a standard normalization layer.\n    :param channels: number of input channels.\n    :return: an nn.Module for normalization.\n    \"\"\"\n    groups = 32",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "documentation": {}
    },
    {
        "label": "QKVAttentionLegacy",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "peekOfCode": "class QKVAttentionLegacy(nn.Module):\n    \"\"\"\n    A module which performs QKV attention. Matches legacy QKVAttention + input/output heads shaping\n    \"\"\"\n    def __init__(self, n_heads):\n        super().__init__()\n        self.n_heads = n_heads\n    def forward(self, qkv, mask=None, rel_pos=None):\n        \"\"\"\n        Apply QKV attention.",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "documentation": {}
    },
    {
        "label": "AttentionBlock",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "peekOfCode": "class AttentionBlock(nn.Module):\n    \"\"\"\n    An attention block that allows spatial positions to attend to each other.\n    Originally ported from here, but adapted to the N-d case.\n    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/models/unet.py#L66.\n    \"\"\"\n    def __init__(\n        self,\n        channels,\n        num_heads=1,",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "documentation": {}
    },
    {
        "label": "Upsample",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "peekOfCode": "class Upsample(nn.Module):\n    \"\"\"\n    An upsampling layer with an optional convolution.\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    \"\"\"\n    def __init__(self, channels, use_conv, out_channels=None, factor=4):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "documentation": {}
    },
    {
        "label": "Downsample",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "peekOfCode": "class Downsample(nn.Module):\n    \"\"\"\n    A downsampling layer with an optional convolution.\n    :param channels: channels in the inputs and outputs.\n    :param use_conv: a bool determining if a convolution is applied.\n    \"\"\"\n    def __init__(self, channels, use_conv, out_channels=None, factor=4, ksize=5, pad=2):\n        super().__init__()\n        self.channels = channels\n        self.out_channels = out_channels or channels",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "documentation": {}
    },
    {
        "label": "ResBlock",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "peekOfCode": "class ResBlock(nn.Module):\n    def __init__(\n        self,\n        channels,\n        dropout,\n        out_channels=None,\n        use_conv=False,\n        use_scale_shift_norm=False,\n        up=False,\n        down=False,",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "documentation": {}
    },
    {
        "label": "AudioMiniEncoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "peekOfCode": "class AudioMiniEncoder(nn.Module):\n    def __init__(\n        self,\n        spec_dim,\n        embedding_dim,\n        base_channels=128,\n        depth=2,\n        resnet_blocks=2,\n        attn_blocks=4,\n        num_attn_heads=4,",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "documentation": {}
    },
    {
        "label": "TorchMelSpectrogram",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "peekOfCode": "class TorchMelSpectrogram(nn.Module):\n    def __init__(\n        self,\n        filter_length=1024,\n        hop_length=256,\n        win_length=1024,\n        n_mel_channels=80,\n        mel_fmin=0,\n        mel_fmax=8000,\n        sampling_rate=22050,",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "documentation": {}
    },
    {
        "label": "CheckpointedLayer",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "peekOfCode": "class CheckpointedLayer(nn.Module):\n    \"\"\"\n    Wraps a module. When forward() is called, passes kwargs that require_grad through torch.checkpoint() and bypasses\n    checkpoint for all other args.\n    \"\"\"\n    def __init__(self, wrap):\n        super().__init__()\n        self.wrap = wrap\n    def forward(self, x, *args, **kwargs):\n        for k, v in kwargs.items():",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "documentation": {}
    },
    {
        "label": "CheckpointedXTransformerEncoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "peekOfCode": "class CheckpointedXTransformerEncoder(nn.Module):\n    \"\"\"\n    Wraps a ContinuousTransformerWrapper and applies CheckpointedLayer to each layer and permutes from channels-mid\n    to channels-last that XTransformer expects.\n    \"\"\"\n    def __init__(self, needs_permute=True, exit_permute=True, checkpoint=True, **xtransformer_kwargs):\n        super().__init__()\n        self.transformer = ContinuousTransformerWrapper(**xtransformer_kwargs)\n        self.needs_permute = needs_permute\n        self.exit_permute = exit_permute",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "documentation": {}
    },
    {
        "label": "TypicalLogitsWarper",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "peekOfCode": "class TypicalLogitsWarper(LogitsWarper):\n    def __init__(\n        self,\n        mass: float = 0.9,\n        filter_value: float = -float(\"Inf\"),\n        min_tokens_to_keep: int = 1,\n    ):\n        self.filter_value = filter_value\n        self.mass = mass\n        self.min_tokens_to_keep = min_tokens_to_keep",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "documentation": {}
    },
    {
        "label": "zero_module",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "peekOfCode": "def zero_module(module):\n    \"\"\"\n    Zero out the parameters of a module and return it.\n    \"\"\"\n    for p in module.parameters():\n        p.detach().zero_()\n    return module\nclass GroupNorm32(nn.GroupNorm):\n    def forward(self, x):\n        return super().forward(x.float()).type(x.dtype)",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "documentation": {}
    },
    {
        "label": "normalization",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "peekOfCode": "def normalization(channels):\n    \"\"\"\n    Make a standard normalization layer.\n    :param channels: number of input channels.\n    :return: an nn.Module for normalization.\n    \"\"\"\n    groups = 32\n    if channels <= 16:\n        groups = 8\n    elif channels <= 64:",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MEL_NORM_FILE",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "peekOfCode": "DEFAULT_MEL_NORM_FILE = \"https://coqui.gateway.scarf.sh/v0.14.1_models/mel_norms.pth\"\nclass TorchMelSpectrogram(nn.Module):\n    def __init__(\n        self,\n        filter_length=1024,\n        hop_length=256,\n        win_length=1024,\n        n_mel_channels=80,\n        mel_fmin=0,\n        mel_fmax=8000,",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.arch_utils",
        "documentation": {}
    },
    {
        "label": "load_wav_to_torch",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "peekOfCode": "def load_wav_to_torch(full_path):\n    sampling_rate, data = read(full_path)\n    if data.dtype == np.int32:\n        norm_fix = 2**31\n    elif data.dtype == np.int16:\n        norm_fix = 2**15\n    elif data.dtype == np.float16 or data.dtype == np.float32:\n        norm_fix = 1.0\n    else:\n        raise NotImplementedError(f\"Provided data dtype not supported: {data.dtype}\")",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "documentation": {}
    },
    {
        "label": "check_audio",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "peekOfCode": "def check_audio(audio, audiopath: str):\n    # Check some assumptions about audio range. This should be automatically fixed in load_wav_to_torch, but might not be in some edge cases, where we should squawk.\n    # '2' is arbitrarily chosen since it seems like audio will often \"overdrive\" the [-1,1] bounds.\n    if torch.any(audio > 2) or not torch.any(audio < 0):\n        print(f\"Error with {audiopath}. Max={audio.max()} min={audio.min()}\")\n    audio.clip_(-1, 1)\ndef read_audio_file(audiopath: str):\n    if audiopath[-4:] == \".wav\":\n        audio, lsr = load_wav_to_torch(audiopath)\n    elif audiopath[-4:] == \".mp3\":",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "documentation": {}
    },
    {
        "label": "read_audio_file",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "peekOfCode": "def read_audio_file(audiopath: str):\n    if audiopath[-4:] == \".wav\":\n        audio, lsr = load_wav_to_torch(audiopath)\n    elif audiopath[-4:] == \".mp3\":\n        audio, lsr = librosa.load(audiopath, sr=None)\n        audio = torch.FloatTensor(audio)\n    else:\n        assert False, f\"Unsupported audio format provided: {audiopath[-4:]}\"\n    # Remove any channel data.\n    if len(audio.shape) > 1:",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "documentation": {}
    },
    {
        "label": "load_required_audio",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "peekOfCode": "def load_required_audio(audiopath: str):\n    audio, lsr = read_audio_file(audiopath)\n    audios = [torchaudio.functional.resample(audio, lsr, sampling_rate) for sampling_rate in (22050, 24000)]\n    for audio in audios:\n        check_audio(audio, audiopath)\n    return [audio.unsqueeze(0) for audio in audios]\ndef load_audio(audiopath, sampling_rate):\n    audio, lsr = read_audio_file(audiopath)\n    if lsr != sampling_rate:\n        audio = torchaudio.functional.resample(audio, lsr, sampling_rate)",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "documentation": {}
    },
    {
        "label": "load_audio",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "peekOfCode": "def load_audio(audiopath, sampling_rate):\n    audio, lsr = read_audio_file(audiopath)\n    if lsr != sampling_rate:\n        audio = torchaudio.functional.resample(audio, lsr, sampling_rate)\n    check_audio(audio, audiopath)\n    return audio.unsqueeze(0)\nTACOTRON_MEL_MAX = 2.3143386840820312\nTACOTRON_MEL_MIN = -11.512925148010254\ndef denormalize_tacotron_mel(norm_mel):\n    return ((norm_mel + 1) / 2) * (TACOTRON_MEL_MAX - TACOTRON_MEL_MIN) + TACOTRON_MEL_MIN",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "documentation": {}
    },
    {
        "label": "denormalize_tacotron_mel",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "peekOfCode": "def denormalize_tacotron_mel(norm_mel):\n    return ((norm_mel + 1) / 2) * (TACOTRON_MEL_MAX - TACOTRON_MEL_MIN) + TACOTRON_MEL_MIN\ndef normalize_tacotron_mel(mel):\n    return 2 * ((mel - TACOTRON_MEL_MIN) / (TACOTRON_MEL_MAX - TACOTRON_MEL_MIN)) - 1\ndef dynamic_range_compression(x, C=1, clip_val=1e-5):\n    \"\"\"\n    PARAMS\n    ------\n    C: compression factor\n    \"\"\"",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "documentation": {}
    },
    {
        "label": "normalize_tacotron_mel",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "peekOfCode": "def normalize_tacotron_mel(mel):\n    return 2 * ((mel - TACOTRON_MEL_MIN) / (TACOTRON_MEL_MAX - TACOTRON_MEL_MIN)) - 1\ndef dynamic_range_compression(x, C=1, clip_val=1e-5):\n    \"\"\"\n    PARAMS\n    ------\n    C: compression factor\n    \"\"\"\n    return torch.log(torch.clamp(x, min=clip_val) * C)\ndef dynamic_range_decompression(x, C=1):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "documentation": {}
    },
    {
        "label": "dynamic_range_compression",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "peekOfCode": "def dynamic_range_compression(x, C=1, clip_val=1e-5):\n    \"\"\"\n    PARAMS\n    ------\n    C: compression factor\n    \"\"\"\n    return torch.log(torch.clamp(x, min=clip_val) * C)\ndef dynamic_range_decompression(x, C=1):\n    \"\"\"\n    PARAMS",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "documentation": {}
    },
    {
        "label": "dynamic_range_decompression",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "peekOfCode": "def dynamic_range_decompression(x, C=1):\n    \"\"\"\n    PARAMS\n    ------\n    C: compression factor used to compress\n    \"\"\"\n    return torch.exp(x) / C\ndef get_voices(extra_voice_dirs: List[str] = []):\n    dirs = extra_voice_dirs\n    voices: Dict[str, List[str]] = {}",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "documentation": {}
    },
    {
        "label": "get_voices",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "peekOfCode": "def get_voices(extra_voice_dirs: List[str] = []):\n    dirs = extra_voice_dirs\n    voices: Dict[str, List[str]] = {}\n    for d in dirs:\n        subs = os.listdir(d)\n        for sub in subs:\n            subj = os.path.join(d, sub)\n            if os.path.isdir(subj):\n                voices[sub] = list(glob(f\"{subj}/*.wav\")) + list(glob(f\"{subj}/*.mp3\")) + list(glob(f\"{subj}/*.pth\"))\n    return voices",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "documentation": {}
    },
    {
        "label": "load_voice",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "peekOfCode": "def load_voice(voice: str, extra_voice_dirs: List[str] = []):\n    if voice == \"random\":\n        return None, None\n    voices = get_voices(extra_voice_dirs)\n    paths = voices[voice]\n    if len(paths) == 1 and paths[0].endswith(\".pth\"):\n        return None, torch.load(paths[0])\n    else:\n        conds = []\n        for cond_path in paths:",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "documentation": {}
    },
    {
        "label": "load_voices",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "peekOfCode": "def load_voices(voices: List[str], extra_voice_dirs: List[str] = []):\n    latents = []\n    clips = []\n    for voice in voices:\n        if voice == \"random\":\n            if len(voices) > 1:\n                print(\"Cannot combine a random voice with a non-random voice. Just using a random voice.\")\n            return None, None\n        clip, latent = load_voice(voice, extra_voice_dirs)\n        if latent is None:",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "documentation": {}
    },
    {
        "label": "wav_to_univnet_mel",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "peekOfCode": "def wav_to_univnet_mel(wav, do_normalization=False, device=\"cuda\"):\n    stft = TorchSTFT(\n        n_fft=1024,\n        hop_length=256,\n        win_length=1024,\n        use_mel=True,\n        n_mels=100,\n        sample_rate=24000,\n        mel_fmin=0,\n        mel_fmax=12000,",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "documentation": {}
    },
    {
        "label": "TACOTRON_MEL_MAX",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "peekOfCode": "TACOTRON_MEL_MAX = 2.3143386840820312\nTACOTRON_MEL_MIN = -11.512925148010254\ndef denormalize_tacotron_mel(norm_mel):\n    return ((norm_mel + 1) / 2) * (TACOTRON_MEL_MAX - TACOTRON_MEL_MIN) + TACOTRON_MEL_MIN\ndef normalize_tacotron_mel(mel):\n    return 2 * ((mel - TACOTRON_MEL_MIN) / (TACOTRON_MEL_MAX - TACOTRON_MEL_MIN)) - 1\ndef dynamic_range_compression(x, C=1, clip_val=1e-5):\n    \"\"\"\n    PARAMS\n    ------",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "documentation": {}
    },
    {
        "label": "TACOTRON_MEL_MIN",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "peekOfCode": "TACOTRON_MEL_MIN = -11.512925148010254\ndef denormalize_tacotron_mel(norm_mel):\n    return ((norm_mel + 1) / 2) * (TACOTRON_MEL_MAX - TACOTRON_MEL_MIN) + TACOTRON_MEL_MIN\ndef normalize_tacotron_mel(mel):\n    return 2 * ((mel - TACOTRON_MEL_MIN) / (TACOTRON_MEL_MAX - TACOTRON_MEL_MIN)) - 1\ndef dynamic_range_compression(x, C=1, clip_val=1e-5):\n    \"\"\"\n    PARAMS\n    ------\n    C: compression factor",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.audio_utils",
        "documentation": {}
    },
    {
        "label": "ResBlock",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.autoregressive",
        "description": "env.src.tts.TTS.tts.layers.tortoise.autoregressive",
        "peekOfCode": "class ResBlock(nn.Module):\n    \"\"\"\n    Basic residual convolutional block that uses GroupNorm.\n    \"\"\"\n    def __init__(self, chan):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv1d(chan, chan, kernel_size=3, padding=1),\n            nn.GroupNorm(chan // 8, chan),\n            nn.ReLU(),",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.autoregressive",
        "documentation": {}
    },
    {
        "label": "GPT2InferenceModel",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.autoregressive",
        "description": "env.src.tts.TTS.tts.layers.tortoise.autoregressive",
        "peekOfCode": "class GPT2InferenceModel(GPT2PreTrainedModel):\n    def __init__(self, config, gpt, text_pos_emb, embeddings, norm, linear, kv_cache):\n        super().__init__(config)\n        self.transformer = gpt\n        self.text_pos_embedding = text_pos_emb\n        self.embeddings = embeddings\n        self.lm_head = nn.Sequential(norm, linear)\n        self.kv_cache = kv_cache\n    def store_mel_emb(self, mel_emb):\n        self.cached_mel_emb = mel_emb",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.autoregressive",
        "documentation": {}
    },
    {
        "label": "ConditioningEncoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.autoregressive",
        "description": "env.src.tts.TTS.tts.layers.tortoise.autoregressive",
        "peekOfCode": "class ConditioningEncoder(nn.Module):\n    def __init__(\n        self,\n        spec_dim,\n        embedding_dim,\n        attn_blocks=6,\n        num_attn_heads=4,\n        do_checkpointing=False,\n        mean=False,\n    ):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.autoregressive",
        "documentation": {}
    },
    {
        "label": "LearnedPositionEmbeddings",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.autoregressive",
        "description": "env.src.tts.TTS.tts.layers.tortoise.autoregressive",
        "peekOfCode": "class LearnedPositionEmbeddings(nn.Module):\n    def __init__(self, seq_len, model_dim, init=0.02):\n        super().__init__()\n        self.emb = nn.Embedding(seq_len, model_dim)\n        # Initializing this way is standard for GPT-2\n        self.emb.weight.data.normal_(mean=0.0, std=init)\n    def forward(self, x):\n        sl = x.shape[1]\n        return self.emb(torch.arange(0, sl, device=x.device))\n    def get_fixed_embedding(self, ind, dev):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.autoregressive",
        "documentation": {}
    },
    {
        "label": "MelEncoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.autoregressive",
        "description": "env.src.tts.TTS.tts.layers.tortoise.autoregressive",
        "peekOfCode": "class MelEncoder(nn.Module):\n    def __init__(self, channels, mel_channels=80, resblocks_per_reduction=2):\n        super().__init__()\n        self.channels = channels\n        self.encoder = nn.Sequential(\n            nn.Conv1d(mel_channels, channels // 4, kernel_size=3, padding=1),\n            nn.Sequential(*[ResBlock(channels // 4) for _ in range(resblocks_per_reduction)]),\n            nn.Conv1d(channels // 4, channels // 2, kernel_size=3, stride=2, padding=1),\n            nn.GroupNorm(channels // 16, channels // 2),\n            nn.ReLU(),",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.autoregressive",
        "documentation": {}
    },
    {
        "label": "UnifiedVoice",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.autoregressive",
        "description": "env.src.tts.TTS.tts.layers.tortoise.autoregressive",
        "peekOfCode": "class UnifiedVoice(nn.Module):\n    def __init__(\n        self,\n        layers=8,\n        model_dim=512,\n        heads=8,\n        max_text_tokens=120,\n        max_mel_tokens=250,\n        max_conditioning_inputs=1,\n        mel_length_compression=1024,",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.autoregressive",
        "documentation": {}
    },
    {
        "label": "null_position_embeddings",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.autoregressive",
        "description": "env.src.tts.TTS.tts.layers.tortoise.autoregressive",
        "peekOfCode": "def null_position_embeddings(range, dim):\n    return torch.zeros((range.shape[0], range.shape[1], dim), device=range.device)\ndef _p(t):\n    return t and (len(t), len(t[0]), t[0][0].shape)  # kv_cache debug\nclass ResBlock(nn.Module):\n    \"\"\"\n    Basic residual convolutional block that uses GroupNorm.\n    \"\"\"\n    def __init__(self, chan):\n        super().__init__()",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.autoregressive",
        "documentation": {}
    },
    {
        "label": "build_hf_gpt_transformer",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.autoregressive",
        "description": "env.src.tts.TTS.tts.layers.tortoise.autoregressive",
        "peekOfCode": "def build_hf_gpt_transformer(layers, model_dim, heads, max_mel_seq_len, max_text_seq_len, checkpointing):\n    \"\"\"\n    GPT-2 implemented by the HuggingFace library.\n    \"\"\"\n    from transformers import GPT2Config, GPT2Model\n    gpt_config = GPT2Config(\n        vocab_size=256,  # Unused.\n        n_positions=max_mel_seq_len + max_text_seq_len,\n        n_ctx=max_mel_seq_len + max_text_seq_len,\n        n_embd=model_dim,",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.autoregressive",
        "documentation": {}
    },
    {
        "label": "ResBlock",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.classifier",
        "description": "env.src.tts.TTS.tts.layers.tortoise.classifier",
        "peekOfCode": "class ResBlock(nn.Module):\n    def __init__(\n        self,\n        channels,\n        dropout,\n        out_channels=None,\n        use_conv=False,\n        use_scale_shift_norm=False,\n        dims=2,\n        up=False,",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.classifier",
        "documentation": {}
    },
    {
        "label": "AudioMiniEncoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.classifier",
        "description": "env.src.tts.TTS.tts.layers.tortoise.classifier",
        "peekOfCode": "class AudioMiniEncoder(nn.Module):\n    def __init__(\n        self,\n        spec_dim,\n        embedding_dim,\n        base_channels=128,\n        depth=2,\n        resnet_blocks=2,\n        attn_blocks=4,\n        num_attn_heads=4,",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.classifier",
        "documentation": {}
    },
    {
        "label": "AudioMiniEncoderWithClassifierHead",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.classifier",
        "description": "env.src.tts.TTS.tts.layers.tortoise.classifier",
        "peekOfCode": "class AudioMiniEncoderWithClassifierHead(nn.Module):\n    def __init__(self, classes, distribute_zero_label=True, **kwargs):\n        super().__init__()\n        self.enc = AudioMiniEncoder(**kwargs)\n        self.head = nn.Linear(self.enc.dim, classes)\n        self.num_classes = classes\n        self.distribute_zero_label = distribute_zero_label\n    def forward(self, x, labels=None):\n        h = self.enc(x)\n        logits = self.head(h)",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.classifier",
        "documentation": {}
    },
    {
        "label": "CLVP",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.clvp",
        "description": "env.src.tts.TTS.tts.layers.tortoise.clvp",
        "peekOfCode": "class CLVP(nn.Module):\n    \"\"\"\n    CLIP model retrofitted for performing contrastive evaluation between tokenized audio data and the corresponding\n    transcribed text.\n    Originally from https://github.com/lucidrains/DALLE-pytorch/blob/main/dalle_pytorch/dalle_pytorch.py\n    \"\"\"\n    def __init__(\n        self,\n        *,\n        dim_text=512,",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.clvp",
        "documentation": {}
    },
    {
        "label": "exists",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.clvp",
        "description": "env.src.tts.TTS.tts.layers.tortoise.clvp",
        "peekOfCode": "def exists(val):\n    return val is not None\ndef masked_mean(t, mask, dim=1):\n    t = t.masked_fill(~mask[:, :, None], 0.0)\n    return t.sum(dim=1) / mask.sum(dim=1)[..., None]\nclass CLVP(nn.Module):\n    \"\"\"\n    CLIP model retrofitted for performing contrastive evaluation between tokenized audio data and the corresponding\n    transcribed text.\n    Originally from https://github.com/lucidrains/DALLE-pytorch/blob/main/dalle_pytorch/dalle_pytorch.py",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.clvp",
        "documentation": {}
    },
    {
        "label": "masked_mean",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.clvp",
        "description": "env.src.tts.TTS.tts.layers.tortoise.clvp",
        "peekOfCode": "def masked_mean(t, mask, dim=1):\n    t = t.masked_fill(~mask[:, :, None], 0.0)\n    return t.sum(dim=1) / mask.sum(dim=1)[..., None]\nclass CLVP(nn.Module):\n    \"\"\"\n    CLIP model retrofitted for performing contrastive evaluation between tokenized audio data and the corresponding\n    transcribed text.\n    Originally from https://github.com/lucidrains/DALLE-pytorch/blob/main/dalle_pytorch/dalle_pytorch.py\n    \"\"\"\n    def __init__(",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.clvp",
        "documentation": {}
    },
    {
        "label": "ModelMeanType",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "description": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "peekOfCode": "class ModelMeanType(enum.Enum):\n    \"\"\"\n    Which type of output the model predicts.\n    \"\"\"\n    PREVIOUS_X = \"previous_x\"  # the model predicts x_{t-1}\n    START_X = \"start_x\"  # the model predicts x_0\n    EPSILON = \"epsilon\"  # the model predicts epsilon\nclass ModelVarType(enum.Enum):\n    \"\"\"\n    What is used as the model's output variance.",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "documentation": {}
    },
    {
        "label": "ModelVarType",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "description": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "peekOfCode": "class ModelVarType(enum.Enum):\n    \"\"\"\n    What is used as the model's output variance.\n    The LEARNED_RANGE option has been added to allow the model to predict\n    values between FIXED_SMALL and FIXED_LARGE, making its job easier.\n    \"\"\"\n    LEARNED = \"learned\"\n    FIXED_SMALL = \"fixed_small\"\n    FIXED_LARGE = \"fixed_large\"\n    LEARNED_RANGE = \"learned_range\"",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "documentation": {}
    },
    {
        "label": "LossType",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "description": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "peekOfCode": "class LossType(enum.Enum):\n    MSE = \"mse\"  # use raw MSE loss (and KL when learning variances)\n    RESCALED_MSE = \"rescaled_mse\"  # use raw MSE loss (with RESCALED_KL when learning variances)\n    KL = \"kl\"  # use the variational lower-bound\n    RESCALED_KL = \"rescaled_kl\"  # like KL, but rescale to estimate the full VLB\n    def is_vb(self):\n        return self == LossType.KL or self == LossType.RESCALED_KL\nclass GaussianDiffusion:\n    \"\"\"\n    Utilities for training and sampling diffusion models.",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "documentation": {}
    },
    {
        "label": "GaussianDiffusion",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "description": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "peekOfCode": "class GaussianDiffusion:\n    \"\"\"\n    Utilities for training and sampling diffusion models.\n    Ported directly from here, and then adapted over time to further experimentation.\n    https://github.com/hojonathanho/diffusion/blob/1e0dceb3b3495bbe19116a5e1b3596cd0706c543/diffusion_tf/diffusion_utils_2.py#L42\n    :param betas: a 1-D numpy array of betas for each diffusion timestep,\n                  starting at T and going to 1.\n    :param model_mean_type: a ModelMeanType determining what the model outputs.\n    :param model_var_type: a ModelVarType determining how variance is output.\n    :param loss_type: a LossType determining the loss function to use.",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "documentation": {}
    },
    {
        "label": "SpacedDiffusion",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "description": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "peekOfCode": "class SpacedDiffusion(GaussianDiffusion):\n    \"\"\"\n    A diffusion process which can skip steps in a base diffusion process.\n    :param use_timesteps: a collection (sequence or set) of timesteps from the\n                          original diffusion process to retain.\n    :param kwargs: the kwargs to create the base diffusion process.\n    \"\"\"\n    def __init__(self, use_timesteps, **kwargs):\n        self.use_timesteps = set(use_timesteps)\n        self.timestep_map = []",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "documentation": {}
    },
    {
        "label": "_WrappedModel",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "description": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "peekOfCode": "class _WrappedModel:\n    def __init__(self, model, timestep_map, rescale_timesteps, original_num_steps):\n        self.model = model\n        self.timestep_map = timestep_map\n        self.rescale_timesteps = rescale_timesteps\n        self.original_num_steps = original_num_steps\n    def __call__(self, x, ts, **kwargs):\n        map_tensor = th.tensor(self.timestep_map, device=ts.device, dtype=ts.dtype)\n        new_ts = map_tensor[ts]\n        if self.rescale_timesteps:",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "documentation": {}
    },
    {
        "label": "_WrappedAutoregressiveModel",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "description": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "peekOfCode": "class _WrappedAutoregressiveModel:\n    def __init__(self, model, timestep_map, rescale_timesteps, original_num_steps):\n        self.model = model\n        self.timestep_map = timestep_map\n        self.rescale_timesteps = rescale_timesteps\n        self.original_num_steps = original_num_steps\n    def __call__(self, x, x0, ts, **kwargs):\n        map_tensor = th.tensor(self.timestep_map, device=ts.device, dtype=ts.dtype)\n        new_ts = map_tensor[ts]\n        if self.rescale_timesteps:",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "documentation": {}
    },
    {
        "label": "normal_kl",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "description": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "peekOfCode": "def normal_kl(mean1, logvar1, mean2, logvar2):\n    \"\"\"\n    Compute the KL divergence between two gaussians.\n    Shapes are automatically broadcasted, so batches can be compared to\n    scalars, among other use cases.\n    \"\"\"\n    tensor = None\n    for obj in (mean1, logvar1, mean2, logvar2):\n        if isinstance(obj, th.Tensor):\n            tensor = obj",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "documentation": {}
    },
    {
        "label": "approx_standard_normal_cdf",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "description": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "peekOfCode": "def approx_standard_normal_cdf(x):\n    \"\"\"\n    A fast approximation of the cumulative distribution function of the\n    standard normal.\n    \"\"\"\n    return 0.5 * (1.0 + th.tanh(np.sqrt(2.0 / np.pi) * (x + 0.044715 * th.pow(x, 3))))\ndef discretized_gaussian_log_likelihood(x, *, means, log_scales):\n    \"\"\"\n    Compute the log-likelihood of a Gaussian distribution discretizing to a\n    given image.",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "documentation": {}
    },
    {
        "label": "discretized_gaussian_log_likelihood",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "description": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "peekOfCode": "def discretized_gaussian_log_likelihood(x, *, means, log_scales):\n    \"\"\"\n    Compute the log-likelihood of a Gaussian distribution discretizing to a\n    given image.\n    :param x: the target images. It is assumed that this was uint8 values,\n              rescaled to the range [-1, 1].\n    :param means: the Gaussian mean Tensor.\n    :param log_scales: the Gaussian log stddev Tensor.\n    :return: a tensor like x of log probabilities (in nats).\n    \"\"\"",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "documentation": {}
    },
    {
        "label": "mean_flat",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "description": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "peekOfCode": "def mean_flat(tensor):\n    \"\"\"\n    Take the mean over all non-batch dimensions.\n    \"\"\"\n    return tensor.mean(dim=list(range(1, len(tensor.shape))))\ndef get_named_beta_schedule(schedule_name, num_diffusion_timesteps):\n    \"\"\"\n    Get a pre-defined beta schedule for the given name.\n    The beta schedule library consists of beta schedules which remain similar\n    in the limit of num_diffusion_timesteps.",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "documentation": {}
    },
    {
        "label": "get_named_beta_schedule",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "description": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "peekOfCode": "def get_named_beta_schedule(schedule_name, num_diffusion_timesteps):\n    \"\"\"\n    Get a pre-defined beta schedule for the given name.\n    The beta schedule library consists of beta schedules which remain similar\n    in the limit of num_diffusion_timesteps.\n    Beta schedules may be added, but should not be removed or changed once\n    they are committed to maintain backwards compatibility.\n    \"\"\"\n    if schedule_name == \"linear\":\n        # Linear schedule from Ho et al, extended to work for any number of",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "documentation": {}
    },
    {
        "label": "betas_for_alpha_bar",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "description": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "peekOfCode": "def betas_for_alpha_bar(num_diffusion_timesteps, alpha_bar, max_beta=0.999):\n    \"\"\"\n    Create a beta schedule that discretizes the given alpha_t_bar function,\n    which defines the cumulative product of (1-beta) over time from t = [0,1].\n    :param num_diffusion_timesteps: the number of betas to produce.\n    :param alpha_bar: a lambda that takes an argument t from 0 to 1 and\n                      produces the cumulative product of (1-beta) up to that\n                      part of the diffusion process.\n    :param max_beta: the maximum beta to use; use values lower than 1 to\n                     prevent singularities.",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "documentation": {}
    },
    {
        "label": "get_named_beta_schedule",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "description": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "peekOfCode": "def get_named_beta_schedule(schedule_name, num_diffusion_timesteps):\n    \"\"\"\n    Get a pre-defined beta schedule for the given name.\n    The beta schedule library consists of beta schedules which remain similar\n    in the limit of num_diffusion_timesteps.\n    Beta schedules may be added, but should not be removed or changed once\n    they are committed to maintain backwards compatibility.\n    \"\"\"\n    if schedule_name == \"linear\":\n        # Linear schedule from Ho et al, extended to work for any number of",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "documentation": {}
    },
    {
        "label": "space_timesteps",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "description": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "peekOfCode": "def space_timesteps(num_timesteps, section_counts):\n    \"\"\"\n    Create a list of timesteps to use from an original diffusion process,\n    given the number of timesteps we want to take from equally-sized portions\n    of the original process.\n    For example, if there's 300 timesteps and the section counts are [10,15,20]\n    then the first 100 timesteps are strided to be 10 timesteps, the second 100\n    are strided to be 15 timesteps, and the final 100 are strided to be 20.\n    If the stride is a string starting with \"ddim\", then the fixed striding\n    from the DDIM paper is used, and only one section is allowed.",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "documentation": {}
    },
    {
        "label": "K_DIFFUSION_SAMPLERS",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "description": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "peekOfCode": "K_DIFFUSION_SAMPLERS = {\"k_euler_a\": sample_euler_ancestral, \"dpm++2m\": sample_dpmpp_2m}\nSAMPLERS = [\"dpm++2m\", \"p\", \"ddim\"]\ndef normal_kl(mean1, logvar1, mean2, logvar2):\n    \"\"\"\n    Compute the KL divergence between two gaussians.\n    Shapes are automatically broadcasted, so batches can be compared to\n    scalars, among other use cases.\n    \"\"\"\n    tensor = None\n    for obj in (mean1, logvar1, mean2, logvar2):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "documentation": {}
    },
    {
        "label": "SAMPLERS",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "description": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "peekOfCode": "SAMPLERS = [\"dpm++2m\", \"p\", \"ddim\"]\ndef normal_kl(mean1, logvar1, mean2, logvar2):\n    \"\"\"\n    Compute the KL divergence between two gaussians.\n    Shapes are automatically broadcasted, so batches can be compared to\n    scalars, among other use cases.\n    \"\"\"\n    tensor = None\n    for obj in (mean1, logvar1, mean2, logvar2):\n        if isinstance(obj, th.Tensor):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.diffusion",
        "documentation": {}
    },
    {
        "label": "TimestepBlock",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.diffusion_decoder",
        "description": "env.src.tts.TTS.tts.layers.tortoise.diffusion_decoder",
        "peekOfCode": "class TimestepBlock(nn.Module):\n    @abstractmethod\n    def forward(self, x, emb):\n        \"\"\"\n        Apply the module to `x` given `emb` timestep embeddings.\n        \"\"\"\nclass TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n    def forward(self, x, emb):\n        for layer in self:\n            if isinstance(layer, TimestepBlock):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.diffusion_decoder",
        "documentation": {}
    },
    {
        "label": "TimestepEmbedSequential",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.diffusion_decoder",
        "description": "env.src.tts.TTS.tts.layers.tortoise.diffusion_decoder",
        "peekOfCode": "class TimestepEmbedSequential(nn.Sequential, TimestepBlock):\n    def forward(self, x, emb):\n        for layer in self:\n            if isinstance(layer, TimestepBlock):\n                x = layer(x, emb)\n            else:\n                x = layer(x)\n        return x\nclass ResBlock(TimestepBlock):\n    def __init__(",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.diffusion_decoder",
        "documentation": {}
    },
    {
        "label": "ResBlock",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.diffusion_decoder",
        "description": "env.src.tts.TTS.tts.layers.tortoise.diffusion_decoder",
        "peekOfCode": "class ResBlock(TimestepBlock):\n    def __init__(\n        self,\n        channels,\n        emb_channels,\n        dropout,\n        out_channels=None,\n        dims=2,\n        kernel_size=3,\n        efficient_config=True,",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.diffusion_decoder",
        "documentation": {}
    },
    {
        "label": "DiffusionLayer",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.diffusion_decoder",
        "description": "env.src.tts.TTS.tts.layers.tortoise.diffusion_decoder",
        "peekOfCode": "class DiffusionLayer(TimestepBlock):\n    def __init__(self, model_channels, dropout, num_heads):\n        super().__init__()\n        self.resblk = ResBlock(\n            model_channels,\n            model_channels,\n            dropout,\n            model_channels,\n            dims=1,\n            use_scale_shift_norm=True,",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.diffusion_decoder",
        "documentation": {}
    },
    {
        "label": "DiffusionTts",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.diffusion_decoder",
        "description": "env.src.tts.TTS.tts.layers.tortoise.diffusion_decoder",
        "peekOfCode": "class DiffusionTts(nn.Module):\n    def __init__(\n        self,\n        model_channels=512,\n        num_layers=8,\n        in_channels=100,\n        in_latent_channels=512,\n        in_tokens=8193,\n        out_channels=200,  # mean and variance\n        dropout=0,",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.diffusion_decoder",
        "documentation": {}
    },
    {
        "label": "is_latent",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.diffusion_decoder",
        "description": "env.src.tts.TTS.tts.layers.tortoise.diffusion_decoder",
        "peekOfCode": "def is_latent(t):\n    return t.dtype == torch.float\ndef is_sequence(t):\n    return t.dtype == torch.long\ndef timestep_embedding(timesteps, dim, max_period=10000):\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.diffusion_decoder",
        "documentation": {}
    },
    {
        "label": "is_sequence",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.diffusion_decoder",
        "description": "env.src.tts.TTS.tts.layers.tortoise.diffusion_decoder",
        "peekOfCode": "def is_sequence(t):\n    return t.dtype == torch.long\ndef timestep_embedding(timesteps, dim, max_period=10000):\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.diffusion_decoder",
        "documentation": {}
    },
    {
        "label": "timestep_embedding",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.diffusion_decoder",
        "description": "env.src.tts.TTS.tts.layers.tortoise.diffusion_decoder",
        "peekOfCode": "def timestep_embedding(timesteps, dim, max_period=10000):\n    \"\"\"\n    Create sinusoidal timestep embeddings.\n    :param timesteps: a 1-D Tensor of N indices, one per batch element.\n                      These may be fractional.\n    :param dim: the dimension of the output.\n    :param max_period: controls the minimum frequency of the embeddings.\n    :return: an [N x dim] Tensor of positional embeddings.\n    \"\"\"\n    half = dim // 2",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.diffusion_decoder",
        "documentation": {}
    },
    {
        "label": "NoiseScheduleVP",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.dpm_solver",
        "description": "env.src.tts.TTS.tts.layers.tortoise.dpm_solver",
        "peekOfCode": "class NoiseScheduleVP:\n    def __init__(\n        self,\n        schedule=\"discrete\",\n        betas=None,\n        alphas_cumprod=None,\n        continuous_beta_0=0.1,\n        continuous_beta_1=20.0,\n        dtype=torch.float32,\n    ):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.dpm_solver",
        "documentation": {}
    },
    {
        "label": "DPM_Solver",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.dpm_solver",
        "description": "env.src.tts.TTS.tts.layers.tortoise.dpm_solver",
        "peekOfCode": "class DPM_Solver:\n    def __init__(\n        self,\n        model_fn,\n        noise_schedule,\n        algorithm_type=\"dpmsolver++\",\n        correcting_x0_fn=None,\n        correcting_xt_fn=None,\n        thresholding_max_val=1.0,\n        dynamic_thresholding_ratio=0.995,",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.dpm_solver",
        "documentation": {}
    },
    {
        "label": "model_wrapper",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.dpm_solver",
        "description": "env.src.tts.TTS.tts.layers.tortoise.dpm_solver",
        "peekOfCode": "def model_wrapper(\n    model,\n    noise_schedule,\n    model_type=\"noise\",\n    model_kwargs={},\n    guidance_type=\"uncond\",\n    condition=None,\n    unconditional_condition=None,\n    guidance_scale=1.0,\n    classifier_fn=None,",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.dpm_solver",
        "documentation": {}
    },
    {
        "label": "interpolate_fn",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.dpm_solver",
        "description": "env.src.tts.TTS.tts.layers.tortoise.dpm_solver",
        "peekOfCode": "def interpolate_fn(x, xp, yp):\n    \"\"\"\n    A piecewise linear function y = f(x), using xp and yp as keypoints.\n    We implement f(x) in a differentiable way (i.e. applicable for autograd).\n    The function f(x) is well-defined for all x-axis. (For x beyond the bounds of xp, we use the outmost points of xp to define the linear function.)\n    Args:\n        x: PyTorch tensor with shape [N, C], where N is the batch size, C is the number of channels (we use C = 1 for DPM-Solver).\n        xp: PyTorch tensor with shape [C, K], where K is the number of keypoints.\n        yp: PyTorch tensor with shape [C, K].\n    Returns:",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.dpm_solver",
        "documentation": {}
    },
    {
        "label": "expand_dims",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.dpm_solver",
        "description": "env.src.tts.TTS.tts.layers.tortoise.dpm_solver",
        "peekOfCode": "def expand_dims(v, dims):\n    \"\"\"\n    Expand the tensor `v` to the dim `dims`.\n    Args:\n        `v`: a PyTorch tensor with shape [N].\n        `dim`: a `int`.\n    Returns:\n        a PyTorch tensor with shape [N, 1, 1, ..., 1] and the total dimension is `dims`.\n    \"\"\"\n    return v[(...,) + (None,) * (dims - 1)]",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.dpm_solver",
        "documentation": {}
    },
    {
        "label": "EqualLinear",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.random_latent_generator",
        "description": "env.src.tts.TTS.tts.layers.tortoise.random_latent_generator",
        "peekOfCode": "class EqualLinear(nn.Module):\n    def __init__(self, in_dim, out_dim, bias=True, bias_init=0, lr_mul=1):\n        super().__init__()\n        self.weight = nn.Parameter(torch.randn(out_dim, in_dim).div_(lr_mul))\n        if bias:\n            self.bias = nn.Parameter(torch.zeros(out_dim).fill_(bias_init))\n        else:\n            self.bias = None\n        self.scale = (1 / math.sqrt(in_dim)) * lr_mul\n        self.lr_mul = lr_mul",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.random_latent_generator",
        "documentation": {}
    },
    {
        "label": "RandomLatentConverter",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.random_latent_generator",
        "description": "env.src.tts.TTS.tts.layers.tortoise.random_latent_generator",
        "peekOfCode": "class RandomLatentConverter(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.layers = nn.Sequential(\n            *[EqualLinear(channels, channels, lr_mul=0.1) for _ in range(5)], nn.Linear(channels, channels)\n        )\n        self.channels = channels\n    def forward(self, ref):\n        r = torch.randn(ref.shape[0], self.channels, device=ref.device)\n        y = self.layers(r)",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.random_latent_generator",
        "documentation": {}
    },
    {
        "label": "fused_leaky_relu",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.random_latent_generator",
        "description": "env.src.tts.TTS.tts.layers.tortoise.random_latent_generator",
        "peekOfCode": "def fused_leaky_relu(input, bias=None, negative_slope=0.2, scale=2**0.5):\n    if bias is not None:\n        rest_dim = [1] * (input.ndim - bias.ndim - 1)\n        return (\n            F.leaky_relu(\n                input + bias.view(1, bias.shape[0], *rest_dim),\n                negative_slope=negative_slope,\n            )\n            * scale\n        )",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.random_latent_generator",
        "documentation": {}
    },
    {
        "label": "VoiceBpeTokenizer",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.tokenizer",
        "description": "env.src.tts.TTS.tts.layers.tortoise.tokenizer",
        "peekOfCode": "class VoiceBpeTokenizer:\n    def __init__(self, vocab_file=DEFAULT_VOCAB_FILE):\n        if vocab_file is not None:\n            self.tokenizer = Tokenizer.from_file(vocab_file)\n    def preprocess_text(self, txt):\n        txt = english_cleaners(txt)\n        return txt\n    def encode(self, txt):\n        txt = self.preprocess_text(txt)\n        txt = txt.replace(\" \", \"[SPACE]\")",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.tokenizer",
        "documentation": {}
    },
    {
        "label": "DEFAULT_VOCAB_FILE",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.tokenizer",
        "description": "env.src.tts.TTS.tts.layers.tortoise.tokenizer",
        "peekOfCode": "DEFAULT_VOCAB_FILE = os.path.join(\n    os.path.dirname(os.path.realpath(__file__)), \"../../utils/assets/tortoise/tokenizer.json\"\n)\nclass VoiceBpeTokenizer:\n    def __init__(self, vocab_file=DEFAULT_VOCAB_FILE):\n        if vocab_file is not None:\n            self.tokenizer = Tokenizer.from_file(vocab_file)\n    def preprocess_text(self, txt):\n        txt = english_cleaners(txt)\n        return txt",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.tokenizer",
        "documentation": {}
    },
    {
        "label": "SequentialSequence",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "description": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "peekOfCode": "class SequentialSequence(nn.Module):\n    def __init__(self, layers, args_route={}, layer_dropout=0.0):\n        super().__init__()\n        assert all(\n            len(route) == len(layers) for route in args_route.values()\n        ), \"each argument route map must have the same depth as the number of sequential layers\"\n        self.layers = layers\n        self.args_route = args_route\n        self.layer_dropout = layer_dropout\n    def forward(self, x, **kwargs):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "documentation": {}
    },
    {
        "label": "DivideMax",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "description": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "peekOfCode": "class DivideMax(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.dim = dim\n    def forward(self, x):\n        maxes = x.amax(dim=self.dim, keepdim=True).detach()\n        return x / maxes\n# https://arxiv.org/abs/2103.17239\nclass LayerScale(nn.Module):\n    def __init__(self, dim, depth, fn):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "documentation": {}
    },
    {
        "label": "LayerScale",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "description": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "peekOfCode": "class LayerScale(nn.Module):\n    def __init__(self, dim, depth, fn):\n        super().__init__()\n        if depth <= 18:\n            init_eps = 0.1\n        elif depth > 18 and depth <= 24:\n            init_eps = 1e-5\n        else:\n            init_eps = 1e-6\n        scale = torch.zeros(1, 1, dim).fill_(init_eps)",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "documentation": {}
    },
    {
        "label": "PreNorm",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "description": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "peekOfCode": "class PreNorm(nn.Module):\n    def __init__(self, dim, fn, sandwich=False):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.norm_out = nn.LayerNorm(dim) if sandwich else nn.Identity()\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        x = self.norm(x)\n        x = self.fn(x, **kwargs)\n        return self.norm_out(x)",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "documentation": {}
    },
    {
        "label": "GEGLU",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "description": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "peekOfCode": "class GEGLU(nn.Module):\n    def forward(self, x):\n        x, gates = x.chunk(2, dim=-1)\n        return x * F.gelu(gates)\nclass FeedForward(nn.Module):\n    def __init__(self, dim, dropout=0.0, mult=4.0):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, dim * mult * 2),\n            GEGLU(),",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "documentation": {}
    },
    {
        "label": "FeedForward",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "description": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "peekOfCode": "class FeedForward(nn.Module):\n    def __init__(self, dim, dropout=0.0, mult=4.0):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, dim * mult * 2),\n            GEGLU(),\n            nn.Dropout(dropout),\n            nn.Linear(dim * mult, dim),\n        )\n    def forward(self, x):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "documentation": {}
    },
    {
        "label": "Attention",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "description": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "peekOfCode": "class Attention(nn.Module):\n    def __init__(self, dim, seq_len, causal=True, heads=8, dim_head=64, dropout=0.0):\n        super().__init__()\n        inner_dim = dim_head * heads\n        self.heads = heads\n        self.seq_len = seq_len\n        self.scale = dim_head**-0.5\n        self.causal = causal\n        self.to_qkv = nn.Linear(dim, inner_dim * 3, bias=False)\n        self.to_out = nn.Sequential(nn.Linear(inner_dim, dim), nn.Dropout(dropout))",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "description": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "peekOfCode": "class Transformer(nn.Module):\n    def __init__(\n        self,\n        *,\n        dim,\n        depth,\n        seq_len,\n        causal=True,\n        heads=8,\n        dim_head=64,",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "documentation": {}
    },
    {
        "label": "exists",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "description": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "peekOfCode": "def exists(val):\n    return val is not None\ndef default(val, d):\n    return val if exists(val) else d\ndef cast_tuple(val, depth=1):\n    if isinstance(val, list):\n        val = tuple(val)\n    return val if isinstance(val, tuple) else (val,) * depth\ndef max_neg_value(t):\n    return -torch.finfo(t.dtype).max",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "documentation": {}
    },
    {
        "label": "default",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "description": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "peekOfCode": "def default(val, d):\n    return val if exists(val) else d\ndef cast_tuple(val, depth=1):\n    if isinstance(val, list):\n        val = tuple(val)\n    return val if isinstance(val, tuple) else (val,) * depth\ndef max_neg_value(t):\n    return -torch.finfo(t.dtype).max\ndef stable_softmax(t, dim=-1, alpha=32**2):\n    t = t / alpha",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "documentation": {}
    },
    {
        "label": "cast_tuple",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "description": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "peekOfCode": "def cast_tuple(val, depth=1):\n    if isinstance(val, list):\n        val = tuple(val)\n    return val if isinstance(val, tuple) else (val,) * depth\ndef max_neg_value(t):\n    return -torch.finfo(t.dtype).max\ndef stable_softmax(t, dim=-1, alpha=32**2):\n    t = t / alpha\n    t = t - torch.amax(t, dim=dim, keepdim=True).detach()\n    return (t * alpha).softmax(dim=dim)",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "documentation": {}
    },
    {
        "label": "max_neg_value",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "description": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "peekOfCode": "def max_neg_value(t):\n    return -torch.finfo(t.dtype).max\ndef stable_softmax(t, dim=-1, alpha=32**2):\n    t = t / alpha\n    t = t - torch.amax(t, dim=dim, keepdim=True).detach()\n    return (t * alpha).softmax(dim=dim)\ndef route_args(router, args, depth):\n    routed_args = [(dict(), dict()) for _ in range(depth)]\n    matched_keys = [key for key in args.keys() if key in router]\n    for key in matched_keys:",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "documentation": {}
    },
    {
        "label": "stable_softmax",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "description": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "peekOfCode": "def stable_softmax(t, dim=-1, alpha=32**2):\n    t = t / alpha\n    t = t - torch.amax(t, dim=dim, keepdim=True).detach()\n    return (t * alpha).softmax(dim=dim)\ndef route_args(router, args, depth):\n    routed_args = [(dict(), dict()) for _ in range(depth)]\n    matched_keys = [key for key in args.keys() if key in router]\n    for key in matched_keys:\n        val = args[key]\n        for depth, ((f_args, g_args), routes) in enumerate(zip(routed_args, router[key])):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "documentation": {}
    },
    {
        "label": "route_args",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "description": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "peekOfCode": "def route_args(router, args, depth):\n    routed_args = [(dict(), dict()) for _ in range(depth)]\n    matched_keys = [key for key in args.keys() if key in router]\n    for key in matched_keys:\n        val = args[key]\n        for depth, ((f_args, g_args), routes) in enumerate(zip(routed_args, router[key])):\n            new_f_args, new_g_args = map(lambda route: ({key: val} if route else {}), routes)\n            routed_args[depth] = ({**f_args, **new_f_args}, {**g_args, **new_g_args})\n    return routed_args\n# classes",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.transformer",
        "documentation": {}
    },
    {
        "label": "download_models",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.utils",
        "peekOfCode": "def download_models(specific_models=None):\n    \"\"\"\n    Call to download all the models that Tortoise uses.\n    \"\"\"\n    os.makedirs(MODELS_DIR, exist_ok=True)\n    for model_name, url in MODELS.items():\n        if specific_models is not None and model_name not in specific_models:\n            continue\n        model_path = os.path.join(MODELS_DIR, model_name)\n        if os.path.exists(model_path):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.utils",
        "documentation": {}
    },
    {
        "label": "get_model_path",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.utils",
        "peekOfCode": "def get_model_path(model_name, models_dir=MODELS_DIR):\n    \"\"\"\n    Get path to given model, download it if it doesn't exist.\n    \"\"\"\n    if model_name not in MODELS:\n        raise ValueError(f\"Model {model_name} not found in available models.\")\n    model_path = os.path.join(models_dir, model_name)\n    if not os.path.exists(model_path) and models_dir == MODELS_DIR:\n        download_models([model_name])\n    return model_path",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.utils",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MODELS_DIR",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.utils",
        "peekOfCode": "DEFAULT_MODELS_DIR = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"tortoise\", \"models\")\nMODELS_DIR = os.environ.get(\"TORTOISE_MODELS_DIR\", DEFAULT_MODELS_DIR)\nMODELS_DIR = \"/data/speech_synth/models/\"\nMODELS = {\n    \"autoregressive.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/autoregressive.pth\",\n    \"classifier.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/classifier.pth\",\n    \"clvp2.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/clvp2.pth\",\n    \"diffusion_decoder.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/diffusion_decoder.pth\",\n    \"vocoder.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/vocoder.pth\",\n    \"rlg_auto.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/rlg_auto.pth\",",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.utils",
        "documentation": {}
    },
    {
        "label": "MODELS_DIR",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.utils",
        "peekOfCode": "MODELS_DIR = os.environ.get(\"TORTOISE_MODELS_DIR\", DEFAULT_MODELS_DIR)\nMODELS_DIR = \"/data/speech_synth/models/\"\nMODELS = {\n    \"autoregressive.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/autoregressive.pth\",\n    \"classifier.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/classifier.pth\",\n    \"clvp2.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/clvp2.pth\",\n    \"diffusion_decoder.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/diffusion_decoder.pth\",\n    \"vocoder.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/vocoder.pth\",\n    \"rlg_auto.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/rlg_auto.pth\",\n    \"rlg_diffuser.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/rlg_diffuser.pth\",",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.utils",
        "documentation": {}
    },
    {
        "label": "MODELS_DIR",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.utils",
        "peekOfCode": "MODELS_DIR = \"/data/speech_synth/models/\"\nMODELS = {\n    \"autoregressive.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/autoregressive.pth\",\n    \"classifier.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/classifier.pth\",\n    \"clvp2.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/clvp2.pth\",\n    \"diffusion_decoder.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/diffusion_decoder.pth\",\n    \"vocoder.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/vocoder.pth\",\n    \"rlg_auto.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/rlg_auto.pth\",\n    \"rlg_diffuser.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/rlg_diffuser.pth\",\n}",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.utils",
        "documentation": {}
    },
    {
        "label": "MODELS",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.utils",
        "description": "env.src.tts.TTS.tts.layers.tortoise.utils",
        "peekOfCode": "MODELS = {\n    \"autoregressive.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/autoregressive.pth\",\n    \"classifier.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/classifier.pth\",\n    \"clvp2.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/clvp2.pth\",\n    \"diffusion_decoder.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/diffusion_decoder.pth\",\n    \"vocoder.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/vocoder.pth\",\n    \"rlg_auto.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/rlg_auto.pth\",\n    \"rlg_diffuser.pth\": \"https://huggingface.co/jbetker/tortoise-tts-v2/resolve/main/.models/rlg_diffuser.pth\",\n}\ndef download_models(specific_models=None):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.utils",
        "documentation": {}
    },
    {
        "label": "KernelPredictor",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.vocoder",
        "description": "env.src.tts.TTS.tts.layers.tortoise.vocoder",
        "peekOfCode": "class KernelPredictor(torch.nn.Module):\n    \"\"\"Kernel predictor for the location-variable convolutions\"\"\"\n    def __init__(\n        self,\n        cond_channels,\n        conv_in_channels,\n        conv_out_channels,\n        conv_layers,\n        conv_kernel_size=3,\n        kpnet_hidden_channels=64,",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.vocoder",
        "documentation": {}
    },
    {
        "label": "LVCBlock",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.vocoder",
        "description": "env.src.tts.TTS.tts.layers.tortoise.vocoder",
        "peekOfCode": "class LVCBlock(torch.nn.Module):\n    \"\"\"the location-variable convolutions\"\"\"\n    def __init__(\n        self,\n        in_channels,\n        cond_channels,\n        stride,\n        dilations=[1, 3, 9, 27],\n        lReLU_slope=0.2,\n        conv_kernel_size=3,",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.vocoder",
        "documentation": {}
    },
    {
        "label": "UnivNetGenerator",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.vocoder",
        "description": "env.src.tts.TTS.tts.layers.tortoise.vocoder",
        "peekOfCode": "class UnivNetGenerator(nn.Module):\n    \"\"\"\n    UnivNet Generator\n    Originally from https://github.com/mindslab-ai/univnet/blob/master/model/generator.py.\n    \"\"\"\n    def __init__(\n        self,\n        noise_dim=64,\n        channel_size=32,\n        dilations=[1, 3, 9, 27],",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.vocoder",
        "documentation": {}
    },
    {
        "label": "VocType",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.vocoder",
        "description": "env.src.tts.TTS.tts.layers.tortoise.vocoder",
        "peekOfCode": "class VocType:\n    constructor: Callable[[], nn.Module]\n    model_path: str\n    subkey: Optional[str] = None\n    def optionally_index(self, model_dict):\n        if self.subkey is not None:\n            return model_dict[self.subkey]\n        return model_dict\nclass VocConf(Enum):\n    Univnet = VocType(UnivNetGenerator, \"vocoder.pth\", \"model_g\")",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.vocoder",
        "documentation": {}
    },
    {
        "label": "VocConf",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.vocoder",
        "description": "env.src.tts.TTS.tts.layers.tortoise.vocoder",
        "peekOfCode": "class VocConf(Enum):\n    Univnet = VocType(UnivNetGenerator, \"vocoder.pth\", \"model_g\")\nif __name__ == \"__main__\":\n    model = UnivNetGenerator()\n    c = torch.randn(3, 100, 10)\n    z = torch.randn(3, 64, 10)\n    print(c.shape)\n    y = model(c, z)\n    print(y.shape)\n    assert y.shape == torch.Size([3, 1, 2560])",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.vocoder",
        "documentation": {}
    },
    {
        "label": "MAX_WAV_VALUE",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.vocoder",
        "description": "env.src.tts.TTS.tts.layers.tortoise.vocoder",
        "peekOfCode": "MAX_WAV_VALUE = 32768.0\nclass KernelPredictor(torch.nn.Module):\n    \"\"\"Kernel predictor for the location-variable convolutions\"\"\"\n    def __init__(\n        self,\n        cond_channels,\n        conv_in_channels,\n        conv_out_channels,\n        conv_layers,\n        conv_kernel_size=3,",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.vocoder",
        "documentation": {}
    },
    {
        "label": "Wav2VecAlignment",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.wav2vec_alignment",
        "description": "env.src.tts.TTS.tts.layers.tortoise.wav2vec_alignment",
        "peekOfCode": "class Wav2VecAlignment:\n    \"\"\"\n    Uses wav2vec2 to perform audio<->text alignment.\n    \"\"\"\n    def __init__(self, device=\"cuda\"):\n        self.model = Wav2Vec2ForCTC.from_pretrained(\"jbetker/wav2vec2-large-robust-ft-libritts-voxpopuli\").cpu()\n        self.feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"facebook/wav2vec2-large-960h\")\n        self.tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\"jbetker/tacotron-symbols\")\n        self.device = device\n    def align(self, audio, expected_text, audio_sample_rate=24000):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.wav2vec_alignment",
        "documentation": {}
    },
    {
        "label": "max_alignment",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.wav2vec_alignment",
        "description": "env.src.tts.TTS.tts.layers.tortoise.wav2vec_alignment",
        "peekOfCode": "def max_alignment(s1, s2, skip_character=\"~\", record=None):\n    \"\"\"\n    A clever function that aligns s1 to s2 as best it can. Wherever a character from s1 is not found in s2, a '~' is\n    used to replace that character.\n    Finally got to use my DP skills!\n    \"\"\"\n    if record is None:\n        record = {}\n    assert skip_character not in s1, f\"Found the skip character {skip_character} in the provided string, {s1}\"\n    if len(s1) == 0:",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.wav2vec_alignment",
        "documentation": {}
    },
    {
        "label": "always",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class always:\n    def __init__(self, val):\n        self.val = val\n    def __call__(self, *args, **kwargs):\n        return self.val\nclass not_equals:\n    def __init__(self, val):\n        self.val = val\n    def __call__(self, x, *args, **kwargs):\n        return x != self.val",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "not_equals",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class not_equals:\n    def __init__(self, val):\n        self.val = val\n    def __call__(self, x, *args, **kwargs):\n        return x != self.val\nclass equals:\n    def __init__(self, val):\n        self.val = val\n    def __call__(self, x, *args, **kwargs):\n        return x == self.val",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "equals",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class equals:\n    def __init__(self, val):\n        self.val = val\n    def __call__(self, x, *args, **kwargs):\n        return x == self.val\ndef max_neg_value(tensor):\n    return -torch.finfo(tensor.dtype).max\ndef l2norm(t):\n    return F.normalize(t, p=2, dim=-1)\n# init helpers",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "ReluSquared",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class ReluSquared(nn.Module):\n    def forward(self, x):\n        return F.relu(x) ** 2\n# positional embeddings\nclass AbsolutePositionalEmbedding(nn.Module):\n    def __init__(self, dim, max_seq_len):\n        super().__init__()\n        self.scale = dim**-0.5\n        self.emb = nn.Embedding(max_seq_len, dim)\n    def forward(self, x):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "AbsolutePositionalEmbedding",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class AbsolutePositionalEmbedding(nn.Module):\n    def __init__(self, dim, max_seq_len):\n        super().__init__()\n        self.scale = dim**-0.5\n        self.emb = nn.Embedding(max_seq_len, dim)\n    def forward(self, x):\n        n = torch.arange(x.shape[1], device=x.device)\n        pos_emb = self.emb(n)\n        pos_emb = rearrange(pos_emb, \"n d -> () n d\")\n        return pos_emb * self.scale",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "FixedPositionalEmbedding",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class FixedPositionalEmbedding(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n    def forward(self, x, seq_dim=1, offset=0):\n        t = torch.arange(x.shape[seq_dim], device=x.device).type_as(self.inv_freq) + offset\n        sinusoid_inp = torch.einsum(\"i , j -> i j\", t, self.inv_freq)\n        emb = torch.cat((sinusoid_inp.sin(), sinusoid_inp.cos()), dim=-1)\n        return rearrange(emb, \"n d -> () n d\")",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "RelativePositionBias",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class RelativePositionBias(nn.Module):\n    def __init__(self, scale, causal=False, num_buckets=32, max_distance=128, heads=8):\n        super().__init__()\n        self.scale = scale\n        self.causal = causal\n        self.num_buckets = num_buckets\n        self.max_distance = max_distance\n        self.relative_attention_bias = nn.Embedding(num_buckets, heads)\n    @staticmethod\n    def _relative_position_bucket(relative_position, causal=True, num_buckets=32, max_distance=128):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "AlibiPositionalBias",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class AlibiPositionalBias(nn.Module):\n    def __init__(self, heads, **kwargs):\n        super().__init__()\n        self.heads = heads\n        slopes = torch.Tensor(self._get_slopes(heads))\n        slopes = rearrange(slopes, \"h -> () h () ()\")\n        self.register_buffer(\"slopes\", slopes, persistent=False)\n        self.register_buffer(\"bias\", None, persistent=False)\n    @staticmethod\n    def _get_slopes(heads):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "LearnedAlibiPositionalBias",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class LearnedAlibiPositionalBias(AlibiPositionalBias):\n    def __init__(self, heads, bidirectional=False):\n        super().__init__(heads)\n        los_slopes = torch.log(self.slopes)\n        self.learned_logslopes = nn.Parameter(los_slopes)\n        self.bidirectional = bidirectional\n        if self.bidirectional:\n            self.learned_logslopes_future = nn.Parameter(los_slopes)\n    def forward(self, qk_dots):\n        h, i, j, device = *qk_dots.shape[-3:], qk_dots.device",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "RotaryEmbedding",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class RotaryEmbedding(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n    def forward(self, max_seq_len, device):\n        t = torch.arange(max_seq_len, device=device).type_as(self.inv_freq)\n        freqs = torch.einsum(\"i , j -> i j\", t, self.inv_freq)\n        emb = torch.cat((freqs, freqs), dim=-1)\n        return rearrange(emb, \"n d -> () () n d\")",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "Scale",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class Scale(nn.Module):\n    def __init__(self, value, fn):\n        super().__init__()\n        self.value = value\n        self.fn = fn\n    def forward(self, x, **kwargs):\n        out = self.fn(x, **kwargs)\n        scale_fn = lambda t: t * self.value\n        if not isinstance(out, tuple):\n            return scale_fn(out)",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "Rezero",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class Rezero(nn.Module):\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n        self.g = nn.Parameter(torch.zeros(1))\n    def forward(self, x, **kwargs):\n        out = self.fn(x, **kwargs)\n        rezero_fn = lambda t: t * self.g\n        if not isinstance(out, tuple):\n            return rezero_fn(out)",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "ScaleNorm",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class ScaleNorm(nn.Module):\n    def __init__(self, dim, eps=1e-5):\n        super().__init__()\n        self.scale = dim**-0.5\n        self.eps = eps\n        self.g = nn.Parameter(torch.ones(1))\n    def forward(self, x):\n        norm = torch.norm(x, dim=-1, keepdim=True) * self.scale\n        return x / norm.clamp(min=self.eps) * self.g\nclass RMSNorm(nn.Module):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "RMSNorm",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class RMSNorm(nn.Module):\n    def __init__(self, dim, eps=1e-8):\n        super().__init__()\n        self.scale = dim**-0.5\n        self.eps = eps\n        self.g = nn.Parameter(torch.ones(dim))\n    def forward(self, x):\n        norm = torch.norm(x, dim=-1, keepdim=True) * self.scale\n        return x / norm.clamp(min=self.eps) * self.g\nclass RMSScaleShiftNorm(nn.Module):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "RMSScaleShiftNorm",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class RMSScaleShiftNorm(nn.Module):\n    def __init__(self, dim, eps=1e-8):\n        super().__init__()\n        self.scale = dim**-0.5\n        self.eps = eps\n        self.g = nn.Parameter(torch.ones(dim))\n        self.scale_shift_process = nn.Linear(dim * 2, dim * 2)\n    def forward(self, x, norm_scale_shift_inp):\n        norm = torch.norm(x, dim=-1, keepdim=True) * self.scale\n        norm = x / norm.clamp(min=self.eps) * self.g",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "Residual",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class Residual(nn.Module):\n    def __init__(self, dim, scale_residual=False):\n        super().__init__()\n        self.residual_scale = nn.Parameter(torch.ones(dim)) if scale_residual else None\n    def forward(self, x, residual):\n        if exists(self.residual_scale):\n            residual = residual * self.residual_scale\n        return x + residual\nclass GRUGating(nn.Module):\n    def __init__(self, dim, scale_residual=False):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "GRUGating",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class GRUGating(nn.Module):\n    def __init__(self, dim, scale_residual=False):\n        super().__init__()\n        self.gru = nn.GRUCell(dim, dim)\n        self.residual_scale = nn.Parameter(torch.ones(dim)) if scale_residual else None\n    def forward(self, x, residual):\n        if exists(self.residual_scale):\n            residual = residual * self.residual_scale\n        gated_output = self.gru(rearrange(x, \"b n d -> (b n) d\"), rearrange(residual, \"b n d -> (b n) d\"))\n        return gated_output.reshape_as(x)",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "ShiftTokens",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class ShiftTokens(nn.Module):\n    def __init__(self, shifts, fn):\n        super().__init__()\n        self.fn = fn\n        self.shifts = tuple(shifts)\n    def forward(self, x, **kwargs):\n        mask = kwargs.get(\"mask\", None)\n        shifts = self.shifts\n        segments = len(shifts)\n        feats_per_shift = x.shape[-1] // segments",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "GLU",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class GLU(nn.Module):\n    def __init__(self, dim_in, dim_out, activation):\n        super().__init__()\n        self.act = activation\n        self.proj = nn.Linear(dim_in, dim_out * 2)\n    def forward(self, x):\n        x, gate = self.proj(x).chunk(2, dim=-1)\n        return x * self.act(gate)\nclass FeedForward(nn.Module):\n    def __init__(",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "FeedForward",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class FeedForward(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_out=None,\n        mult=4,\n        glu=False,\n        relu_squared=False,\n        post_act_ln=False,\n        dropout=0.0,",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "Attention",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class Attention(nn.Module):\n    def __init__(\n        self,\n        dim,\n        dim_head=DEFAULT_DIM_HEAD,\n        heads=8,\n        causal=False,\n        talking_heads=False,\n        head_scale=False,\n        collab_heads=False,",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "AttentionLayers",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class AttentionLayers(nn.Module):\n    def __init__(\n        self,\n        dim,\n        depth,\n        heads=8,\n        causal=False,\n        cross_attend=False,\n        only_cross=False,\n        use_scalenorm=False,",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class Encoder(AttentionLayers):\n    def __init__(self, **kwargs):\n        assert \"causal\" not in kwargs, \"cannot set causality on encoder\"\n        super().__init__(causal=False, **kwargs)\nclass Decoder(AttentionLayers):\n    def __init__(self, **kwargs):\n        assert \"causal\" not in kwargs, \"cannot set causality on decoder\"\n        super().__init__(causal=True, **kwargs)\nclass CrossAttender(AttentionLayers):\n    def __init__(self, **kwargs):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class Decoder(AttentionLayers):\n    def __init__(self, **kwargs):\n        assert \"causal\" not in kwargs, \"cannot set causality on decoder\"\n        super().__init__(causal=True, **kwargs)\nclass CrossAttender(AttentionLayers):\n    def __init__(self, **kwargs):\n        super().__init__(cross_attend=True, only_cross=True, **kwargs)\nclass ViTransformerWrapper(nn.Module):\n    def __init__(self, *, image_size, patch_size, attn_layers, num_classes=None, dropout=0.0, emb_dropout=0.0):\n        super().__init__()",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "CrossAttender",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class CrossAttender(AttentionLayers):\n    def __init__(self, **kwargs):\n        super().__init__(cross_attend=True, only_cross=True, **kwargs)\nclass ViTransformerWrapper(nn.Module):\n    def __init__(self, *, image_size, patch_size, attn_layers, num_classes=None, dropout=0.0, emb_dropout=0.0):\n        super().__init__()\n        assert isinstance(attn_layers, Encoder), \"attention layers must be an Encoder\"\n        assert image_size % patch_size == 0, \"image dimensions must be divisible by the patch size\"\n        dim = attn_layers.dim\n        num_patches = (image_size // patch_size) ** 2",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "ViTransformerWrapper",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class ViTransformerWrapper(nn.Module):\n    def __init__(self, *, image_size, patch_size, attn_layers, num_classes=None, dropout=0.0, emb_dropout=0.0):\n        super().__init__()\n        assert isinstance(attn_layers, Encoder), \"attention layers must be an Encoder\"\n        assert image_size % patch_size == 0, \"image dimensions must be divisible by the patch size\"\n        dim = attn_layers.dim\n        num_patches = (image_size // patch_size) ** 2\n        patch_dim = 3 * patch_size**2\n        self.patch_size = patch_size\n        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "TransformerWrapper",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class TransformerWrapper(nn.Module):\n    def __init__(\n        self,\n        *,\n        num_tokens,\n        max_seq_len,\n        attn_layers,\n        emb_dim=None,\n        max_mem_len=0.0,\n        shift_mem_down=0,",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "ContinuousTransformerWrapper",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "class ContinuousTransformerWrapper(nn.Module):\n    def __init__(\n        self, *, max_seq_len, attn_layers, dim_in=None, dim_out=None, emb_dim=None, emb_dropout=0.0, use_pos_emb=True\n    ):\n        super().__init__()\n        assert isinstance(attn_layers, AttentionLayers), \"attention layers must be one of Encoder or Decoder\"\n        dim = attn_layers.dim\n        self.max_seq_len = max_seq_len\n        self.pos_emb = (\n            AbsolutePositionalEmbedding(dim, max_seq_len)",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "exists",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "def exists(val):\n    return val is not None\ndef default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d\ndef cast_tuple(val, depth):\n    return val if isinstance(val, tuple) else (val,) * depth\nclass always:\n    def __init__(self, val):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "default",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "def default(val, d):\n    if exists(val):\n        return val\n    return d() if isfunction(d) else d\ndef cast_tuple(val, depth):\n    return val if isinstance(val, tuple) else (val,) * depth\nclass always:\n    def __init__(self, val):\n        self.val = val\n    def __call__(self, *args, **kwargs):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "cast_tuple",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "def cast_tuple(val, depth):\n    return val if isinstance(val, tuple) else (val,) * depth\nclass always:\n    def __init__(self, val):\n        self.val = val\n    def __call__(self, *args, **kwargs):\n        return self.val\nclass not_equals:\n    def __init__(self, val):\n        self.val = val",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "max_neg_value",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "def max_neg_value(tensor):\n    return -torch.finfo(tensor.dtype).max\ndef l2norm(t):\n    return F.normalize(t, p=2, dim=-1)\n# init helpers\ndef init_zero_(layer):\n    nn.init.constant_(layer.weight, 0.0)\n    if exists(layer.bias):\n        nn.init.constant_(layer.bias, 0.0)\n# keyword argument helpers",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "l2norm",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "def l2norm(t):\n    return F.normalize(t, p=2, dim=-1)\n# init helpers\ndef init_zero_(layer):\n    nn.init.constant_(layer.weight, 0.0)\n    if exists(layer.bias):\n        nn.init.constant_(layer.bias, 0.0)\n# keyword argument helpers\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "init_zero_",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "def init_zero_(layer):\n    nn.init.constant_(layer.weight, 0.0)\n    if exists(layer.bias):\n        nn.init.constant_(layer.bias, 0.0)\n# keyword argument helpers\ndef pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\ndef group_dict_by_key(cond, d):\n    return_val = [dict(), dict()]",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "pick_and_pop",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "def pick_and_pop(keys, d):\n    values = list(map(lambda key: d.pop(key), keys))\n    return dict(zip(keys, values))\ndef group_dict_by_key(cond, d):\n    return_val = [dict(), dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "group_dict_by_key",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "def group_dict_by_key(cond, d):\n    return_val = [dict(), dict()]\n    for key in d.keys():\n        match = bool(cond(key))\n        ind = int(not match)\n        return_val[ind][key] = d[key]\n    return (*return_val,)\ndef string_begins_with(prefix, str):\n    return str.startswith(prefix)\ndef group_by_key_prefix(prefix, d):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "string_begins_with",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "def string_begins_with(prefix, str):\n    return str.startswith(prefix)\ndef group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix) :], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n# activations\nclass ReluSquared(nn.Module):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "group_by_key_prefix",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "def group_by_key_prefix(prefix, d):\n    return group_dict_by_key(partial(string_begins_with, prefix), d)\ndef groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix) :], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n# activations\nclass ReluSquared(nn.Module):\n    def forward(self, x):\n        return F.relu(x) ** 2",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "groupby_prefix_and_trim",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "def groupby_prefix_and_trim(prefix, d):\n    kwargs_with_prefix, kwargs = group_dict_by_key(partial(string_begins_with, prefix), d)\n    kwargs_without_prefix = dict(map(lambda x: (x[0][len(prefix) :], x[1]), tuple(kwargs_with_prefix.items())))\n    return kwargs_without_prefix, kwargs\n# activations\nclass ReluSquared(nn.Module):\n    def forward(self, x):\n        return F.relu(x) ** 2\n# positional embeddings\nclass AbsolutePositionalEmbedding(nn.Module):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "rotate_half",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "def rotate_half(x):\n    x = rearrange(x, \"... (j d) -> ... j d\", j=2)\n    x1, x2 = x.unbind(dim=-2)\n    return torch.cat((-x2, x1), dim=-1)\ndef apply_rotary_pos_emb(t, freqs):\n    seq_len = t.shape[-2]\n    freqs = freqs[:, :, -seq_len:]\n    return (t * freqs.cos()) + (rotate_half(t) * freqs.sin())\n# norms\nclass Scale(nn.Module):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "apply_rotary_pos_emb",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "def apply_rotary_pos_emb(t, freqs):\n    seq_len = t.shape[-2]\n    freqs = freqs[:, :, -seq_len:]\n    return (t * freqs.cos()) + (rotate_half(t) * freqs.sin())\n# norms\nclass Scale(nn.Module):\n    def __init__(self, value, fn):\n        super().__init__()\n        self.value = value\n        self.fn = fn",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "shift",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "def shift(t, amount, mask=None):\n    if amount == 0:\n        return t\n    if exists(mask):\n        t = t.masked_fill(~mask[..., None], 0.0)\n    return F.pad(t, (0, 0, amount, -amount), value=0.0)\nclass ShiftTokens(nn.Module):\n    def __init__(self, shifts, fn):\n        super().__init__()\n        self.fn = fn",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "DEFAULT_DIM_HEAD",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "DEFAULT_DIM_HEAD = 64\nIntermediates = namedtuple(\"Intermediates\", [\"pre_softmax_attn\", \"post_softmax_attn\"])\nLayerIntermediates = namedtuple(\n    \"Intermediates\",\n    [\n        \"hiddens\",\n        \"attn_intermediates\",\n        \"past_key_values\",\n    ],\n)",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "Intermediates",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "Intermediates = namedtuple(\"Intermediates\", [\"pre_softmax_attn\", \"post_softmax_attn\"])\nLayerIntermediates = namedtuple(\n    \"Intermediates\",\n    [\n        \"hiddens\",\n        \"attn_intermediates\",\n        \"past_key_values\",\n    ],\n)\n# helpers",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "LayerIntermediates",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "description": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "peekOfCode": "LayerIntermediates = namedtuple(\n    \"Intermediates\",\n    [\n        \"hiddens\",\n        \"attn_intermediates\",\n        \"past_key_values\",\n    ],\n)\n# helpers\ndef exists(val):",
        "detail": "env.src.tts.TTS.tts.layers.tortoise.xtransformers",
        "documentation": {}
    },
    {
        "label": "DiscriminatorS",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.vits.discriminator",
        "description": "env.src.tts.TTS.tts.layers.vits.discriminator",
        "peekOfCode": "class DiscriminatorS(torch.nn.Module):\n    \"\"\"HiFiGAN Scale Discriminator. Channel sizes are different from the original HiFiGAN.\n    Args:\n        use_spectral_norm (bool): if `True` swith to spectral norm instead of weight norm.\n    \"\"\"\n    def __init__(self, use_spectral_norm=False):\n        super().__init__()\n        norm_f = nn.utils.spectral_norm if use_spectral_norm else nn.utils.weight_norm\n        self.convs = nn.ModuleList(\n            [",
        "detail": "env.src.tts.TTS.tts.layers.vits.discriminator",
        "documentation": {}
    },
    {
        "label": "VitsDiscriminator",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.vits.discriminator",
        "description": "env.src.tts.TTS.tts.layers.vits.discriminator",
        "peekOfCode": "class VitsDiscriminator(nn.Module):\n    \"\"\"VITS discriminator wrapping one Scale Discriminator and a stack of Period Discriminator.\n    ::\n        waveform -> ScaleDiscriminator() -> scores_sd, feats_sd --> append() -> scores, feats\n               |--> MultiPeriodDiscriminator() -> scores_mpd, feats_mpd ^\n    Args:\n        use_spectral_norm (bool): if `True` swith to spectral norm instead of weight norm.\n    \"\"\"\n    def __init__(self, periods=(2, 3, 5, 7, 11), use_spectral_norm=False):\n        super().__init__()",
        "detail": "env.src.tts.TTS.tts.layers.vits.discriminator",
        "documentation": {}
    },
    {
        "label": "TextEncoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.vits.networks",
        "description": "env.src.tts.TTS.tts.layers.vits.networks",
        "peekOfCode": "class TextEncoder(nn.Module):\n    def __init__(\n        self,\n        n_vocab: int,\n        out_channels: int,\n        hidden_channels: int,\n        hidden_channels_ffn: int,\n        num_heads: int,\n        num_layers: int,\n        kernel_size: int,",
        "detail": "env.src.tts.TTS.tts.layers.vits.networks",
        "documentation": {}
    },
    {
        "label": "ResidualCouplingBlock",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.vits.networks",
        "description": "env.src.tts.TTS.tts.layers.vits.networks",
        "peekOfCode": "class ResidualCouplingBlock(nn.Module):\n    def __init__(\n        self,\n        channels,\n        hidden_channels,\n        kernel_size,\n        dilation_rate,\n        num_layers,\n        dropout_p=0,\n        cond_channels=0,",
        "detail": "env.src.tts.TTS.tts.layers.vits.networks",
        "documentation": {}
    },
    {
        "label": "ResidualCouplingBlocks",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.vits.networks",
        "description": "env.src.tts.TTS.tts.layers.vits.networks",
        "peekOfCode": "class ResidualCouplingBlocks(nn.Module):\n    def __init__(\n        self,\n        channels: int,\n        hidden_channels: int,\n        kernel_size: int,\n        dilation_rate: int,\n        num_layers: int,\n        num_flows=4,\n        cond_channels=0,",
        "detail": "env.src.tts.TTS.tts.layers.vits.networks",
        "documentation": {}
    },
    {
        "label": "PosteriorEncoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.vits.networks",
        "description": "env.src.tts.TTS.tts.layers.vits.networks",
        "peekOfCode": "class PosteriorEncoder(nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        hidden_channels: int,\n        kernel_size: int,\n        dilation_rate: int,\n        num_layers: int,\n        cond_channels=0,",
        "detail": "env.src.tts.TTS.tts.layers.vits.networks",
        "documentation": {}
    },
    {
        "label": "convert_pad_shape",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.vits.networks",
        "description": "env.src.tts.TTS.tts.layers.vits.networks",
        "peekOfCode": "def convert_pad_shape(pad_shape):\n    l = pad_shape[::-1]\n    pad_shape = [item for sublist in l for item in sublist]\n    return pad_shape\ndef init_weights(m, mean=0.0, std=0.01):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        m.weight.data.normal_(mean, std)\ndef get_padding(kernel_size, dilation=1):\n    return int((kernel_size * dilation - dilation) / 2)",
        "detail": "env.src.tts.TTS.tts.layers.vits.networks",
        "documentation": {}
    },
    {
        "label": "init_weights",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.vits.networks",
        "description": "env.src.tts.TTS.tts.layers.vits.networks",
        "peekOfCode": "def init_weights(m, mean=0.0, std=0.01):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        m.weight.data.normal_(mean, std)\ndef get_padding(kernel_size, dilation=1):\n    return int((kernel_size * dilation - dilation) / 2)\nclass TextEncoder(nn.Module):\n    def __init__(\n        self,\n        n_vocab: int,",
        "detail": "env.src.tts.TTS.tts.layers.vits.networks",
        "documentation": {}
    },
    {
        "label": "get_padding",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.vits.networks",
        "description": "env.src.tts.TTS.tts.layers.vits.networks",
        "peekOfCode": "def get_padding(kernel_size, dilation=1):\n    return int((kernel_size * dilation - dilation) / 2)\nclass TextEncoder(nn.Module):\n    def __init__(\n        self,\n        n_vocab: int,\n        out_channels: int,\n        hidden_channels: int,\n        hidden_channels_ffn: int,\n        num_heads: int,",
        "detail": "env.src.tts.TTS.tts.layers.vits.networks",
        "documentation": {}
    },
    {
        "label": "LRELU_SLOPE",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.layers.vits.networks",
        "description": "env.src.tts.TTS.tts.layers.vits.networks",
        "peekOfCode": "LRELU_SLOPE = 0.1\ndef convert_pad_shape(pad_shape):\n    l = pad_shape[::-1]\n    pad_shape = [item for sublist in l for item in sublist]\n    return pad_shape\ndef init_weights(m, mean=0.0, std=0.01):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        m.weight.data.normal_(mean, std)\ndef get_padding(kernel_size, dilation=1):",
        "detail": "env.src.tts.TTS.tts.layers.vits.networks",
        "documentation": {}
    },
    {
        "label": "DilatedDepthSeparableConv",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.vits.stochastic_duration_predictor",
        "description": "env.src.tts.TTS.tts.layers.vits.stochastic_duration_predictor",
        "peekOfCode": "class DilatedDepthSeparableConv(nn.Module):\n    def __init__(self, channels, kernel_size, num_layers, dropout_p=0.0) -> torch.tensor:\n        \"\"\"Dilated Depth-wise Separable Convolution module.\n        ::\n            x |-> DDSConv(x) -> LayerNorm(x) -> GeLU(x) -> Conv1x1(x) -> LayerNorm(x) -> GeLU(x) -> + -> o\n              |-------------------------------------------------------------------------------------^\n        Args:\n            channels ([type]): [description]\n            kernel_size ([type]): [description]\n            num_layers ([type]): [description]",
        "detail": "env.src.tts.TTS.tts.layers.vits.stochastic_duration_predictor",
        "documentation": {}
    },
    {
        "label": "ElementwiseAffine",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.vits.stochastic_duration_predictor",
        "description": "env.src.tts.TTS.tts.layers.vits.stochastic_duration_predictor",
        "peekOfCode": "class ElementwiseAffine(nn.Module):\n    \"\"\"Element-wise affine transform like no-population stats BatchNorm alternative.\n    Args:\n        channels (int): Number of input tensor channels.\n    \"\"\"\n    def __init__(self, channels):\n        super().__init__()\n        self.translation = nn.Parameter(torch.zeros(channels, 1))\n        self.log_scale = nn.Parameter(torch.zeros(channels, 1))\n    def forward(self, x, x_mask, reverse=False, **kwargs):  # pylint: disable=unused-argument",
        "detail": "env.src.tts.TTS.tts.layers.vits.stochastic_duration_predictor",
        "documentation": {}
    },
    {
        "label": "ConvFlow",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.vits.stochastic_duration_predictor",
        "description": "env.src.tts.TTS.tts.layers.vits.stochastic_duration_predictor",
        "peekOfCode": "class ConvFlow(nn.Module):\n    \"\"\"Dilated depth separable convolutional based spline flow.\n    Args:\n        in_channels (int): Number of input tensor channels.\n        hidden_channels (int): Number of in network channels.\n        kernel_size (int): Convolutional kernel size.\n        num_layers (int): Number of convolutional layers.\n        num_bins (int, optional): Number of spline bins. Defaults to 10.\n        tail_bound (float, optional): Tail bound for PRQT. Defaults to 5.0.\n    \"\"\"",
        "detail": "env.src.tts.TTS.tts.layers.vits.stochastic_duration_predictor",
        "documentation": {}
    },
    {
        "label": "StochasticDurationPredictor",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.vits.stochastic_duration_predictor",
        "description": "env.src.tts.TTS.tts.layers.vits.stochastic_duration_predictor",
        "peekOfCode": "class StochasticDurationPredictor(nn.Module):\n    \"\"\"Stochastic duration predictor with Spline Flows.\n    It applies Variational Dequantization and Variational Data Augmentation.\n    Paper:\n        SDP: https://arxiv.org/pdf/2106.06103.pdf\n        Spline Flow: https://arxiv.org/abs/1906.04032\n    ::\n        ## Inference\n        x -> TextCondEncoder() -> Flow() -> dr_hat\n        noise ----------------------^",
        "detail": "env.src.tts.TTS.tts.layers.vits.stochastic_duration_predictor",
        "documentation": {}
    },
    {
        "label": "piecewise_rational_quadratic_transform",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.vits.transforms",
        "description": "env.src.tts.TTS.tts.layers.vits.transforms",
        "peekOfCode": "def piecewise_rational_quadratic_transform(\n    inputs,\n    unnormalized_widths,\n    unnormalized_heights,\n    unnormalized_derivatives,\n    inverse=False,\n    tails=None,\n    tail_bound=1.0,\n    min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n    min_bin_height=DEFAULT_MIN_BIN_HEIGHT,",
        "detail": "env.src.tts.TTS.tts.layers.vits.transforms",
        "documentation": {}
    },
    {
        "label": "searchsorted",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.vits.transforms",
        "description": "env.src.tts.TTS.tts.layers.vits.transforms",
        "peekOfCode": "def searchsorted(bin_locations, inputs, eps=1e-6):\n    bin_locations[..., -1] += eps\n    return torch.sum(inputs[..., None] >= bin_locations, dim=-1) - 1\ndef unconstrained_rational_quadratic_spline(\n    inputs,\n    unnormalized_widths,\n    unnormalized_heights,\n    unnormalized_derivatives,\n    inverse=False,\n    tails=\"linear\",",
        "detail": "env.src.tts.TTS.tts.layers.vits.transforms",
        "documentation": {}
    },
    {
        "label": "unconstrained_rational_quadratic_spline",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.vits.transforms",
        "description": "env.src.tts.TTS.tts.layers.vits.transforms",
        "peekOfCode": "def unconstrained_rational_quadratic_spline(\n    inputs,\n    unnormalized_widths,\n    unnormalized_heights,\n    unnormalized_derivatives,\n    inverse=False,\n    tails=\"linear\",\n    tail_bound=1.0,\n    min_bin_width=DEFAULT_MIN_BIN_WIDTH,\n    min_bin_height=DEFAULT_MIN_BIN_HEIGHT,",
        "detail": "env.src.tts.TTS.tts.layers.vits.transforms",
        "documentation": {}
    },
    {
        "label": "rational_quadratic_spline",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.vits.transforms",
        "description": "env.src.tts.TTS.tts.layers.vits.transforms",
        "peekOfCode": "def rational_quadratic_spline(\n    inputs,\n    unnormalized_widths,\n    unnormalized_heights,\n    unnormalized_derivatives,\n    inverse=False,\n    left=0.0,\n    right=1.0,\n    bottom=0.0,\n    top=1.0,",
        "detail": "env.src.tts.TTS.tts.layers.vits.transforms",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MIN_BIN_WIDTH",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.layers.vits.transforms",
        "description": "env.src.tts.TTS.tts.layers.vits.transforms",
        "peekOfCode": "DEFAULT_MIN_BIN_WIDTH = 1e-3\nDEFAULT_MIN_BIN_HEIGHT = 1e-3\nDEFAULT_MIN_DERIVATIVE = 1e-3\ndef piecewise_rational_quadratic_transform(\n    inputs,\n    unnormalized_widths,\n    unnormalized_heights,\n    unnormalized_derivatives,\n    inverse=False,\n    tails=None,",
        "detail": "env.src.tts.TTS.tts.layers.vits.transforms",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MIN_BIN_HEIGHT",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.layers.vits.transforms",
        "description": "env.src.tts.TTS.tts.layers.vits.transforms",
        "peekOfCode": "DEFAULT_MIN_BIN_HEIGHT = 1e-3\nDEFAULT_MIN_DERIVATIVE = 1e-3\ndef piecewise_rational_quadratic_transform(\n    inputs,\n    unnormalized_widths,\n    unnormalized_heights,\n    unnormalized_derivatives,\n    inverse=False,\n    tails=None,\n    tail_bound=1.0,",
        "detail": "env.src.tts.TTS.tts.layers.vits.transforms",
        "documentation": {}
    },
    {
        "label": "DEFAULT_MIN_DERIVATIVE",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.layers.vits.transforms",
        "description": "env.src.tts.TTS.tts.layers.vits.transforms",
        "peekOfCode": "DEFAULT_MIN_DERIVATIVE = 1e-3\ndef piecewise_rational_quadratic_transform(\n    inputs,\n    unnormalized_widths,\n    unnormalized_heights,\n    unnormalized_derivatives,\n    inverse=False,\n    tails=None,\n    tail_bound=1.0,\n    min_bin_width=DEFAULT_MIN_BIN_WIDTH,",
        "detail": "env.src.tts.TTS.tts.layers.vits.transforms",
        "documentation": {}
    },
    {
        "label": "L1LossMasked",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.losses",
        "description": "env.src.tts.TTS.tts.layers.losses",
        "peekOfCode": "class L1LossMasked(nn.Module):\n    def __init__(self, seq_len_norm):\n        super().__init__()\n        self.seq_len_norm = seq_len_norm\n    def forward(self, x, target, length):\n        \"\"\"\n        Args:\n            x: A Variable containing a FloatTensor of size\n                (batch, max_len, dim) which contains the\n                unnormalized probability for each class.",
        "detail": "env.src.tts.TTS.tts.layers.losses",
        "documentation": {}
    },
    {
        "label": "MSELossMasked",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.losses",
        "description": "env.src.tts.TTS.tts.layers.losses",
        "peekOfCode": "class MSELossMasked(nn.Module):\n    def __init__(self, seq_len_norm):\n        super().__init__()\n        self.seq_len_norm = seq_len_norm\n    def forward(self, x, target, length):\n        \"\"\"\n        Args:\n            x: A Variable containing a FloatTensor of size\n                (batch, max_len, dim) which contains the\n                unnormalized probability for each class.",
        "detail": "env.src.tts.TTS.tts.layers.losses",
        "documentation": {}
    },
    {
        "label": "SSIMLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.losses",
        "description": "env.src.tts.TTS.tts.layers.losses",
        "peekOfCode": "class SSIMLoss(torch.nn.Module):\n    \"\"\"SSIM loss as (1 - SSIM)\n    SSIM is explained here https://en.wikipedia.org/wiki/Structural_similarity\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.loss_func = _SSIMLoss()\n    def forward(self, y_hat, y, length):\n        \"\"\"\n        Args:",
        "detail": "env.src.tts.TTS.tts.layers.losses",
        "documentation": {}
    },
    {
        "label": "AttentionEntropyLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.losses",
        "description": "env.src.tts.TTS.tts.layers.losses",
        "peekOfCode": "class AttentionEntropyLoss(nn.Module):\n    # pylint: disable=R0201\n    def forward(self, align):\n        \"\"\"\n        Forces attention to be more decisive by penalizing\n        soft attention weights\n        \"\"\"\n        entropy = torch.distributions.Categorical(probs=align).entropy()\n        loss = (entropy / np.log(align.shape[1])).mean()\n        return loss",
        "detail": "env.src.tts.TTS.tts.layers.losses",
        "documentation": {}
    },
    {
        "label": "BCELossMasked",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.losses",
        "description": "env.src.tts.TTS.tts.layers.losses",
        "peekOfCode": "class BCELossMasked(nn.Module):\n    \"\"\"BCE loss with masking.\n    Used mainly for stopnet in autoregressive models.\n    Args:\n        pos_weight (float): weight for positive samples. If set < 1, penalize early stopping. Defaults to None.\n    \"\"\"\n    def __init__(self, pos_weight: float = None):\n        super().__init__()\n        self.pos_weight = nn.Parameter(torch.tensor([pos_weight]), requires_grad=False)\n    def forward(self, x, target, length):",
        "detail": "env.src.tts.TTS.tts.layers.losses",
        "documentation": {}
    },
    {
        "label": "DifferentialSpectralLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.losses",
        "description": "env.src.tts.TTS.tts.layers.losses",
        "peekOfCode": "class DifferentialSpectralLoss(nn.Module):\n    \"\"\"Differential Spectral Loss\n    https://arxiv.org/ftp/arxiv/papers/1909/1909.10302.pdf\"\"\"\n    def __init__(self, loss_func):\n        super().__init__()\n        self.loss_func = loss_func\n    def forward(self, x, target, length=None):\n        \"\"\"\n         Shapes:\n            x: B x T",
        "detail": "env.src.tts.TTS.tts.layers.losses",
        "documentation": {}
    },
    {
        "label": "GuidedAttentionLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.losses",
        "description": "env.src.tts.TTS.tts.layers.losses",
        "peekOfCode": "class GuidedAttentionLoss(torch.nn.Module):\n    def __init__(self, sigma=0.4):\n        super().__init__()\n        self.sigma = sigma\n    def _make_ga_masks(self, ilens, olens):\n        B = len(ilens)\n        max_ilen = max(ilens)\n        max_olen = max(olens)\n        ga_masks = torch.zeros((B, max_olen, max_ilen))\n        for idx, (ilen, olen) in enumerate(zip(ilens, olens)):",
        "detail": "env.src.tts.TTS.tts.layers.losses",
        "documentation": {}
    },
    {
        "label": "Huber",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.losses",
        "description": "env.src.tts.TTS.tts.layers.losses",
        "peekOfCode": "class Huber(nn.Module):\n    # pylint: disable=R0201\n    def forward(self, x, y, length=None):\n        \"\"\"\n        Shapes:\n            x: B x T\n            y: B x T\n            length: B\n        \"\"\"\n        mask = sequence_mask(sequence_length=length, max_len=y.size(1)).unsqueeze(2).float()",
        "detail": "env.src.tts.TTS.tts.layers.losses",
        "documentation": {}
    },
    {
        "label": "ForwardSumLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.losses",
        "description": "env.src.tts.TTS.tts.layers.losses",
        "peekOfCode": "class ForwardSumLoss(nn.Module):\n    def __init__(self, blank_logprob=-1):\n        super().__init__()\n        self.log_softmax = torch.nn.LogSoftmax(dim=3)\n        self.ctc_loss = torch.nn.CTCLoss(zero_infinity=True)\n        self.blank_logprob = blank_logprob\n    def forward(self, attn_logprob, in_lens, out_lens):\n        key_lens = in_lens\n        query_lens = out_lens\n        attn_logprob_padded = torch.nn.functional.pad(input=attn_logprob, pad=(1, 0), value=self.blank_logprob)",
        "detail": "env.src.tts.TTS.tts.layers.losses",
        "documentation": {}
    },
    {
        "label": "TacotronLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.losses",
        "description": "env.src.tts.TTS.tts.layers.losses",
        "peekOfCode": "class TacotronLoss(torch.nn.Module):\n    \"\"\"Collection of Tacotron set-up based on provided config.\"\"\"\n    def __init__(self, c, ga_sigma=0.4):\n        super().__init__()\n        self.stopnet_pos_weight = c.stopnet_pos_weight\n        self.use_capacitron_vae = c.use_capacitron_vae\n        if self.use_capacitron_vae:\n            self.capacitron_capacity = c.capacitron_vae.capacitron_capacity\n            self.capacitron_vae_loss_alpha = c.capacitron_vae.capacitron_VAE_loss_alpha\n        self.ga_alpha = c.ga_alpha",
        "detail": "env.src.tts.TTS.tts.layers.losses",
        "documentation": {}
    },
    {
        "label": "GlowTTSLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.losses",
        "description": "env.src.tts.TTS.tts.layers.losses",
        "peekOfCode": "class GlowTTSLoss(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.constant_factor = 0.5 * math.log(2 * math.pi)\n    def forward(self, z, means, scales, log_det, y_lengths, o_dur_log, o_attn_dur, x_lengths):\n        return_dict = {}\n        # flow loss - neg log likelihood\n        pz = torch.sum(scales) + 0.5 * torch.sum(torch.exp(-2 * scales) * (z - means) ** 2)\n        log_mle = self.constant_factor + (pz - torch.sum(log_det)) / (torch.sum(y_lengths) * z.shape[2])\n        # duration loss - MSE",
        "detail": "env.src.tts.TTS.tts.layers.losses",
        "documentation": {}
    },
    {
        "label": "MDNLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.losses",
        "description": "env.src.tts.TTS.tts.layers.losses",
        "peekOfCode": "class MDNLoss(nn.Module):\n    \"\"\"Mixture of Density Network Loss as described in https://arxiv.org/pdf/2003.01950.pdf.\"\"\"\n    def forward(self, logp, text_lengths, mel_lengths):  # pylint: disable=no-self-use\n        \"\"\"\n        Shapes:\n            mu: [B, D, T]\n            log_sigma: [B, D, T]\n            mel_spec: [B, D, T]\n        \"\"\"\n        B, T_seq, T_mel = logp.shape",
        "detail": "env.src.tts.TTS.tts.layers.losses",
        "documentation": {}
    },
    {
        "label": "AlignTTSLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.losses",
        "description": "env.src.tts.TTS.tts.layers.losses",
        "peekOfCode": "class AlignTTSLoss(nn.Module):\n    \"\"\"Modified AlignTTS Loss.\n    Computes\n        - L1 and SSIM losses from output spectrograms.\n        - Huber loss for duration predictor.\n        - MDNLoss for Mixture of Density Network.\n    All loss values are aggregated by a weighted sum of the alpha values.\n    Args:\n        c (dict): TTS model configuration.\n    \"\"\"",
        "detail": "env.src.tts.TTS.tts.layers.losses",
        "documentation": {}
    },
    {
        "label": "VitsGeneratorLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.losses",
        "description": "env.src.tts.TTS.tts.layers.losses",
        "peekOfCode": "class VitsGeneratorLoss(nn.Module):\n    def __init__(self, c: Coqpit):\n        super().__init__()\n        self.kl_loss_alpha = c.kl_loss_alpha\n        self.gen_loss_alpha = c.gen_loss_alpha\n        self.feat_loss_alpha = c.feat_loss_alpha\n        self.dur_loss_alpha = c.dur_loss_alpha\n        self.mel_loss_alpha = c.mel_loss_alpha\n        self.spk_encoder_loss_alpha = c.speaker_encoder_loss_alpha\n        self.stft = TorchSTFT(",
        "detail": "env.src.tts.TTS.tts.layers.losses",
        "documentation": {}
    },
    {
        "label": "VitsDiscriminatorLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.losses",
        "description": "env.src.tts.TTS.tts.layers.losses",
        "peekOfCode": "class VitsDiscriminatorLoss(nn.Module):\n    def __init__(self, c: Coqpit):\n        super().__init__()\n        self.disc_loss_alpha = c.disc_loss_alpha\n    @staticmethod\n    def discriminator_loss(scores_real, scores_fake):\n        loss = 0\n        real_losses = []\n        fake_losses = []\n        for dr, dg in zip(scores_real, scores_fake):",
        "detail": "env.src.tts.TTS.tts.layers.losses",
        "documentation": {}
    },
    {
        "label": "ForwardTTSLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.layers.losses",
        "description": "env.src.tts.TTS.tts.layers.losses",
        "peekOfCode": "class ForwardTTSLoss(nn.Module):\n    \"\"\"Generic configurable ForwardTTS loss.\"\"\"\n    def __init__(self, c):\n        super().__init__()\n        if c.spec_loss_type == \"mse\":\n            self.spec_loss = MSELossMasked(False)\n        elif c.spec_loss_type == \"l1\":\n            self.spec_loss = L1LossMasked(False)\n        else:\n            raise ValueError(\" [!] Unknown spec_loss_type {}\".format(c.spec_loss_type))",
        "detail": "env.src.tts.TTS.tts.layers.losses",
        "documentation": {}
    },
    {
        "label": "sample_wise_min_max",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.losses",
        "description": "env.src.tts.TTS.tts.layers.losses",
        "peekOfCode": "def sample_wise_min_max(x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n    \"\"\"Min-Max normalize tensor through first dimension\n    Shapes:\n        - x: :math:`[B, D1, D2]`\n        - m: :math:`[B, D1, 1]`\n    \"\"\"\n    maximum = torch.amax(x.masked_fill(~mask, 0), dim=(1, 2), keepdim=True)\n    minimum = torch.amin(x.masked_fill(~mask, np.inf), dim=(1, 2), keepdim=True)\n    return (x - minimum) / (maximum - minimum + 1e-8)\nclass SSIMLoss(torch.nn.Module):",
        "detail": "env.src.tts.TTS.tts.layers.losses",
        "documentation": {}
    },
    {
        "label": "mse_loss_custom",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.layers.losses",
        "description": "env.src.tts.TTS.tts.layers.losses",
        "peekOfCode": "def mse_loss_custom(x, y):\n    \"\"\"MSE loss using the torch back-end without reduction.\n    It uses less VRAM than the raw code\"\"\"\n    expanded_x, expanded_y = torch.broadcast_tensors(x, y)\n    return torch._C._nn.mse_loss(expanded_x, expanded_y, 0)  # pylint: disable=protected-access, c-extension-no-member\nclass MDNLoss(nn.Module):\n    \"\"\"Mixture of Density Network Loss as described in https://arxiv.org/pdf/2003.01950.pdf.\"\"\"\n    def forward(self, logp, text_lengths, mel_lengths):  # pylint: disable=no-self-use\n        \"\"\"\n        Shapes:",
        "detail": "env.src.tts.TTS.tts.layers.losses",
        "documentation": {}
    },
    {
        "label": "AlignTTSArgs",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.models.align_tts",
        "description": "env.src.tts.TTS.tts.models.align_tts",
        "peekOfCode": "class AlignTTSArgs(Coqpit):\n    \"\"\"\n    Args:\n        num_chars (int):\n            number of unique input to characters\n        out_channels (int):\n            number of output tensor channels. It is equal to the expected spectrogram size.\n        hidden_channels (int):\n            number of channels in all the model layers.\n        hidden_channels_ffn (int):",
        "detail": "env.src.tts.TTS.tts.models.align_tts",
        "documentation": {}
    },
    {
        "label": "AlignTTS",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.models.align_tts",
        "description": "env.src.tts.TTS.tts.models.align_tts",
        "peekOfCode": "class AlignTTS(BaseTTS):\n    \"\"\"AlignTTS with modified duration predictor.\n    https://arxiv.org/pdf/2003.01950.pdf\n    Encoder -> DurationPredictor -> Decoder\n    Check :class:`AlignTTSArgs` for the class arguments.\n    Paper Abstract:\n        Targeting at both high efficiency and performance, we propose AlignTTS to predict the\n        mel-spectrum in parallel. AlignTTS is based on a Feed-Forward Transformer which generates mel-spectrum from a\n        sequence of characters, and the duration of each character is determined by a duration predictor.Instead of\n        adopting the attention mechanism in Transformer TTS to align text to mel-spectrum, the alignment loss is presented",
        "detail": "env.src.tts.TTS.tts.models.align_tts",
        "documentation": {}
    },
    {
        "label": "BaseTacotron",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.models.base_tacotron",
        "description": "env.src.tts.TTS.tts.models.base_tacotron",
        "peekOfCode": "class BaseTacotron(BaseTTS):\n    \"\"\"Base class shared by Tacotron and Tacotron2\"\"\"\n    def __init__(\n        self,\n        config: \"TacotronConfig\",\n        ap: \"AudioProcessor\",\n        tokenizer: \"TTSTokenizer\",\n        speaker_manager: SpeakerManager = None,\n    ):\n        super().__init__(config, ap, tokenizer, speaker_manager)",
        "detail": "env.src.tts.TTS.tts.models.base_tacotron",
        "documentation": {}
    },
    {
        "label": "BaseTTS",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.models.base_tts",
        "description": "env.src.tts.TTS.tts.models.base_tts",
        "peekOfCode": "class BaseTTS(BaseTrainerModel):\n    \"\"\"Base `tts` class. Every new `tts` model must inherit this.\n    It defines common `tts` specific functions on top of `Model` implementation.\n    \"\"\"\n    MODEL_TYPE = \"tts\"\n    def __init__(\n        self,\n        config: Coqpit,\n        ap: \"AudioProcessor\",\n        tokenizer: \"TTSTokenizer\",",
        "detail": "env.src.tts.TTS.tts.models.base_tts",
        "documentation": {}
    },
    {
        "label": "ForwardTTSArgs",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.models.forward_tts",
        "description": "env.src.tts.TTS.tts.models.forward_tts",
        "peekOfCode": "class ForwardTTSArgs(Coqpit):\n    \"\"\"ForwardTTS Model arguments.\n    Args:\n        num_chars (int):\n            Number of characters in the vocabulary. Defaults to 100.\n        out_channels (int):\n            Number of output channels. Defaults to 80.\n        hidden_channels (int):\n            Number of base hidden channels of the model. Defaults to 512.\n        use_aligner (bool):",
        "detail": "env.src.tts.TTS.tts.models.forward_tts",
        "documentation": {}
    },
    {
        "label": "ForwardTTS",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.models.forward_tts",
        "description": "env.src.tts.TTS.tts.models.forward_tts",
        "peekOfCode": "class ForwardTTS(BaseTTS):\n    \"\"\"General forward TTS model implementation that uses an encoder-decoder architecture with an optional alignment\n    network and a pitch predictor.\n    If the alignment network is used, the model learns the text-to-speech alignment\n    from the data instead of using pre-computed durations.\n    If the pitch predictor is used, the model trains a pitch predictor that predicts average pitch value for each\n    input character as in the FastPitch model.\n    `ForwardTTS` can be configured to one of these architectures,\n        - FastPitch\n        - SpeedySpeech",
        "detail": "env.src.tts.TTS.tts.models.forward_tts",
        "documentation": {}
    },
    {
        "label": "GlowTTS",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.models.glow_tts",
        "description": "env.src.tts.TTS.tts.models.glow_tts",
        "peekOfCode": "class GlowTTS(BaseTTS):\n    \"\"\"GlowTTS model.\n    Paper::\n        https://arxiv.org/abs/2005.11129\n    Paper abstract::\n        Recently, text-to-speech (TTS) models such as FastSpeech and ParaNet have been proposed to generate\n        mel-spectrograms from text in parallel. Despite the advantage, the parallel TTS models cannot be trained\n        without guidance from autoregressive TTS models as their external aligners. In this work, we propose Glow-TTS,\n        a flow-based generative model for parallel TTS that does not require any external aligner. By combining the\n        properties of flows and dynamic programming, the proposed model searches for the most probable monotonic",
        "detail": "env.src.tts.TTS.tts.models.glow_tts",
        "documentation": {}
    },
    {
        "label": "NeuralhmmTTS",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.models.neuralhmm_tts",
        "description": "env.src.tts.TTS.tts.models.neuralhmm_tts",
        "peekOfCode": "class NeuralhmmTTS(BaseTTS):\n    \"\"\"Neural HMM TTS model.\n    Paper::\n        https://arxiv.org/abs/2108.13320\n    Paper abstract::\n        Neural sequence-to-sequence TTS has achieved significantly better output quality\n    than statistical speech synthesis using HMMs.However, neural TTS is generally not probabilistic\n    and uses non-monotonic attention. Attention failures increase training time and can make\n    synthesis babble incoherently. This paper describes how the old and new paradigms can be\n    combined to obtain the advantages of both worlds, by replacing attention in neural TTS with",
        "detail": "env.src.tts.TTS.tts.models.neuralhmm_tts",
        "documentation": {}
    },
    {
        "label": "NLLLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.models.neuralhmm_tts",
        "description": "env.src.tts.TTS.tts.models.neuralhmm_tts",
        "peekOfCode": "class NLLLoss(nn.Module):\n    \"\"\"Negative log likelihood loss.\"\"\"\n    def forward(self, log_prob: torch.Tensor) -> dict:  # pylint: disable=no-self-use\n        \"\"\"Compute the loss.\n        Args:\n            logits (Tensor): [B, T, D]\n        Returns:\n            Tensor: [1]\n        \"\"\"\n        return_dict = {}",
        "detail": "env.src.tts.TTS.tts.models.neuralhmm_tts",
        "documentation": {}
    },
    {
        "label": "Overflow",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.models.overflow",
        "description": "env.src.tts.TTS.tts.models.overflow",
        "peekOfCode": "class Overflow(BaseTTS):\n    \"\"\"OverFlow TTS model.\n    Paper::\n        https://arxiv.org/abs/2211.06892\n    Paper abstract::\n        Neural HMMs are a type of neural transducer recently proposed for\n    sequence-to-sequence modelling in text-to-speech. They combine the best features\n    of classic statistical speech synthesis and modern neural TTS, requiring less\n    data and fewer training updates, and are less prone to gibberish output caused\n    by neural attention failures. In this paper, we combine neural HMM TTS with",
        "detail": "env.src.tts.TTS.tts.models.overflow",
        "documentation": {}
    },
    {
        "label": "NLLLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.models.overflow",
        "description": "env.src.tts.TTS.tts.models.overflow",
        "peekOfCode": "class NLLLoss(nn.Module):\n    \"\"\"Negative log likelihood loss.\"\"\"\n    def forward(self, log_prob: torch.Tensor) -> dict:  # pylint: disable=no-self-use\n        \"\"\"Compute the loss.\n        Args:\n            logits (Tensor): [B, T, D]\n        Returns:\n            Tensor: [1]\n        \"\"\"\n        return_dict = {}",
        "detail": "env.src.tts.TTS.tts.models.overflow",
        "documentation": {}
    },
    {
        "label": "Tacotron",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.models.tacotron",
        "description": "env.src.tts.TTS.tts.models.tacotron",
        "peekOfCode": "class Tacotron(BaseTacotron):\n    \"\"\"Tacotron as in https://arxiv.org/abs/1703.10135\n    It's an autoregressive encoder-attention-decoder-postnet architecture.\n    Check `TacotronConfig` for the arguments.\n    Args:\n        config (TacotronConfig): Configuration for the Tacotron model.\n        speaker_manager (SpeakerManager): Speaker manager to handle multi-speaker settings. Only use if the model is\n            a multi-speaker model. Defaults to None.\n    \"\"\"\n    def __init__(",
        "detail": "env.src.tts.TTS.tts.models.tacotron",
        "documentation": {}
    },
    {
        "label": "Tacotron2",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.models.tacotron2",
        "description": "env.src.tts.TTS.tts.models.tacotron2",
        "peekOfCode": "class Tacotron2(BaseTacotron):\n    \"\"\"Tacotron2 model implementation inherited from :class:`TTS.tts.models.base_tacotron.BaseTacotron`.\n    Paper::\n        https://arxiv.org/abs/1712.05884\n    Paper abstract::\n        This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text.\n        The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character\n        embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize\n        timedomain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable\n        to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation",
        "detail": "env.src.tts.TTS.tts.models.tacotron2",
        "documentation": {}
    },
    {
        "label": "TortoiseAudioConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.models.tortoise",
        "description": "env.src.tts.TTS.tts.models.tortoise",
        "peekOfCode": "class TortoiseAudioConfig(Coqpit):\n    sample_rate: int = 22050\n    diffusion_sample_rate: int = 24000\n    output_sample_rate: int = 24000\n@dataclass\nclass TortoiseArgs(Coqpit):\n    \"\"\"A dataclass to represent Tortoise model arguments that define the model structure.\n    Args:\n        autoregressive_batch_size (int): The size of the auto-regressive batch.\n        enable_redaction (bool, optional): Whether to enable redaction. Defaults to True.",
        "detail": "env.src.tts.TTS.tts.models.tortoise",
        "documentation": {}
    },
    {
        "label": "TortoiseArgs",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.models.tortoise",
        "description": "env.src.tts.TTS.tts.models.tortoise",
        "peekOfCode": "class TortoiseArgs(Coqpit):\n    \"\"\"A dataclass to represent Tortoise model arguments that define the model structure.\n    Args:\n        autoregressive_batch_size (int): The size of the auto-regressive batch.\n        enable_redaction (bool, optional): Whether to enable redaction. Defaults to True.\n        high_vram (bool, optional): Whether to use high VRAM. Defaults to False.\n        kv_cache (bool, optional): Whether to use the kv_cache. Defaults to True.\n        ar_checkpoint (str, optional): The checkpoint for the autoregressive model. Defaults to None.\n        clvp_checkpoint (str, optional): The checkpoint for the ConditionalLatentVariablePerseq model. Defaults to None.\n        diff_checkpoint (str, optional): The checkpoint for the DiffTTS model. Defaults to None.",
        "detail": "env.src.tts.TTS.tts.models.tortoise",
        "documentation": {}
    },
    {
        "label": "Tortoise",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.models.tortoise",
        "description": "env.src.tts.TTS.tts.models.tortoise",
        "peekOfCode": "class Tortoise(BaseTTS):\n    \"\"\"Tortoise model class.\n    Currently only supports inference.\n    Examples:\n        >>> from TTS.tts.configs.tortoise_config import TortoiseConfig\n        >>> from TTS.tts.models.tortoise import Tortoise\n        >>> config = TortoiseConfig()\n        >>> model = Tortoise.inif_from_config(config)\n        >>> model.load_checkpoint(config, checkpoint_dir=\"paths/to/models_dir/\", eval=True)\n    \"\"\"",
        "detail": "env.src.tts.TTS.tts.models.tortoise",
        "documentation": {}
    },
    {
        "label": "pad_or_truncate",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.models.tortoise",
        "description": "env.src.tts.TTS.tts.models.tortoise",
        "peekOfCode": "def pad_or_truncate(t, length):\n    \"\"\"\n    Utility function for forcing <t> to have the specified sequence length, whether by clipping it or padding it with 0s.\n    \"\"\"\n    tp = t[..., :length]\n    if t.shape[-1] == length:\n        tp = t\n    elif t.shape[-1] < length:\n        tp = F.pad(t, (0, length - t.shape[-1]))\n    return tp",
        "detail": "env.src.tts.TTS.tts.models.tortoise",
        "documentation": {}
    },
    {
        "label": "deterministic_state",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.models.tortoise",
        "description": "env.src.tts.TTS.tts.models.tortoise",
        "peekOfCode": "def deterministic_state(seed=None):\n    \"\"\"\n    Sets the random seeds that tortoise uses to the current time() and returns that seed so results can be\n    reproduced.\n    \"\"\"\n    seed = int(time()) if seed is None else seed\n    torch.manual_seed(seed)\n    random.seed(seed)\n    # Can't currently set this because of CUBLAS. TODO: potentially enable it if necessary.\n    # torch.use_deterministic_algorithms(True)",
        "detail": "env.src.tts.TTS.tts.models.tortoise",
        "documentation": {}
    },
    {
        "label": "load_discrete_vocoder_diffuser",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.models.tortoise",
        "description": "env.src.tts.TTS.tts.models.tortoise",
        "peekOfCode": "def load_discrete_vocoder_diffuser(\n    trained_diffusion_steps=4000,\n    desired_diffusion_steps=200,\n    cond_free=True,\n    cond_free_k=1,\n    sampler=\"ddim\",\n):\n    \"\"\"\n    Helper function to load a GaussianDiffusion instance configured for use as a vocoder.\n    \"\"\"",
        "detail": "env.src.tts.TTS.tts.models.tortoise",
        "documentation": {}
    },
    {
        "label": "format_conditioning",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.models.tortoise",
        "description": "env.src.tts.TTS.tts.models.tortoise",
        "peekOfCode": "def format_conditioning(clip, cond_length=132300, device=\"cuda\"):\n    \"\"\"\n    Converts the given conditioning signal to a MEL spectrogram and clips it as expected by the models.\n    \"\"\"\n    gap = clip.shape[-1] - cond_length\n    if gap < 0:\n        clip = F.pad(clip, pad=(0, abs(gap)))\n    elif gap > 0:\n        rand_start = random.randint(0, gap)\n        clip = clip[:, rand_start : rand_start + cond_length]",
        "detail": "env.src.tts.TTS.tts.models.tortoise",
        "documentation": {}
    },
    {
        "label": "fix_autoregressive_output",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.models.tortoise",
        "description": "env.src.tts.TTS.tts.models.tortoise",
        "peekOfCode": "def fix_autoregressive_output(codes, stop_token, complain=True):\n    \"\"\"\n    This function performs some padding on coded audio that fixes a mismatch issue between what the diffusion model was\n    trained on and what the autoregressive code generator creates (which has no padding or end).\n    This is highly specific to the DVAE being used, so this particular coding will not necessarily work if used with\n    a different DVAE. This can be inferred by feeding a audio clip padded with lots of zeros on the end through the DVAE\n    and copying out the last few codes.\n    Failing to do this padding will produce speech with a harsh end that sounds like \"BLAH\" or similar.\n    \"\"\"\n    # Strip off the autoregressive stop token and add padding.",
        "detail": "env.src.tts.TTS.tts.models.tortoise",
        "documentation": {}
    },
    {
        "label": "do_spectrogram_diffusion",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.models.tortoise",
        "description": "env.src.tts.TTS.tts.models.tortoise",
        "peekOfCode": "def do_spectrogram_diffusion(\n    diffusion_model,\n    diffuser,\n    latents,\n    conditioning_latents,\n    temperature=1,\n    verbose=True,\n):\n    \"\"\"\n    Uses the specified diffusion model to convert discrete codes into a spectrogram.",
        "detail": "env.src.tts.TTS.tts.models.tortoise",
        "documentation": {}
    },
    {
        "label": "classify_audio_clip",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.models.tortoise",
        "description": "env.src.tts.TTS.tts.models.tortoise",
        "peekOfCode": "def classify_audio_clip(clip, model_dir):\n    \"\"\"\n    Returns whether or not Tortoises' classifier thinks the given clip came from Tortoise.\n    :param clip: torch tensor containing audio waveform data (get it from load_audio)\n    :return: True if the clip was classified as coming from Tortoise and false if it was classified as real.\n    \"\"\"\n    classifier = AudioMiniEncoderWithClassifierHead(\n        2,\n        spec_dim=1,\n        embedding_dim=512,",
        "detail": "env.src.tts.TTS.tts.models.tortoise",
        "documentation": {}
    },
    {
        "label": "pick_best_batch_size_for_gpu",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.models.tortoise",
        "description": "env.src.tts.TTS.tts.models.tortoise",
        "peekOfCode": "def pick_best_batch_size_for_gpu():\n    \"\"\"\n    Tries to pick a batch size that will fit in your GPU. These sizes aren't guaranteed to work, but they should give\n    you a good shot.\n    \"\"\"\n    if torch.cuda.is_available():\n        _, available = torch.cuda.mem_get_info()\n        availableGb = available / (1024**3)\n        batch_size = 1\n        if availableGb > 14:",
        "detail": "env.src.tts.TTS.tts.models.tortoise",
        "documentation": {}
    },
    {
        "label": "VitsAudioConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.models.vits",
        "description": "env.src.tts.TTS.tts.models.vits",
        "peekOfCode": "class VitsAudioConfig(Coqpit):\n    fft_size: int = 1024\n    sample_rate: int = 22050\n    win_length: int = 1024\n    hop_length: int = 256\n    num_mels: int = 80\n    mel_fmin: int = 0\n    mel_fmax: int = None\n##############################\n# DATASET",
        "detail": "env.src.tts.TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "VitsDataset",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.models.vits",
        "description": "env.src.tts.TTS.tts.models.vits",
        "peekOfCode": "class VitsDataset(TTSDataset):\n    def __init__(self, model_args, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.pad_id = self.tokenizer.characters.pad_id\n        self.model_args = model_args\n    def __getitem__(self, idx):\n        item = self.samples[idx]\n        raw_text = item[\"text\"]\n        wav, _ = load_audio(item[\"audio_file\"])\n        if self.model_args.encoder_sample_rate is not None:",
        "detail": "env.src.tts.TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "VitsArgs",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.models.vits",
        "description": "env.src.tts.TTS.tts.models.vits",
        "peekOfCode": "class VitsArgs(Coqpit):\n    \"\"\"VITS model arguments.\n    Args:\n        num_chars (int):\n            Number of characters in the vocabulary. Defaults to 100.\n        out_channels (int):\n            Number of output channels of the decoder. Defaults to 513.\n        spec_segment_size (int):\n            Decoder input segment size. Defaults to 32 `(32 * hoplength = waveform length)`.\n        hidden_channels (int):",
        "detail": "env.src.tts.TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "Vits",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.models.vits",
        "description": "env.src.tts.TTS.tts.models.vits",
        "peekOfCode": "class Vits(BaseTTS):\n    \"\"\"VITS TTS model\n    Paper::\n        https://arxiv.org/pdf/2106.06103.pdf\n    Paper Abstract::\n        Several recent end-to-end text-to-speech (TTS) models enabling single-stage training and parallel\n        sampling have been proposed, but their sample quality does not match that of two-stage TTS systems.\n        In this work, we present a parallel endto-end TTS method that generates more natural sounding audio than\n        current two-stage models. Our method adopts variational inference augmented with normalizing flows and\n        an adversarial training process, which improves the expressive power of generative modeling. We also propose a",
        "detail": "env.src.tts.TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "VitsCharacters",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.models.vits",
        "description": "env.src.tts.TTS.tts.models.vits",
        "peekOfCode": "class VitsCharacters(BaseCharacters):\n    \"\"\"Characters class for VITs model for compatibility with pre-trained models\"\"\"\n    def __init__(\n        self,\n        graphemes: str = _characters,\n        punctuations: str = _punctuations,\n        pad: str = _pad,\n        ipa_characters: str = _phonemes,\n    ) -> None:\n        if ipa_characters is not None:",
        "detail": "env.src.tts.TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "FairseqVocab",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.models.vits",
        "description": "env.src.tts.TTS.tts.models.vits",
        "peekOfCode": "class FairseqVocab(BaseVocabulary):\n    def __init__(self, vocab: str):\n        super(FairseqVocab).__init__()\n        self.vocab = vocab\n    @property\n    def vocab(self):\n        \"\"\"Return the vocabulary dictionary.\"\"\"\n        return self._vocab\n    @vocab.setter\n    def vocab(self, vocab_file):",
        "detail": "env.src.tts.TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "weights_reset",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.models.vits",
        "description": "env.src.tts.TTS.tts.models.vits",
        "peekOfCode": "def weights_reset(m: nn.Module):\n    # check if the current module has reset_parameters and if it is reset the weight\n    reset_parameters = getattr(m, \"reset_parameters\", None)\n    if callable(reset_parameters):\n        m.reset_parameters()\ndef get_module_weights_sum(mdl: nn.Module):\n    dict_sums = {}\n    for name, w in mdl.named_parameters():\n        if \"weight\" in name:\n            value = w.data.sum().item()",
        "detail": "env.src.tts.TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "get_module_weights_sum",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.models.vits",
        "description": "env.src.tts.TTS.tts.models.vits",
        "peekOfCode": "def get_module_weights_sum(mdl: nn.Module):\n    dict_sums = {}\n    for name, w in mdl.named_parameters():\n        if \"weight\" in name:\n            value = w.data.sum().item()\n            dict_sums[name] = value\n    return dict_sums\ndef load_audio(file_path):\n    \"\"\"Load the audio file normalized in [-1, 1]\n    Return Shapes:",
        "detail": "env.src.tts.TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "load_audio",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.models.vits",
        "description": "env.src.tts.TTS.tts.models.vits",
        "peekOfCode": "def load_audio(file_path):\n    \"\"\"Load the audio file normalized in [-1, 1]\n    Return Shapes:\n        - x: :math:`[1, T]`\n    \"\"\"\n    x, sr = torchaudio.load(file_path)\n    assert (x > 1).sum() + (x < -1).sum() == 0\n    return x, sr\ndef _amp_to_db(x, C=1, clip_val=1e-5):\n    return torch.log(torch.clamp(x, min=clip_val) * C)",
        "detail": "env.src.tts.TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "amp_to_db",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.models.vits",
        "description": "env.src.tts.TTS.tts.models.vits",
        "peekOfCode": "def amp_to_db(magnitudes):\n    output = _amp_to_db(magnitudes)\n    return output\ndef db_to_amp(magnitudes):\n    output = _db_to_amp(magnitudes)\n    return output\ndef wav_to_spec(y, n_fft, hop_length, win_length, center=False):\n    \"\"\"\n    Args Shapes:\n        - y : :math:`[B, 1, T]`",
        "detail": "env.src.tts.TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "db_to_amp",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.models.vits",
        "description": "env.src.tts.TTS.tts.models.vits",
        "peekOfCode": "def db_to_amp(magnitudes):\n    output = _db_to_amp(magnitudes)\n    return output\ndef wav_to_spec(y, n_fft, hop_length, win_length, center=False):\n    \"\"\"\n    Args Shapes:\n        - y : :math:`[B, 1, T]`\n    Return Shapes:\n        - spec : :math:`[B,C,T]`\n    \"\"\"",
        "detail": "env.src.tts.TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "wav_to_spec",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.models.vits",
        "description": "env.src.tts.TTS.tts.models.vits",
        "peekOfCode": "def wav_to_spec(y, n_fft, hop_length, win_length, center=False):\n    \"\"\"\n    Args Shapes:\n        - y : :math:`[B, 1, T]`\n    Return Shapes:\n        - spec : :math:`[B,C,T]`\n    \"\"\"\n    y = y.squeeze(1)\n    if torch.min(y) < -1.0:\n        print(\"min value is \", torch.min(y))",
        "detail": "env.src.tts.TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "spec_to_mel",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.models.vits",
        "description": "env.src.tts.TTS.tts.models.vits",
        "peekOfCode": "def spec_to_mel(spec, n_fft, num_mels, sample_rate, fmin, fmax):\n    \"\"\"\n    Args Shapes:\n        - spec : :math:`[B,C,T]`\n    Return Shapes:\n        - mel : :math:`[B,C,T]`\n    \"\"\"\n    global mel_basis\n    dtype_device = str(spec.dtype) + \"_\" + str(spec.device)\n    fmax_dtype_device = str(fmax) + \"_\" + dtype_device",
        "detail": "env.src.tts.TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "wav_to_mel",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.models.vits",
        "description": "env.src.tts.TTS.tts.models.vits",
        "peekOfCode": "def wav_to_mel(y, n_fft, num_mels, sample_rate, hop_length, win_length, fmin, fmax, center=False):\n    \"\"\"\n    Args Shapes:\n        - y : :math:`[B, 1, T]`\n    Return Shapes:\n        - spec : :math:`[B,C,T]`\n    \"\"\"\n    y = y.squeeze(1)\n    if torch.min(y) < -1.0:\n        print(\"min value is \", torch.min(y))",
        "detail": "env.src.tts.TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "get_attribute_balancer_weights",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.models.vits",
        "description": "env.src.tts.TTS.tts.models.vits",
        "peekOfCode": "def get_attribute_balancer_weights(items: list, attr_name: str, multi_dict: dict = None):\n    \"\"\"Create inverse frequency weights for balancing the dataset.\n    Use `multi_dict` to scale relative weights.\"\"\"\n    attr_names_samples = np.array([item[attr_name] for item in items])\n    unique_attr_names = np.unique(attr_names_samples).tolist()\n    attr_idx = [unique_attr_names.index(l) for l in attr_names_samples]\n    attr_count = np.array([len(np.where(attr_names_samples == l)[0]) for l in unique_attr_names])\n    weight_attr = 1.0 / attr_count\n    dataset_samples_weight = np.array([weight_attr[l] for l in attr_idx])\n    dataset_samples_weight = dataset_samples_weight / np.linalg.norm(dataset_samples_weight)",
        "detail": "env.src.tts.TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "hann_window",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.models.vits",
        "description": "env.src.tts.TTS.tts.models.vits",
        "peekOfCode": "hann_window = {}\nmel_basis = {}\n@torch.no_grad()\ndef weights_reset(m: nn.Module):\n    # check if the current module has reset_parameters and if it is reset the weight\n    reset_parameters = getattr(m, \"reset_parameters\", None)\n    if callable(reset_parameters):\n        m.reset_parameters()\ndef get_module_weights_sum(mdl: nn.Module):\n    dict_sums = {}",
        "detail": "env.src.tts.TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "mel_basis",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.models.vits",
        "description": "env.src.tts.TTS.tts.models.vits",
        "peekOfCode": "mel_basis = {}\n@torch.no_grad()\ndef weights_reset(m: nn.Module):\n    # check if the current module has reset_parameters and if it is reset the weight\n    reset_parameters = getattr(m, \"reset_parameters\", None)\n    if callable(reset_parameters):\n        m.reset_parameters()\ndef get_module_weights_sum(mdl: nn.Module):\n    dict_sums = {}\n    for name, w in mdl.named_parameters():",
        "detail": "env.src.tts.TTS.tts.models.vits",
        "documentation": {}
    },
    {
        "label": "tag_text",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.bangla.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.bangla.phonemizer",
        "peekOfCode": "def tag_text(text: str):\n    # remove multiple spaces\n    text = re.sub(\" +\", \" \", text)\n    # create start and end\n    text = \"start\" + text + \"end\"\n    # tag text\n    parts = re.split(\"[\\u0600-\\u06FF]+\", text)\n    # remove non chars\n    parts = [p for p in parts if p.strip()]\n    # unique parts",
        "detail": "env.src.tts.TTS.tts.utils.text.bangla.phonemizer",
        "documentation": {}
    },
    {
        "label": "normalize",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.bangla.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.bangla.phonemizer",
        "peekOfCode": "def normalize(sen):\n    global bnorm  # pylint: disable=global-statement\n    _words = [bnorm(word)[\"normalized\"] for word in sen.split()]\n    return \" \".join([word for word in _words if word is not None])\ndef expand_full_attribution(text):\n    for word, attr in attribution_dict.items():\n        if word in text:\n            text = text.replace(word, normalize(attr))\n    return text\ndef collapse_whitespace(text):",
        "detail": "env.src.tts.TTS.tts.utils.text.bangla.phonemizer",
        "documentation": {}
    },
    {
        "label": "expand_full_attribution",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.bangla.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.bangla.phonemizer",
        "peekOfCode": "def expand_full_attribution(text):\n    for word, attr in attribution_dict.items():\n        if word in text:\n            text = text.replace(word, normalize(attr))\n    return text\ndef collapse_whitespace(text):\n    # Regular expression matching whitespace:\n    _whitespace_re = re.compile(r\"\\s+\")\n    return re.sub(_whitespace_re, \" \", text)\ndef bangla_text_to_phonemes(text: str) -> str:",
        "detail": "env.src.tts.TTS.tts.utils.text.bangla.phonemizer",
        "documentation": {}
    },
    {
        "label": "collapse_whitespace",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.bangla.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.bangla.phonemizer",
        "peekOfCode": "def collapse_whitespace(text):\n    # Regular expression matching whitespace:\n    _whitespace_re = re.compile(r\"\\s+\")\n    return re.sub(_whitespace_re, \" \", text)\ndef bangla_text_to_phonemes(text: str) -> str:\n    # english numbers to bangla conversion\n    res = re.search(\"[0-9]\", text)\n    if res is not None:\n        text = bangla.convert_english_digit_to_bangla_digit(text)\n    # replace ':' in between two bangla numbers with '  '",
        "detail": "env.src.tts.TTS.tts.utils.text.bangla.phonemizer",
        "documentation": {}
    },
    {
        "label": "bangla_text_to_phonemes",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.bangla.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.bangla.phonemizer",
        "peekOfCode": "def bangla_text_to_phonemes(text: str) -> str:\n    # english numbers to bangla conversion\n    res = re.search(\"[0-9]\", text)\n    if res is not None:\n        text = bangla.convert_english_digit_to_bangla_digit(text)\n    # replace ':' in between two bangla numbers with '  '\n    pattern = r\"[, , , , , , , , , ]:[, , , , , , , , , ]\"\n    matches = re.findall(pattern, text)\n    for m in matches:\n        r = m.replace(\":\", \"  \")",
        "detail": "env.src.tts.TTS.tts.utils.text.bangla.phonemizer",
        "documentation": {}
    },
    {
        "label": "bnorm",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.bangla.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.bangla.phonemizer",
        "peekOfCode": "bnorm = Normalizer()\nattribution_dict = {\n    \"\": \"   \",\n    \"\": \" \",\n    \"\": \" \",\n    \"\": \" \",\n    \"\": \"\",\n    \"\": \"\",\n    \"\": \"\",\n    \"\": \" , \",",
        "detail": "env.src.tts.TTS.tts.utils.text.bangla.phonemizer",
        "documentation": {}
    },
    {
        "label": "attribution_dict",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.bangla.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.bangla.phonemizer",
        "peekOfCode": "attribution_dict = {\n    \"\": \"   \",\n    \"\": \" \",\n    \"\": \" \",\n    \"\": \" \",\n    \"\": \"\",\n    \"\": \"\",\n    \"\": \"\",\n    \"\": \" , \",\n    # \"\" : \"\",#",
        "detail": "env.src.tts.TTS.tts.utils.text.bangla.phonemizer",
        "documentation": {}
    },
    {
        "label": "replace_numbers_to_characters_in_text",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.chinese_mandarin.numbers",
        "description": "env.src.tts.TTS.tts.utils.text.chinese_mandarin.numbers",
        "peekOfCode": "def replace_numbers_to_characters_in_text(text: str) -> str:\n    \"\"\"Replace all arabic numbers in a text by their equivalent in chinese characters (simplified)\n    Args:\n        text (str): input text to transform\n    Returns:\n        str: output text\n    \"\"\"\n    text = re.sub(r\"[0-9]+\", _number_replace, text)\n    return text",
        "detail": "env.src.tts.TTS.tts.utils.text.chinese_mandarin.numbers",
        "documentation": {}
    },
    {
        "label": "chinese_text_to_phonemes",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.chinese_mandarin.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.chinese_mandarin.phonemizer",
        "peekOfCode": "def chinese_text_to_phonemes(text: str, seperator: str = \"|\") -> str:\n    tokenized_text = jieba.cut(text, HMM=False)\n    tokenized_text = \" \".join(tokenized_text)\n    pinyined_text: List[str] = _chinese_character_to_pinyin(tokenized_text)\n    results: List[str] = []\n    for token in pinyined_text:\n        if token[-1] in \"12345\":  # TODO transform to is_pinyin()\n            pinyin_phonemes = _chinese_pinyin_to_phoneme(token)\n            results += list(pinyin_phonemes)\n        else:  # is ponctuation or other",
        "detail": "env.src.tts.TTS.tts.utils.text.chinese_mandarin.phonemizer",
        "documentation": {}
    },
    {
        "label": "PINYIN_DICT",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.chinese_mandarin.pinyinToPhonemes",
        "description": "env.src.tts.TTS.tts.utils.text.chinese_mandarin.pinyinToPhonemes",
        "peekOfCode": "PINYIN_DICT = {\n    \"a\": [\"a\"],\n    \"ai\": [\"ai\"],\n    \"an\": [\"an\"],\n    \"ang\": [\"\"],\n    \"ao\": [\"a\"],\n    \"ba\": [\"ba\"],\n    \"bai\": [\"bai\"],\n    \"ban\": [\"ban\"],\n    \"bang\": [\"b\"],",
        "detail": "env.src.tts.TTS.tts.utils.text.chinese_mandarin.pinyinToPhonemes",
        "documentation": {}
    },
    {
        "label": "abbreviations_en",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.english.abbreviations",
        "description": "env.src.tts.TTS.tts.utils.text.english.abbreviations",
        "peekOfCode": "abbreviations_en = [\n    (re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1])\n    for x in [\n        (\"mrs\", \"misess\"),\n        (\"mr\", \"mister\"),\n        (\"dr\", \"doctor\"),\n        (\"st\", \"saint\"),\n        (\"co\", \"company\"),\n        (\"jr\", \"junior\"),\n        (\"maj\", \"major\"),",
        "detail": "env.src.tts.TTS.tts.utils.text.english.abbreviations",
        "documentation": {}
    },
    {
        "label": "normalize_numbers",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.english.number_norm",
        "description": "env.src.tts.TTS.tts.utils.text.english.number_norm",
        "peekOfCode": "def normalize_numbers(text):\n    text = re.sub(_comma_number_re, _remove_commas, text)\n    text = re.sub(_currency_re, _expand_currency, text)\n    text = re.sub(_decimal_number_re, _expand_decimal_point, text)\n    text = re.sub(_ordinal_re, _expand_ordinal, text)\n    text = re.sub(_number_re, _expand_number, text)\n    return text",
        "detail": "env.src.tts.TTS.tts.utils.text.english.number_norm",
        "documentation": {}
    },
    {
        "label": "_inflect",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.english.number_norm",
        "description": "env.src.tts.TTS.tts.utils.text.english.number_norm",
        "peekOfCode": "_inflect = inflect.engine()\n_comma_number_re = re.compile(r\"([0-9][0-9\\,]+[0-9])\")\n_decimal_number_re = re.compile(r\"([0-9]+\\.[0-9]+)\")\n_currency_re = re.compile(r\"(|\\$|)([0-9\\,\\.]*[0-9]+)\")\n_ordinal_re = re.compile(r\"[0-9]+(st|nd|rd|th)\")\n_number_re = re.compile(r\"-?[0-9]+\")\ndef _remove_commas(m):\n    return m.group(1).replace(\",\", \"\")\ndef _expand_decimal_point(m):\n    return m.group(1).replace(\".\", \" point \")",
        "detail": "env.src.tts.TTS.tts.utils.text.english.number_norm",
        "documentation": {}
    },
    {
        "label": "_comma_number_re",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.english.number_norm",
        "description": "env.src.tts.TTS.tts.utils.text.english.number_norm",
        "peekOfCode": "_comma_number_re = re.compile(r\"([0-9][0-9\\,]+[0-9])\")\n_decimal_number_re = re.compile(r\"([0-9]+\\.[0-9]+)\")\n_currency_re = re.compile(r\"(|\\$|)([0-9\\,\\.]*[0-9]+)\")\n_ordinal_re = re.compile(r\"[0-9]+(st|nd|rd|th)\")\n_number_re = re.compile(r\"-?[0-9]+\")\ndef _remove_commas(m):\n    return m.group(1).replace(\",\", \"\")\ndef _expand_decimal_point(m):\n    return m.group(1).replace(\".\", \" point \")\ndef __expand_currency(value: str, inflection: Dict[float, str]) -> str:",
        "detail": "env.src.tts.TTS.tts.utils.text.english.number_norm",
        "documentation": {}
    },
    {
        "label": "_decimal_number_re",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.english.number_norm",
        "description": "env.src.tts.TTS.tts.utils.text.english.number_norm",
        "peekOfCode": "_decimal_number_re = re.compile(r\"([0-9]+\\.[0-9]+)\")\n_currency_re = re.compile(r\"(|\\$|)([0-9\\,\\.]*[0-9]+)\")\n_ordinal_re = re.compile(r\"[0-9]+(st|nd|rd|th)\")\n_number_re = re.compile(r\"-?[0-9]+\")\ndef _remove_commas(m):\n    return m.group(1).replace(\",\", \"\")\ndef _expand_decimal_point(m):\n    return m.group(1).replace(\".\", \" point \")\ndef __expand_currency(value: str, inflection: Dict[float, str]) -> str:\n    parts = value.replace(\",\", \"\").split(\".\")",
        "detail": "env.src.tts.TTS.tts.utils.text.english.number_norm",
        "documentation": {}
    },
    {
        "label": "_currency_re",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.english.number_norm",
        "description": "env.src.tts.TTS.tts.utils.text.english.number_norm",
        "peekOfCode": "_currency_re = re.compile(r\"(|\\$|)([0-9\\,\\.]*[0-9]+)\")\n_ordinal_re = re.compile(r\"[0-9]+(st|nd|rd|th)\")\n_number_re = re.compile(r\"-?[0-9]+\")\ndef _remove_commas(m):\n    return m.group(1).replace(\",\", \"\")\ndef _expand_decimal_point(m):\n    return m.group(1).replace(\".\", \" point \")\ndef __expand_currency(value: str, inflection: Dict[float, str]) -> str:\n    parts = value.replace(\",\", \"\").split(\".\")\n    if len(parts) > 2:",
        "detail": "env.src.tts.TTS.tts.utils.text.english.number_norm",
        "documentation": {}
    },
    {
        "label": "_ordinal_re",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.english.number_norm",
        "description": "env.src.tts.TTS.tts.utils.text.english.number_norm",
        "peekOfCode": "_ordinal_re = re.compile(r\"[0-9]+(st|nd|rd|th)\")\n_number_re = re.compile(r\"-?[0-9]+\")\ndef _remove_commas(m):\n    return m.group(1).replace(\",\", \"\")\ndef _expand_decimal_point(m):\n    return m.group(1).replace(\".\", \" point \")\ndef __expand_currency(value: str, inflection: Dict[float, str]) -> str:\n    parts = value.replace(\",\", \"\").split(\".\")\n    if len(parts) > 2:\n        return f\"{value} {inflection[2]}\"  # Unexpected format",
        "detail": "env.src.tts.TTS.tts.utils.text.english.number_norm",
        "documentation": {}
    },
    {
        "label": "_number_re",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.english.number_norm",
        "description": "env.src.tts.TTS.tts.utils.text.english.number_norm",
        "peekOfCode": "_number_re = re.compile(r\"-?[0-9]+\")\ndef _remove_commas(m):\n    return m.group(1).replace(\",\", \"\")\ndef _expand_decimal_point(m):\n    return m.group(1).replace(\".\", \" point \")\ndef __expand_currency(value: str, inflection: Dict[float, str]) -> str:\n    parts = value.replace(\",\", \"\").split(\".\")\n    if len(parts) > 2:\n        return f\"{value} {inflection[2]}\"  # Unexpected format\n    text = []",
        "detail": "env.src.tts.TTS.tts.utils.text.english.number_norm",
        "documentation": {}
    },
    {
        "label": "expand_time_english",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.english.time_norm",
        "description": "env.src.tts.TTS.tts.utils.text.english.time_norm",
        "peekOfCode": "def expand_time_english(text: str) -> str:\n    return re.sub(_time_re, _expand_time_english, text)",
        "detail": "env.src.tts.TTS.tts.utils.text.english.time_norm",
        "documentation": {}
    },
    {
        "label": "_inflect",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.english.time_norm",
        "description": "env.src.tts.TTS.tts.utils.text.english.time_norm",
        "peekOfCode": "_inflect = inflect.engine()\n_time_re = re.compile(\n    r\"\"\"\\b\n                          ((0?[0-9])|(1[0-1])|(1[2-9])|(2[0-3]))  # hours\n                          :\n                          ([0-5][0-9])                            # minutes\n                          \\s*(a\\\\.m\\\\.|am|pm|p\\\\.m\\\\.|a\\\\.m|p\\\\.m)? # am/pm\n                          \\b\"\"\",\n    re.IGNORECASE | re.X,\n)",
        "detail": "env.src.tts.TTS.tts.utils.text.english.time_norm",
        "documentation": {}
    },
    {
        "label": "_time_re",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.english.time_norm",
        "description": "env.src.tts.TTS.tts.utils.text.english.time_norm",
        "peekOfCode": "_time_re = re.compile(\n    r\"\"\"\\b\n                          ((0?[0-9])|(1[0-1])|(1[2-9])|(2[0-3]))  # hours\n                          :\n                          ([0-5][0-9])                            # minutes\n                          \\s*(a\\\\.m\\\\.|am|pm|p\\\\.m\\\\.|a\\\\.m|p\\\\.m)? # am/pm\n                          \\b\"\"\",\n    re.IGNORECASE | re.X,\n)\ndef _expand_num(n: int) -> str:",
        "detail": "env.src.tts.TTS.tts.utils.text.english.time_norm",
        "documentation": {}
    },
    {
        "label": "abbreviations_fr",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.french.abbreviations",
        "description": "env.src.tts.TTS.tts.utils.text.french.abbreviations",
        "peekOfCode": "abbreviations_fr = [\n    (re.compile(\"\\\\b%s\\\\.\" % x[0], re.IGNORECASE), x[1])\n    for x in [\n        (\"M\", \"monsieur\"),\n        (\"Mlle\", \"mademoiselle\"),\n        (\"Mlles\", \"mesdemoiselles\"),\n        (\"Mme\", \"Madame\"),\n        (\"Mmes\", \"Mesdames\"),\n        (\"N.B\", \"nota bene\"),\n        (\"M\", \"monsieur\"),",
        "detail": "env.src.tts.TTS.tts.utils.text.french.abbreviations",
        "documentation": {}
    },
    {
        "label": "kata2phoneme",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "peekOfCode": "def kata2phoneme(text: str) -> str:\n    \"\"\"Convert katakana text to phonemes.\"\"\"\n    text = text.strip()\n    res = \"\"\n    while text:\n        if len(text) >= 2:\n            x = _RULEMAP2.get(text[:2])\n            if x is not None:\n                text = text[2:]\n                res += x",
        "detail": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "documentation": {}
    },
    {
        "label": "hira2kata",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "peekOfCode": "def hira2kata(text: str) -> str:\n    text = text.translate(_HIRA2KATATRANS)\n    return text.replace(\"\", \"\")\n_SYMBOL_TOKENS = set(list(\"\"))\n_NO_YOMI_TOKENS = set(list(\"[]\"))\n_TAGGER = MeCab.Tagger()\ndef text2kata(text: str) -> str:\n    parsed = _TAGGER.parse(text)\n    res = []\n    for line in parsed.split(\"\\n\"):",
        "detail": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "documentation": {}
    },
    {
        "label": "text2kata",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "peekOfCode": "def text2kata(text: str) -> str:\n    parsed = _TAGGER.parse(text)\n    res = []\n    for line in parsed.split(\"\\n\"):\n        if line == \"EOS\":\n            break\n        parts = line.split(\"\\t\")\n        word, yomi = parts[0], parts[1]\n        if yomi:\n            res.append(yomi)",
        "detail": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "documentation": {}
    },
    {
        "label": "japanese_convert_numbers_to_words",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "peekOfCode": "def japanese_convert_numbers_to_words(text: str) -> str:\n    res = _NUMBER_WITH_SEPARATOR_RX.sub(lambda m: m[0].replace(\",\", \"\"), text)\n    res = _CURRENCY_RX.sub(lambda m: m[2] + _CURRENCY_MAP.get(m[1], m[1]), res)\n    res = _NUMBER_RX.sub(lambda m: num2words(m[0], lang=\"ja\"), res)\n    return res\ndef japanese_convert_alpha_symbols_to_words(text: str) -> str:\n    return \"\".join([_ALPHASYMBOL_YOMI.get(ch, ch) for ch in text.lower()])\ndef japanese_text_to_phonemes(text: str) -> str:\n    \"\"\"Convert Japanese text to phonemes.\"\"\"\n    res = unicodedata.normalize(\"NFKC\", text)",
        "detail": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "documentation": {}
    },
    {
        "label": "japanese_convert_alpha_symbols_to_words",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "peekOfCode": "def japanese_convert_alpha_symbols_to_words(text: str) -> str:\n    return \"\".join([_ALPHASYMBOL_YOMI.get(ch, ch) for ch in text.lower()])\ndef japanese_text_to_phonemes(text: str) -> str:\n    \"\"\"Convert Japanese text to phonemes.\"\"\"\n    res = unicodedata.normalize(\"NFKC\", text)\n    res = japanese_convert_numbers_to_words(res)\n    res = japanese_convert_alpha_symbols_to_words(res)\n    res = text2kata(res)\n    res = kata2phoneme(res)\n    return res.replace(\" \", \"\")",
        "detail": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "documentation": {}
    },
    {
        "label": "japanese_text_to_phonemes",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "peekOfCode": "def japanese_text_to_phonemes(text: str) -> str:\n    \"\"\"Convert Japanese text to phonemes.\"\"\"\n    res = unicodedata.normalize(\"NFKC\", text)\n    res = japanese_convert_numbers_to_words(res)\n    res = japanese_convert_alpha_symbols_to_words(res)\n    res = text2kata(res)\n    res = kata2phoneme(res)\n    return res.replace(\" \", \"\")",
        "detail": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "documentation": {}
    },
    {
        "label": "_CONVRULES",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "peekOfCode": "_CONVRULES = [\n    # Conversion of 2 letters\n    \"/ a a\",\n    \"/ i i\",\n    \"/ i e\",\n    \"/ y a\",\n    \"/ u:\",\n    \"/ e e\",\n    \"/ o:\",\n    \"/ k a:\",",
        "detail": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "documentation": {}
    },
    {
        "label": "_COLON_RX",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "peekOfCode": "_COLON_RX = re.compile(\":+\")\n_REJECT_RX = re.compile(\"[^ a-zA-Z:,.?]\")\ndef _makerulemap():\n    l = [tuple(x.split(\"/\")) for x in _CONVRULES]\n    return tuple({k: v for k, v in l if len(k) == i} for i in (1, 2))\n_RULEMAP1, _RULEMAP2 = _makerulemap()\ndef kata2phoneme(text: str) -> str:\n    \"\"\"Convert katakana text to phonemes.\"\"\"\n    text = text.strip()\n    res = \"\"",
        "detail": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "documentation": {}
    },
    {
        "label": "_REJECT_RX",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "peekOfCode": "_REJECT_RX = re.compile(\"[^ a-zA-Z:,.?]\")\ndef _makerulemap():\n    l = [tuple(x.split(\"/\")) for x in _CONVRULES]\n    return tuple({k: v for k, v in l if len(k) == i} for i in (1, 2))\n_RULEMAP1, _RULEMAP2 = _makerulemap()\ndef kata2phoneme(text: str) -> str:\n    \"\"\"Convert katakana text to phonemes.\"\"\"\n    text = text.strip()\n    res = \"\"\n    while text:",
        "detail": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "documentation": {}
    },
    {
        "label": "_KATAKANA",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "peekOfCode": "_KATAKANA = \"\".join(chr(ch) for ch in range(ord(\"\"), ord(\"\") + 1))\n_HIRAGANA = \"\".join(chr(ch) for ch in range(ord(\"\"), ord(\"\") + 1))\n_HIRA2KATATRANS = str.maketrans(_HIRAGANA, _KATAKANA)\ndef hira2kata(text: str) -> str:\n    text = text.translate(_HIRA2KATATRANS)\n    return text.replace(\"\", \"\")\n_SYMBOL_TOKENS = set(list(\"\"))\n_NO_YOMI_TOKENS = set(list(\"[]\"))\n_TAGGER = MeCab.Tagger()\ndef text2kata(text: str) -> str:",
        "detail": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "documentation": {}
    },
    {
        "label": "_HIRAGANA",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "peekOfCode": "_HIRAGANA = \"\".join(chr(ch) for ch in range(ord(\"\"), ord(\"\") + 1))\n_HIRA2KATATRANS = str.maketrans(_HIRAGANA, _KATAKANA)\ndef hira2kata(text: str) -> str:\n    text = text.translate(_HIRA2KATATRANS)\n    return text.replace(\"\", \"\")\n_SYMBOL_TOKENS = set(list(\"\"))\n_NO_YOMI_TOKENS = set(list(\"[]\"))\n_TAGGER = MeCab.Tagger()\ndef text2kata(text: str) -> str:\n    parsed = _TAGGER.parse(text)",
        "detail": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "documentation": {}
    },
    {
        "label": "_HIRA2KATATRANS",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "peekOfCode": "_HIRA2KATATRANS = str.maketrans(_HIRAGANA, _KATAKANA)\ndef hira2kata(text: str) -> str:\n    text = text.translate(_HIRA2KATATRANS)\n    return text.replace(\"\", \"\")\n_SYMBOL_TOKENS = set(list(\"\"))\n_NO_YOMI_TOKENS = set(list(\"[]\"))\n_TAGGER = MeCab.Tagger()\ndef text2kata(text: str) -> str:\n    parsed = _TAGGER.parse(text)\n    res = []",
        "detail": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "documentation": {}
    },
    {
        "label": "_SYMBOL_TOKENS",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "peekOfCode": "_SYMBOL_TOKENS = set(list(\"\"))\n_NO_YOMI_TOKENS = set(list(\"[]\"))\n_TAGGER = MeCab.Tagger()\ndef text2kata(text: str) -> str:\n    parsed = _TAGGER.parse(text)\n    res = []\n    for line in parsed.split(\"\\n\"):\n        if line == \"EOS\":\n            break\n        parts = line.split(\"\\t\")",
        "detail": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "documentation": {}
    },
    {
        "label": "_NO_YOMI_TOKENS",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "peekOfCode": "_NO_YOMI_TOKENS = set(list(\"[]\"))\n_TAGGER = MeCab.Tagger()\ndef text2kata(text: str) -> str:\n    parsed = _TAGGER.parse(text)\n    res = []\n    for line in parsed.split(\"\\n\"):\n        if line == \"EOS\":\n            break\n        parts = line.split(\"\\t\")\n        word, yomi = parts[0], parts[1]",
        "detail": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "documentation": {}
    },
    {
        "label": "_TAGGER",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "peekOfCode": "_TAGGER = MeCab.Tagger()\ndef text2kata(text: str) -> str:\n    parsed = _TAGGER.parse(text)\n    res = []\n    for line in parsed.split(\"\\n\"):\n        if line == \"EOS\":\n            break\n        parts = line.split(\"\\t\")\n        word, yomi = parts[0], parts[1]\n        if yomi:",
        "detail": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "documentation": {}
    },
    {
        "label": "_ALPHASYMBOL_YOMI",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "peekOfCode": "_ALPHASYMBOL_YOMI = {\n    \"#\": \"\",\n    \"%\": \"\",\n    \"&\": \"\",\n    \"+\": \"\",\n    \"-\": \"\",\n    \":\": \"\",\n    \";\": \"\",\n    \"<\": \"\",\n    \"=\": \"\",",
        "detail": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "documentation": {}
    },
    {
        "label": "_NUMBER_WITH_SEPARATOR_RX",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "peekOfCode": "_NUMBER_WITH_SEPARATOR_RX = re.compile(\"[0-9]{1,3}(,[0-9]{3})+\")\n_CURRENCY_MAP = {\"$\": \"\", \"\": \"\", \"\": \"\", \"\": \"\"}\n_CURRENCY_RX = re.compile(r\"([$])([0-9.]*[0-9])\")\n_NUMBER_RX = re.compile(r\"[0-9]+(\\.[0-9]+)?\")\ndef japanese_convert_numbers_to_words(text: str) -> str:\n    res = _NUMBER_WITH_SEPARATOR_RX.sub(lambda m: m[0].replace(\",\", \"\"), text)\n    res = _CURRENCY_RX.sub(lambda m: m[2] + _CURRENCY_MAP.get(m[1], m[1]), res)\n    res = _NUMBER_RX.sub(lambda m: num2words(m[0], lang=\"ja\"), res)\n    return res\ndef japanese_convert_alpha_symbols_to_words(text: str) -> str:",
        "detail": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "documentation": {}
    },
    {
        "label": "_CURRENCY_MAP",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "peekOfCode": "_CURRENCY_MAP = {\"$\": \"\", \"\": \"\", \"\": \"\", \"\": \"\"}\n_CURRENCY_RX = re.compile(r\"([$])([0-9.]*[0-9])\")\n_NUMBER_RX = re.compile(r\"[0-9]+(\\.[0-9]+)?\")\ndef japanese_convert_numbers_to_words(text: str) -> str:\n    res = _NUMBER_WITH_SEPARATOR_RX.sub(lambda m: m[0].replace(\",\", \"\"), text)\n    res = _CURRENCY_RX.sub(lambda m: m[2] + _CURRENCY_MAP.get(m[1], m[1]), res)\n    res = _NUMBER_RX.sub(lambda m: num2words(m[0], lang=\"ja\"), res)\n    return res\ndef japanese_convert_alpha_symbols_to_words(text: str) -> str:\n    return \"\".join([_ALPHASYMBOL_YOMI.get(ch, ch) for ch in text.lower()])",
        "detail": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "documentation": {}
    },
    {
        "label": "_CURRENCY_RX",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "peekOfCode": "_CURRENCY_RX = re.compile(r\"([$])([0-9.]*[0-9])\")\n_NUMBER_RX = re.compile(r\"[0-9]+(\\.[0-9]+)?\")\ndef japanese_convert_numbers_to_words(text: str) -> str:\n    res = _NUMBER_WITH_SEPARATOR_RX.sub(lambda m: m[0].replace(\",\", \"\"), text)\n    res = _CURRENCY_RX.sub(lambda m: m[2] + _CURRENCY_MAP.get(m[1], m[1]), res)\n    res = _NUMBER_RX.sub(lambda m: num2words(m[0], lang=\"ja\"), res)\n    return res\ndef japanese_convert_alpha_symbols_to_words(text: str) -> str:\n    return \"\".join([_ALPHASYMBOL_YOMI.get(ch, ch) for ch in text.lower()])\ndef japanese_text_to_phonemes(text: str) -> str:",
        "detail": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "documentation": {}
    },
    {
        "label": "_NUMBER_RX",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "peekOfCode": "_NUMBER_RX = re.compile(r\"[0-9]+(\\.[0-9]+)?\")\ndef japanese_convert_numbers_to_words(text: str) -> str:\n    res = _NUMBER_WITH_SEPARATOR_RX.sub(lambda m: m[0].replace(\",\", \"\"), text)\n    res = _CURRENCY_RX.sub(lambda m: m[2] + _CURRENCY_MAP.get(m[1], m[1]), res)\n    res = _NUMBER_RX.sub(lambda m: num2words(m[0], lang=\"ja\"), res)\n    return res\ndef japanese_convert_alpha_symbols_to_words(text: str) -> str:\n    return \"\".join([_ALPHASYMBOL_YOMI.get(ch, ch) for ch in text.lower()])\ndef japanese_text_to_phonemes(text: str) -> str:\n    \"\"\"Convert Japanese text to phonemes.\"\"\"",
        "detail": "env.src.tts.TTS.tts.utils.text.japanese.phonemizer",
        "documentation": {}
    },
    {
        "label": "etc_dictionary",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.korean.ko_dictionary",
        "description": "env.src.tts.TTS.tts.utils.text.korean.ko_dictionary",
        "peekOfCode": "etc_dictionary = {\"1+1\": \"\", \"2+1\": \"\"}\nenglish_dictionary = {\n    \"KOREA\": \"\",\n    \"IDOL\": \"\",\n    \"IT\": \"\",\n    \"IQ\": \"\",\n    \"UP\": \"\",\n    \"DOWN\": \"\",\n    \"PC\": \"\",\n    \"CCTV\": \"\",",
        "detail": "env.src.tts.TTS.tts.utils.text.korean.ko_dictionary",
        "documentation": {}
    },
    {
        "label": "english_dictionary",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.korean.ko_dictionary",
        "description": "env.src.tts.TTS.tts.utils.text.korean.ko_dictionary",
        "peekOfCode": "english_dictionary = {\n    \"KOREA\": \"\",\n    \"IDOL\": \"\",\n    \"IT\": \"\",\n    \"IQ\": \"\",\n    \"UP\": \"\",\n    \"DOWN\": \"\",\n    \"PC\": \"\",\n    \"CCTV\": \"\",\n    \"SNS\": \"\",",
        "detail": "env.src.tts.TTS.tts.utils.text.korean.ko_dictionary",
        "documentation": {}
    },
    {
        "label": "normalize",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.korean.korean",
        "description": "env.src.tts.TTS.tts.utils.text.korean.korean",
        "peekOfCode": "def normalize(text):\n    text = text.strip()\n    text = re.sub(\"[----------]\", \"\", text)\n    text = normalize_with_dictionary(text, etc_dictionary)\n    text = normalize_english(text)\n    text = text.lower()\n    return text\ndef normalize_with_dictionary(text, dic):\n    if any(key in text for key in dic.keys()):\n        pattern = re.compile(\"|\".join(re.escape(key) for key in dic.keys()))",
        "detail": "env.src.tts.TTS.tts.utils.text.korean.korean",
        "documentation": {}
    },
    {
        "label": "normalize_with_dictionary",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.korean.korean",
        "description": "env.src.tts.TTS.tts.utils.text.korean.korean",
        "peekOfCode": "def normalize_with_dictionary(text, dic):\n    if any(key in text for key in dic.keys()):\n        pattern = re.compile(\"|\".join(re.escape(key) for key in dic.keys()))\n        return pattern.sub(lambda x: dic[x.group()], text)\n    return text\ndef normalize_english(text):\n    def fn(m):\n        word = m.group()\n        if word in english_dictionary:\n            return english_dictionary.get(word)",
        "detail": "env.src.tts.TTS.tts.utils.text.korean.korean",
        "documentation": {}
    },
    {
        "label": "normalize_english",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.korean.korean",
        "description": "env.src.tts.TTS.tts.utils.text.korean.korean",
        "peekOfCode": "def normalize_english(text):\n    def fn(m):\n        word = m.group()\n        if word in english_dictionary:\n            return english_dictionary.get(word)\n        return word\n    text = re.sub(\"([A-Za-z]+)\", fn, text)\n    return text",
        "detail": "env.src.tts.TTS.tts.utils.text.korean.korean",
        "documentation": {}
    },
    {
        "label": "korean_text_to_phonemes",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.korean.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.korean.phonemizer",
        "peekOfCode": "def korean_text_to_phonemes(text, character: str = \"hangeul\") -> str:\n    \"\"\"\n    The input and output values look the same, but they are different in Unicode.\n    example :\n        input = '' (Unicode : \\ud558\\ub298), ( + )\n        output = '' (Unicode :\\u1112\\u1161\\u1102\\u1173\\u11af), ( +  +  +  + )\n    \"\"\"\n    global g2p  # pylint: disable=global-statement\n    if g2p is None:\n        from g2pkk import G2p",
        "detail": "env.src.tts.TTS.tts.utils.text.korean.phonemizer",
        "documentation": {}
    },
    {
        "label": "g2p",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.korean.phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.korean.phonemizer",
        "peekOfCode": "g2p = None\ndef korean_text_to_phonemes(text, character: str = \"hangeul\") -> str:\n    \"\"\"\n    The input and output values look the same, but they are different in Unicode.\n    example :\n        input = '' (Unicode : \\ud558\\ub298), ( + )\n        output = '' (Unicode :\\u1112\\u1161\\u1102\\u1173\\u11af), ( +  +  +  + )\n    \"\"\"\n    global g2p  # pylint: disable=global-statement\n    if g2p is None:",
        "detail": "env.src.tts.TTS.tts.utils.text.korean.phonemizer",
        "documentation": {}
    },
    {
        "label": "BN_Phonemizer",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.utils.text.phonemizers.bangla_phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.phonemizers.bangla_phonemizer",
        "peekOfCode": "class BN_Phonemizer(BasePhonemizer):\n    \"\"\"TTS bn phonemizer using functions in `TTS.tts.utils.text.bangla.phonemizer`\n    Args:\n        punctuations (str):\n            Set of characters to be treated as punctuation. Defaults to `_DEF_ZH_PUNCS`.\n        keep_puncs (bool):\n            If True, keep the punctuations after phonemization. Defaults to False.\n    Example ::\n        \"\" -> `d|||4| |||4| || |i|||4|b||n|3| |d||o||1|w||n|2| |`\n    TODO: someone with Bangla knowledge should check this implementation",
        "detail": "env.src.tts.TTS.tts.utils.text.phonemizers.bangla_phonemizer",
        "documentation": {}
    },
    {
        "label": "_DEF_ZH_PUNCS",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.phonemizers.bangla_phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.phonemizers.bangla_phonemizer",
        "peekOfCode": "_DEF_ZH_PUNCS = \".,[]()?!~\"\nclass BN_Phonemizer(BasePhonemizer):\n    \"\"\"TTS bn phonemizer using functions in `TTS.tts.utils.text.bangla.phonemizer`\n    Args:\n        punctuations (str):\n            Set of characters to be treated as punctuation. Defaults to `_DEF_ZH_PUNCS`.\n        keep_puncs (bool):\n            If True, keep the punctuations after phonemization. Defaults to False.\n    Example ::\n        \"\" -> `d|||4| |||4| || |i|||4|b||n|3| |d||o||1|w||n|2| |`",
        "detail": "env.src.tts.TTS.tts.utils.text.phonemizers.bangla_phonemizer",
        "documentation": {}
    },
    {
        "label": "BasePhonemizer",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.utils.text.phonemizers.base",
        "description": "env.src.tts.TTS.tts.utils.text.phonemizers.base",
        "peekOfCode": "class BasePhonemizer(abc.ABC):\n    \"\"\"Base phonemizer class\n    Phonemization follows the following steps:\n        1. Preprocessing:\n            - remove empty lines\n            - remove punctuation\n            - keep track of punctuation marks\n        2. Phonemization:\n            - convert text to phonemes\n        3. Postprocessing:",
        "detail": "env.src.tts.TTS.tts.utils.text.phonemizers.base",
        "documentation": {}
    },
    {
        "label": "ESpeak",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.utils.text.phonemizers.espeak_wrapper",
        "description": "env.src.tts.TTS.tts.utils.text.phonemizers.espeak_wrapper",
        "peekOfCode": "class ESpeak(BasePhonemizer):\n    \"\"\"ESpeak wrapper calling `espeak` or `espeak-ng` from the command-line the perform G2P\n    Args:\n        language (str):\n            Valid language code for the used backend.\n        backend (str):\n            Name of the backend library to use. `espeak` or `espeak-ng`. If None, set automatically\n            prefering `espeak-ng` over `espeak`. Defaults to None.\n        punctuations (str):\n            Characters to be treated as punctuation. Defaults to Punctuation.default_puncs().",
        "detail": "env.src.tts.TTS.tts.utils.text.phonemizers.espeak_wrapper",
        "documentation": {}
    },
    {
        "label": "is_tool",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.phonemizers.espeak_wrapper",
        "description": "env.src.tts.TTS.tts.utils.text.phonemizers.espeak_wrapper",
        "peekOfCode": "def is_tool(name):\n    from shutil import which\n    return which(name) is not None\n# Use a regex pattern to match the espeak version, because it may be\n# symlinked to espeak-ng, which moves the version bits to another spot.\nespeak_version_pattern = re.compile(r\"text-to-speech:\\s(?P<version>\\d+\\.\\d+(\\.\\d+)?)\")\ndef get_espeak_version():\n    output = subprocess.getoutput(\"espeak --version\")\n    match = espeak_version_pattern.search(output)\n    return match.group(\"version\")",
        "detail": "env.src.tts.TTS.tts.utils.text.phonemizers.espeak_wrapper",
        "documentation": {}
    },
    {
        "label": "get_espeak_version",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.phonemizers.espeak_wrapper",
        "description": "env.src.tts.TTS.tts.utils.text.phonemizers.espeak_wrapper",
        "peekOfCode": "def get_espeak_version():\n    output = subprocess.getoutput(\"espeak --version\")\n    match = espeak_version_pattern.search(output)\n    return match.group(\"version\")\ndef get_espeakng_version():\n    output = subprocess.getoutput(\"espeak-ng --version\")\n    return output.split()[3]\n# priority: espeakng > espeak\nif is_tool(\"espeak-ng\"):\n    _DEF_ESPEAK_LIB = \"espeak-ng\"",
        "detail": "env.src.tts.TTS.tts.utils.text.phonemizers.espeak_wrapper",
        "documentation": {}
    },
    {
        "label": "get_espeakng_version",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.phonemizers.espeak_wrapper",
        "description": "env.src.tts.TTS.tts.utils.text.phonemizers.espeak_wrapper",
        "peekOfCode": "def get_espeakng_version():\n    output = subprocess.getoutput(\"espeak-ng --version\")\n    return output.split()[3]\n# priority: espeakng > espeak\nif is_tool(\"espeak-ng\"):\n    _DEF_ESPEAK_LIB = \"espeak-ng\"\n    _DEF_ESPEAK_VER = get_espeakng_version()\nelif is_tool(\"espeak\"):\n    _DEF_ESPEAK_LIB = \"espeak\"\n    _DEF_ESPEAK_VER = get_espeak_version()",
        "detail": "env.src.tts.TTS.tts.utils.text.phonemizers.espeak_wrapper",
        "documentation": {}
    },
    {
        "label": "espeak_version_pattern",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.phonemizers.espeak_wrapper",
        "description": "env.src.tts.TTS.tts.utils.text.phonemizers.espeak_wrapper",
        "peekOfCode": "espeak_version_pattern = re.compile(r\"text-to-speech:\\s(?P<version>\\d+\\.\\d+(\\.\\d+)?)\")\ndef get_espeak_version():\n    output = subprocess.getoutput(\"espeak --version\")\n    match = espeak_version_pattern.search(output)\n    return match.group(\"version\")\ndef get_espeakng_version():\n    output = subprocess.getoutput(\"espeak-ng --version\")\n    return output.split()[3]\n# priority: espeakng > espeak\nif is_tool(\"espeak-ng\"):",
        "detail": "env.src.tts.TTS.tts.utils.text.phonemizers.espeak_wrapper",
        "documentation": {}
    },
    {
        "label": "Gruut",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.utils.text.phonemizers.gruut_wrapper",
        "description": "env.src.tts.TTS.tts.utils.text.phonemizers.gruut_wrapper",
        "peekOfCode": "class Gruut(BasePhonemizer):\n    \"\"\"Gruut wrapper for G2P\n    Args:\n        language (str):\n            Valid language code for the used backend.\n        punctuations (str):\n            Characters to be treated as punctuation. Defaults to `Punctuation.default_puncs()`.\n        keep_puncs (bool):\n            If true, keep the punctuations after phonemization. Defaults to True.\n        use_espeak_phonemes (bool):",
        "detail": "env.src.tts.TTS.tts.utils.text.phonemizers.gruut_wrapper",
        "documentation": {}
    },
    {
        "label": "GRUUT_TRANS_TABLE",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.phonemizers.gruut_wrapper",
        "description": "env.src.tts.TTS.tts.utils.text.phonemizers.gruut_wrapper",
        "peekOfCode": "GRUUT_TRANS_TABLE = str.maketrans(\"g\", \"\")\nclass Gruut(BasePhonemizer):\n    \"\"\"Gruut wrapper for G2P\n    Args:\n        language (str):\n            Valid language code for the used backend.\n        punctuations (str):\n            Characters to be treated as punctuation. Defaults to `Punctuation.default_puncs()`.\n        keep_puncs (bool):\n            If true, keep the punctuations after phonemization. Defaults to True.",
        "detail": "env.src.tts.TTS.tts.utils.text.phonemizers.gruut_wrapper",
        "documentation": {}
    },
    {
        "label": "JA_JP_Phonemizer",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.utils.text.phonemizers.ja_jp_phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.phonemizers.ja_jp_phonemizer",
        "peekOfCode": "class JA_JP_Phonemizer(BasePhonemizer):\n    \"\"\"TTS Ja-Jp phonemizer using functions in `TTS.tts.utils.text.japanese.phonemizer`\n    TODO: someone with JA knowledge should check this implementation\n    Example:\n        >>> from TTS.tts.utils.text.phonemizers import JA_JP_Phonemizer\n        >>> phonemizer = JA_JP_Phonemizer()\n        >>> phonemizer.phonemize(\"\", separator=\"|\")\n        'd|o|c|h|i|r|a|n|i|i|k|i|m|a|s|u|k|a|?'\n    \"\"\"\n    language = \"ja-jp\"",
        "detail": "env.src.tts.TTS.tts.utils.text.phonemizers.ja_jp_phonemizer",
        "documentation": {}
    },
    {
        "label": "trans",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.phonemizers.ja_jp_phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.phonemizers.ja_jp_phonemizer",
        "peekOfCode": "def trans(text):\n    for i, j in _TRANS_TABLE.items():\n        text = text.replace(i, j)\n    return text\nclass JA_JP_Phonemizer(BasePhonemizer):\n    \"\"\"TTS Ja-Jp phonemizer using functions in `TTS.tts.utils.text.japanese.phonemizer`\n    TODO: someone with JA knowledge should check this implementation\n    Example:\n        >>> from TTS.tts.utils.text.phonemizers import JA_JP_Phonemizer\n        >>> phonemizer = JA_JP_Phonemizer()",
        "detail": "env.src.tts.TTS.tts.utils.text.phonemizers.ja_jp_phonemizer",
        "documentation": {}
    },
    {
        "label": "_DEF_JA_PUNCS",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.phonemizers.ja_jp_phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.phonemizers.ja_jp_phonemizer",
        "peekOfCode": "_DEF_JA_PUNCS = \".,[]()?!~\"\n_TRANS_TABLE = {\"\": \",\"}\ndef trans(text):\n    for i, j in _TRANS_TABLE.items():\n        text = text.replace(i, j)\n    return text\nclass JA_JP_Phonemizer(BasePhonemizer):\n    \"\"\"TTS Ja-Jp phonemizer using functions in `TTS.tts.utils.text.japanese.phonemizer`\n    TODO: someone with JA knowledge should check this implementation\n    Example:",
        "detail": "env.src.tts.TTS.tts.utils.text.phonemizers.ja_jp_phonemizer",
        "documentation": {}
    },
    {
        "label": "_TRANS_TABLE",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.phonemizers.ja_jp_phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.phonemizers.ja_jp_phonemizer",
        "peekOfCode": "_TRANS_TABLE = {\"\": \",\"}\ndef trans(text):\n    for i, j in _TRANS_TABLE.items():\n        text = text.replace(i, j)\n    return text\nclass JA_JP_Phonemizer(BasePhonemizer):\n    \"\"\"TTS Ja-Jp phonemizer using functions in `TTS.tts.utils.text.japanese.phonemizer`\n    TODO: someone with JA knowledge should check this implementation\n    Example:\n        >>> from TTS.tts.utils.text.phonemizers import JA_JP_Phonemizer",
        "detail": "env.src.tts.TTS.tts.utils.text.phonemizers.ja_jp_phonemizer",
        "documentation": {}
    },
    {
        "label": "KO_KR_Phonemizer",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.utils.text.phonemizers.ko_kr_phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.phonemizers.ko_kr_phonemizer",
        "peekOfCode": "class KO_KR_Phonemizer(BasePhonemizer):\n    \"\"\"TTS ko_kr_phonemizer using functions in `TTS.tts.utils.text.korean.phonemizer`\n    TODO: Add Korean to character ()\n    Example:\n        >>> from TTS.tts.utils.text.phonemizers import KO_KR_Phonemizer\n        >>> phonemizer = KO_KR_Phonemizer()\n        >>> phonemizer.phonemize(\"     .\", separator=\"|\")\n        '|| ||||||||| ||||||||||||| ||||||||| |||||| ||||||||||||||.'\n        >>> from TTS.tts.utils.text.phonemizers import KO_KR_Phonemizer\n        >>> phonemizer = KO_KR_Phonemizer()",
        "detail": "env.src.tts.TTS.tts.utils.text.phonemizers.ko_kr_phonemizer",
        "documentation": {}
    },
    {
        "label": "_DEF_KO_PUNCS",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.phonemizers.ko_kr_phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.phonemizers.ko_kr_phonemizer",
        "peekOfCode": "_DEF_KO_PUNCS = \".,[]()?!~\"\nclass KO_KR_Phonemizer(BasePhonemizer):\n    \"\"\"TTS ko_kr_phonemizer using functions in `TTS.tts.utils.text.korean.phonemizer`\n    TODO: Add Korean to character ()\n    Example:\n        >>> from TTS.tts.utils.text.phonemizers import KO_KR_Phonemizer\n        >>> phonemizer = KO_KR_Phonemizer()\n        >>> phonemizer.phonemize(\"     .\", separator=\"|\")\n        '|| ||||||||| ||||||||||||| ||||||||| |||||| ||||||||||||||.'\n        >>> from TTS.tts.utils.text.phonemizers import KO_KR_Phonemizer",
        "detail": "env.src.tts.TTS.tts.utils.text.phonemizers.ko_kr_phonemizer",
        "documentation": {}
    },
    {
        "label": "MultiPhonemizer",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.utils.text.phonemizers.multi_phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.phonemizers.multi_phonemizer",
        "peekOfCode": "class MultiPhonemizer:\n    \"\"\"TTS multi-phonemizer that operates phonemizers for multiple langugages\n    Args:\n        custom_lang_to_phonemizer (Dict):\n            Custom phonemizer mapping if you want to change the defaults. In the format of\n            `{\"lang_code\", \"phonemizer_name\"}`. When it is None, `DEF_LANG_TO_PHONEMIZER` is used. Defaults to `{}`.\n    TODO: find a way to pass custom kwargs to the phonemizers\n    \"\"\"\n    lang_to_phonemizer = {}\n    def __init__(self, lang_to_phonemizer_name: Dict = {}) -> None:  # pylint: disable=dangerous-default-value",
        "detail": "env.src.tts.TTS.tts.utils.text.phonemizers.multi_phonemizer",
        "documentation": {}
    },
    {
        "label": "ZH_CN_Phonemizer",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.utils.text.phonemizers.zh_cn_phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.phonemizers.zh_cn_phonemizer",
        "peekOfCode": "class ZH_CN_Phonemizer(BasePhonemizer):\n    \"\"\"TTS Zh-Cn phonemizer using functions in `TTS.tts.utils.text.chinese_mandarin.phonemizer`\n    Args:\n        punctuations (str):\n            Set of characters to be treated as punctuation. Defaults to `_DEF_ZH_PUNCS`.\n        keep_puncs (bool):\n            If True, keep the punctuations after phonemization. Defaults to False.\n    Example ::\n        \"\" -> `d|||4| |||4| || |i|||4|b||n|3| |d||o||1|w||n|2| |`\n    TODO: someone with Mandarin knowledge should check this implementation",
        "detail": "env.src.tts.TTS.tts.utils.text.phonemizers.zh_cn_phonemizer",
        "documentation": {}
    },
    {
        "label": "_DEF_ZH_PUNCS",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.phonemizers.zh_cn_phonemizer",
        "description": "env.src.tts.TTS.tts.utils.text.phonemizers.zh_cn_phonemizer",
        "peekOfCode": "_DEF_ZH_PUNCS = \".,[]()?!~\"\nclass ZH_CN_Phonemizer(BasePhonemizer):\n    \"\"\"TTS Zh-Cn phonemizer using functions in `TTS.tts.utils.text.chinese_mandarin.phonemizer`\n    Args:\n        punctuations (str):\n            Set of characters to be treated as punctuation. Defaults to `_DEF_ZH_PUNCS`.\n        keep_puncs (bool):\n            If True, keep the punctuations after phonemization. Defaults to False.\n    Example ::\n        \"\" -> `d|||4| |||4| || |i|||4|b||n|3| |d||o||1|w||n|2| |`",
        "detail": "env.src.tts.TTS.tts.utils.text.phonemizers.zh_cn_phonemizer",
        "documentation": {}
    },
    {
        "label": "BaseVocabulary",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.utils.text.characters",
        "description": "env.src.tts.TTS.tts.utils.text.characters",
        "peekOfCode": "class BaseVocabulary:\n    \"\"\"Base Vocabulary class.\n    This class only needs a vocabulary dictionary without specifying the characters.\n    Args:\n        vocab (Dict): A dictionary of characters and their corresponding indices.\n    \"\"\"\n    def __init__(self, vocab: Dict, pad: str = None, blank: str = None, bos: str = None, eos: str = None):\n        self.vocab = vocab\n        self.pad = pad\n        self.blank = blank",
        "detail": "env.src.tts.TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "BaseCharacters",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.utils.text.characters",
        "description": "env.src.tts.TTS.tts.utils.text.characters",
        "peekOfCode": "class BaseCharacters:\n    \"\"\"BaseCharacters class\n        Every new character class should inherit from this.\n        Characters are oredered as follows ```[PAD, EOS, BOS, BLANK, CHARACTERS, PUNCTUATIONS]```.\n        If you need a custom order, you need to define inherit from this class and override the ```_create_vocab``` method.\n        Args:\n            characters (str):\n                Main set of characters to be used in the vocabulary.\n            punctuations (str):\n                Characters to be treated as punctuation.",
        "detail": "env.src.tts.TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "IPAPhonemes",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.utils.text.characters",
        "description": "env.src.tts.TTS.tts.utils.text.characters",
        "peekOfCode": "class IPAPhonemes(BaseCharacters):\n    \"\"\"IPAPhonemes class to manage `TTS.tts` model vocabulary\n    Intended to be used with models using IPAPhonemes as input.\n    It uses system defaults for the undefined class arguments.\n    Args:\n        characters (str):\n            Main set of case-sensitive characters to be used in the vocabulary. Defaults to `_phonemes`.\n        punctuations (str):\n            Characters to be treated as punctuation. Defaults to `_punctuations`.\n        pad (str):",
        "detail": "env.src.tts.TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "Graphemes",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.utils.text.characters",
        "description": "env.src.tts.TTS.tts.utils.text.characters",
        "peekOfCode": "class Graphemes(BaseCharacters):\n    \"\"\"Graphemes class to manage `TTS.tts` model vocabulary\n    Intended to be used with models using graphemes as input.\n    It uses system defaults for the undefined class arguments.\n    Args:\n        characters (str):\n            Main set of case-sensitive characters to be used in the vocabulary. Defaults to `_characters`.\n        punctuations (str):\n            Characters to be treated as punctuation. Defaults to `_punctuations`.\n        pad (str):",
        "detail": "env.src.tts.TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "parse_symbols",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.characters",
        "description": "env.src.tts.TTS.tts.utils.text.characters",
        "peekOfCode": "def parse_symbols():\n    return {\n        \"pad\": _pad,\n        \"eos\": _eos,\n        \"bos\": _bos,\n        \"characters\": _characters,\n        \"punctuations\": _punctuations,\n        \"phonemes\": _phonemes,\n    }\n# DEFAULT SET OF GRAPHEMES",
        "detail": "env.src.tts.TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "_pad",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.characters",
        "description": "env.src.tts.TTS.tts.utils.text.characters",
        "peekOfCode": "_pad = \"<PAD>\"\n_eos = \"<EOS>\"\n_bos = \"<BOS>\"\n_blank = \"<BLNK>\"  # TODO: check if we need this alongside with PAD\n_characters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n_punctuations = \"!'(),-.:;? \"\n# DEFAULT SET OF IPA PHONEMES\n# Phonemes definition (All IPA characters)\n_vowels = \"iyueoa\"\n_non_pulmonic_consonants = \"\"",
        "detail": "env.src.tts.TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "_eos",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.characters",
        "description": "env.src.tts.TTS.tts.utils.text.characters",
        "peekOfCode": "_eos = \"<EOS>\"\n_bos = \"<BOS>\"\n_blank = \"<BLNK>\"  # TODO: check if we need this alongside with PAD\n_characters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n_punctuations = \"!'(),-.:;? \"\n# DEFAULT SET OF IPA PHONEMES\n# Phonemes definition (All IPA characters)\n_vowels = \"iyueoa\"\n_non_pulmonic_consonants = \"\"\n_pulmonic_consonants = \"pbtdckqnmrfvszxhjl\"",
        "detail": "env.src.tts.TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "_bos",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.characters",
        "description": "env.src.tts.TTS.tts.utils.text.characters",
        "peekOfCode": "_bos = \"<BOS>\"\n_blank = \"<BLNK>\"  # TODO: check if we need this alongside with PAD\n_characters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n_punctuations = \"!'(),-.:;? \"\n# DEFAULT SET OF IPA PHONEMES\n# Phonemes definition (All IPA characters)\n_vowels = \"iyueoa\"\n_non_pulmonic_consonants = \"\"\n_pulmonic_consonants = \"pbtdckqnmrfvszxhjl\"\n_suprasegmentals = \"\"",
        "detail": "env.src.tts.TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "_blank",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.characters",
        "description": "env.src.tts.TTS.tts.utils.text.characters",
        "peekOfCode": "_blank = \"<BLNK>\"  # TODO: check if we need this alongside with PAD\n_characters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n_punctuations = \"!'(),-.:;? \"\n# DEFAULT SET OF IPA PHONEMES\n# Phonemes definition (All IPA characters)\n_vowels = \"iyueoa\"\n_non_pulmonic_consonants = \"\"\n_pulmonic_consonants = \"pbtdckqnmrfvszxhjl\"\n_suprasegmentals = \"\"\n_other_symbols = \"w\"",
        "detail": "env.src.tts.TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "_characters",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.characters",
        "description": "env.src.tts.TTS.tts.utils.text.characters",
        "peekOfCode": "_characters = \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\"\n_punctuations = \"!'(),-.:;? \"\n# DEFAULT SET OF IPA PHONEMES\n# Phonemes definition (All IPA characters)\n_vowels = \"iyueoa\"\n_non_pulmonic_consonants = \"\"\n_pulmonic_consonants = \"pbtdckqnmrfvszxhjl\"\n_suprasegmentals = \"\"\n_other_symbols = \"w\"\n_diacrilics = \"\"",
        "detail": "env.src.tts.TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "_punctuations",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.characters",
        "description": "env.src.tts.TTS.tts.utils.text.characters",
        "peekOfCode": "_punctuations = \"!'(),-.:;? \"\n# DEFAULT SET OF IPA PHONEMES\n# Phonemes definition (All IPA characters)\n_vowels = \"iyueoa\"\n_non_pulmonic_consonants = \"\"\n_pulmonic_consonants = \"pbtdckqnmrfvszxhjl\"\n_suprasegmentals = \"\"\n_other_symbols = \"w\"\n_diacrilics = \"\"\n_phonemes = _vowels + _non_pulmonic_consonants + _pulmonic_consonants + _suprasegmentals + _other_symbols + _diacrilics",
        "detail": "env.src.tts.TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "_vowels",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.characters",
        "description": "env.src.tts.TTS.tts.utils.text.characters",
        "peekOfCode": "_vowels = \"iyueoa\"\n_non_pulmonic_consonants = \"\"\n_pulmonic_consonants = \"pbtdckqnmrfvszxhjl\"\n_suprasegmentals = \"\"\n_other_symbols = \"w\"\n_diacrilics = \"\"\n_phonemes = _vowels + _non_pulmonic_consonants + _pulmonic_consonants + _suprasegmentals + _other_symbols + _diacrilics\nclass BaseVocabulary:\n    \"\"\"Base Vocabulary class.\n    This class only needs a vocabulary dictionary without specifying the characters.",
        "detail": "env.src.tts.TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "_non_pulmonic_consonants",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.characters",
        "description": "env.src.tts.TTS.tts.utils.text.characters",
        "peekOfCode": "_non_pulmonic_consonants = \"\"\n_pulmonic_consonants = \"pbtdckqnmrfvszxhjl\"\n_suprasegmentals = \"\"\n_other_symbols = \"w\"\n_diacrilics = \"\"\n_phonemes = _vowels + _non_pulmonic_consonants + _pulmonic_consonants + _suprasegmentals + _other_symbols + _diacrilics\nclass BaseVocabulary:\n    \"\"\"Base Vocabulary class.\n    This class only needs a vocabulary dictionary without specifying the characters.\n    Args:",
        "detail": "env.src.tts.TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "_pulmonic_consonants",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.characters",
        "description": "env.src.tts.TTS.tts.utils.text.characters",
        "peekOfCode": "_pulmonic_consonants = \"pbtdckqnmrfvszxhjl\"\n_suprasegmentals = \"\"\n_other_symbols = \"w\"\n_diacrilics = \"\"\n_phonemes = _vowels + _non_pulmonic_consonants + _pulmonic_consonants + _suprasegmentals + _other_symbols + _diacrilics\nclass BaseVocabulary:\n    \"\"\"Base Vocabulary class.\n    This class only needs a vocabulary dictionary without specifying the characters.\n    Args:\n        vocab (Dict): A dictionary of characters and their corresponding indices.",
        "detail": "env.src.tts.TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "_suprasegmentals",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.characters",
        "description": "env.src.tts.TTS.tts.utils.text.characters",
        "peekOfCode": "_suprasegmentals = \"\"\n_other_symbols = \"w\"\n_diacrilics = \"\"\n_phonemes = _vowels + _non_pulmonic_consonants + _pulmonic_consonants + _suprasegmentals + _other_symbols + _diacrilics\nclass BaseVocabulary:\n    \"\"\"Base Vocabulary class.\n    This class only needs a vocabulary dictionary without specifying the characters.\n    Args:\n        vocab (Dict): A dictionary of characters and their corresponding indices.\n    \"\"\"",
        "detail": "env.src.tts.TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "_other_symbols",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.characters",
        "description": "env.src.tts.TTS.tts.utils.text.characters",
        "peekOfCode": "_other_symbols = \"w\"\n_diacrilics = \"\"\n_phonemes = _vowels + _non_pulmonic_consonants + _pulmonic_consonants + _suprasegmentals + _other_symbols + _diacrilics\nclass BaseVocabulary:\n    \"\"\"Base Vocabulary class.\n    This class only needs a vocabulary dictionary without specifying the characters.\n    Args:\n        vocab (Dict): A dictionary of characters and their corresponding indices.\n    \"\"\"\n    def __init__(self, vocab: Dict, pad: str = None, blank: str = None, bos: str = None, eos: str = None):",
        "detail": "env.src.tts.TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "_diacrilics",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.characters",
        "description": "env.src.tts.TTS.tts.utils.text.characters",
        "peekOfCode": "_diacrilics = \"\"\n_phonemes = _vowels + _non_pulmonic_consonants + _pulmonic_consonants + _suprasegmentals + _other_symbols + _diacrilics\nclass BaseVocabulary:\n    \"\"\"Base Vocabulary class.\n    This class only needs a vocabulary dictionary without specifying the characters.\n    Args:\n        vocab (Dict): A dictionary of characters and their corresponding indices.\n    \"\"\"\n    def __init__(self, vocab: Dict, pad: str = None, blank: str = None, bos: str = None, eos: str = None):\n        self.vocab = vocab",
        "detail": "env.src.tts.TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "_phonemes",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.characters",
        "description": "env.src.tts.TTS.tts.utils.text.characters",
        "peekOfCode": "_phonemes = _vowels + _non_pulmonic_consonants + _pulmonic_consonants + _suprasegmentals + _other_symbols + _diacrilics\nclass BaseVocabulary:\n    \"\"\"Base Vocabulary class.\n    This class only needs a vocabulary dictionary without specifying the characters.\n    Args:\n        vocab (Dict): A dictionary of characters and their corresponding indices.\n    \"\"\"\n    def __init__(self, vocab: Dict, pad: str = None, blank: str = None, bos: str = None, eos: str = None):\n        self.vocab = vocab\n        self.pad = pad",
        "detail": "env.src.tts.TTS.tts.utils.text.characters",
        "documentation": {}
    },
    {
        "label": "expand_abbreviations",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.cleaners",
        "description": "env.src.tts.TTS.tts.utils.text.cleaners",
        "peekOfCode": "def expand_abbreviations(text, lang=\"en\"):\n    if lang == \"en\":\n        _abbreviations = abbreviations_en\n    elif lang == \"fr\":\n        _abbreviations = abbreviations_fr\n    for regex, replacement in _abbreviations:\n        text = re.sub(regex, replacement, text)\n    return text\ndef lowercase(text):\n    return text.lower()",
        "detail": "env.src.tts.TTS.tts.utils.text.cleaners",
        "documentation": {}
    },
    {
        "label": "lowercase",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.cleaners",
        "description": "env.src.tts.TTS.tts.utils.text.cleaners",
        "peekOfCode": "def lowercase(text):\n    return text.lower()\ndef collapse_whitespace(text):\n    return re.sub(_whitespace_re, \" \", text).strip()\ndef convert_to_ascii(text):\n    return anyascii(text)\ndef remove_aux_symbols(text):\n    text = re.sub(r\"[\\<\\>\\(\\)\\[\\]\\\"]+\", \"\", text)\n    return text\ndef replace_symbols(text, lang=\"en\"):",
        "detail": "env.src.tts.TTS.tts.utils.text.cleaners",
        "documentation": {}
    },
    {
        "label": "collapse_whitespace",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.cleaners",
        "description": "env.src.tts.TTS.tts.utils.text.cleaners",
        "peekOfCode": "def collapse_whitespace(text):\n    return re.sub(_whitespace_re, \" \", text).strip()\ndef convert_to_ascii(text):\n    return anyascii(text)\ndef remove_aux_symbols(text):\n    text = re.sub(r\"[\\<\\>\\(\\)\\[\\]\\\"]+\", \"\", text)\n    return text\ndef replace_symbols(text, lang=\"en\"):\n    \"\"\"Replace symbols based on the lenguage tag.\n    Args:",
        "detail": "env.src.tts.TTS.tts.utils.text.cleaners",
        "documentation": {}
    },
    {
        "label": "convert_to_ascii",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.cleaners",
        "description": "env.src.tts.TTS.tts.utils.text.cleaners",
        "peekOfCode": "def convert_to_ascii(text):\n    return anyascii(text)\ndef remove_aux_symbols(text):\n    text = re.sub(r\"[\\<\\>\\(\\)\\[\\]\\\"]+\", \"\", text)\n    return text\ndef replace_symbols(text, lang=\"en\"):\n    \"\"\"Replace symbols based on the lenguage tag.\n    Args:\n      text:\n       Input text.",
        "detail": "env.src.tts.TTS.tts.utils.text.cleaners",
        "documentation": {}
    },
    {
        "label": "remove_aux_symbols",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.cleaners",
        "description": "env.src.tts.TTS.tts.utils.text.cleaners",
        "peekOfCode": "def remove_aux_symbols(text):\n    text = re.sub(r\"[\\<\\>\\(\\)\\[\\]\\\"]+\", \"\", text)\n    return text\ndef replace_symbols(text, lang=\"en\"):\n    \"\"\"Replace symbols based on the lenguage tag.\n    Args:\n      text:\n       Input text.\n      lang:\n        Lenguage identifier. ex: \"en\", \"fr\", \"pt\", \"ca\".",
        "detail": "env.src.tts.TTS.tts.utils.text.cleaners",
        "documentation": {}
    },
    {
        "label": "replace_symbols",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.cleaners",
        "description": "env.src.tts.TTS.tts.utils.text.cleaners",
        "peekOfCode": "def replace_symbols(text, lang=\"en\"):\n    \"\"\"Replace symbols based on the lenguage tag.\n    Args:\n      text:\n       Input text.\n      lang:\n        Lenguage identifier. ex: \"en\", \"fr\", \"pt\", \"ca\".\n    Returns:\n      The modified text\n      example:",
        "detail": "env.src.tts.TTS.tts.utils.text.cleaners",
        "documentation": {}
    },
    {
        "label": "basic_cleaners",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.cleaners",
        "description": "env.src.tts.TTS.tts.utils.text.cleaners",
        "peekOfCode": "def basic_cleaners(text):\n    \"\"\"Basic pipeline that lowercases and collapses whitespace without transliteration.\"\"\"\n    text = lowercase(text)\n    text = collapse_whitespace(text)\n    return text\ndef transliteration_cleaners(text):\n    \"\"\"Pipeline for non-English text that transliterates to ASCII.\"\"\"\n    # text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = collapse_whitespace(text)",
        "detail": "env.src.tts.TTS.tts.utils.text.cleaners",
        "documentation": {}
    },
    {
        "label": "transliteration_cleaners",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.cleaners",
        "description": "env.src.tts.TTS.tts.utils.text.cleaners",
        "peekOfCode": "def transliteration_cleaners(text):\n    \"\"\"Pipeline for non-English text that transliterates to ASCII.\"\"\"\n    # text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = collapse_whitespace(text)\n    return text\ndef basic_german_cleaners(text):\n    \"\"\"Pipeline for German text\"\"\"\n    text = lowercase(text)\n    text = collapse_whitespace(text)",
        "detail": "env.src.tts.TTS.tts.utils.text.cleaners",
        "documentation": {}
    },
    {
        "label": "basic_german_cleaners",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.cleaners",
        "description": "env.src.tts.TTS.tts.utils.text.cleaners",
        "peekOfCode": "def basic_german_cleaners(text):\n    \"\"\"Pipeline for German text\"\"\"\n    text = lowercase(text)\n    text = collapse_whitespace(text)\n    return text\n# TODO: elaborate it\ndef basic_turkish_cleaners(text):\n    \"\"\"Pipeline for Turkish text\"\"\"\n    text = text.replace(\"I\", \"\")\n    text = lowercase(text)",
        "detail": "env.src.tts.TTS.tts.utils.text.cleaners",
        "documentation": {}
    },
    {
        "label": "basic_turkish_cleaners",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.cleaners",
        "description": "env.src.tts.TTS.tts.utils.text.cleaners",
        "peekOfCode": "def basic_turkish_cleaners(text):\n    \"\"\"Pipeline for Turkish text\"\"\"\n    text = text.replace(\"I\", \"\")\n    text = lowercase(text)\n    text = collapse_whitespace(text)\n    return text\ndef english_cleaners(text):\n    \"\"\"Pipeline for English text, including number and abbreviation expansion.\"\"\"\n    # text = convert_to_ascii(text)\n    text = lowercase(text)",
        "detail": "env.src.tts.TTS.tts.utils.text.cleaners",
        "documentation": {}
    },
    {
        "label": "english_cleaners",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.cleaners",
        "description": "env.src.tts.TTS.tts.utils.text.cleaners",
        "peekOfCode": "def english_cleaners(text):\n    \"\"\"Pipeline for English text, including number and abbreviation expansion.\"\"\"\n    # text = convert_to_ascii(text)\n    text = lowercase(text)\n    text = expand_time_english(text)\n    text = en_normalize_numbers(text)\n    text = expand_abbreviations(text)\n    text = replace_symbols(text)\n    text = remove_aux_symbols(text)\n    text = collapse_whitespace(text)",
        "detail": "env.src.tts.TTS.tts.utils.text.cleaners",
        "documentation": {}
    },
    {
        "label": "phoneme_cleaners",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.cleaners",
        "description": "env.src.tts.TTS.tts.utils.text.cleaners",
        "peekOfCode": "def phoneme_cleaners(text):\n    \"\"\"Pipeline for phonemes mode, including number and abbreviation expansion.\"\"\"\n    text = en_normalize_numbers(text)\n    text = expand_abbreviations(text)\n    text = replace_symbols(text)\n    text = remove_aux_symbols(text)\n    text = collapse_whitespace(text)\n    return text\ndef french_cleaners(text):\n    \"\"\"Pipeline for French text. There is no need to expand numbers, phonemizer already does that\"\"\"",
        "detail": "env.src.tts.TTS.tts.utils.text.cleaners",
        "documentation": {}
    },
    {
        "label": "french_cleaners",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.cleaners",
        "description": "env.src.tts.TTS.tts.utils.text.cleaners",
        "peekOfCode": "def french_cleaners(text):\n    \"\"\"Pipeline for French text. There is no need to expand numbers, phonemizer already does that\"\"\"\n    text = expand_abbreviations(text, lang=\"fr\")\n    text = lowercase(text)\n    text = replace_symbols(text, lang=\"fr\")\n    text = remove_aux_symbols(text)\n    text = collapse_whitespace(text)\n    return text\ndef portuguese_cleaners(text):\n    \"\"\"Basic pipeline for Portuguese text. There is no need to expand abbreviation and",
        "detail": "env.src.tts.TTS.tts.utils.text.cleaners",
        "documentation": {}
    },
    {
        "label": "portuguese_cleaners",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.cleaners",
        "description": "env.src.tts.TTS.tts.utils.text.cleaners",
        "peekOfCode": "def portuguese_cleaners(text):\n    \"\"\"Basic pipeline for Portuguese text. There is no need to expand abbreviation and\n    numbers, phonemizer already does that\"\"\"\n    text = lowercase(text)\n    text = replace_symbols(text, lang=\"pt\")\n    text = remove_aux_symbols(text)\n    text = collapse_whitespace(text)\n    return text\ndef chinese_mandarin_cleaners(text: str) -> str:\n    \"\"\"Basic pipeline for chinese\"\"\"",
        "detail": "env.src.tts.TTS.tts.utils.text.cleaners",
        "documentation": {}
    },
    {
        "label": "chinese_mandarin_cleaners",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.cleaners",
        "description": "env.src.tts.TTS.tts.utils.text.cleaners",
        "peekOfCode": "def chinese_mandarin_cleaners(text: str) -> str:\n    \"\"\"Basic pipeline for chinese\"\"\"\n    text = replace_numbers_to_characters_in_text(text)\n    return text\ndef multilingual_cleaners(text):\n    \"\"\"Pipeline for multilingual text\"\"\"\n    text = lowercase(text)\n    text = replace_symbols(text, lang=None)\n    text = remove_aux_symbols(text)\n    text = collapse_whitespace(text)",
        "detail": "env.src.tts.TTS.tts.utils.text.cleaners",
        "documentation": {}
    },
    {
        "label": "multilingual_cleaners",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.text.cleaners",
        "description": "env.src.tts.TTS.tts.utils.text.cleaners",
        "peekOfCode": "def multilingual_cleaners(text):\n    \"\"\"Pipeline for multilingual text\"\"\"\n    text = lowercase(text)\n    text = replace_symbols(text, lang=None)\n    text = remove_aux_symbols(text)\n    text = collapse_whitespace(text)\n    return text",
        "detail": "env.src.tts.TTS.tts.utils.text.cleaners",
        "documentation": {}
    },
    {
        "label": "_whitespace_re",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.cleaners",
        "description": "env.src.tts.TTS.tts.utils.text.cleaners",
        "peekOfCode": "_whitespace_re = re.compile(r\"\\s+\")\ndef expand_abbreviations(text, lang=\"en\"):\n    if lang == \"en\":\n        _abbreviations = abbreviations_en\n    elif lang == \"fr\":\n        _abbreviations = abbreviations_fr\n    for regex, replacement in _abbreviations:\n        text = re.sub(regex, replacement, text)\n    return text\ndef lowercase(text):",
        "detail": "env.src.tts.TTS.tts.utils.text.cleaners",
        "documentation": {}
    },
    {
        "label": "CMUDict",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.utils.text.cmudict",
        "description": "env.src.tts.TTS.tts.utils.text.cmudict",
        "peekOfCode": "class CMUDict:\n    \"\"\"Thin wrapper around CMUDict data. http://www.speech.cs.cmu.edu/cgi-bin/cmudict\"\"\"\n    def __init__(self, file_or_path, keep_ambiguous=True):\n        if isinstance(file_or_path, str):\n            with open(file_or_path, encoding=\"latin-1\") as f:\n                entries = _parse_cmudict(f)\n        else:\n            entries = _parse_cmudict(file_or_path)\n        if not keep_ambiguous:\n            entries = {word: pron for word, pron in entries.items() if len(pron) == 1}",
        "detail": "env.src.tts.TTS.tts.utils.text.cmudict",
        "documentation": {}
    },
    {
        "label": "VALID_SYMBOLS",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.cmudict",
        "description": "env.src.tts.TTS.tts.utils.text.cmudict",
        "peekOfCode": "VALID_SYMBOLS = [\n    \"AA\",\n    \"AA0\",\n    \"AA1\",\n    \"AA2\",\n    \"AE\",\n    \"AE0\",\n    \"AE1\",\n    \"AE2\",\n    \"AH\",",
        "detail": "env.src.tts.TTS.tts.utils.text.cmudict",
        "documentation": {}
    },
    {
        "label": "_alt_re",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.cmudict",
        "description": "env.src.tts.TTS.tts.utils.text.cmudict",
        "peekOfCode": "_alt_re = re.compile(r\"\\([0-9]+\\)\")\ndef _parse_cmudict(file):\n    cmudict = {}\n    for line in file:\n        if line and (line[0] >= \"A\" and line[0] <= \"Z\" or line[0] == \"'\"):\n            parts = line.split(\"  \")\n            word = re.sub(_alt_re, \"\", parts[0])\n            pronunciation = _get_pronunciation(parts[1])\n            if pronunciation:\n                if word in cmudict:",
        "detail": "env.src.tts.TTS.tts.utils.text.cmudict",
        "documentation": {}
    },
    {
        "label": "PuncPosition",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.utils.text.punctuation",
        "description": "env.src.tts.TTS.tts.utils.text.punctuation",
        "peekOfCode": "class PuncPosition(Enum):\n    \"\"\"Enum for the punctuations positions\"\"\"\n    BEGIN = 0\n    END = 1\n    MIDDLE = 2\n    ALONE = 3\nclass Punctuation:\n    \"\"\"Handle punctuations in text.\n    Just strip punctuations from text or strip and restore them later.\n    Args:",
        "detail": "env.src.tts.TTS.tts.utils.text.punctuation",
        "documentation": {}
    },
    {
        "label": "Punctuation",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.utils.text.punctuation",
        "description": "env.src.tts.TTS.tts.utils.text.punctuation",
        "peekOfCode": "class Punctuation:\n    \"\"\"Handle punctuations in text.\n    Just strip punctuations from text or strip and restore them later.\n    Args:\n        puncs (str): The punctuations to be processed. Defaults to `_DEF_PUNCS`.\n    Example:\n        >>> punc = Punctuation()\n        >>> punc.strip(\"This is. example !\")\n        'This is example'\n        >>> text_striped, punc_map = punc.strip_to_restore(\"This is. example !\")",
        "detail": "env.src.tts.TTS.tts.utils.text.punctuation",
        "documentation": {}
    },
    {
        "label": "_DEF_PUNCS",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.punctuation",
        "description": "env.src.tts.TTS.tts.utils.text.punctuation",
        "peekOfCode": "_DEF_PUNCS = ';:,.!?\"'\n_PUNC_IDX = collections.namedtuple(\"_punc_index\", [\"punc\", \"position\"])\nclass PuncPosition(Enum):\n    \"\"\"Enum for the punctuations positions\"\"\"\n    BEGIN = 0\n    END = 1\n    MIDDLE = 2\n    ALONE = 3\nclass Punctuation:\n    \"\"\"Handle punctuations in text.",
        "detail": "env.src.tts.TTS.tts.utils.text.punctuation",
        "documentation": {}
    },
    {
        "label": "_PUNC_IDX",
        "kind": 5,
        "importPath": "env.src.tts.TTS.tts.utils.text.punctuation",
        "description": "env.src.tts.TTS.tts.utils.text.punctuation",
        "peekOfCode": "_PUNC_IDX = collections.namedtuple(\"_punc_index\", [\"punc\", \"position\"])\nclass PuncPosition(Enum):\n    \"\"\"Enum for the punctuations positions\"\"\"\n    BEGIN = 0\n    END = 1\n    MIDDLE = 2\n    ALONE = 3\nclass Punctuation:\n    \"\"\"Handle punctuations in text.\n    Just strip punctuations from text or strip and restore them later.",
        "detail": "env.src.tts.TTS.tts.utils.text.punctuation",
        "documentation": {}
    },
    {
        "label": "TTSTokenizer",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.utils.text.tokenizer",
        "description": "env.src.tts.TTS.tts.utils.text.tokenizer",
        "peekOfCode": "class TTSTokenizer:\n    \"\"\"TTS tokenizer to convert input characters to token IDs and back.\n    Token IDs for OOV chars are discarded but those are stored in `self.not_found_characters` for later.\n    Args:\n        use_phonemes (bool):\n            Whether to use phonemes instead of characters. Defaults to False.\n        characters (Characters):\n            A Characters object to use for character-to-ID and ID-to-character mappings.\n        text_cleaner (callable):\n            A function to pre-process the text before tokenization and phonemization. Defaults to None.",
        "detail": "env.src.tts.TTS.tts.utils.text.tokenizer",
        "documentation": {}
    },
    {
        "label": "prepare_data",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.data",
        "description": "env.src.tts.TTS.tts.utils.data",
        "peekOfCode": "def prepare_data(inputs):\n    max_len = max((len(x) for x in inputs))\n    return np.stack([_pad_data(x, max_len) for x in inputs])\ndef _pad_tensor(x, length):\n    _pad = 0.0\n    assert x.ndim == 2\n    x = np.pad(x, [[0, 0], [0, length - x.shape[1]]], mode=\"constant\", constant_values=_pad)\n    return x\ndef prepare_tensor(inputs, out_steps):\n    max_len = max((x.shape[1] for x in inputs))",
        "detail": "env.src.tts.TTS.tts.utils.data",
        "documentation": {}
    },
    {
        "label": "prepare_tensor",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.data",
        "description": "env.src.tts.TTS.tts.utils.data",
        "peekOfCode": "def prepare_tensor(inputs, out_steps):\n    max_len = max((x.shape[1] for x in inputs))\n    remainder = max_len % out_steps\n    pad_len = max_len + (out_steps - remainder) if remainder > 0 else max_len\n    return np.stack([_pad_tensor(x, pad_len) for x in inputs])\ndef _pad_stop_target(x: np.ndarray, length: int, pad_val=1) -> np.ndarray:\n    \"\"\"Pad stop target array.\n    Args:\n        x (np.ndarray): Stop target array.\n        length (int): Length after padding.",
        "detail": "env.src.tts.TTS.tts.utils.data",
        "documentation": {}
    },
    {
        "label": "prepare_stop_target",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.data",
        "description": "env.src.tts.TTS.tts.utils.data",
        "peekOfCode": "def prepare_stop_target(inputs, out_steps):\n    \"\"\"Pad row vectors with 1.\"\"\"\n    max_len = max((x.shape[0] for x in inputs))\n    remainder = max_len % out_steps\n    pad_len = max_len + (out_steps - remainder) if remainder > 0 else max_len\n    return np.stack([_pad_stop_target(x, pad_len) for x in inputs])\ndef pad_per_step(inputs, pad_len):\n    return np.pad(inputs, [[0, 0], [0, 0], [0, pad_len]], mode=\"constant\", constant_values=0.0)\ndef get_length_balancer_weights(items: list, num_buckets=10):\n    # get all durations",
        "detail": "env.src.tts.TTS.tts.utils.data",
        "documentation": {}
    },
    {
        "label": "pad_per_step",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.data",
        "description": "env.src.tts.TTS.tts.utils.data",
        "peekOfCode": "def pad_per_step(inputs, pad_len):\n    return np.pad(inputs, [[0, 0], [0, 0], [0, pad_len]], mode=\"constant\", constant_values=0.0)\ndef get_length_balancer_weights(items: list, num_buckets=10):\n    # get all durations\n    audio_lengths = np.array([item[\"audio_length\"] for item in items])\n    # create the $num_buckets buckets classes based in the dataset max and min length\n    max_length = int(max(audio_lengths))\n    min_length = int(min(audio_lengths))\n    step = int((max_length - min_length) / num_buckets) + 1\n    buckets_classes = [i + step for i in range(min_length, (max_length - step) + num_buckets + 1, step)]",
        "detail": "env.src.tts.TTS.tts.utils.data",
        "documentation": {}
    },
    {
        "label": "get_length_balancer_weights",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.data",
        "description": "env.src.tts.TTS.tts.utils.data",
        "peekOfCode": "def get_length_balancer_weights(items: list, num_buckets=10):\n    # get all durations\n    audio_lengths = np.array([item[\"audio_length\"] for item in items])\n    # create the $num_buckets buckets classes based in the dataset max and min length\n    max_length = int(max(audio_lengths))\n    min_length = int(min(audio_lengths))\n    step = int((max_length - min_length) / num_buckets) + 1\n    buckets_classes = [i + step for i in range(min_length, (max_length - step) + num_buckets + 1, step)]\n    # add each sample in their respective length bucket\n    buckets_names = np.array(",
        "detail": "env.src.tts.TTS.tts.utils.data",
        "documentation": {}
    },
    {
        "label": "rehash_fairseq_vits_checkpoint",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.fairseq",
        "description": "env.src.tts.TTS.tts.utils.fairseq",
        "peekOfCode": "def rehash_fairseq_vits_checkpoint(checkpoint_file):\n    chk = torch.load(checkpoint_file, map_location=torch.device(\"cpu\"))[\"model\"]\n    new_chk = {}\n    for k, v in chk.items():\n        if \"enc_p.\" in k:\n            new_chk[k.replace(\"enc_p.\", \"text_encoder.\")] = v\n        elif \"dec.\" in k:\n            new_chk[k.replace(\"dec.\", \"waveform_decoder.\")] = v\n        elif \"enc_q.\" in k:\n            new_chk[k.replace(\"enc_q.\", \"posterior_encoder.\")] = v",
        "detail": "env.src.tts.TTS.tts.utils.fairseq",
        "documentation": {}
    },
    {
        "label": "StandardScaler",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.utils.helpers",
        "description": "env.src.tts.TTS.tts.utils.helpers",
        "peekOfCode": "class StandardScaler:\n    \"\"\"StandardScaler for mean-scale normalization with the given mean and scale values.\"\"\"\n    def __init__(self, mean: np.ndarray = None, scale: np.ndarray = None) -> None:\n        self.mean_ = mean\n        self.scale_ = scale\n    def set_stats(self, mean, scale):\n        self.mean_ = mean\n        self.scale_ = scale\n    def reset_stats(self):\n        delattr(self, \"mean_\")",
        "detail": "env.src.tts.TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "sequence_mask",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.helpers",
        "description": "env.src.tts.TTS.tts.utils.helpers",
        "peekOfCode": "def sequence_mask(sequence_length, max_len=None):\n    \"\"\"Create a sequence mask for filtering padding in a sequence tensor.\n    Args:\n        sequence_length (torch.tensor): Sequence lengths.\n        max_len (int, Optional): Maximum sequence length. Defaults to None.\n    Shapes:\n        - mask: :math:`[B, T_max]`\n    \"\"\"\n    if max_len is None:\n        max_len = sequence_length.max()",
        "detail": "env.src.tts.TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "segment",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.helpers",
        "description": "env.src.tts.TTS.tts.utils.helpers",
        "peekOfCode": "def segment(x: torch.tensor, segment_indices: torch.tensor, segment_size=4, pad_short=False):\n    \"\"\"Segment each sample in a batch based on the provided segment indices\n    Args:\n        x (torch.tensor): Input tensor.\n        segment_indices (torch.tensor): Segment indices.\n        segment_size (int): Expected output segment size.\n        pad_short (bool): Pad the end of input tensor with zeros if shorter than the segment size.\n    \"\"\"\n    # pad the input tensor if it is shorter than the segment size\n    if pad_short and x.shape[-1] < segment_size:",
        "detail": "env.src.tts.TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "rand_segments",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.helpers",
        "description": "env.src.tts.TTS.tts.utils.helpers",
        "peekOfCode": "def rand_segments(\n    x: torch.tensor, x_lengths: torch.tensor = None, segment_size=4, let_short_samples=False, pad_short=False\n):\n    \"\"\"Create random segments based on the input lengths.\n    Args:\n        x (torch.tensor): Input tensor.\n        x_lengths (torch.tensor): Input lengths.\n        segment_size (int): Expected output segment size.\n        let_short_samples (bool): Allow shorter samples than the segment size.\n        pad_short (bool): Pad the end of input tensor with zeros if shorter than the segment size.",
        "detail": "env.src.tts.TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "average_over_durations",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.helpers",
        "description": "env.src.tts.TTS.tts.utils.helpers",
        "peekOfCode": "def average_over_durations(values, durs):\n    \"\"\"Average values over durations.\n    Shapes:\n        - values: :math:`[B, 1, T_de]`\n        - durs: :math:`[B, T_en]`\n        - avg: :math:`[B, 1, T_en]`\n    \"\"\"\n    durs_cums_ends = torch.cumsum(durs, dim=1).long()\n    durs_cums_starts = torch.nn.functional.pad(durs_cums_ends[:, :-1], (1, 0))\n    values_nonzero_cums = torch.nn.functional.pad(torch.cumsum(values != 0.0, dim=2), (1, 0))",
        "detail": "env.src.tts.TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "convert_pad_shape",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.helpers",
        "description": "env.src.tts.TTS.tts.utils.helpers",
        "peekOfCode": "def convert_pad_shape(pad_shape):\n    l = pad_shape[::-1]\n    pad_shape = [item for sublist in l for item in sublist]\n    return pad_shape\ndef generate_path(duration, mask):\n    \"\"\"\n    Shapes:\n        - duration: :math:`[B, T_en]`\n        - mask: :math:'[B, T_en, T_de]`\n        - path: :math:`[B, T_en, T_de]`",
        "detail": "env.src.tts.TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "generate_path",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.helpers",
        "description": "env.src.tts.TTS.tts.utils.helpers",
        "peekOfCode": "def generate_path(duration, mask):\n    \"\"\"\n    Shapes:\n        - duration: :math:`[B, T_en]`\n        - mask: :math:'[B, T_en, T_de]`\n        - path: :math:`[B, T_en, T_de]`\n    \"\"\"\n    b, t_x, t_y = mask.shape\n    cum_duration = torch.cumsum(duration, 1)\n    cum_duration_flat = cum_duration.view(b * t_x)",
        "detail": "env.src.tts.TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "maximum_path",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.helpers",
        "description": "env.src.tts.TTS.tts.utils.helpers",
        "peekOfCode": "def maximum_path(value, mask):\n    if CYTHON:\n        return maximum_path_cython(value, mask)\n    return maximum_path_numpy(value, mask)\ndef maximum_path_cython(value, mask):\n    \"\"\"Cython optimised version.\n    Shapes:\n        - value: :math:`[B, T_en, T_de]`\n        - mask: :math:`[B, T_en, T_de]`\n    \"\"\"",
        "detail": "env.src.tts.TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "maximum_path_cython",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.helpers",
        "description": "env.src.tts.TTS.tts.utils.helpers",
        "peekOfCode": "def maximum_path_cython(value, mask):\n    \"\"\"Cython optimised version.\n    Shapes:\n        - value: :math:`[B, T_en, T_de]`\n        - mask: :math:`[B, T_en, T_de]`\n    \"\"\"\n    value = value * mask\n    device = value.device\n    dtype = value.dtype\n    value = value.data.cpu().numpy().astype(np.float32)",
        "detail": "env.src.tts.TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "maximum_path_numpy",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.helpers",
        "description": "env.src.tts.TTS.tts.utils.helpers",
        "peekOfCode": "def maximum_path_numpy(value, mask, max_neg_val=None):\n    \"\"\"\n    Monotonic alignment search algorithm\n    Numpy-friendly version. It's about 4 times faster than torch version.\n    value: [b, t_x, t_y]\n    mask: [b, t_x, t_y]\n    \"\"\"\n    if max_neg_val is None:\n        max_neg_val = -np.inf  # Patch for Sphinx complaint\n    value = value * mask",
        "detail": "env.src.tts.TTS.tts.utils.helpers",
        "documentation": {}
    },
    {
        "label": "LanguageManager",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.utils.languages",
        "description": "env.src.tts.TTS.tts.utils.languages",
        "peekOfCode": "class LanguageManager(BaseIDManager):\n    \"\"\"Manage the languages for multi-lingual TTS models. Load a datafile and parse the information\n    in a way that can be queried by language.\n    Args:\n        language_ids_file_path (str, optional): Path to the metafile that maps language names to ids used by\n        TTS models. Defaults to \"\".\n        config (Coqpit, optional): Coqpit config that contains the language information in the datasets filed.\n        Defaults to None.\n    Examples:\n        >>> manager = LanguageManager(language_ids_file_path=language_ids_file_path)",
        "detail": "env.src.tts.TTS.tts.utils.languages",
        "documentation": {}
    },
    {
        "label": "get_language_balancer_weights",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.languages",
        "description": "env.src.tts.TTS.tts.utils.languages",
        "peekOfCode": "def get_language_balancer_weights(items: list):\n    language_names = np.array([item[\"language\"] for item in items])\n    unique_language_names = np.unique(language_names).tolist()\n    language_ids = [unique_language_names.index(l) for l in language_names]\n    language_count = np.array([len(np.where(language_names == l)[0]) for l in unique_language_names])\n    weight_language = 1.0 / language_count\n    # get weight for each sample\n    dataset_samples_weight = np.array([weight_language[l] for l in language_ids])\n    # normalize\n    dataset_samples_weight = dataset_samples_weight / np.linalg.norm(dataset_samples_weight)",
        "detail": "env.src.tts.TTS.tts.utils.languages",
        "documentation": {}
    },
    {
        "label": "BaseIDManager",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.utils.managers",
        "description": "env.src.tts.TTS.tts.utils.managers",
        "peekOfCode": "class BaseIDManager:\n    \"\"\"Base `ID` Manager class. Every new `ID` manager must inherit this.\n    It defines common `ID` manager specific functions.\n    \"\"\"\n    def __init__(self, id_file_path: str = \"\"):\n        self.name_to_id = {}\n        if id_file_path:\n            self.load_ids_from_file(id_file_path)\n    @staticmethod\n    def _load_json(json_file_path: str) -> Dict:",
        "detail": "env.src.tts.TTS.tts.utils.managers",
        "documentation": {}
    },
    {
        "label": "EmbeddingManager",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.utils.managers",
        "description": "env.src.tts.TTS.tts.utils.managers",
        "peekOfCode": "class EmbeddingManager(BaseIDManager):\n    \"\"\"Base `Embedding` Manager class. Every new `Embedding` manager must inherit this.\n    It defines common `Embedding` manager specific functions.\n    It expects embeddings files in the following format:\n    ::\n        {\n            'audio_file_key':{\n                'name': 'category_name',\n                'embedding'[<embedding_values>]\n            },",
        "detail": "env.src.tts.TTS.tts.utils.managers",
        "documentation": {}
    },
    {
        "label": "load_file",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.managers",
        "description": "env.src.tts.TTS.tts.utils.managers",
        "peekOfCode": "def load_file(path: str):\n    if path.endswith(\".json\"):\n        with fsspec.open(path, \"r\") as f:\n            return json.load(f)\n    elif path.endswith(\".pth\"):\n        with fsspec.open(path, \"rb\") as f:\n            return torch.load(f, map_location=\"cpu\")\n    else:\n        raise ValueError(\"Unsupported file type\")\ndef save_file(obj: Any, path: str):",
        "detail": "env.src.tts.TTS.tts.utils.managers",
        "documentation": {}
    },
    {
        "label": "save_file",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.managers",
        "description": "env.src.tts.TTS.tts.utils.managers",
        "peekOfCode": "def save_file(obj: Any, path: str):\n    if path.endswith(\".json\"):\n        with fsspec.open(path, \"w\") as f:\n            json.dump(obj, f, indent=4)\n    elif path.endswith(\".pth\"):\n        with fsspec.open(path, \"wb\") as f:\n            torch.save(obj, f)\n    else:\n        raise ValueError(\"Unsupported file type\")\nclass BaseIDManager:",
        "detail": "env.src.tts.TTS.tts.utils.managers",
        "documentation": {}
    },
    {
        "label": "alignment_diagonal_score",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.measures",
        "description": "env.src.tts.TTS.tts.utils.measures",
        "peekOfCode": "def alignment_diagonal_score(alignments, binary=False):\n    \"\"\"\n    Compute how diagonal alignment predictions are. It is useful\n    to measure the alignment consistency of a model\n    Args:\n        alignments (torch.Tensor): batch of alignments.\n        binary (bool): if True, ignore scores and consider attention\n        as a binary mask.\n    Shape:\n        - alignments : :math:`[B, T_de, T_en]`",
        "detail": "env.src.tts.TTS.tts.utils.measures",
        "documentation": {}
    },
    {
        "label": "SpeakerManager",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.utils.speakers",
        "description": "env.src.tts.TTS.tts.utils.speakers",
        "peekOfCode": "class SpeakerManager(EmbeddingManager):\n    \"\"\"Manage the speakers for multi-speaker TTS models. Load a datafile and parse the information\n    in a way that can be queried by speaker or clip.\n    There are 3 different scenarios considered:\n    1. Models using speaker embedding layers. The datafile only maps speaker names to ids used by the embedding layer.\n    2. Models using d-vectors. The datafile includes a dictionary in the following format.\n    ::\n        {\n            'clip_name.wav':{\n                'name': 'speakerA',",
        "detail": "env.src.tts.TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "load_speaker_mapping",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.speakers",
        "description": "env.src.tts.TTS.tts.utils.speakers",
        "peekOfCode": "def load_speaker_mapping(out_path):\n    \"\"\"Loads speaker mapping if already present.\"\"\"\n    if os.path.splitext(out_path)[1] == \".json\":\n        json_file = out_path\n    else:\n        json_file = _set_file_path(out_path)\n    with fsspec.open(json_file, \"r\") as f:\n        return json.load(f)\ndef save_speaker_mapping(out_path, speaker_mapping):\n    \"\"\"Saves speaker mapping if not yet present.\"\"\"",
        "detail": "env.src.tts.TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "save_speaker_mapping",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.speakers",
        "description": "env.src.tts.TTS.tts.utils.speakers",
        "peekOfCode": "def save_speaker_mapping(out_path, speaker_mapping):\n    \"\"\"Saves speaker mapping if not yet present.\"\"\"\n    if out_path is not None:\n        speakers_json_path = _set_file_path(out_path)\n        with fsspec.open(speakers_json_path, \"w\") as f:\n            json.dump(speaker_mapping, f, indent=4)\ndef get_speaker_manager(c: Coqpit, data: List = None, restore_path: str = None, out_path: str = None) -> SpeakerManager:\n    \"\"\"Initiate a `SpeakerManager` instance by the provided config.\n    Args:\n        c (Coqpit): Model configuration.",
        "detail": "env.src.tts.TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "get_speaker_manager",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.speakers",
        "description": "env.src.tts.TTS.tts.utils.speakers",
        "peekOfCode": "def get_speaker_manager(c: Coqpit, data: List = None, restore_path: str = None, out_path: str = None) -> SpeakerManager:\n    \"\"\"Initiate a `SpeakerManager` instance by the provided config.\n    Args:\n        c (Coqpit): Model configuration.\n        restore_path (str): Path to a previous training folder.\n        data (List): Data samples used in training to infer speakers from. It must be provided if speaker embedding\n            layers is used. Defaults to None.\n        out_path (str, optional): Save the generated speaker IDs to a output path. Defaults to None.\n    Returns:\n        SpeakerManager: initialized and ready to use instance.",
        "detail": "env.src.tts.TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "get_speaker_balancer_weights",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.speakers",
        "description": "env.src.tts.TTS.tts.utils.speakers",
        "peekOfCode": "def get_speaker_balancer_weights(items: list):\n    speaker_names = np.array([item[\"speaker_name\"] for item in items])\n    unique_speaker_names = np.unique(speaker_names).tolist()\n    speaker_ids = [unique_speaker_names.index(l) for l in speaker_names]\n    speaker_count = np.array([len(np.where(speaker_names == l)[0]) for l in unique_speaker_names])\n    weight_speaker = 1.0 / speaker_count\n    dataset_samples_weight = np.array([weight_speaker[l] for l in speaker_ids])\n    # normalize\n    dataset_samples_weight = dataset_samples_weight / np.linalg.norm(dataset_samples_weight)\n    return torch.from_numpy(dataset_samples_weight).float()",
        "detail": "env.src.tts.TTS.tts.utils.speakers",
        "documentation": {}
    },
    {
        "label": "SSIMLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.tts.utils.ssim",
        "description": "env.src.tts.TTS.tts.utils.ssim",
        "peekOfCode": "class SSIMLoss(_Loss):\n    r\"\"\"Creates a criterion that measures the structural similarity index error between\n    each element in the input :math:`x` and target :math:`y`.\n    To match performance with skimage and tensorflow set ``'downsample' = True``.\n    The unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:\n    .. math::\n        SSIM = \\{ssim_1,\\dots,ssim_{N \\times C}\\}\\\\\n        ssim_{l}(x, y) = \\frac{(2 \\mu_x \\mu_y + c_1) (2 \\sigma_{xy} + c_2)}\n        {(\\mu_x^2 +\\mu_y^2 + c_1)(\\sigma_x^2 +\\sigma_y^2 + c_2)},\n    where :math:`N` is the batch size, `C` is the channel size. If :attr:`reduction` is not ``'none'``",
        "detail": "env.src.tts.TTS.tts.utils.ssim",
        "documentation": {}
    },
    {
        "label": "gaussian_filter",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.ssim",
        "description": "env.src.tts.TTS.tts.utils.ssim",
        "peekOfCode": "def gaussian_filter(kernel_size: int, sigma: float) -> torch.Tensor:\n    r\"\"\"Returns 2D Gaussian kernel N(0,`sigma`^2)\n    Args:\n        size: Size of the kernel\n        sigma: Std of the distribution\n    Returns:\n        gaussian_kernel: Tensor with shape (1, kernel_size, kernel_size)\n    \"\"\"\n    coords = torch.arange(kernel_size, dtype=torch.float32)\n    coords -= (kernel_size - 1) / 2.0",
        "detail": "env.src.tts.TTS.tts.utils.ssim",
        "documentation": {}
    },
    {
        "label": "ssim",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.ssim",
        "description": "env.src.tts.TTS.tts.utils.ssim",
        "peekOfCode": "def ssim(\n    x: torch.Tensor,\n    y: torch.Tensor,\n    kernel_size: int = 11,\n    kernel_sigma: float = 1.5,\n    data_range: Union[int, float] = 1.0,\n    reduction: str = \"mean\",\n    full: bool = False,\n    downsample: bool = True,\n    k1: float = 0.01,",
        "detail": "env.src.tts.TTS.tts.utils.ssim",
        "documentation": {}
    },
    {
        "label": "numpy_to_torch",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.synthesis",
        "description": "env.src.tts.TTS.tts.utils.synthesis",
        "peekOfCode": "def numpy_to_torch(np_array, dtype, cuda=False):\n    if np_array is None:\n        return None\n    tensor = torch.as_tensor(np_array, dtype=dtype)\n    if cuda:\n        return tensor.cuda()\n    return tensor\ndef compute_style_mel(style_wav, ap, cuda=False):\n    style_mel = torch.FloatTensor(ap.melspectrogram(ap.load_wav(style_wav, sr=ap.sample_rate))).unsqueeze(0)\n    if cuda:",
        "detail": "env.src.tts.TTS.tts.utils.synthesis",
        "documentation": {}
    },
    {
        "label": "compute_style_mel",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.synthesis",
        "description": "env.src.tts.TTS.tts.utils.synthesis",
        "peekOfCode": "def compute_style_mel(style_wav, ap, cuda=False):\n    style_mel = torch.FloatTensor(ap.melspectrogram(ap.load_wav(style_wav, sr=ap.sample_rate))).unsqueeze(0)\n    if cuda:\n        return style_mel.cuda()\n    return style_mel\ndef run_model_torch(\n    model: nn.Module,\n    inputs: torch.Tensor,\n    speaker_id: int = None,\n    style_mel: torch.Tensor = None,",
        "detail": "env.src.tts.TTS.tts.utils.synthesis",
        "documentation": {}
    },
    {
        "label": "run_model_torch",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.synthesis",
        "description": "env.src.tts.TTS.tts.utils.synthesis",
        "peekOfCode": "def run_model_torch(\n    model: nn.Module,\n    inputs: torch.Tensor,\n    speaker_id: int = None,\n    style_mel: torch.Tensor = None,\n    style_text: str = None,\n    d_vector: torch.Tensor = None,\n    language_id: torch.Tensor = None,\n) -> Dict:\n    \"\"\"Run a torch model for inference. It does not support batch inference.",
        "detail": "env.src.tts.TTS.tts.utils.synthesis",
        "documentation": {}
    },
    {
        "label": "trim_silence",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.synthesis",
        "description": "env.src.tts.TTS.tts.utils.synthesis",
        "peekOfCode": "def trim_silence(wav, ap):\n    return wav[: ap.find_endpoint(wav)]\ndef inv_spectrogram(postnet_output, ap, CONFIG):\n    if CONFIG.model.lower() in [\"tacotron\"]:\n        wav = ap.inv_spectrogram(postnet_output.T)\n    else:\n        wav = ap.inv_melspectrogram(postnet_output.T)\n    return wav\ndef id_to_torch(aux_id, cuda=False):\n    if aux_id is not None:",
        "detail": "env.src.tts.TTS.tts.utils.synthesis",
        "documentation": {}
    },
    {
        "label": "inv_spectrogram",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.synthesis",
        "description": "env.src.tts.TTS.tts.utils.synthesis",
        "peekOfCode": "def inv_spectrogram(postnet_output, ap, CONFIG):\n    if CONFIG.model.lower() in [\"tacotron\"]:\n        wav = ap.inv_spectrogram(postnet_output.T)\n    else:\n        wav = ap.inv_melspectrogram(postnet_output.T)\n    return wav\ndef id_to_torch(aux_id, cuda=False):\n    if aux_id is not None:\n        aux_id = np.asarray(aux_id)\n        aux_id = torch.from_numpy(aux_id)",
        "detail": "env.src.tts.TTS.tts.utils.synthesis",
        "documentation": {}
    },
    {
        "label": "id_to_torch",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.synthesis",
        "description": "env.src.tts.TTS.tts.utils.synthesis",
        "peekOfCode": "def id_to_torch(aux_id, cuda=False):\n    if aux_id is not None:\n        aux_id = np.asarray(aux_id)\n        aux_id = torch.from_numpy(aux_id)\n    if cuda:\n        return aux_id.cuda()\n    return aux_id\ndef embedding_to_torch(d_vector, cuda=False):\n    if d_vector is not None:\n        d_vector = np.asarray(d_vector)",
        "detail": "env.src.tts.TTS.tts.utils.synthesis",
        "documentation": {}
    },
    {
        "label": "embedding_to_torch",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.synthesis",
        "description": "env.src.tts.TTS.tts.utils.synthesis",
        "peekOfCode": "def embedding_to_torch(d_vector, cuda=False):\n    if d_vector is not None:\n        d_vector = np.asarray(d_vector)\n        d_vector = torch.from_numpy(d_vector).type(torch.FloatTensor)\n        d_vector = d_vector.squeeze().unsqueeze(0)\n    if cuda:\n        return d_vector.cuda()\n    return d_vector\n# TODO: perform GL with pytorch for batching\ndef apply_griffin_lim(inputs, input_lens, CONFIG, ap):",
        "detail": "env.src.tts.TTS.tts.utils.synthesis",
        "documentation": {}
    },
    {
        "label": "apply_griffin_lim",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.synthesis",
        "description": "env.src.tts.TTS.tts.utils.synthesis",
        "peekOfCode": "def apply_griffin_lim(inputs, input_lens, CONFIG, ap):\n    \"\"\"Apply griffin-lim to each sample iterating throught the first dimension.\n    Args:\n        inputs (Tensor or np.Array): Features to be converted by GL. First dimension is the batch size.\n        input_lens (Tensor or np.Array): 1D array of sample lengths.\n        CONFIG (Dict): TTS config.\n        ap (AudioProcessor): TTS audio processor.\n    \"\"\"\n    wavs = []\n    for idx, spec in enumerate(inputs):",
        "detail": "env.src.tts.TTS.tts.utils.synthesis",
        "documentation": {}
    },
    {
        "label": "synthesis",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.synthesis",
        "description": "env.src.tts.TTS.tts.utils.synthesis",
        "peekOfCode": "def synthesis(\n    model,\n    text,\n    CONFIG,\n    use_cuda,\n    speaker_id=None,\n    style_wav=None,\n    style_text=None,\n    use_griffin_lim=False,\n    do_trim_silence=False,",
        "detail": "env.src.tts.TTS.tts.utils.synthesis",
        "documentation": {}
    },
    {
        "label": "transfer_voice",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.synthesis",
        "description": "env.src.tts.TTS.tts.utils.synthesis",
        "peekOfCode": "def transfer_voice(\n    model,\n    CONFIG,\n    use_cuda,\n    reference_wav,\n    speaker_id=None,\n    d_vector=None,\n    reference_speaker_id=None,\n    reference_d_vector=None,\n    do_trim_silence=False,",
        "detail": "env.src.tts.TTS.tts.utils.synthesis",
        "documentation": {}
    },
    {
        "label": "plot_alignment",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.visual",
        "description": "env.src.tts.TTS.tts.utils.visual",
        "peekOfCode": "def plot_alignment(alignment, info=None, fig_size=(16, 10), title=None, output_fig=False, plot_log=False):\n    if isinstance(alignment, torch.Tensor):\n        alignment_ = alignment.detach().cpu().numpy().squeeze()\n    else:\n        alignment_ = alignment\n    alignment_ = alignment_.astype(np.float32) if alignment_.dtype == np.float16 else alignment_\n    fig, ax = plt.subplots(figsize=fig_size)\n    im = ax.imshow(\n        alignment_.T, aspect=\"auto\", origin=\"lower\", interpolation=\"none\", norm=LogNorm() if plot_log else None\n    )",
        "detail": "env.src.tts.TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_spectrogram",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.visual",
        "description": "env.src.tts.TTS.tts.utils.visual",
        "peekOfCode": "def plot_spectrogram(spectrogram, ap=None, fig_size=(16, 10), output_fig=False):\n    if isinstance(spectrogram, torch.Tensor):\n        spectrogram_ = spectrogram.detach().cpu().numpy().squeeze().T\n    else:\n        spectrogram_ = spectrogram.T\n    spectrogram_ = spectrogram_.astype(np.float32) if spectrogram_.dtype == np.float16 else spectrogram_\n    if ap is not None:\n        spectrogram_ = ap.denormalize(spectrogram_)  # pylint: disable=protected-access\n    fig = plt.figure(figsize=fig_size)\n    plt.imshow(spectrogram_, aspect=\"auto\", origin=\"lower\")",
        "detail": "env.src.tts.TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_pitch",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.visual",
        "description": "env.src.tts.TTS.tts.utils.visual",
        "peekOfCode": "def plot_pitch(pitch, spectrogram, ap=None, fig_size=(30, 10), output_fig=False):\n    \"\"\"Plot pitch curves on top of the spectrogram.\n    Args:\n        pitch (np.array): Pitch values.\n        spectrogram (np.array): Spectrogram values.\n    Shapes:\n        pitch: :math:`(T,)`\n        spec: :math:`(C, T)`\n    \"\"\"\n    if isinstance(spectrogram, torch.Tensor):",
        "detail": "env.src.tts.TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_avg_pitch",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.visual",
        "description": "env.src.tts.TTS.tts.utils.visual",
        "peekOfCode": "def plot_avg_pitch(pitch, chars, fig_size=(30, 10), output_fig=False):\n    \"\"\"Plot pitch curves on top of the input characters.\n    Args:\n        pitch (np.array): Pitch values.\n        chars (str): Characters to place to the x-axis.\n    Shapes:\n        pitch: :math:`(T,)`\n    \"\"\"\n    old_fig_size = plt.rcParams[\"figure.figsize\"]\n    if fig_size is not None:",
        "detail": "env.src.tts.TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "plot_avg_energy",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.visual",
        "description": "env.src.tts.TTS.tts.utils.visual",
        "peekOfCode": "def plot_avg_energy(energy, chars, fig_size=(30, 10), output_fig=False):\n    \"\"\"Plot energy curves on top of the input characters.\n    Args:\n        energy (np.array): energy values.\n        chars (str): Characters to place to the x-axis.\n    Shapes:\n        energy: :math:`(T,)`\n    \"\"\"\n    old_fig_size = plt.rcParams[\"figure.figsize\"]\n    if fig_size is not None:",
        "detail": "env.src.tts.TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "visualize",
        "kind": 2,
        "importPath": "env.src.tts.TTS.tts.utils.visual",
        "description": "env.src.tts.TTS.tts.utils.visual",
        "peekOfCode": "def visualize(\n    alignment,\n    postnet_output,\n    text,\n    hop_length,\n    CONFIG,\n    tokenizer,\n    stop_tokens=None,\n    decoder_output=None,\n    output_path=None,",
        "detail": "env.src.tts.TTS.tts.utils.visual",
        "documentation": {}
    },
    {
        "label": "build_mel_basis",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def build_mel_basis(\n    *,\n    sample_rate: int = None,\n    fft_size: int = None,\n    num_mels: int = None,\n    mel_fmax: int = None,\n    mel_fmin: int = None,\n    **kwargs,\n) -> np.ndarray:\n    \"\"\"Build melspectrogram basis.",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "millisec_to_length",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def millisec_to_length(\n    *, frame_length_ms: int = None, frame_shift_ms: int = None, sample_rate: int = None, **kwargs\n) -> Tuple[int, int]:\n    \"\"\"Compute hop and window length from milliseconds.\n    Returns:\n        Tuple[int, int]: hop length and window length for STFT.\n    \"\"\"\n    factor = frame_length_ms / frame_shift_ms\n    assert (factor).is_integer(), \" [!] frame_shift_ms should divide frame_length_ms\"\n    win_length = int(frame_length_ms / 1000.0 * sample_rate)",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "amp_to_db",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def amp_to_db(*, x: np.ndarray = None, gain: float = 1, base: int = 10, **kwargs) -> np.ndarray:\n    \"\"\"Convert amplitude values to decibels.\n    Args:\n        x (np.ndarray): Amplitude spectrogram.\n        gain (float): Gain factor. Defaults to 1.\n        base (int): Logarithm base. Defaults to 10.\n    Returns:\n        np.ndarray: Decibels spectrogram.\n    \"\"\"\n    assert (x < 0).sum() == 0, \" [!] Input values must be non-negative.\"",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "db_to_amp",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def db_to_amp(*, x: np.ndarray = None, gain: float = 1, base: int = 10, **kwargs) -> np.ndarray:\n    \"\"\"Convert decibels spectrogram to amplitude spectrogram.\n    Args:\n        x (np.ndarray): Decibels spectrogram.\n        gain (float): Gain factor. Defaults to 1.\n        base (int): Logarithm base. Defaults to 10.\n    Returns:\n        np.ndarray: Amplitude spectrogram.\n    \"\"\"\n    return _exp(x / gain, base)",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "preemphasis",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def preemphasis(*, x: np.ndarray, coef: float = 0.97, **kwargs) -> np.ndarray:\n    \"\"\"Apply pre-emphasis to the audio signal. Useful to reduce the correlation between neighbouring signal values.\n    Args:\n        x (np.ndarray): Audio signal.\n    Raises:\n        RuntimeError: Preemphasis coeff is set to 0.\n    Returns:\n        np.ndarray: Decorrelated audio signal.\n    \"\"\"\n    if coef == 0:",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "deemphasis",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def deemphasis(*, x: np.ndarray = None, coef: float = 0.97, **kwargs) -> np.ndarray:\n    \"\"\"Reverse pre-emphasis.\"\"\"\n    if coef == 0:\n        raise RuntimeError(\" [!] Preemphasis is set 0.0.\")\n    return scipy.signal.lfilter([1], [1, -coef], x)\ndef spec_to_mel(*, spec: np.ndarray, mel_basis: np.ndarray = None, **kwargs) -> np.ndarray:\n    \"\"\"Convert a full scale linear spectrogram output of a network to a melspectrogram.\n    Args:\n        spec (np.ndarray): Normalized full scale linear spectrogram.\n    Shapes:",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "spec_to_mel",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def spec_to_mel(*, spec: np.ndarray, mel_basis: np.ndarray = None, **kwargs) -> np.ndarray:\n    \"\"\"Convert a full scale linear spectrogram output of a network to a melspectrogram.\n    Args:\n        spec (np.ndarray): Normalized full scale linear spectrogram.\n    Shapes:\n        - spec: :math:`[C, T]`\n    Returns:\n        np.ndarray: Normalized melspectrogram.\n    \"\"\"\n    return np.dot(mel_basis, spec)",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "mel_to_spec",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def mel_to_spec(*, mel: np.ndarray = None, mel_basis: np.ndarray = None, **kwargs) -> np.ndarray:\n    \"\"\"Convert a melspectrogram to full scale spectrogram.\"\"\"\n    assert (mel < 0).sum() == 0, \" [!] Input values must be non-negative.\"\n    inv_mel_basis = np.linalg.pinv(mel_basis)\n    return np.maximum(1e-10, np.dot(inv_mel_basis, mel))\ndef wav_to_spec(*, wav: np.ndarray = None, **kwargs) -> np.ndarray:\n    \"\"\"Compute a spectrogram from a waveform.\n    Args:\n        wav (np.ndarray): Waveform. Shape :math:`[T_wav,]`\n    Returns:",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "wav_to_spec",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def wav_to_spec(*, wav: np.ndarray = None, **kwargs) -> np.ndarray:\n    \"\"\"Compute a spectrogram from a waveform.\n    Args:\n        wav (np.ndarray): Waveform. Shape :math:`[T_wav,]`\n    Returns:\n        np.ndarray: Spectrogram. Shape :math:`[C, T_spec]`. :math:`T_spec == T_wav / hop_length`\n    \"\"\"\n    D = stft(y=wav, **kwargs)\n    S = np.abs(D)\n    return S.astype(np.float32)",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "wav_to_mel",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def wav_to_mel(*, wav: np.ndarray = None, mel_basis=None, **kwargs) -> np.ndarray:\n    \"\"\"Compute a melspectrogram from a waveform.\"\"\"\n    D = stft(y=wav, **kwargs)\n    S = spec_to_mel(spec=np.abs(D), mel_basis=mel_basis, **kwargs)\n    return S.astype(np.float32)\ndef spec_to_wav(*, spec: np.ndarray, power: float = 1.5, **kwargs) -> np.ndarray:\n    \"\"\"Convert a spectrogram to a waveform using Griffi-Lim vocoder.\"\"\"\n    S = spec.copy()\n    return griffin_lim(spec=S**power, **kwargs)\ndef mel_to_wav(*, mel: np.ndarray = None, power: float = 1.5, **kwargs) -> np.ndarray:",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "spec_to_wav",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def spec_to_wav(*, spec: np.ndarray, power: float = 1.5, **kwargs) -> np.ndarray:\n    \"\"\"Convert a spectrogram to a waveform using Griffi-Lim vocoder.\"\"\"\n    S = spec.copy()\n    return griffin_lim(spec=S**power, **kwargs)\ndef mel_to_wav(*, mel: np.ndarray = None, power: float = 1.5, **kwargs) -> np.ndarray:\n    \"\"\"Convert a melspectrogram to a waveform using Griffi-Lim vocoder.\"\"\"\n    S = mel.copy()\n    S = mel_to_spec(mel=S, mel_basis=kwargs[\"mel_basis\"])  # Convert back to linear\n    return griffin_lim(spec=S**power, **kwargs)\n### STFT and ISTFT ###",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "mel_to_wav",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def mel_to_wav(*, mel: np.ndarray = None, power: float = 1.5, **kwargs) -> np.ndarray:\n    \"\"\"Convert a melspectrogram to a waveform using Griffi-Lim vocoder.\"\"\"\n    S = mel.copy()\n    S = mel_to_spec(mel=S, mel_basis=kwargs[\"mel_basis\"])  # Convert back to linear\n    return griffin_lim(spec=S**power, **kwargs)\n### STFT and ISTFT ###\ndef stft(\n    *,\n    y: np.ndarray = None,\n    fft_size: int = None,",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "stft",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def stft(\n    *,\n    y: np.ndarray = None,\n    fft_size: int = None,\n    hop_length: int = None,\n    win_length: int = None,\n    pad_mode: str = \"reflect\",\n    window: str = \"hann\",\n    center: bool = True,\n    **kwargs,",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "istft",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def istft(\n    *,\n    y: np.ndarray = None,\n    fft_size: int = None,\n    hop_length: int = None,\n    win_length: int = None,\n    window: str = \"hann\",\n    center: bool = True,\n    **kwargs,\n) -> np.ndarray:",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "griffin_lim",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def griffin_lim(*, spec: np.ndarray = None, num_iter=60, **kwargs) -> np.ndarray:\n    angles = np.exp(2j * np.pi * np.random.rand(*spec.shape))\n    S_complex = np.abs(spec).astype(np.complex)\n    y = istft(y=S_complex * angles, **kwargs)\n    if not np.isfinite(y).all():\n        print(\" [!] Waveform is not finite everywhere. Skipping the GL.\")\n        return np.array([0.0])\n    for _ in range(num_iter):\n        angles = np.exp(1j * np.angle(stft(y=y, **kwargs)))\n        y = istft(y=S_complex * angles, **kwargs)",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "compute_stft_paddings",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def compute_stft_paddings(\n    *, x: np.ndarray = None, hop_length: int = None, pad_two_sides: bool = False, **kwargs\n) -> Tuple[int, int]:\n    \"\"\"Compute paddings used by Librosa's STFT. Compute right padding (final frame) or both sides padding\n    (first and final frames)\"\"\"\n    pad = (x.shape[0] // hop_length + 1) * hop_length - x.shape[0]\n    if not pad_two_sides:\n        return 0, pad\n    return pad // 2, pad // 2 + pad % 2\ndef compute_f0(",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "compute_f0",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def compute_f0(\n    *,\n    x: np.ndarray = None,\n    pitch_fmax: float = None,\n    pitch_fmin: float = None,\n    hop_length: int = None,\n    win_length: int = None,\n    sample_rate: int = None,\n    stft_pad_mode: str = \"reflect\",\n    center: bool = True,",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "compute_energy",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def compute_energy(y: np.ndarray, **kwargs) -> np.ndarray:\n    \"\"\"Compute energy of a waveform using the same parameters used for computing melspectrogram.\n    Args:\n      x (np.ndarray): Waveform. Shape :math:`[T_wav,]`\n    Returns:\n      np.ndarray: energy. Shape :math:`[T_energy,]`. :math:`T_energy == T_wav / hop_length`\n    Examples:\n      >>> WAV_FILE = filename = librosa.example('vibeace')\n      >>> from TTS.config import BaseAudioConfig\n      >>> from TTS.utils.audio import AudioProcessor",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "find_endpoint",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def find_endpoint(\n    *,\n    wav: np.ndarray = None,\n    trim_db: float = -40,\n    sample_rate: int = None,\n    min_silence_sec=0.8,\n    gain: float = None,\n    base: int = None,\n    **kwargs,\n) -> int:",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "trim_silence",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def trim_silence(\n    *,\n    wav: np.ndarray = None,\n    sample_rate: int = None,\n    trim_db: float = None,\n    win_length: int = None,\n    hop_length: int = None,\n    **kwargs,\n) -> np.ndarray:\n    \"\"\"Trim silent parts with a threshold and 0.01 sec margin\"\"\"",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "volume_norm",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def volume_norm(*, x: np.ndarray = None, coef: float = 0.95, **kwargs) -> np.ndarray:\n    \"\"\"Normalize the volume of an audio signal.\n    Args:\n        x (np.ndarray): Raw waveform.\n        coef (float): Coefficient to rescale the maximum value. Defaults to 0.95.\n    Returns:\n        np.ndarray: Volume normalized waveform.\n    \"\"\"\n    return x / abs(x).max() * coef\ndef rms_norm(*, wav: np.ndarray = None, db_level: float = -27.0, **kwargs) -> np.ndarray:",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "rms_norm",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def rms_norm(*, wav: np.ndarray = None, db_level: float = -27.0, **kwargs) -> np.ndarray:\n    r = 10 ** (db_level / 20)\n    a = np.sqrt((len(wav) * (r**2)) / np.sum(wav**2))\n    return wav * a\ndef rms_volume_norm(*, x: np.ndarray, db_level: float = -27.0, **kwargs) -> np.ndarray:\n    \"\"\"Normalize the volume based on RMS of the signal.\n    Args:\n        x (np.ndarray): Raw waveform.\n        db_level (float): Target dB level in RMS. Defaults to -27.0.\n    Returns:",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "rms_volume_norm",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def rms_volume_norm(*, x: np.ndarray, db_level: float = -27.0, **kwargs) -> np.ndarray:\n    \"\"\"Normalize the volume based on RMS of the signal.\n    Args:\n        x (np.ndarray): Raw waveform.\n        db_level (float): Target dB level in RMS. Defaults to -27.0.\n    Returns:\n        np.ndarray: RMS normalized waveform.\n    \"\"\"\n    assert -99 <= db_level <= 0, \" [!] db_level should be between -99 and 0\"\n    wav = rms_norm(wav=x, db_level=db_level)",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "load_wav",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def load_wav(*, filename: str, sample_rate: int = None, resample: bool = False, **kwargs) -> np.ndarray:\n    \"\"\"Read a wav file using Librosa and optionally resample, silence trim, volume normalize.\n    Resampling slows down loading the file significantly. Therefore it is recommended to resample the file before.\n    Args:\n        filename (str): Path to the wav file.\n        sr (int, optional): Sampling rate for resampling. Defaults to None.\n        resample (bool, optional): Resample the audio file when loading. Slows down the I/O time. Defaults to False.\n    Returns:\n        np.ndarray: Loaded waveform.\n    \"\"\"",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "save_wav",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def save_wav(*, wav: np.ndarray, path: str, sample_rate: int = None, **kwargs) -> None:\n    \"\"\"Save float waveform to a file using Scipy.\n    Args:\n        wav (np.ndarray): Waveform with float values in range [-1, 1] to save.\n        path (str): Path to a output file.\n        sr (int, optional): Sampling rate used for saving to the file. Defaults to None.\n    \"\"\"\n    wav_norm = wav * (32767 / max(0.01, np.max(np.abs(wav))))\n    scipy.io.wavfile.write(path, sample_rate, wav_norm.astype(np.int16))\ndef mulaw_encode(*, wav: np.ndarray, mulaw_qc: int, **kwargs) -> np.ndarray:",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "mulaw_encode",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def mulaw_encode(*, wav: np.ndarray, mulaw_qc: int, **kwargs) -> np.ndarray:\n    mu = 2**mulaw_qc - 1\n    signal = np.sign(wav) * np.log(1 + mu * np.abs(wav)) / np.log(1.0 + mu)\n    signal = (signal + 1) / 2 * mu + 0.5\n    return np.floor(\n        signal,\n    )\ndef mulaw_decode(*, wav, mulaw_qc: int, **kwargs) -> np.ndarray:\n    \"\"\"Recovers waveform from quantized values.\"\"\"\n    mu = 2**mulaw_qc - 1",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "mulaw_decode",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def mulaw_decode(*, wav, mulaw_qc: int, **kwargs) -> np.ndarray:\n    \"\"\"Recovers waveform from quantized values.\"\"\"\n    mu = 2**mulaw_qc - 1\n    x = np.sign(wav) / mu * ((1 + mu) ** np.abs(wav) - 1)\n    return x\ndef encode_16bits(*, x: np.ndarray, **kwargs) -> np.ndarray:\n    return np.clip(x * 2**15, -(2**15), 2**15 - 1).astype(np.int16)\ndef quantize(*, x: np.ndarray, quantize_bits: int, **kwargs) -> np.ndarray:\n    \"\"\"Quantize a waveform to a given number of bits.\n    Args:",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "encode_16bits",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def encode_16bits(*, x: np.ndarray, **kwargs) -> np.ndarray:\n    return np.clip(x * 2**15, -(2**15), 2**15 - 1).astype(np.int16)\ndef quantize(*, x: np.ndarray, quantize_bits: int, **kwargs) -> np.ndarray:\n    \"\"\"Quantize a waveform to a given number of bits.\n    Args:\n        x (np.ndarray): Waveform to quantize. Must be normalized into the range `[-1, 1]`.\n        quantize_bits (int): Number of quantization bits.\n    Returns:\n        np.ndarray: Quantized waveform.\n    \"\"\"",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "quantize",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def quantize(*, x: np.ndarray, quantize_bits: int, **kwargs) -> np.ndarray:\n    \"\"\"Quantize a waveform to a given number of bits.\n    Args:\n        x (np.ndarray): Waveform to quantize. Must be normalized into the range `[-1, 1]`.\n        quantize_bits (int): Number of quantization bits.\n    Returns:\n        np.ndarray: Quantized waveform.\n    \"\"\"\n    return (x + 1.0) * (2**quantize_bits - 1) / 2\ndef dequantize(*, x, quantize_bits, **kwargs) -> np.ndarray:",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "dequantize",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "description": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "peekOfCode": "def dequantize(*, x, quantize_bits, **kwargs) -> np.ndarray:\n    \"\"\"Dequantize a waveform from the given number of bits.\"\"\"\n    return 2 * x / (2**quantize_bits - 1) - 1",
        "detail": "env.src.tts.TTS.utils.audio.numpy_transforms",
        "documentation": {}
    },
    {
        "label": "AudioProcessor",
        "kind": 6,
        "importPath": "env.src.tts.TTS.utils.audio.processor",
        "description": "env.src.tts.TTS.utils.audio.processor",
        "peekOfCode": "class AudioProcessor(object):\n    \"\"\"Audio Processor for TTS.\n    Note:\n        All the class arguments are set to default values to enable a flexible initialization\n        of the class with the model config. They are not meaningful for all the arguments.\n    Args:\n        sample_rate (int, optional):\n            target audio sampling rate. Defaults to None.\n        resample (bool, optional):\n            enable/disable resampling of the audio clips when the target sampling rate does not match the original sampling rate. Defaults to False.",
        "detail": "env.src.tts.TTS.utils.audio.processor",
        "documentation": {}
    },
    {
        "label": "TorchSTFT",
        "kind": 6,
        "importPath": "env.src.tts.TTS.utils.audio.torch_transforms",
        "description": "env.src.tts.TTS.utils.audio.torch_transforms",
        "peekOfCode": "class TorchSTFT(nn.Module):  # pylint: disable=abstract-method\n    \"\"\"Some of the audio processing funtions using Torch for faster batch processing.\n    Args:\n        n_fft (int):\n            FFT window size for STFT.\n        hop_length (int):\n            number of frames between STFT columns.\n        win_length (int, optional):\n            STFT window length.\n        pad_wav (bool, optional):",
        "detail": "env.src.tts.TTS.utils.audio.torch_transforms",
        "documentation": {}
    },
    {
        "label": "TrainerCallback",
        "kind": 6,
        "importPath": "env.src.tts.TTS.utils.callbacks",
        "description": "env.src.tts.TTS.utils.callbacks",
        "peekOfCode": "class TrainerCallback:\n    @staticmethod\n    def on_init_start(trainer) -> None:\n        if hasattr(trainer.model, \"module\"):\n            if hasattr(trainer.model.module, \"on_init_start\"):\n                trainer.model.module.on_init_start(trainer)\n        else:\n            if hasattr(trainer.model, \"on_init_start\"):\n                trainer.model.on_init_start(trainer)\n        if hasattr(trainer.criterion, \"on_init_start\"):",
        "detail": "env.src.tts.TTS.utils.callbacks",
        "documentation": {}
    },
    {
        "label": "CapacitronOptimizer",
        "kind": 6,
        "importPath": "env.src.tts.TTS.utils.capacitron_optimizer",
        "description": "env.src.tts.TTS.utils.capacitron_optimizer",
        "peekOfCode": "class CapacitronOptimizer:\n    \"\"\"Double optimizer class for the Capacitron model.\"\"\"\n    def __init__(self, config: dict, model_params: Generator) -> None:\n        self.primary_params, self.secondary_params = self.split_model_parameters(model_params)\n        optimizer_names = list(config.optimizer_params.keys())\n        optimizer_parameters = list(config.optimizer_params.values())\n        self.primary_optimizer = get_optimizer(\n            optimizer_names[0],\n            optimizer_parameters[0],\n            config.lr,",
        "detail": "env.src.tts.TTS.utils.capacitron_optimizer",
        "documentation": {}
    },
    {
        "label": "reduce_tensor",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.distribute",
        "description": "env.src.tts.TTS.utils.distribute",
        "peekOfCode": "def reduce_tensor(tensor, num_gpus):\n    rt = tensor.clone()\n    dist.all_reduce(rt, op=dist.reduce_op.SUM)\n    rt /= num_gpus\n    return rt\ndef init_distributed(rank, num_gpus, group_name, dist_backend, dist_url):\n    assert torch.cuda.is_available(), \"Distributed mode requires CUDA.\"\n    # Set cuda device so everything is done on the right GPU.\n    torch.cuda.set_device(rank % torch.cuda.device_count())\n    # Initialize distributed communication",
        "detail": "env.src.tts.TTS.utils.distribute",
        "documentation": {}
    },
    {
        "label": "init_distributed",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.distribute",
        "description": "env.src.tts.TTS.utils.distribute",
        "peekOfCode": "def init_distributed(rank, num_gpus, group_name, dist_backend, dist_url):\n    assert torch.cuda.is_available(), \"Distributed mode requires CUDA.\"\n    # Set cuda device so everything is done on the right GPU.\n    torch.cuda.set_device(rank % torch.cuda.device_count())\n    # Initialize distributed communication\n    dist.init_process_group(dist_backend, init_method=dist_url, world_size=num_gpus, rank=rank, group_name=group_name)",
        "detail": "env.src.tts.TTS.utils.distribute",
        "documentation": {}
    },
    {
        "label": "stream_url",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.download",
        "description": "env.src.tts.TTS.utils.download",
        "peekOfCode": "def stream_url(\n    url: str, start_byte: Optional[int] = None, block_size: int = 32 * 1024, progress_bar: bool = True\n) -> Iterable:\n    \"\"\"Stream url by chunk\n    Args:\n        url (str): Url.\n        start_byte (int or None, optional): Start streaming at that point (Default: ``None``).\n        block_size (int, optional): Size of chunks to stream (Default: ``32 * 1024``).\n        progress_bar (bool, optional): Display a progress bar (Default: ``True``).\n    \"\"\"",
        "detail": "env.src.tts.TTS.utils.download",
        "documentation": {}
    },
    {
        "label": "download_url",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.download",
        "description": "env.src.tts.TTS.utils.download",
        "peekOfCode": "def download_url(\n    url: str,\n    download_folder: str,\n    filename: Optional[str] = None,\n    hash_value: Optional[str] = None,\n    hash_type: str = \"sha256\",\n    progress_bar: bool = True,\n    resume: bool = False,\n) -> None:\n    \"\"\"Download file to disk.",
        "detail": "env.src.tts.TTS.utils.download",
        "documentation": {}
    },
    {
        "label": "validate_file",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.download",
        "description": "env.src.tts.TTS.utils.download",
        "peekOfCode": "def validate_file(file_obj: Any, hash_value: str, hash_type: str = \"sha256\") -> bool:\n    \"\"\"Validate a given file object with its hash.\n    Args:\n        file_obj: File object to read from.\n        hash_value (str): Hash for url.\n        hash_type (str, optional): Hash type, among \"sha256\" and \"md5\" (Default: ``\"sha256\"``).\n    Returns:\n        bool: return True if its a valid file, else False.\n    \"\"\"\n    if hash_type == \"sha256\":",
        "detail": "env.src.tts.TTS.utils.download",
        "documentation": {}
    },
    {
        "label": "extract_archive",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.download",
        "description": "env.src.tts.TTS.utils.download",
        "peekOfCode": "def extract_archive(from_path: str, to_path: Optional[str] = None, overwrite: bool = False) -> List[str]:\n    \"\"\"Extract archive.\n    Args:\n        from_path (str): the path of the archive.\n        to_path (str or None, optional): the root path of the extraced files (directory of from_path)\n            (Default: ``None``)\n        overwrite (bool, optional): overwrite existing files (Default: ``False``)\n    Returns:\n        list: List of paths to extracted files even if not overwritten.\n    \"\"\"",
        "detail": "env.src.tts.TTS.utils.download",
        "documentation": {}
    },
    {
        "label": "download_kaggle_dataset",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.download",
        "description": "env.src.tts.TTS.utils.download",
        "peekOfCode": "def download_kaggle_dataset(dataset_path: str, dataset_name: str, output_path: str):\n    \"\"\"Download dataset from kaggle.\n    Args:\n        dataset_path (str):\n        This the kaggle link to the dataset. for example vctk is 'mfekadu/english-multispeaker-corpus-for-voice-cloning'\n        dataset_name (str): Name of the folder the dataset will be saved in.\n        output_path (str): Path of the location you want the dataset folder to be saved to.\n    \"\"\"\n    data_path = os.path.join(output_path, dataset_name)\n    try:",
        "detail": "env.src.tts.TTS.utils.download",
        "documentation": {}
    },
    {
        "label": "download_ljspeech",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.downloaders",
        "description": "env.src.tts.TTS.utils.downloaders",
        "peekOfCode": "def download_ljspeech(path: str):\n    \"\"\"Download and extract LJSpeech dataset\n    Args:\n        path (str): path to the directory where the dataset will be stored.\n    \"\"\"\n    os.makedirs(path, exist_ok=True)\n    url = \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\"\n    download_url(url, path)\n    basename = os.path.basename(url)\n    archive = os.path.join(path, basename)",
        "detail": "env.src.tts.TTS.utils.downloaders",
        "documentation": {}
    },
    {
        "label": "download_vctk",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.downloaders",
        "description": "env.src.tts.TTS.utils.downloaders",
        "peekOfCode": "def download_vctk(path: str, use_kaggle: Optional[bool] = False):\n    \"\"\"Download and extract VCTK dataset.\n    Args:\n        path (str): path to the directory where the dataset will be stored.\n        use_kaggle (bool, optional): Downloads vctk dataset from kaggle. Is generally faster. Defaults to False.\n    \"\"\"\n    if use_kaggle:\n        download_kaggle_dataset(\"mfekadu/english-multispeaker-corpus-for-voice-cloning\", \"VCTK\", path)\n    else:\n        os.makedirs(path, exist_ok=True)",
        "detail": "env.src.tts.TTS.utils.downloaders",
        "documentation": {}
    },
    {
        "label": "download_tweb",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.downloaders",
        "description": "env.src.tts.TTS.utils.downloaders",
        "peekOfCode": "def download_tweb(path: str):\n    \"\"\"Download and extract Tweb dataset\n    Args:\n        path (str): Path to the directory where the dataset will be stored.\n    \"\"\"\n    download_kaggle_dataset(\"bryanpark/the-world-english-bible-speech-dataset\", \"TWEB\", path)\ndef download_libri_tts(path: str, subset: Optional[str] = \"all\"):\n    \"\"\"Download and extract libri tts dataset.\n    Args:\n        path (str): Path to the directory where the dataset will be stored.",
        "detail": "env.src.tts.TTS.utils.downloaders",
        "documentation": {}
    },
    {
        "label": "download_libri_tts",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.downloaders",
        "description": "env.src.tts.TTS.utils.downloaders",
        "peekOfCode": "def download_libri_tts(path: str, subset: Optional[str] = \"all\"):\n    \"\"\"Download and extract libri tts dataset.\n    Args:\n        path (str): Path to the directory where the dataset will be stored.\n        subset (str, optional): Name of the subset to download. If you only want to download a certain\n        portion specify it here. Defaults to 'all'.\n    \"\"\"\n    subset_dict = {\n        \"libri-tts-clean-100\": \"http://www.openslr.org/resources/60/train-clean-100.tar.gz\",\n        \"libri-tts-clean-360\": \"http://www.openslr.org/resources/60/train-clean-360.tar.gz\",",
        "detail": "env.src.tts.TTS.utils.downloaders",
        "documentation": {}
    },
    {
        "label": "download_thorsten_de",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.downloaders",
        "description": "env.src.tts.TTS.utils.downloaders",
        "peekOfCode": "def download_thorsten_de(path: str):\n    \"\"\"Download and extract Thorsten german male voice dataset.\n    Args:\n        path (str): Path to the directory where the dataset will be stored.\n    \"\"\"\n    os.makedirs(path, exist_ok=True)\n    url = \"https://www.openslr.org/resources/95/thorsten-de_v02.tgz\"\n    download_url(url, path)\n    basename = os.path.basename(url)\n    archive = os.path.join(path, basename)",
        "detail": "env.src.tts.TTS.utils.downloaders",
        "documentation": {}
    },
    {
        "label": "download_mailabs",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.downloaders",
        "description": "env.src.tts.TTS.utils.downloaders",
        "peekOfCode": "def download_mailabs(path: str, language: str = \"english\"):\n    \"\"\"Download and extract Mailabs dataset.\n    Args:\n        path (str): Path to the directory where the dataset will be stored.\n        language (str): Language subset to download. Defaults to english.\n    \"\"\"\n    language_dict = {\n        \"english\": \"https://data.solak.de/data/Training/stt_tts/en_US.tgz\",\n        \"german\": \"https://data.solak.de/data/Training/stt_tts/de_DE.tgz\",\n        \"french\": \"https://data.solak.de/data/Training/stt_tts/fr_FR.tgz\",",
        "detail": "env.src.tts.TTS.utils.downloaders",
        "documentation": {}
    },
    {
        "label": "KeepAverage",
        "kind": 6,
        "importPath": "env.src.tts.TTS.utils.generic_utils",
        "description": "env.src.tts.TTS.utils.generic_utils",
        "peekOfCode": "class KeepAverage:\n    def __init__(self):\n        self.avg_values = {}\n        self.iters = {}\n    def __getitem__(self, key):\n        return self.avg_values[key]\n    def items(self):\n        return self.avg_values.items()\n    def add_value(self, name, init_val=0, init_iter=0):\n        self.avg_values[name] = init_val",
        "detail": "env.src.tts.TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "to_cuda",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.generic_utils",
        "description": "env.src.tts.TTS.utils.generic_utils",
        "peekOfCode": "def to_cuda(x: torch.Tensor) -> torch.Tensor:\n    if x is None:\n        return None\n    if torch.is_tensor(x):\n        x = x.contiguous()\n        if torch.cuda.is_available():\n            x = x.cuda(non_blocking=True)\n    return x\ndef get_cuda():\n    use_cuda = torch.cuda.is_available()",
        "detail": "env.src.tts.TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "get_cuda",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.generic_utils",
        "description": "env.src.tts.TTS.utils.generic_utils",
        "peekOfCode": "def get_cuda():\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    return use_cuda, device\ndef get_git_branch():\n    try:\n        out = subprocess.check_output([\"git\", \"branch\"]).decode(\"utf8\")\n        current = next(line for line in out.split(\"\\n\") if line.startswith(\"*\"))\n        current.replace(\"* \", \"\")\n    except subprocess.CalledProcessError:",
        "detail": "env.src.tts.TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "get_git_branch",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.generic_utils",
        "description": "env.src.tts.TTS.utils.generic_utils",
        "peekOfCode": "def get_git_branch():\n    try:\n        out = subprocess.check_output([\"git\", \"branch\"]).decode(\"utf8\")\n        current = next(line for line in out.split(\"\\n\") if line.startswith(\"*\"))\n        current.replace(\"* \", \"\")\n    except subprocess.CalledProcessError:\n        current = \"inside_docker\"\n    except FileNotFoundError:\n        current = \"unknown\"\n    return current",
        "detail": "env.src.tts.TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "get_commit_hash",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.generic_utils",
        "description": "env.src.tts.TTS.utils.generic_utils",
        "peekOfCode": "def get_commit_hash():\n    \"\"\"https://stackoverflow.com/questions/14989858/get-the-current-git-hash-in-a-python-script\"\"\"\n    # try:\n    #     subprocess.check_output(['git', 'diff-index', '--quiet',\n    #                              'HEAD'])  # Verify client is clean\n    # except:\n    #     raise RuntimeError(\n    #         \" !! Commit before training to get the commit hash.\")\n    try:\n        commit = subprocess.check_output([\"git\", \"rev-parse\", \"--short\", \"HEAD\"]).decode().strip()",
        "detail": "env.src.tts.TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "get_experiment_folder_path",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.generic_utils",
        "description": "env.src.tts.TTS.utils.generic_utils",
        "peekOfCode": "def get_experiment_folder_path(root_path, model_name):\n    \"\"\"Get an experiment folder path with the current date and time\"\"\"\n    date_str = datetime.datetime.now().strftime(\"%B-%d-%Y_%I+%M%p\")\n    commit_hash = get_commit_hash()\n    output_folder = os.path.join(root_path, model_name + \"-\" + date_str + \"-\" + commit_hash)\n    return output_folder\ndef remove_experiment_folder(experiment_path):\n    \"\"\"Check folder if there is a checkpoint, otherwise remove the folder\"\"\"\n    fs = fsspec.get_mapper(experiment_path).fs\n    checkpoint_files = fs.glob(experiment_path + \"/*.pth\")",
        "detail": "env.src.tts.TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "remove_experiment_folder",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.generic_utils",
        "description": "env.src.tts.TTS.utils.generic_utils",
        "peekOfCode": "def remove_experiment_folder(experiment_path):\n    \"\"\"Check folder if there is a checkpoint, otherwise remove the folder\"\"\"\n    fs = fsspec.get_mapper(experiment_path).fs\n    checkpoint_files = fs.glob(experiment_path + \"/*.pth\")\n    if not checkpoint_files:\n        if fs.exists(experiment_path):\n            fs.rm(experiment_path, recursive=True)\n            print(\" ! Run is removed from {}\".format(experiment_path))\n    else:\n        print(\" ! Run is kept in {}\".format(experiment_path))",
        "detail": "env.src.tts.TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "count_parameters",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.generic_utils",
        "description": "env.src.tts.TTS.utils.generic_utils",
        "peekOfCode": "def count_parameters(model):\n    r\"\"\"Count number of trainable parameters in a network\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\ndef to_camel(text):\n    text = text.capitalize()\n    text = re.sub(r\"(?!^)_([a-zA-Z])\", lambda m: m.group(1).upper(), text)\n    text = text.replace(\"Tts\", \"TTS\")\n    text = text.replace(\"vc\", \"VC\")\n    return text\ndef find_module(module_path: str, module_name: str) -> object:",
        "detail": "env.src.tts.TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "to_camel",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.generic_utils",
        "description": "env.src.tts.TTS.utils.generic_utils",
        "peekOfCode": "def to_camel(text):\n    text = text.capitalize()\n    text = re.sub(r\"(?!^)_([a-zA-Z])\", lambda m: m.group(1).upper(), text)\n    text = text.replace(\"Tts\", \"TTS\")\n    text = text.replace(\"vc\", \"VC\")\n    return text\ndef find_module(module_path: str, module_name: str) -> object:\n    module_name = module_name.lower()\n    module = importlib.import_module(module_path + \".\" + module_name)\n    class_name = to_camel(module_name)",
        "detail": "env.src.tts.TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "find_module",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.generic_utils",
        "description": "env.src.tts.TTS.utils.generic_utils",
        "peekOfCode": "def find_module(module_path: str, module_name: str) -> object:\n    module_name = module_name.lower()\n    module = importlib.import_module(module_path + \".\" + module_name)\n    class_name = to_camel(module_name)\n    return getattr(module, class_name)\ndef import_class(module_path: str) -> object:\n    \"\"\"Import a class from a module path.\n    Args:\n        module_path (str): The module path of the class.\n    Returns:",
        "detail": "env.src.tts.TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "import_class",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.generic_utils",
        "description": "env.src.tts.TTS.utils.generic_utils",
        "peekOfCode": "def import_class(module_path: str) -> object:\n    \"\"\"Import a class from a module path.\n    Args:\n        module_path (str): The module path of the class.\n    Returns:\n        object: The imported class.\n    \"\"\"\n    class_name = module_path.split(\".\")[-1]\n    module_path = \".\".join(module_path.split(\".\")[:-1])\n    module = importlib.import_module(module_path)",
        "detail": "env.src.tts.TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "get_import_path",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.generic_utils",
        "description": "env.src.tts.TTS.utils.generic_utils",
        "peekOfCode": "def get_import_path(obj: object) -> str:\n    \"\"\"Get the import path of a class.\n    Args:\n        obj (object): The class object.\n    Returns:\n        str: The import path of the class.\n    \"\"\"\n    return \".\".join([type(obj).__module__, type(obj).__name__])\ndef get_user_data_dir(appname):\n    if sys.platform == \"win32\":",
        "detail": "env.src.tts.TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "get_user_data_dir",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.generic_utils",
        "description": "env.src.tts.TTS.utils.generic_utils",
        "peekOfCode": "def get_user_data_dir(appname):\n    if sys.platform == \"win32\":\n        import winreg  # pylint: disable=import-outside-toplevel\n        key = winreg.OpenKey(\n            winreg.HKEY_CURRENT_USER, r\"Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders\"\n        )\n        dir_, _ = winreg.QueryValueEx(key, \"Local AppData\")\n        ans = Path(dir_).resolve(strict=False)\n    elif sys.platform == \"darwin\":\n        ans = Path(\"~/Library/Application Support/\").expanduser()",
        "detail": "env.src.tts.TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "set_init_dict",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.generic_utils",
        "description": "env.src.tts.TTS.utils.generic_utils",
        "peekOfCode": "def set_init_dict(model_dict, checkpoint_state, c):\n    # Partial initialization: if there is a mismatch with new and old layer, it is skipped.\n    for k, v in checkpoint_state.items():\n        if k not in model_dict:\n            print(\" | > Layer missing in the model definition: {}\".format(k))\n    # 1. filter out unnecessary keys\n    pretrained_dict = {k: v for k, v in checkpoint_state.items() if k in model_dict}\n    # 2. filter out different size layers\n    pretrained_dict = {k: v for k, v in pretrained_dict.items() if v.numel() == model_dict[k].numel()}\n    # 3. skip reinit layers",
        "detail": "env.src.tts.TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "format_aux_input",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.generic_utils",
        "description": "env.src.tts.TTS.utils.generic_utils",
        "peekOfCode": "def format_aux_input(def_args: Dict, kwargs: Dict) -> Dict:\n    \"\"\"Format kwargs to hande auxilary inputs to models.\n    Args:\n        def_args (Dict): A dictionary of argument names and their default values if not defined in `kwargs`.\n        kwargs (Dict): A `dict` or `kwargs` that includes auxilary inputs to the model.\n    Returns:\n        Dict: arguments with formatted auxilary inputs.\n    \"\"\"\n    kwargs = kwargs.copy()\n    for name in def_args:",
        "detail": "env.src.tts.TTS.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "RenamingUnpickler",
        "kind": 6,
        "importPath": "env.src.tts.TTS.utils.io",
        "description": "env.src.tts.TTS.utils.io",
        "peekOfCode": "class RenamingUnpickler(pickle_tts.Unpickler):\n    \"\"\"Overload default pickler to solve module renaming problem\"\"\"\n    def find_class(self, module, name):\n        return super().find_class(module.replace(\"mozilla_voice_tts\", \"TTS\"), name)\nclass AttrDict(dict):\n    \"\"\"A custom dict which converts dict keys\n    to class attributes\"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.__dict__ = self",
        "detail": "env.src.tts.TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "AttrDict",
        "kind": 6,
        "importPath": "env.src.tts.TTS.utils.io",
        "description": "env.src.tts.TTS.utils.io",
        "peekOfCode": "class AttrDict(dict):\n    \"\"\"A custom dict which converts dict keys\n    to class attributes\"\"\"\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.__dict__ = self\ndef copy_model_files(config: Coqpit, out_path, new_fields=None):\n    \"\"\"Copy config.json and other model files to training folder and add\n    new fields.\n    Args:",
        "detail": "env.src.tts.TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "copy_model_files",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.io",
        "description": "env.src.tts.TTS.utils.io",
        "peekOfCode": "def copy_model_files(config: Coqpit, out_path, new_fields=None):\n    \"\"\"Copy config.json and other model files to training folder and add\n    new fields.\n    Args:\n        config (Coqpit): Coqpit config defining the training run.\n        out_path (str): output path to copy the file.\n        new_fields (dict): new fileds to be added or edited\n            in the config file.\n    \"\"\"\n    copy_config_path = os.path.join(out_path, \"config.json\")",
        "detail": "env.src.tts.TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "load_fsspec",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.io",
        "description": "env.src.tts.TTS.utils.io",
        "peekOfCode": "def load_fsspec(\n    path: str,\n    map_location: Union[str, Callable, torch.device, Dict[Union[str, torch.device], Union[str, torch.device]]] = None,\n    cache: bool = True,\n    **kwargs,\n) -> Any:\n    \"\"\"Like torch.load but can load from other locations (e.g. s3:// , gs://).\n    Args:\n        path: Any path or url supported by fsspec.\n        map_location: torch.device or str.",
        "detail": "env.src.tts.TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "load_checkpoint",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.io",
        "description": "env.src.tts.TTS.utils.io",
        "peekOfCode": "def load_checkpoint(\n    model, checkpoint_path, use_cuda=False, eval=False, cache=False\n):  # pylint: disable=redefined-builtin\n    try:\n        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"), cache=cache)\n    except ModuleNotFoundError:\n        pickle_tts.Unpickler = RenamingUnpickler\n        state = load_fsspec(checkpoint_path, map_location=torch.device(\"cpu\"), pickle_module=pickle_tts, cache=cache)\n    model.load_state_dict(state[\"model\"])\n    if use_cuda:",
        "detail": "env.src.tts.TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "save_fsspec",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.io",
        "description": "env.src.tts.TTS.utils.io",
        "peekOfCode": "def save_fsspec(state: Any, path: str, **kwargs):\n    \"\"\"Like torch.save but can save to other locations (e.g. s3:// , gs://).\n    Args:\n        state: State object to save\n        path: Any path or url supported by fsspec.\n        **kwargs: Keyword arguments forwarded to torch.save.\n    \"\"\"\n    with fsspec.open(path, \"wb\") as f:\n        torch.save(state, f, **kwargs)\ndef save_model(config, model, optimizer, scaler, current_step, epoch, output_path, **kwargs):",
        "detail": "env.src.tts.TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "save_model",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.io",
        "description": "env.src.tts.TTS.utils.io",
        "peekOfCode": "def save_model(config, model, optimizer, scaler, current_step, epoch, output_path, **kwargs):\n    if hasattr(model, \"module\"):\n        model_state = model.module.state_dict()\n    else:\n        model_state = model.state_dict()\n    if isinstance(optimizer, list):\n        optimizer_state = [optim.state_dict() for optim in optimizer]\n    elif optimizer.__class__.__name__ == \"CapacitronOptimizer\":\n        optimizer_state = [optimizer.primary_optimizer.state_dict(), optimizer.secondary_optimizer.state_dict()]\n    else:",
        "detail": "env.src.tts.TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "save_checkpoint",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.io",
        "description": "env.src.tts.TTS.utils.io",
        "peekOfCode": "def save_checkpoint(\n    config,\n    model,\n    optimizer,\n    scaler,\n    current_step,\n    epoch,\n    output_folder,\n    **kwargs,\n):",
        "detail": "env.src.tts.TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "save_best_model",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.io",
        "description": "env.src.tts.TTS.utils.io",
        "peekOfCode": "def save_best_model(\n    current_loss,\n    best_loss,\n    config,\n    model,\n    optimizer,\n    scaler,\n    current_step,\n    epoch,\n    out_path,",
        "detail": "env.src.tts.TTS.utils.io",
        "documentation": {}
    },
    {
        "label": "ModelManager",
        "kind": 6,
        "importPath": "env.src.tts.TTS.utils.manage",
        "description": "env.src.tts.TTS.utils.manage",
        "peekOfCode": "class ModelManager(object):\n    \"\"\"Manage TTS models defined in .models.json.\n    It provides an interface to list and download\n    models defines in '.model.json'\n    Models are downloaded under '.TTS' folder in the user's\n    home path.\n    Args:\n        models_file (str): path to .model.json file. Defaults to None.\n        output_prefix (str): prefix to `tts` to download models. Defaults to None\n        progress_bar (bool): print a progress bar when donwloading a file. Defaults to False.",
        "detail": "env.src.tts.TTS.utils.manage",
        "documentation": {}
    },
    {
        "label": "LICENSE_URLS",
        "kind": 5,
        "importPath": "env.src.tts.TTS.utils.manage",
        "description": "env.src.tts.TTS.utils.manage",
        "peekOfCode": "LICENSE_URLS = {\n    \"cc by-nc-nd 4.0\": \"https://creativecommons.org/licenses/by-nc-nd/4.0/\",\n    \"mpl\": \"https://www.mozilla.org/en-US/MPL/2.0/\",\n    \"mpl2\": \"https://www.mozilla.org/en-US/MPL/2.0/\",\n    \"mpl 2.0\": \"https://www.mozilla.org/en-US/MPL/2.0/\",\n    \"mit\": \"https://choosealicense.com/licenses/mit/\",\n    \"apache 2.0\": \"https://choosealicense.com/licenses/apache-2.0/\",\n    \"apache2\": \"https://choosealicense.com/licenses/apache-2.0/\",\n    \"cc-by-sa 4.0\": \"https://creativecommons.org/licenses/by-sa/4.0/\",\n}",
        "detail": "env.src.tts.TTS.utils.manage",
        "documentation": {}
    },
    {
        "label": "RAdam",
        "kind": 6,
        "importPath": "env.src.tts.TTS.utils.radam",
        "description": "env.src.tts.TTS.utils.radam",
        "peekOfCode": "class RAdam(Optimizer):\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n        if lr < 0.0:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if eps < 0.0:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))",
        "detail": "env.src.tts.TTS.utils.radam",
        "documentation": {}
    },
    {
        "label": "SubsetSampler",
        "kind": 6,
        "importPath": "env.src.tts.TTS.utils.samplers",
        "description": "env.src.tts.TTS.utils.samplers",
        "peekOfCode": "class SubsetSampler(Sampler):\n    \"\"\"\n    Samples elements sequentially from a given list of indices.\n    Args:\n        indices (list): a sequence of indices\n    \"\"\"\n    def __init__(self, indices):\n        super().__init__(indices)\n        self.indices = indices\n    def __iter__(self):",
        "detail": "env.src.tts.TTS.utils.samplers",
        "documentation": {}
    },
    {
        "label": "PerfectBatchSampler",
        "kind": 6,
        "importPath": "env.src.tts.TTS.utils.samplers",
        "description": "env.src.tts.TTS.utils.samplers",
        "peekOfCode": "class PerfectBatchSampler(Sampler):\n    \"\"\"\n    Samples a mini-batch of indices for a balanced class batching\n    Args:\n        dataset_items(list): dataset items to sample from.\n        classes (list): list of classes of dataset_items to sample from.\n        batch_size (int): total number of samples to be sampled in a mini-batch.\n        num_gpus (int): number of GPU in the data parallel mode.\n        shuffle (bool): if True, samples randomly, otherwise samples sequentially.\n        drop_last (bool): if True, drops last incomplete batch.",
        "detail": "env.src.tts.TTS.utils.samplers",
        "documentation": {}
    },
    {
        "label": "SortedSampler",
        "kind": 6,
        "importPath": "env.src.tts.TTS.utils.samplers",
        "description": "env.src.tts.TTS.utils.samplers",
        "peekOfCode": "class SortedSampler(Sampler):\n    \"\"\"Samples elements sequentially, always in the same order.\n    Taken from https://github.com/PetrochukM/PyTorch-NLP\n    Args:\n        data (iterable): Iterable data.\n        sort_key (callable): Specifies a function of one argument that is used to extract a\n            numerical comparison key from each list element.\n    Example:\n        >>> list(SortedSampler(range(10), sort_key=lambda i: -i))\n        [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]",
        "detail": "env.src.tts.TTS.utils.samplers",
        "documentation": {}
    },
    {
        "label": "BucketBatchSampler",
        "kind": 6,
        "importPath": "env.src.tts.TTS.utils.samplers",
        "description": "env.src.tts.TTS.utils.samplers",
        "peekOfCode": "class BucketBatchSampler(BatchSampler):\n    \"\"\"Bucket batch sampler\n    Adapted from https://github.com/PetrochukM/PyTorch-NLP\n    Args:\n        sampler (torch.data.utils.sampler.Sampler):\n        batch_size (int): Size of mini-batch.\n        drop_last (bool): If `True` the sampler will drop the last batch if its size would be less\n            than `batch_size`.\n        data (list): List of data samples.\n        sort_key (callable, optional): Callable to specify a comparison key for sorting.",
        "detail": "env.src.tts.TTS.utils.samplers",
        "documentation": {}
    },
    {
        "label": "identity",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.samplers",
        "description": "env.src.tts.TTS.utils.samplers",
        "peekOfCode": "def identity(x):\n    return x\nclass SortedSampler(Sampler):\n    \"\"\"Samples elements sequentially, always in the same order.\n    Taken from https://github.com/PetrochukM/PyTorch-NLP\n    Args:\n        data (iterable): Iterable data.\n        sort_key (callable): Specifies a function of one argument that is used to extract a\n            numerical comparison key from each list element.\n    Example:",
        "detail": "env.src.tts.TTS.utils.samplers",
        "documentation": {}
    },
    {
        "label": "Synthesizer",
        "kind": 6,
        "importPath": "env.src.tts.TTS.utils.synthesizer",
        "description": "env.src.tts.TTS.utils.synthesizer",
        "peekOfCode": "class Synthesizer(object):\n    def __init__(\n        self,\n        tts_checkpoint: str = \"\",\n        tts_config_path: str = \"\",\n        tts_speakers_file: str = \"\",\n        tts_languages_file: str = \"\",\n        vocoder_checkpoint: str = \"\",\n        vocoder_config: str = \"\",\n        encoder_checkpoint: str = \"\",",
        "detail": "env.src.tts.TTS.utils.synthesizer",
        "documentation": {}
    },
    {
        "label": "check_update",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.training",
        "description": "env.src.tts.TTS.utils.training",
        "peekOfCode": "def check_update(model, grad_clip, ignore_stopnet=False, amp_opt_params=None):\n    r\"\"\"Check model gradient against unexpected jumps and failures\"\"\"\n    skip_flag = False\n    if ignore_stopnet:\n        if not amp_opt_params:\n            grad_norm = torch.nn.utils.clip_grad_norm_(\n                [param for name, param in model.named_parameters() if \"stopnet\" not in name], grad_clip\n            )\n        else:\n            grad_norm = torch.nn.utils.clip_grad_norm_(amp_opt_params, grad_clip)",
        "detail": "env.src.tts.TTS.utils.training",
        "documentation": {}
    },
    {
        "label": "gradual_training_scheduler",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.training",
        "description": "env.src.tts.TTS.utils.training",
        "peekOfCode": "def gradual_training_scheduler(global_step, config):\n    \"\"\"Setup the gradual training schedule wrt number\n    of active GPUs\"\"\"\n    num_gpus = torch.cuda.device_count()\n    if num_gpus == 0:\n        num_gpus = 1\n    new_values = None\n    # we set the scheduling wrt num_gpus\n    for values in config.gradual_training:\n        if global_step * num_gpus >= values[0]:",
        "detail": "env.src.tts.TTS.utils.training",
        "documentation": {}
    },
    {
        "label": "read_audio",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.vad",
        "description": "env.src.tts.TTS.utils.vad",
        "peekOfCode": "def read_audio(path):\n    wav, sr = torchaudio.load(path)\n    if wav.size(0) > 1:\n        wav = wav.mean(dim=0, keepdim=True)\n    return wav.squeeze(0), sr\ndef resample_wav(wav, sr, new_sr):\n    wav = wav.unsqueeze(0)\n    transform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=new_sr)\n    wav = transform(wav)\n    return wav.squeeze(0)",
        "detail": "env.src.tts.TTS.utils.vad",
        "documentation": {}
    },
    {
        "label": "resample_wav",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.vad",
        "description": "env.src.tts.TTS.utils.vad",
        "peekOfCode": "def resample_wav(wav, sr, new_sr):\n    wav = wav.unsqueeze(0)\n    transform = torchaudio.transforms.Resample(orig_freq=sr, new_freq=new_sr)\n    wav = transform(wav)\n    return wav.squeeze(0)\ndef map_timestamps_to_new_sr(vad_sr, new_sr, timestamps, just_begging_end=False):\n    factor = new_sr / vad_sr\n    new_timestamps = []\n    if just_begging_end and timestamps:\n        # get just the start and end timestamps",
        "detail": "env.src.tts.TTS.utils.vad",
        "documentation": {}
    },
    {
        "label": "map_timestamps_to_new_sr",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.vad",
        "description": "env.src.tts.TTS.utils.vad",
        "peekOfCode": "def map_timestamps_to_new_sr(vad_sr, new_sr, timestamps, just_begging_end=False):\n    factor = new_sr / vad_sr\n    new_timestamps = []\n    if just_begging_end and timestamps:\n        # get just the start and end timestamps\n        new_dict = {\"start\": int(timestamps[0][\"start\"] * factor), \"end\": int(timestamps[-1][\"end\"] * factor)}\n        new_timestamps.append(new_dict)\n    else:\n        for ts in timestamps:\n            # map to the new SR",
        "detail": "env.src.tts.TTS.utils.vad",
        "documentation": {}
    },
    {
        "label": "get_vad_model_and_utils",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.vad",
        "description": "env.src.tts.TTS.utils.vad",
        "peekOfCode": "def get_vad_model_and_utils(use_cuda=False, use_onnx=False):\n    model, utils = torch.hub.load(\n        repo_or_dir=\"snakers4/silero-vad\", model=\"silero_vad\", force_reload=True, onnx=use_onnx, force_onnx_cpu=True\n    )\n    if use_cuda:\n        model = model.cuda()\n    get_speech_timestamps, save_audio, _, _, collect_chunks = utils\n    return model, get_speech_timestamps, save_audio, collect_chunks\ndef remove_silence(\n    model_and_utils, audio_path, out_path, vad_sample_rate=8000, trim_just_beginning_and_end=True, use_cuda=False",
        "detail": "env.src.tts.TTS.utils.vad",
        "documentation": {}
    },
    {
        "label": "remove_silence",
        "kind": 2,
        "importPath": "env.src.tts.TTS.utils.vad",
        "description": "env.src.tts.TTS.utils.vad",
        "peekOfCode": "def remove_silence(\n    model_and_utils, audio_path, out_path, vad_sample_rate=8000, trim_just_beginning_and_end=True, use_cuda=False\n):\n    # get the VAD model and utils functions\n    model, get_speech_timestamps, _, collect_chunks = model_and_utils\n    # read ground truth wav and resample the audio for the VAD\n    try:\n        wav, gt_sample_rate = read_audio(audio_path)\n    except:\n        print(f\">  Failed to read {audio_path}\")",
        "detail": "env.src.tts.TTS.utils.vad",
        "documentation": {}
    },
    {
        "label": "BaseVCConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.configs.shared_configs",
        "description": "env.src.tts.TTS.vc.configs.shared_configs",
        "peekOfCode": "class BaseVCConfig(BaseTrainingConfig):\n    \"\"\"Shared parameters among all the tts models.\n    Args:\n        audio (BaseAudioConfig):\n            Audio processor config object instance.\n        batch_group_size (int):\n            Size of the batch groups used for bucketing. By default, the dataloader orders samples by the sequence\n            length for a more efficient and stable training. If `batch_group_size > 1` then it performs bucketing to\n            prevent using the same batches for each epoch.\n        loss_masking (bool):",
        "detail": "env.src.tts.TTS.vc.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseVC",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.models.base_vc",
        "description": "env.src.tts.TTS.vc.models.base_vc",
        "peekOfCode": "class BaseVC(BaseTrainerModel):\n    \"\"\"Base `vc` class. Every new `vc` model must inherit this.\n    It defines common `vc` specific functions on top of `Model` implementation.\n    \"\"\"\n    MODEL_TYPE = \"vc\"\n    def __init__(\n        self,\n        config: Coqpit,\n        ap: \"AudioProcessor\",\n        speaker_manager: SpeakerManager = None,",
        "detail": "env.src.tts.TTS.vc.models.base_vc",
        "documentation": {}
    },
    {
        "label": "ResidualCouplingBlock",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.models.freevc",
        "description": "env.src.tts.TTS.vc.models.freevc",
        "peekOfCode": "class ResidualCouplingBlock(nn.Module):\n    def __init__(self, channels, hidden_channels, kernel_size, dilation_rate, n_layers, n_flows=4, gin_channels=0):\n        super().__init__()\n        self.channels = channels\n        self.hidden_channels = hidden_channels\n        self.kernel_size = kernel_size\n        self.dilation_rate = dilation_rate\n        self.n_layers = n_layers\n        self.n_flows = n_flows\n        self.gin_channels = gin_channels",
        "detail": "env.src.tts.TTS.vc.models.freevc",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.models.freevc",
        "description": "env.src.tts.TTS.vc.models.freevc",
        "peekOfCode": "class Encoder(nn.Module):\n    def __init__(\n        self, in_channels, out_channels, hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=0\n    ):\n        super().__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.hidden_channels = hidden_channels\n        self.kernel_size = kernel_size\n        self.dilation_rate = dilation_rate",
        "detail": "env.src.tts.TTS.vc.models.freevc",
        "documentation": {}
    },
    {
        "label": "Generator",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.models.freevc",
        "description": "env.src.tts.TTS.vc.models.freevc",
        "peekOfCode": "class Generator(torch.nn.Module):\n    def __init__(\n        self,\n        initial_channel,\n        resblock,\n        resblock_kernel_sizes,\n        resblock_dilation_sizes,\n        upsample_rates,\n        upsample_initial_channel,\n        upsample_kernel_sizes,",
        "detail": "env.src.tts.TTS.vc.models.freevc",
        "documentation": {}
    },
    {
        "label": "DiscriminatorP",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.models.freevc",
        "description": "env.src.tts.TTS.vc.models.freevc",
        "peekOfCode": "class DiscriminatorP(torch.nn.Module):\n    def __init__(self, period, kernel_size=5, stride=3, use_spectral_norm=False):\n        super(DiscriminatorP, self).__init__()\n        self.period = period\n        self.use_spectral_norm = use_spectral_norm\n        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n        self.convs = nn.ModuleList(\n            [\n                norm_f(Conv2d(1, 32, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),\n                norm_f(Conv2d(32, 128, (kernel_size, 1), (stride, 1), padding=(get_padding(kernel_size, 1), 0))),",
        "detail": "env.src.tts.TTS.vc.models.freevc",
        "documentation": {}
    },
    {
        "label": "DiscriminatorS",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.models.freevc",
        "description": "env.src.tts.TTS.vc.models.freevc",
        "peekOfCode": "class DiscriminatorS(torch.nn.Module):\n    def __init__(self, use_spectral_norm=False):\n        super(DiscriminatorS, self).__init__()\n        norm_f = weight_norm if use_spectral_norm == False else spectral_norm\n        self.convs = nn.ModuleList(\n            [\n                norm_f(Conv1d(1, 16, 15, 1, padding=7)),\n                norm_f(Conv1d(16, 64, 41, 4, groups=4, padding=20)),\n                norm_f(Conv1d(64, 256, 41, 4, groups=16, padding=20)),\n                norm_f(Conv1d(256, 1024, 41, 4, groups=64, padding=20)),",
        "detail": "env.src.tts.TTS.vc.models.freevc",
        "documentation": {}
    },
    {
        "label": "MultiPeriodDiscriminator",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.models.freevc",
        "description": "env.src.tts.TTS.vc.models.freevc",
        "peekOfCode": "class MultiPeriodDiscriminator(torch.nn.Module):\n    def __init__(self, use_spectral_norm=False):\n        super(MultiPeriodDiscriminator, self).__init__()\n        periods = [2, 3, 5, 7, 11]\n        discs = [DiscriminatorS(use_spectral_norm=use_spectral_norm)]\n        discs = discs + [DiscriminatorP(i, use_spectral_norm=use_spectral_norm) for i in periods]\n        self.discriminators = nn.ModuleList(discs)\n    def forward(self, y, y_hat):\n        y_d_rs = []\n        y_d_gs = []",
        "detail": "env.src.tts.TTS.vc.models.freevc",
        "documentation": {}
    },
    {
        "label": "SpeakerEncoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.models.freevc",
        "description": "env.src.tts.TTS.vc.models.freevc",
        "peekOfCode": "class SpeakerEncoder(torch.nn.Module):\n    def __init__(self, mel_n_channels=80, model_num_layers=3, model_hidden_size=256, model_embedding_size=256):\n        super(SpeakerEncoder, self).__init__()\n        self.lstm = nn.LSTM(mel_n_channels, model_hidden_size, model_num_layers, batch_first=True)\n        self.linear = nn.Linear(model_hidden_size, model_embedding_size)\n        self.relu = nn.ReLU()\n    def forward(self, mels):\n        self.lstm.flatten_parameters()\n        _, (hidden, _) = self.lstm(mels)\n        embeds_raw = self.relu(self.linear(hidden[-1]))",
        "detail": "env.src.tts.TTS.vc.models.freevc",
        "documentation": {}
    },
    {
        "label": "FreeVCAudioConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.models.freevc",
        "description": "env.src.tts.TTS.vc.models.freevc",
        "peekOfCode": "class FreeVCAudioConfig(Coqpit):\n    \"\"\"Audio configuration\n    Args:\n        max_wav_value (float):\n            The maximum value of the waveform.\n        input_sample_rate (int):\n            The sampling rate of the input waveform.\n        output_sample_rate (int):\n            The sampling rate of the output waveform.\n        filter_length (int):",
        "detail": "env.src.tts.TTS.vc.models.freevc",
        "documentation": {}
    },
    {
        "label": "FreeVCArgs",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.models.freevc",
        "description": "env.src.tts.TTS.vc.models.freevc",
        "peekOfCode": "class FreeVCArgs(Coqpit):\n    \"\"\"FreeVC model arguments\n    Args:\n        spec_channels (int):\n            The number of channels in the spectrogram.\n        inter_channels (int):\n            The number of channels in the intermediate layers.\n        hidden_channels (int):\n            The number of channels in the hidden layers.\n        filter_channels (int):",
        "detail": "env.src.tts.TTS.vc.models.freevc",
        "documentation": {}
    },
    {
        "label": "FreeVC",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.models.freevc",
        "description": "env.src.tts.TTS.vc.models.freevc",
        "peekOfCode": "class FreeVC(BaseVC):\n    \"\"\"\n    Papaer::\n        https://arxiv.org/abs/2210.15418#\n    Paper Abstract::\n        Voice conversion (VC) can be achieved by first extracting source content information and target speaker\n        information, and then reconstructing waveform with these information. However, current approaches normally\n        either extract dirty content information with speaker information leaked in, or demand a large amount of\n        annotated data for training. Besides, the quality of reconstructed waveform can be degraded by the\n        mismatch between conversion model and vocoder. In this paper, we adopt the end-to-end framework of VITS for",
        "detail": "env.src.tts.TTS.vc.models.freevc",
        "documentation": {}
    },
    {
        "label": "FreeVCConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.models.freevc",
        "description": "env.src.tts.TTS.vc.models.freevc",
        "peekOfCode": "class FreeVCConfig(BaseVCConfig):\n    \"\"\"Defines parameters for FreeVC End2End TTS model.\n    Args:\n        model (str):\n            Model name. Do not change unless you know what you are doing.\n        model_args (FreeVCArgs):\n            Model architecture arguments. Defaults to `FreeVCArgs()`.\n        audio (FreeVCAudioConfig):\n            Audio processing configuration. Defaults to `FreeVCAudioConfig()`.\n        grad_clip (List):",
        "detail": "env.src.tts.TTS.vc.models.freevc",
        "documentation": {}
    },
    {
        "label": "preprocess_wav",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.audio",
        "description": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.audio",
        "peekOfCode": "def preprocess_wav(fpath_or_wav: Union[str, Path, np.ndarray], source_sr: Optional[int] = None):\n    \"\"\"\n    Applies the preprocessing operations used in training the Speaker Encoder to a waveform\n    either on disk or in memory. The waveform will be resampled to match the data hyperparameters.\n    :param fpath_or_wav: either a filepath to an audio file (many extensions are supported, not\n    just .wav), either the waveform as a numpy array of floats.\n    :param source_sr: if passing an audio waveform, the sampling rate of the waveform before\n    preprocessing. After preprocessing, the waveform's sampling rate will match the data\n    hyperparameters. If passing a filepath, the sampling rate will be automatically detected and\n    this argument will be ignored.",
        "detail": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.audio",
        "documentation": {}
    },
    {
        "label": "wav_to_mel_spectrogram",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.audio",
        "description": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.audio",
        "peekOfCode": "def wav_to_mel_spectrogram(wav):\n    \"\"\"\n    Derives a mel spectrogram ready to be used by the encoder from a preprocessed audio waveform.\n    Note: this not a log-mel spectrogram.\n    \"\"\"\n    frames = librosa.feature.melspectrogram(\n        y=wav,\n        sr=sampling_rate,\n        n_fft=int(sampling_rate * mel_window_length / 1000),\n        hop_length=int(sampling_rate * mel_window_step / 1000),",
        "detail": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.audio",
        "documentation": {}
    },
    {
        "label": "normalize_volume",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.audio",
        "description": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.audio",
        "peekOfCode": "def normalize_volume(wav, target_dBFS, increase_only=False, decrease_only=False):\n    if increase_only and decrease_only:\n        raise ValueError(\"Both increase only and decrease only are set\")\n    dBFS_change = target_dBFS - 10 * np.log10(np.mean(wav**2))\n    if (dBFS_change < 0 and increase_only) or (dBFS_change > 0 and decrease_only):\n        return wav\n    return wav * (10 ** (dBFS_change / 20))",
        "detail": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.audio",
        "documentation": {}
    },
    {
        "label": "int16_max",
        "kind": 5,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.audio",
        "description": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.audio",
        "peekOfCode": "int16_max = (2**15) - 1\ndef preprocess_wav(fpath_or_wav: Union[str, Path, np.ndarray], source_sr: Optional[int] = None):\n    \"\"\"\n    Applies the preprocessing operations used in training the Speaker Encoder to a waveform\n    either on disk or in memory. The waveform will be resampled to match the data hyperparameters.\n    :param fpath_or_wav: either a filepath to an audio file (many extensions are supported, not\n    just .wav), either the waveform as a numpy array of floats.\n    :param source_sr: if passing an audio waveform, the sampling rate of the waveform before\n    preprocessing. After preprocessing, the waveform's sampling rate will match the data\n    hyperparameters. If passing a filepath, the sampling rate will be automatically detected and",
        "detail": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.audio",
        "documentation": {}
    },
    {
        "label": "mel_window_length",
        "kind": 5,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "description": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "peekOfCode": "mel_window_length = 25  # In milliseconds\nmel_window_step = 10  # In milliseconds\nmel_n_channels = 40\n## Audio\nsampling_rate = 16000\n# Number of spectrogram frames in a partial utterance\npartials_n_frames = 160  # 1600 ms\n## Voice Activation Detection\n# Window size of the VAD. Must be either 10, 20 or 30 milliseconds.\n# This sets the granularity of the VAD. Should not need to be changed.",
        "detail": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "documentation": {}
    },
    {
        "label": "mel_window_step",
        "kind": 5,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "description": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "peekOfCode": "mel_window_step = 10  # In milliseconds\nmel_n_channels = 40\n## Audio\nsampling_rate = 16000\n# Number of spectrogram frames in a partial utterance\npartials_n_frames = 160  # 1600 ms\n## Voice Activation Detection\n# Window size of the VAD. Must be either 10, 20 or 30 milliseconds.\n# This sets the granularity of the VAD. Should not need to be changed.\nvad_window_length = 30  # In milliseconds",
        "detail": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "documentation": {}
    },
    {
        "label": "mel_n_channels",
        "kind": 5,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "description": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "peekOfCode": "mel_n_channels = 40\n## Audio\nsampling_rate = 16000\n# Number of spectrogram frames in a partial utterance\npartials_n_frames = 160  # 1600 ms\n## Voice Activation Detection\n# Window size of the VAD. Must be either 10, 20 or 30 milliseconds.\n# This sets the granularity of the VAD. Should not need to be changed.\nvad_window_length = 30  # In milliseconds\n# Number of frames to average together when performing the moving average smoothing.",
        "detail": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "documentation": {}
    },
    {
        "label": "sampling_rate",
        "kind": 5,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "description": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "peekOfCode": "sampling_rate = 16000\n# Number of spectrogram frames in a partial utterance\npartials_n_frames = 160  # 1600 ms\n## Voice Activation Detection\n# Window size of the VAD. Must be either 10, 20 or 30 milliseconds.\n# This sets the granularity of the VAD. Should not need to be changed.\nvad_window_length = 30  # In milliseconds\n# Number of frames to average together when performing the moving average smoothing.\n# The larger this value, the larger the VAD variations must be to not get smoothed out.\nvad_moving_average_width = 8",
        "detail": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "documentation": {}
    },
    {
        "label": "partials_n_frames",
        "kind": 5,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "description": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "peekOfCode": "partials_n_frames = 160  # 1600 ms\n## Voice Activation Detection\n# Window size of the VAD. Must be either 10, 20 or 30 milliseconds.\n# This sets the granularity of the VAD. Should not need to be changed.\nvad_window_length = 30  # In milliseconds\n# Number of frames to average together when performing the moving average smoothing.\n# The larger this value, the larger the VAD variations must be to not get smoothed out.\nvad_moving_average_width = 8\n# Maximum number of consecutive silent frames a segment can have.\nvad_max_silence_length = 6",
        "detail": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "documentation": {}
    },
    {
        "label": "vad_window_length",
        "kind": 5,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "description": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "peekOfCode": "vad_window_length = 30  # In milliseconds\n# Number of frames to average together when performing the moving average smoothing.\n# The larger this value, the larger the VAD variations must be to not get smoothed out.\nvad_moving_average_width = 8\n# Maximum number of consecutive silent frames a segment can have.\nvad_max_silence_length = 6\n## Audio volume normalization\naudio_norm_target_dBFS = -30\n## Model parameters\nmodel_hidden_size = 256",
        "detail": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "documentation": {}
    },
    {
        "label": "vad_moving_average_width",
        "kind": 5,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "description": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "peekOfCode": "vad_moving_average_width = 8\n# Maximum number of consecutive silent frames a segment can have.\nvad_max_silence_length = 6\n## Audio volume normalization\naudio_norm_target_dBFS = -30\n## Model parameters\nmodel_hidden_size = 256\nmodel_embedding_size = 256\nmodel_num_layers = 3",
        "detail": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "documentation": {}
    },
    {
        "label": "vad_max_silence_length",
        "kind": 5,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "description": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "peekOfCode": "vad_max_silence_length = 6\n## Audio volume normalization\naudio_norm_target_dBFS = -30\n## Model parameters\nmodel_hidden_size = 256\nmodel_embedding_size = 256\nmodel_num_layers = 3",
        "detail": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "documentation": {}
    },
    {
        "label": "audio_norm_target_dBFS",
        "kind": 5,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "description": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "peekOfCode": "audio_norm_target_dBFS = -30\n## Model parameters\nmodel_hidden_size = 256\nmodel_embedding_size = 256\nmodel_num_layers = 3",
        "detail": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "documentation": {}
    },
    {
        "label": "model_hidden_size",
        "kind": 5,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "description": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "peekOfCode": "model_hidden_size = 256\nmodel_embedding_size = 256\nmodel_num_layers = 3",
        "detail": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "documentation": {}
    },
    {
        "label": "model_embedding_size",
        "kind": 5,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "description": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "peekOfCode": "model_embedding_size = 256\nmodel_num_layers = 3",
        "detail": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "documentation": {}
    },
    {
        "label": "model_num_layers",
        "kind": 5,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "description": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "peekOfCode": "model_num_layers = 3",
        "detail": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.hparams",
        "documentation": {}
    },
    {
        "label": "SpeakerEncoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.speaker_encoder",
        "description": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.speaker_encoder",
        "peekOfCode": "class SpeakerEncoder(nn.Module):\n    def __init__(self, weights_fpath, device: Union[str, torch.device] = None, verbose=True):\n        \"\"\"\n        :param device: either a torch device or the name of a torch device (e.g. \"cpu\", \"cuda\").\n        If None, defaults to cuda if it is available on your machine, otherwise the model will\n        run on cpu. Outputs are always returned on the cpu, as numpy arrays.\n        \"\"\"\n        super().__init__()\n        # Define the network\n        self.lstm = nn.LSTM(mel_n_channels, model_hidden_size, model_num_layers, batch_first=True)",
        "detail": "env.src.tts.TTS.vc.modules.freevc.speaker_encoder.speaker_encoder",
        "documentation": {}
    },
    {
        "label": "TransposeLast",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "description": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "peekOfCode": "class TransposeLast(nn.Module):\n    def __init__(self, deconstruct_idx=None):\n        super().__init__()\n        self.deconstruct_idx = deconstruct_idx\n    def forward(self, x):\n        if self.deconstruct_idx is not None:\n            x = x[self.deconstruct_idx]\n        return x.transpose(-2, -1)\nclass Fp32LayerNorm(nn.LayerNorm):\n    def __init__(self, *args, **kwargs):",
        "detail": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "documentation": {}
    },
    {
        "label": "Fp32LayerNorm",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "description": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "peekOfCode": "class Fp32LayerNorm(nn.LayerNorm):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n    def forward(self, input):\n        output = F.layer_norm(\n            input.float(),\n            self.normalized_shape,\n            self.weight.float() if self.weight is not None else None,\n            self.bias.float() if self.bias is not None else None,\n            self.eps,",
        "detail": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "documentation": {}
    },
    {
        "label": "Fp32GroupNorm",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "description": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "peekOfCode": "class Fp32GroupNorm(nn.GroupNorm):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n    def forward(self, input):\n        output = F.group_norm(\n            input.float(),\n            self.num_groups,\n            self.weight.float() if self.weight is not None else None,\n            self.bias.float() if self.bias is not None else None,\n            self.eps,",
        "detail": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "documentation": {}
    },
    {
        "label": "GradMultiply",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "description": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "peekOfCode": "class GradMultiply(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x, scale):\n        ctx.scale = scale\n        res = x.new(x)\n        return res\n    @staticmethod\n    def backward(ctx, grad):\n        return grad * ctx.scale, None\nclass SamePad(nn.Module):",
        "detail": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "documentation": {}
    },
    {
        "label": "SamePad",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "description": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "peekOfCode": "class SamePad(nn.Module):\n    def __init__(self, kernel_size, causal=False):\n        super().__init__()\n        if causal:\n            self.remove = kernel_size - 1\n        else:\n            self.remove = 1 if kernel_size % 2 == 0 else 0\n    def forward(self, x):\n        if self.remove > 0:\n            x = x[:, :, : -self.remove]",
        "detail": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "documentation": {}
    },
    {
        "label": "Swish",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "description": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "peekOfCode": "class Swish(nn.Module):\n    \"\"\"Swish function\"\"\"\n    def __init__(self):\n        \"\"\"Construct an MultiHeadedAttention object.\"\"\"\n        super(Swish, self).__init__()\n        self.act = torch.nn.Sigmoid()\n    def forward(self, x):\n        return x * self.act(x)\nclass GLU_Linear(nn.Module):\n    def __init__(self, input_dim, output_dim, glu_type=\"sigmoid\", bias_in_glu=True):",
        "detail": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "documentation": {}
    },
    {
        "label": "GLU_Linear",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "description": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "peekOfCode": "class GLU_Linear(nn.Module):\n    def __init__(self, input_dim, output_dim, glu_type=\"sigmoid\", bias_in_glu=True):\n        super(GLU_Linear, self).__init__()\n        self.glu_type = glu_type\n        self.output_dim = output_dim\n        if glu_type == \"sigmoid\":\n            self.glu_act = torch.nn.Sigmoid()\n        elif glu_type == \"swish\":\n            self.glu_act = Swish()\n        elif glu_type == \"relu\":",
        "detail": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "documentation": {}
    },
    {
        "label": "MultiheadAttention",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "description": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "peekOfCode": "class MultiheadAttention(nn.Module):\n    \"\"\"Multi-headed attention.\n    See \"Attention Is All You Need\" for more details.\n    \"\"\"\n    def __init__(\n        self,\n        embed_dim,\n        num_heads,\n        kdim=None,\n        vdim=None,",
        "detail": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "documentation": {}
    },
    {
        "label": "gelu_accurate",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "description": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "peekOfCode": "def gelu_accurate(x):\n    if not hasattr(gelu_accurate, \"_a\"):\n        gelu_accurate._a = math.sqrt(2 / math.pi)\n    return 0.5 * x * (1 + torch.tanh(gelu_accurate._a * (x + 0.044715 * torch.pow(x, 3))))\ndef gelu(x: torch.Tensor) -> torch.Tensor:\n    return torch.nn.functional.gelu(x.float()).type_as(x)\ndef get_activation_fn(activation: str):\n    \"\"\"Returns the activation function corresponding to `activation`\"\"\"\n    if activation == \"relu\":\n        return F.relu",
        "detail": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "documentation": {}
    },
    {
        "label": "gelu",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "description": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "peekOfCode": "def gelu(x: torch.Tensor) -> torch.Tensor:\n    return torch.nn.functional.gelu(x.float()).type_as(x)\ndef get_activation_fn(activation: str):\n    \"\"\"Returns the activation function corresponding to `activation`\"\"\"\n    if activation == \"relu\":\n        return F.relu\n    elif activation == \"gelu\":\n        return gelu\n    elif activation == \"gelu_fast\":\n        warnings.warn(\"--activation-fn=gelu_fast has been renamed to gelu_accurate\")",
        "detail": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "documentation": {}
    },
    {
        "label": "get_activation_fn",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "description": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "peekOfCode": "def get_activation_fn(activation: str):\n    \"\"\"Returns the activation function corresponding to `activation`\"\"\"\n    if activation == \"relu\":\n        return F.relu\n    elif activation == \"gelu\":\n        return gelu\n    elif activation == \"gelu_fast\":\n        warnings.warn(\"--activation-fn=gelu_fast has been renamed to gelu_accurate\")\n        return gelu_accurate\n    elif activation == \"gelu_accurate\":",
        "detail": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "documentation": {}
    },
    {
        "label": "init_bert_params",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "description": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "peekOfCode": "def init_bert_params(module):\n    \"\"\"\n    Initialize the weights specific to the BERT Model.\n    This overrides the default initializations depending on the specified arguments.\n        1. If normal_init_linear_weights is set then weights of linear\n           layer will be initialized using the normal distribution and\n           bais will be set to the specified value.\n        2. If normal_init_embed_weights is set then weights of embedding\n           layer will be initialized using the normal distribution.\n        3. If normal_init_proj_weights is set then weights of",
        "detail": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "documentation": {}
    },
    {
        "label": "quant_noise",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "description": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "peekOfCode": "def quant_noise(module, p, block_size):\n    \"\"\"\n    Wraps modules and applies quantization noise to the weights for\n    subsequent quantization with Iterative Product Quantization as\n    described in \"Training with Quantization Noise for Extreme Model Compression\"\n    Args:\n        - module: nn.Module\n        - p: amount of Quantization Noise\n        - block_size: size of the blocks for subsequent quantization with iPQ\n    Remarks:",
        "detail": "env.src.tts.TTS.vc.modules.freevc.wavlm.modules",
        "documentation": {}
    },
    {
        "label": "WavLMConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.wavlm.wavlm",
        "description": "env.src.tts.TTS.vc.modules.freevc.wavlm.wavlm",
        "peekOfCode": "class WavLMConfig:\n    def __init__(self, cfg=None):\n        self.extractor_mode: str = \"default\"  # mode for feature extractor. default has a single group norm with d groups in the first conv block, whereas layer_norm has layer norms in every block (meant to use with normalize=True)\n        self.encoder_layers: int = 12  # num encoder layers in the transformer\n        self.encoder_embed_dim: int = 768  # encoder embedding dimension\n        self.encoder_ffn_embed_dim: int = 3072  # encoder embedding dimension for FFN\n        self.encoder_attention_heads: int = 12  # num encoder attention heads\n        self.activation_fn: str = \"gelu\"  # activation function to use\n        self.layer_norm_first: bool = False  # apply layernorm first in the transformer\n        self.conv_feature_layers: str = \"[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2\"  # string describing convolutional feature extraction layers in form of a python list that contains [(dim, kernel_size, stride), ...]",
        "detail": "env.src.tts.TTS.vc.modules.freevc.wavlm.wavlm",
        "documentation": {}
    },
    {
        "label": "WavLM",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.wavlm.wavlm",
        "description": "env.src.tts.TTS.vc.modules.freevc.wavlm.wavlm",
        "peekOfCode": "class WavLM(nn.Module):\n    def __init__(\n        self,\n        cfg: WavLMConfig,\n    ) -> None:\n        super().__init__()\n        logger.info(f\"WavLM Config: {cfg.__dict__}\")\n        self.cfg = cfg\n        feature_enc_layers = eval(cfg.conv_feature_layers)\n        self.embed = feature_enc_layers[-1][0]",
        "detail": "env.src.tts.TTS.vc.modules.freevc.wavlm.wavlm",
        "documentation": {}
    },
    {
        "label": "ConvFeatureExtractionModel",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.wavlm.wavlm",
        "description": "env.src.tts.TTS.vc.modules.freevc.wavlm.wavlm",
        "peekOfCode": "class ConvFeatureExtractionModel(nn.Module):\n    def __init__(\n        self,\n        conv_layers: List[Tuple[int, int, int]],\n        dropout: float = 0.0,\n        mode: str = \"default\",\n        conv_bias: bool = False,\n        conv_type: str = \"default\",\n    ):\n        super().__init__()",
        "detail": "env.src.tts.TTS.vc.modules.freevc.wavlm.wavlm",
        "documentation": {}
    },
    {
        "label": "TransformerEncoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.wavlm.wavlm",
        "description": "env.src.tts.TTS.vc.modules.freevc.wavlm.wavlm",
        "peekOfCode": "class TransformerEncoder(nn.Module):\n    def __init__(self, args):\n        super().__init__()\n        self.dropout = args.dropout\n        self.embedding_dim = args.encoder_embed_dim\n        self.pos_conv = nn.Conv1d(\n            self.embedding_dim,\n            self.embedding_dim,\n            kernel_size=args.conv_pos,\n            padding=args.conv_pos // 2,",
        "detail": "env.src.tts.TTS.vc.modules.freevc.wavlm.wavlm",
        "documentation": {}
    },
    {
        "label": "TransformerSentenceEncoderLayer",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.wavlm.wavlm",
        "description": "env.src.tts.TTS.vc.modules.freevc.wavlm.wavlm",
        "peekOfCode": "class TransformerSentenceEncoderLayer(nn.Module):\n    \"\"\"\n    Implements a Transformer Encoder Layer used in BERT/XLM style pre-trained\n    models.\n    \"\"\"\n    def __init__(\n        self,\n        embedding_dim: float = 768,\n        ffn_embedding_dim: float = 3072,\n        num_attention_heads: float = 8,",
        "detail": "env.src.tts.TTS.vc.modules.freevc.wavlm.wavlm",
        "documentation": {}
    },
    {
        "label": "compute_mask_indices",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.wavlm.wavlm",
        "description": "env.src.tts.TTS.vc.modules.freevc.wavlm.wavlm",
        "peekOfCode": "def compute_mask_indices(\n    shape: Tuple[int, int],\n    padding_mask: Optional[torch.Tensor],\n    mask_prob: float,\n    mask_length: int,\n    mask_type: str = \"static\",\n    mask_other: float = 0.0,\n    min_masks: int = 0,\n    no_overlap: bool = False,\n    min_space: int = 0,",
        "detail": "env.src.tts.TTS.vc.modules.freevc.wavlm.wavlm",
        "documentation": {}
    },
    {
        "label": "logger",
        "kind": 5,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.wavlm.wavlm",
        "description": "env.src.tts.TTS.vc.modules.freevc.wavlm.wavlm",
        "peekOfCode": "logger = logging.getLogger(__name__)\ndef compute_mask_indices(\n    shape: Tuple[int, int],\n    padding_mask: Optional[torch.Tensor],\n    mask_prob: float,\n    mask_length: int,\n    mask_type: str = \"static\",\n    mask_other: float = 0.0,\n    min_masks: int = 0,\n    no_overlap: bool = False,",
        "detail": "env.src.tts.TTS.vc.modules.freevc.wavlm.wavlm",
        "documentation": {}
    },
    {
        "label": "init_weights",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.commons",
        "description": "env.src.tts.TTS.vc.modules.freevc.commons",
        "peekOfCode": "def init_weights(m, mean=0.0, std=0.01):\n    classname = m.__class__.__name__\n    if classname.find(\"Conv\") != -1:\n        m.weight.data.normal_(mean, std)\ndef get_padding(kernel_size, dilation=1):\n    return int((kernel_size * dilation - dilation) / 2)\ndef convert_pad_shape(pad_shape):\n    l = pad_shape[::-1]\n    pad_shape = [item for sublist in l for item in sublist]\n    return pad_shape",
        "detail": "env.src.tts.TTS.vc.modules.freevc.commons",
        "documentation": {}
    },
    {
        "label": "get_padding",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.commons",
        "description": "env.src.tts.TTS.vc.modules.freevc.commons",
        "peekOfCode": "def get_padding(kernel_size, dilation=1):\n    return int((kernel_size * dilation - dilation) / 2)\ndef convert_pad_shape(pad_shape):\n    l = pad_shape[::-1]\n    pad_shape = [item for sublist in l for item in sublist]\n    return pad_shape\ndef intersperse(lst, item):\n    result = [item] * (len(lst) * 2 + 1)\n    result[1::2] = lst\n    return result",
        "detail": "env.src.tts.TTS.vc.modules.freevc.commons",
        "documentation": {}
    },
    {
        "label": "convert_pad_shape",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.commons",
        "description": "env.src.tts.TTS.vc.modules.freevc.commons",
        "peekOfCode": "def convert_pad_shape(pad_shape):\n    l = pad_shape[::-1]\n    pad_shape = [item for sublist in l for item in sublist]\n    return pad_shape\ndef intersperse(lst, item):\n    result = [item] * (len(lst) * 2 + 1)\n    result[1::2] = lst\n    return result\ndef kl_divergence(m_p, logs_p, m_q, logs_q):\n    \"\"\"KL(P||Q)\"\"\"",
        "detail": "env.src.tts.TTS.vc.modules.freevc.commons",
        "documentation": {}
    },
    {
        "label": "intersperse",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.commons",
        "description": "env.src.tts.TTS.vc.modules.freevc.commons",
        "peekOfCode": "def intersperse(lst, item):\n    result = [item] * (len(lst) * 2 + 1)\n    result[1::2] = lst\n    return result\ndef kl_divergence(m_p, logs_p, m_q, logs_q):\n    \"\"\"KL(P||Q)\"\"\"\n    kl = (logs_q - logs_p) - 0.5\n    kl += 0.5 * (torch.exp(2.0 * logs_p) + ((m_p - m_q) ** 2)) * torch.exp(-2.0 * logs_q)\n    return kl\ndef rand_gumbel(shape):",
        "detail": "env.src.tts.TTS.vc.modules.freevc.commons",
        "documentation": {}
    },
    {
        "label": "kl_divergence",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.commons",
        "description": "env.src.tts.TTS.vc.modules.freevc.commons",
        "peekOfCode": "def kl_divergence(m_p, logs_p, m_q, logs_q):\n    \"\"\"KL(P||Q)\"\"\"\n    kl = (logs_q - logs_p) - 0.5\n    kl += 0.5 * (torch.exp(2.0 * logs_p) + ((m_p - m_q) ** 2)) * torch.exp(-2.0 * logs_q)\n    return kl\ndef rand_gumbel(shape):\n    \"\"\"Sample from the Gumbel distribution, protect from overflows.\"\"\"\n    uniform_samples = torch.rand(shape) * 0.99998 + 0.00001\n    return -torch.log(-torch.log(uniform_samples))\ndef rand_gumbel_like(x):",
        "detail": "env.src.tts.TTS.vc.modules.freevc.commons",
        "documentation": {}
    },
    {
        "label": "rand_gumbel",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.commons",
        "description": "env.src.tts.TTS.vc.modules.freevc.commons",
        "peekOfCode": "def rand_gumbel(shape):\n    \"\"\"Sample from the Gumbel distribution, protect from overflows.\"\"\"\n    uniform_samples = torch.rand(shape) * 0.99998 + 0.00001\n    return -torch.log(-torch.log(uniform_samples))\ndef rand_gumbel_like(x):\n    g = rand_gumbel(x.size()).to(dtype=x.dtype, device=x.device)\n    return g\ndef slice_segments(x, ids_str, segment_size=4):\n    ret = torch.zeros_like(x[:, :, :segment_size])\n    for i in range(x.size(0)):",
        "detail": "env.src.tts.TTS.vc.modules.freevc.commons",
        "documentation": {}
    },
    {
        "label": "rand_gumbel_like",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.commons",
        "description": "env.src.tts.TTS.vc.modules.freevc.commons",
        "peekOfCode": "def rand_gumbel_like(x):\n    g = rand_gumbel(x.size()).to(dtype=x.dtype, device=x.device)\n    return g\ndef slice_segments(x, ids_str, segment_size=4):\n    ret = torch.zeros_like(x[:, :, :segment_size])\n    for i in range(x.size(0)):\n        idx_str = ids_str[i]\n        idx_end = idx_str + segment_size\n        ret[i] = x[i, :, idx_str:idx_end]\n    return ret",
        "detail": "env.src.tts.TTS.vc.modules.freevc.commons",
        "documentation": {}
    },
    {
        "label": "slice_segments",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.commons",
        "description": "env.src.tts.TTS.vc.modules.freevc.commons",
        "peekOfCode": "def slice_segments(x, ids_str, segment_size=4):\n    ret = torch.zeros_like(x[:, :, :segment_size])\n    for i in range(x.size(0)):\n        idx_str = ids_str[i]\n        idx_end = idx_str + segment_size\n        ret[i] = x[i, :, idx_str:idx_end]\n    return ret\ndef rand_slice_segments(x, x_lengths=None, segment_size=4):\n    b, d, t = x.size()\n    if x_lengths is None:",
        "detail": "env.src.tts.TTS.vc.modules.freevc.commons",
        "documentation": {}
    },
    {
        "label": "rand_slice_segments",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.commons",
        "description": "env.src.tts.TTS.vc.modules.freevc.commons",
        "peekOfCode": "def rand_slice_segments(x, x_lengths=None, segment_size=4):\n    b, d, t = x.size()\n    if x_lengths is None:\n        x_lengths = t\n    ids_str_max = x_lengths - segment_size + 1\n    ids_str = (torch.rand([b]).to(device=x.device) * ids_str_max).to(dtype=torch.long)\n    ret = slice_segments(x, ids_str, segment_size)\n    return ret, ids_str\ndef rand_spec_segments(x, x_lengths=None, segment_size=4):\n    b, d, t = x.size()",
        "detail": "env.src.tts.TTS.vc.modules.freevc.commons",
        "documentation": {}
    },
    {
        "label": "rand_spec_segments",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.commons",
        "description": "env.src.tts.TTS.vc.modules.freevc.commons",
        "peekOfCode": "def rand_spec_segments(x, x_lengths=None, segment_size=4):\n    b, d, t = x.size()\n    if x_lengths is None:\n        x_lengths = t\n    ids_str_max = x_lengths - segment_size\n    ids_str = (torch.rand([b]).to(device=x.device) * ids_str_max).to(dtype=torch.long)\n    ret = slice_segments(x, ids_str, segment_size)\n    return ret, ids_str\ndef get_timing_signal_1d(length, channels, min_timescale=1.0, max_timescale=1.0e4):\n    position = torch.arange(length, dtype=torch.float)",
        "detail": "env.src.tts.TTS.vc.modules.freevc.commons",
        "documentation": {}
    },
    {
        "label": "get_timing_signal_1d",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.commons",
        "description": "env.src.tts.TTS.vc.modules.freevc.commons",
        "peekOfCode": "def get_timing_signal_1d(length, channels, min_timescale=1.0, max_timescale=1.0e4):\n    position = torch.arange(length, dtype=torch.float)\n    num_timescales = channels // 2\n    log_timescale_increment = math.log(float(max_timescale) / float(min_timescale)) / (num_timescales - 1)\n    inv_timescales = min_timescale * torch.exp(\n        torch.arange(num_timescales, dtype=torch.float) * -log_timescale_increment\n    )\n    scaled_time = position.unsqueeze(0) * inv_timescales.unsqueeze(1)\n    signal = torch.cat([torch.sin(scaled_time), torch.cos(scaled_time)], 0)\n    signal = F.pad(signal, [0, 0, 0, channels % 2])",
        "detail": "env.src.tts.TTS.vc.modules.freevc.commons",
        "documentation": {}
    },
    {
        "label": "add_timing_signal_1d",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.commons",
        "description": "env.src.tts.TTS.vc.modules.freevc.commons",
        "peekOfCode": "def add_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4):\n    b, channels, length = x.size()\n    signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale)\n    return x + signal.to(dtype=x.dtype, device=x.device)\ndef cat_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4, axis=1):\n    b, channels, length = x.size()\n    signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale)\n    return torch.cat([x, signal.to(dtype=x.dtype, device=x.device)], axis)\ndef subsequent_mask(length):\n    mask = torch.tril(torch.ones(length, length)).unsqueeze(0).unsqueeze(0)",
        "detail": "env.src.tts.TTS.vc.modules.freevc.commons",
        "documentation": {}
    },
    {
        "label": "cat_timing_signal_1d",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.commons",
        "description": "env.src.tts.TTS.vc.modules.freevc.commons",
        "peekOfCode": "def cat_timing_signal_1d(x, min_timescale=1.0, max_timescale=1.0e4, axis=1):\n    b, channels, length = x.size()\n    signal = get_timing_signal_1d(length, channels, min_timescale, max_timescale)\n    return torch.cat([x, signal.to(dtype=x.dtype, device=x.device)], axis)\ndef subsequent_mask(length):\n    mask = torch.tril(torch.ones(length, length)).unsqueeze(0).unsqueeze(0)\n    return mask\n@torch.jit.script\ndef fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):\n    n_channels_int = n_channels[0]",
        "detail": "env.src.tts.TTS.vc.modules.freevc.commons",
        "documentation": {}
    },
    {
        "label": "subsequent_mask",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.commons",
        "description": "env.src.tts.TTS.vc.modules.freevc.commons",
        "peekOfCode": "def subsequent_mask(length):\n    mask = torch.tril(torch.ones(length, length)).unsqueeze(0).unsqueeze(0)\n    return mask\n@torch.jit.script\ndef fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):\n    n_channels_int = n_channels[0]\n    in_act = input_a + input_b\n    t_act = torch.tanh(in_act[:, :n_channels_int, :])\n    s_act = torch.sigmoid(in_act[:, n_channels_int:, :])\n    acts = t_act * s_act",
        "detail": "env.src.tts.TTS.vc.modules.freevc.commons",
        "documentation": {}
    },
    {
        "label": "fused_add_tanh_sigmoid_multiply",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.commons",
        "description": "env.src.tts.TTS.vc.modules.freevc.commons",
        "peekOfCode": "def fused_add_tanh_sigmoid_multiply(input_a, input_b, n_channels):\n    n_channels_int = n_channels[0]\n    in_act = input_a + input_b\n    t_act = torch.tanh(in_act[:, :n_channels_int, :])\n    s_act = torch.sigmoid(in_act[:, n_channels_int:, :])\n    acts = t_act * s_act\n    return acts\ndef convert_pad_shape(pad_shape):\n    l = pad_shape[::-1]\n    pad_shape = [item for sublist in l for item in sublist]",
        "detail": "env.src.tts.TTS.vc.modules.freevc.commons",
        "documentation": {}
    },
    {
        "label": "convert_pad_shape",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.commons",
        "description": "env.src.tts.TTS.vc.modules.freevc.commons",
        "peekOfCode": "def convert_pad_shape(pad_shape):\n    l = pad_shape[::-1]\n    pad_shape = [item for sublist in l for item in sublist]\n    return pad_shape\ndef shift_1d(x):\n    x = F.pad(x, convert_pad_shape([[0, 0], [0, 0], [1, 0]]))[:, :, :-1]\n    return x\ndef sequence_mask(length, max_length=None):\n    if max_length is None:\n        max_length = length.max()",
        "detail": "env.src.tts.TTS.vc.modules.freevc.commons",
        "documentation": {}
    },
    {
        "label": "shift_1d",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.commons",
        "description": "env.src.tts.TTS.vc.modules.freevc.commons",
        "peekOfCode": "def shift_1d(x):\n    x = F.pad(x, convert_pad_shape([[0, 0], [0, 0], [1, 0]]))[:, :, :-1]\n    return x\ndef sequence_mask(length, max_length=None):\n    if max_length is None:\n        max_length = length.max()\n    x = torch.arange(max_length, dtype=length.dtype, device=length.device)\n    return x.unsqueeze(0) < length.unsqueeze(1)\ndef generate_path(duration, mask):\n    \"\"\"",
        "detail": "env.src.tts.TTS.vc.modules.freevc.commons",
        "documentation": {}
    },
    {
        "label": "sequence_mask",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.commons",
        "description": "env.src.tts.TTS.vc.modules.freevc.commons",
        "peekOfCode": "def sequence_mask(length, max_length=None):\n    if max_length is None:\n        max_length = length.max()\n    x = torch.arange(max_length, dtype=length.dtype, device=length.device)\n    return x.unsqueeze(0) < length.unsqueeze(1)\ndef generate_path(duration, mask):\n    \"\"\"\n    duration: [b, 1, t_x]\n    mask: [b, 1, t_y, t_x]\n    \"\"\"",
        "detail": "env.src.tts.TTS.vc.modules.freevc.commons",
        "documentation": {}
    },
    {
        "label": "generate_path",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.commons",
        "description": "env.src.tts.TTS.vc.modules.freevc.commons",
        "peekOfCode": "def generate_path(duration, mask):\n    \"\"\"\n    duration: [b, 1, t_x]\n    mask: [b, 1, t_y, t_x]\n    \"\"\"\n    device = duration.device\n    b, _, t_y, t_x = mask.shape\n    cum_duration = torch.cumsum(duration, -1)\n    cum_duration_flat = cum_duration.view(b * t_x)\n    path = sequence_mask(cum_duration_flat, t_y).to(mask.dtype)",
        "detail": "env.src.tts.TTS.vc.modules.freevc.commons",
        "documentation": {}
    },
    {
        "label": "clip_grad_value_",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.commons",
        "description": "env.src.tts.TTS.vc.modules.freevc.commons",
        "peekOfCode": "def clip_grad_value_(parameters, clip_value, norm_type=2):\n    if isinstance(parameters, torch.Tensor):\n        parameters = [parameters]\n    parameters = list(filter(lambda p: p.grad is not None, parameters))\n    norm_type = float(norm_type)\n    if clip_value is not None:\n        clip_value = float(clip_value)\n    total_norm = 0\n    for p in parameters:\n        param_norm = p.grad.data.norm(norm_type)",
        "detail": "env.src.tts.TTS.vc.modules.freevc.commons",
        "documentation": {}
    },
    {
        "label": "dynamic_range_compression_torch",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "description": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "peekOfCode": "def dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\n    \"\"\"\n    PARAMS\n    ------\n    C: compression factor\n    \"\"\"\n    return torch.log(torch.clamp(x, min=clip_val) * C)\ndef dynamic_range_decompression_torch(x, C=1):\n    \"\"\"\n    PARAMS",
        "detail": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "documentation": {}
    },
    {
        "label": "dynamic_range_decompression_torch",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "description": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "peekOfCode": "def dynamic_range_decompression_torch(x, C=1):\n    \"\"\"\n    PARAMS\n    ------\n    C: compression factor used to compress\n    \"\"\"\n    return torch.exp(x) / C\ndef spectral_normalize_torch(magnitudes):\n    output = dynamic_range_compression_torch(magnitudes)\n    return output",
        "detail": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "documentation": {}
    },
    {
        "label": "spectral_normalize_torch",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "description": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "peekOfCode": "def spectral_normalize_torch(magnitudes):\n    output = dynamic_range_compression_torch(magnitudes)\n    return output\ndef spectral_de_normalize_torch(magnitudes):\n    output = dynamic_range_decompression_torch(magnitudes)\n    return output\nmel_basis = {}\nhann_window = {}\ndef spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n    if torch.min(y) < -1.0:",
        "detail": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "documentation": {}
    },
    {
        "label": "spectral_de_normalize_torch",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "description": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "peekOfCode": "def spectral_de_normalize_torch(magnitudes):\n    output = dynamic_range_decompression_torch(magnitudes)\n    return output\nmel_basis = {}\nhann_window = {}\ndef spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n    if torch.min(y) < -1.0:\n        print(\"min value is \", torch.min(y))\n    if torch.max(y) > 1.0:\n        print(\"max value is \", torch.max(y))",
        "detail": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "documentation": {}
    },
    {
        "label": "spectrogram_torch",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "description": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "peekOfCode": "def spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n    if torch.min(y) < -1.0:\n        print(\"min value is \", torch.min(y))\n    if torch.max(y) > 1.0:\n        print(\"max value is \", torch.max(y))\n    global hann_window\n    dtype_device = str(y.dtype) + \"_\" + str(y.device)\n    wnsize_dtype_device = str(win_size) + \"_\" + dtype_device\n    if wnsize_dtype_device not in hann_window:\n        hann_window[wnsize_dtype_device] = torch.hann_window(win_size).to(dtype=y.dtype, device=y.device)",
        "detail": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "documentation": {}
    },
    {
        "label": "spec_to_mel_torch",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "description": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "peekOfCode": "def spec_to_mel_torch(spec, n_fft, num_mels, sampling_rate, fmin, fmax):\n    global mel_basis\n    dtype_device = str(spec.dtype) + \"_\" + str(spec.device)\n    fmax_dtype_device = str(fmax) + \"_\" + dtype_device\n    if fmax_dtype_device not in mel_basis:\n        mel = librosa_mel_fn(sr=sampling_rate, n_fft=n_fft, n_mels=num_mels, fmin=fmin, fmax=fmax)\n        mel_basis[fmax_dtype_device] = torch.from_numpy(mel).to(dtype=spec.dtype, device=spec.device)\n    spec = torch.matmul(mel_basis[fmax_dtype_device], spec)\n    spec = spectral_normalize_torch(spec)\n    return spec",
        "detail": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "documentation": {}
    },
    {
        "label": "mel_spectrogram_torch",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "description": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "peekOfCode": "def mel_spectrogram_torch(y, n_fft, num_mels, sampling_rate, hop_size, win_size, fmin, fmax, center=False):\n    if torch.min(y) < -1.0:\n        print(\"min value is \", torch.min(y))\n    if torch.max(y) > 1.0:\n        print(\"max value is \", torch.max(y))\n    global mel_basis, hann_window\n    dtype_device = str(y.dtype) + \"_\" + str(y.device)\n    fmax_dtype_device = str(fmax) + \"_\" + dtype_device\n    wnsize_dtype_device = str(win_size) + \"_\" + dtype_device\n    if fmax_dtype_device not in mel_basis:",
        "detail": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "documentation": {}
    },
    {
        "label": "MAX_WAV_VALUE",
        "kind": 5,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "description": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "peekOfCode": "MAX_WAV_VALUE = 32768.0\ndef dynamic_range_compression_torch(x, C=1, clip_val=1e-5):\n    \"\"\"\n    PARAMS\n    ------\n    C: compression factor\n    \"\"\"\n    return torch.log(torch.clamp(x, min=clip_val) * C)\ndef dynamic_range_decompression_torch(x, C=1):\n    \"\"\"",
        "detail": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "documentation": {}
    },
    {
        "label": "mel_basis",
        "kind": 5,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "description": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "peekOfCode": "mel_basis = {}\nhann_window = {}\ndef spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n    if torch.min(y) < -1.0:\n        print(\"min value is \", torch.min(y))\n    if torch.max(y) > 1.0:\n        print(\"max value is \", torch.max(y))\n    global hann_window\n    dtype_device = str(y.dtype) + \"_\" + str(y.device)\n    wnsize_dtype_device = str(win_size) + \"_\" + dtype_device",
        "detail": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "documentation": {}
    },
    {
        "label": "hann_window",
        "kind": 5,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "description": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "peekOfCode": "hann_window = {}\ndef spectrogram_torch(y, n_fft, sampling_rate, hop_size, win_size, center=False):\n    if torch.min(y) < -1.0:\n        print(\"min value is \", torch.min(y))\n    if torch.max(y) > 1.0:\n        print(\"max value is \", torch.max(y))\n    global hann_window\n    dtype_device = str(y.dtype) + \"_\" + str(y.device)\n    wnsize_dtype_device = str(win_size) + \"_\" + dtype_device\n    if wnsize_dtype_device not in hann_window:",
        "detail": "env.src.tts.TTS.vc.modules.freevc.mel_processing",
        "documentation": {}
    },
    {
        "label": "LayerNorm",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.modules",
        "description": "env.src.tts.TTS.vc.modules.freevc.modules",
        "peekOfCode": "class LayerNorm(nn.Module):\n    def __init__(self, channels, eps=1e-5):\n        super().__init__()\n        self.channels = channels\n        self.eps = eps\n        self.gamma = nn.Parameter(torch.ones(channels))\n        self.beta = nn.Parameter(torch.zeros(channels))\n    def forward(self, x):\n        x = x.transpose(1, -1)\n        x = F.layer_norm(x, (self.channels,), self.gamma, self.beta, self.eps)",
        "detail": "env.src.tts.TTS.vc.modules.freevc.modules",
        "documentation": {}
    },
    {
        "label": "ConvReluNorm",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.modules",
        "description": "env.src.tts.TTS.vc.modules.freevc.modules",
        "peekOfCode": "class ConvReluNorm(nn.Module):\n    def __init__(self, in_channels, hidden_channels, out_channels, kernel_size, n_layers, p_dropout):\n        super().__init__()\n        self.in_channels = in_channels\n        self.hidden_channels = hidden_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.n_layers = n_layers\n        self.p_dropout = p_dropout\n        assert n_layers > 1, \"Number of layers should be larger than 0.\"",
        "detail": "env.src.tts.TTS.vc.modules.freevc.modules",
        "documentation": {}
    },
    {
        "label": "DDSConv",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.modules",
        "description": "env.src.tts.TTS.vc.modules.freevc.modules",
        "peekOfCode": "class DDSConv(nn.Module):\n    \"\"\"\n    Dialted and Depth-Separable Convolution\n    \"\"\"\n    def __init__(self, channels, kernel_size, n_layers, p_dropout=0.0):\n        super().__init__()\n        self.channels = channels\n        self.kernel_size = kernel_size\n        self.n_layers = n_layers\n        self.p_dropout = p_dropout",
        "detail": "env.src.tts.TTS.vc.modules.freevc.modules",
        "documentation": {}
    },
    {
        "label": "WN",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.modules",
        "description": "env.src.tts.TTS.vc.modules.freevc.modules",
        "peekOfCode": "class WN(torch.nn.Module):\n    def __init__(self, hidden_channels, kernel_size, dilation_rate, n_layers, gin_channels=0, p_dropout=0):\n        super(WN, self).__init__()\n        assert kernel_size % 2 == 1\n        self.hidden_channels = hidden_channels\n        self.kernel_size = (kernel_size,)\n        self.dilation_rate = dilation_rate\n        self.n_layers = n_layers\n        self.gin_channels = gin_channels\n        self.p_dropout = p_dropout",
        "detail": "env.src.tts.TTS.vc.modules.freevc.modules",
        "documentation": {}
    },
    {
        "label": "ResBlock1",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.modules",
        "description": "env.src.tts.TTS.vc.modules.freevc.modules",
        "peekOfCode": "class ResBlock1(torch.nn.Module):\n    def __init__(self, channels, kernel_size=3, dilation=(1, 3, 5)):\n        super(ResBlock1, self).__init__()\n        self.convs1 = nn.ModuleList(\n            [\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,",
        "detail": "env.src.tts.TTS.vc.modules.freevc.modules",
        "documentation": {}
    },
    {
        "label": "ResBlock2",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.modules",
        "description": "env.src.tts.TTS.vc.modules.freevc.modules",
        "peekOfCode": "class ResBlock2(torch.nn.Module):\n    def __init__(self, channels, kernel_size=3, dilation=(1, 3)):\n        super(ResBlock2, self).__init__()\n        self.convs = nn.ModuleList(\n            [\n                weight_norm(\n                    Conv1d(\n                        channels,\n                        channels,\n                        kernel_size,",
        "detail": "env.src.tts.TTS.vc.modules.freevc.modules",
        "documentation": {}
    },
    {
        "label": "Log",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.modules",
        "description": "env.src.tts.TTS.vc.modules.freevc.modules",
        "peekOfCode": "class Log(nn.Module):\n    def forward(self, x, x_mask, reverse=False, **kwargs):\n        if not reverse:\n            y = torch.log(torch.clamp_min(x, 1e-5)) * x_mask\n            logdet = torch.sum(-y, [1, 2])\n            return y, logdet\n        else:\n            x = torch.exp(x) * x_mask\n            return x\nclass Flip(nn.Module):",
        "detail": "env.src.tts.TTS.vc.modules.freevc.modules",
        "documentation": {}
    },
    {
        "label": "Flip",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.modules",
        "description": "env.src.tts.TTS.vc.modules.freevc.modules",
        "peekOfCode": "class Flip(nn.Module):\n    def forward(self, x, *args, reverse=False, **kwargs):\n        x = torch.flip(x, [1])\n        if not reverse:\n            logdet = torch.zeros(x.size(0)).to(dtype=x.dtype, device=x.device)\n            return x, logdet\n        else:\n            return x\nclass ElementwiseAffine(nn.Module):\n    def __init__(self, channels):",
        "detail": "env.src.tts.TTS.vc.modules.freevc.modules",
        "documentation": {}
    },
    {
        "label": "ElementwiseAffine",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.modules",
        "description": "env.src.tts.TTS.vc.modules.freevc.modules",
        "peekOfCode": "class ElementwiseAffine(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.channels = channels\n        self.m = nn.Parameter(torch.zeros(channels, 1))\n        self.logs = nn.Parameter(torch.zeros(channels, 1))\n    def forward(self, x, x_mask, reverse=False, **kwargs):\n        if not reverse:\n            y = self.m + torch.exp(self.logs) * x\n            y = y * x_mask",
        "detail": "env.src.tts.TTS.vc.modules.freevc.modules",
        "documentation": {}
    },
    {
        "label": "ResidualCouplingLayer",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.modules",
        "description": "env.src.tts.TTS.vc.modules.freevc.modules",
        "peekOfCode": "class ResidualCouplingLayer(nn.Module):\n    def __init__(\n        self,\n        channels,\n        hidden_channels,\n        kernel_size,\n        dilation_rate,\n        n_layers,\n        p_dropout=0,\n        gin_channels=0,",
        "detail": "env.src.tts.TTS.vc.modules.freevc.modules",
        "documentation": {}
    },
    {
        "label": "LRELU_SLOPE",
        "kind": 5,
        "importPath": "env.src.tts.TTS.vc.modules.freevc.modules",
        "description": "env.src.tts.TTS.vc.modules.freevc.modules",
        "peekOfCode": "LRELU_SLOPE = 0.1\nclass LayerNorm(nn.Module):\n    def __init__(self, channels, eps=1e-5):\n        super().__init__()\n        self.channels = channels\n        self.eps = eps\n        self.gamma = nn.Parameter(torch.ones(channels))\n        self.beta = nn.Parameter(torch.zeros(channels))\n    def forward(self, x):\n        x = x.transpose(1, -1)",
        "detail": "env.src.tts.TTS.vc.modules.freevc.modules",
        "documentation": {}
    },
    {
        "label": "FullbandMelganConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.configs.fullband_melgan_config",
        "description": "env.src.tts.TTS.vocoder.configs.fullband_melgan_config",
        "peekOfCode": "class FullbandMelganConfig(BaseGANVocoderConfig):\n    \"\"\"Defines parameters for FullBand MelGAN vocoder.\n    Example:\n        >>> from TTS.vocoder.configs import FullbandMelganConfig\n        >>> config = FullbandMelganConfig()\n    Args:\n        model (str):\n            Model name used for selecting the right model at initialization. Defaults to `fullband_melgan`.\n        discriminator_model (str): One of the discriminators from `TTS.vocoder.models.*_discriminator`. Defaults to\n            'melgan_multiscale_discriminator`.",
        "detail": "env.src.tts.TTS.vocoder.configs.fullband_melgan_config",
        "documentation": {}
    },
    {
        "label": "HifiganConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.configs.hifigan_config",
        "description": "env.src.tts.TTS.vocoder.configs.hifigan_config",
        "peekOfCode": "class HifiganConfig(BaseGANVocoderConfig):\n    \"\"\"Defines parameters for FullBand MelGAN vocoder.\n    Example:\n        >>> from TTS.vocoder.configs import HifiganConfig\n        >>> config = HifiganConfig()\n    Args:\n        model (str):\n            Model name used for selecting the right model at initialization. Defaults to `hifigan`.\n        discriminator_model (str): One of the discriminators from `TTS.vocoder.models.*_discriminator`. Defaults to\n            'hifigan_discriminator`.",
        "detail": "env.src.tts.TTS.vocoder.configs.hifigan_config",
        "documentation": {}
    },
    {
        "label": "MelganConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.configs.melgan_config",
        "description": "env.src.tts.TTS.vocoder.configs.melgan_config",
        "peekOfCode": "class MelganConfig(BaseGANVocoderConfig):\n    \"\"\"Defines parameters for MelGAN vocoder.\n    Example:\n        >>> from TTS.vocoder.configs import MelganConfig\n        >>> config = MelganConfig()\n    Args:\n        model (str):\n            Model name used for selecting the right model at initialization. Defaults to `melgan`.\n        discriminator_model (str): One of the discriminators from `TTS.vocoder.models.*_discriminator`. Defaults to\n            'melgan_multiscale_discriminator`.",
        "detail": "env.src.tts.TTS.vocoder.configs.melgan_config",
        "documentation": {}
    },
    {
        "label": "MultibandMelganConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.configs.multiband_melgan_config",
        "description": "env.src.tts.TTS.vocoder.configs.multiband_melgan_config",
        "peekOfCode": "class MultibandMelganConfig(BaseGANVocoderConfig):\n    \"\"\"Defines parameters for MultiBandMelGAN vocoder.\n    Example:\n        >>> from TTS.vocoder.configs import MultibandMelganConfig\n        >>> config = MultibandMelganConfig()\n    Args:\n        model (str):\n            Model name used for selecting the right model at initialization. Defaults to `multiband_melgan`.\n        discriminator_model (str): One of the discriminators from `TTS.vocoder.models.*_discriminator`. Defaults to\n            'melgan_multiscale_discriminator`.",
        "detail": "env.src.tts.TTS.vocoder.configs.multiband_melgan_config",
        "documentation": {}
    },
    {
        "label": "ParallelWaveganConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.configs.parallel_wavegan_config",
        "description": "env.src.tts.TTS.vocoder.configs.parallel_wavegan_config",
        "peekOfCode": "class ParallelWaveganConfig(BaseGANVocoderConfig):\n    \"\"\"Defines parameters for ParallelWavegan vocoder.\n    Args:\n        model (str):\n            Model name used for selecting the right configuration at initialization. Defaults to `gan`.\n        discriminator_model (str): One of the discriminators from `TTS.vocoder.models.*_discriminator`. Defaults to\n            'parallel_wavegan_discriminator`.\n        discriminator_model_params (dict): The discriminator model kwargs. Defaults to\n            '{\"num_layers\": 10}`\n        generator_model (str): One of the generators from TTS.vocoder.models.*`. Every other non-GAN vocoder model is",
        "detail": "env.src.tts.TTS.vocoder.configs.parallel_wavegan_config",
        "documentation": {}
    },
    {
        "label": "BaseVocoderConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.configs.shared_configs",
        "description": "env.src.tts.TTS.vocoder.configs.shared_configs",
        "peekOfCode": "class BaseVocoderConfig(BaseTrainingConfig):\n    \"\"\"Shared parameters among all the vocoder models.\n    Args:\n        audio (BaseAudioConfig):\n            Audio processor config instance. Defaultsto `BaseAudioConfig()`.\n        use_noise_augment (bool):\n            Augment the input audio with random noise. Defaults to False/\n        eval_split_size (int):\n            Number of instances used for evaluation. Defaults to 10.\n        data_path (str):",
        "detail": "env.src.tts.TTS.vocoder.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "BaseGANVocoderConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.configs.shared_configs",
        "description": "env.src.tts.TTS.vocoder.configs.shared_configs",
        "peekOfCode": "class BaseGANVocoderConfig(BaseVocoderConfig):\n    \"\"\"Base config class used among all the GAN based vocoders.\n    Args:\n        use_stft_loss (bool):\n            enable / disable the use of STFT loss. Defaults to True.\n        use_subband_stft_loss (bool):\n            enable / disable the use of Subband STFT loss. Defaults to True.\n        use_mse_gan_loss (bool):\n            enable / disable the use of Mean Squared Error based GAN loss. Defaults to True.\n        use_hinge_gan_loss (bool):",
        "detail": "env.src.tts.TTS.vocoder.configs.shared_configs",
        "documentation": {}
    },
    {
        "label": "UnivnetConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.configs.univnet_config",
        "description": "env.src.tts.TTS.vocoder.configs.univnet_config",
        "peekOfCode": "class UnivnetConfig(BaseGANVocoderConfig):\n    \"\"\"Defines parameters for UnivNet vocoder.\n    Example:\n        >>> from TTS.vocoder.configs import UnivNetConfig\n        >>> config = UnivNetConfig()\n    Args:\n        model (str):\n            Model name used for selecting the right model at initialization. Defaults to `UnivNet`.\n        discriminator_model (str): One of the discriminators from `TTS.vocoder.models.*_discriminator`. Defaults to\n            'UnivNet_discriminator`.",
        "detail": "env.src.tts.TTS.vocoder.configs.univnet_config",
        "documentation": {}
    },
    {
        "label": "WavegradConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.configs.wavegrad_config",
        "description": "env.src.tts.TTS.vocoder.configs.wavegrad_config",
        "peekOfCode": "class WavegradConfig(BaseVocoderConfig):\n    \"\"\"Defines parameters for WaveGrad vocoder.\n    Example:\n        >>> from TTS.vocoder.configs import WavegradConfig\n        >>> config = WavegradConfig()\n    Args:\n        model (str):\n            Model name used for selecting the right model at initialization. Defaults to `wavegrad`.\n        generator_model (str): One of the generators from TTS.vocoder.models.*`. Every other non-GAN vocoder model is\n            considered as a generator too. Defaults to `wavegrad`.",
        "detail": "env.src.tts.TTS.vocoder.configs.wavegrad_config",
        "documentation": {}
    },
    {
        "label": "WavernnConfig",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.configs.wavernn_config",
        "description": "env.src.tts.TTS.vocoder.configs.wavernn_config",
        "peekOfCode": "class WavernnConfig(BaseVocoderConfig):\n    \"\"\"Defines parameters for Wavernn vocoder.\n    Example:\n        >>> from TTS.vocoder.configs import WavernnConfig\n        >>> config = WavernnConfig()\n    Args:\n        model (str):\n            Model name used for selecting the right model at initialization. Defaults to `wavernn`.\n        mode (str):\n            Output mode of the WaveRNN vocoder. `mold` for Mixture of Logistic Distribution, `gauss` for a single",
        "detail": "env.src.tts.TTS.vocoder.configs.wavernn_config",
        "documentation": {}
    },
    {
        "label": "GANDataset",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.datasets.gan_dataset",
        "description": "env.src.tts.TTS.vocoder.datasets.gan_dataset",
        "peekOfCode": "class GANDataset(Dataset):\n    \"\"\"\n    GAN Dataset searchs for all the wav files under root path\n    and converts them to acoustic features on the fly and returns\n    random segments of (audio, feature) couples.\n    \"\"\"\n    def __init__(\n        self,\n        ap,\n        items,",
        "detail": "env.src.tts.TTS.vocoder.datasets.gan_dataset",
        "documentation": {}
    },
    {
        "label": "preprocess_wav_files",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vocoder.datasets.preprocess",
        "description": "env.src.tts.TTS.vocoder.datasets.preprocess",
        "peekOfCode": "def preprocess_wav_files(out_path: str, config: Coqpit, ap: AudioProcessor):\n    \"\"\"Process wav and compute mel and quantized wave signal.\n    It is mainly used by WaveRNN dataloader.\n    Args:\n        out_path (str): Parent folder path to save the files.\n        config (Coqpit): Model config.\n        ap (AudioProcessor): Audio processor.\n    \"\"\"\n    os.makedirs(os.path.join(out_path, \"quant\"), exist_ok=True)\n    os.makedirs(os.path.join(out_path, \"mel\"), exist_ok=True)",
        "detail": "env.src.tts.TTS.vocoder.datasets.preprocess",
        "documentation": {}
    },
    {
        "label": "find_wav_files",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vocoder.datasets.preprocess",
        "description": "env.src.tts.TTS.vocoder.datasets.preprocess",
        "peekOfCode": "def find_wav_files(data_path, file_ext=\"wav\"):\n    wav_paths = glob.glob(os.path.join(data_path, \"**\", f\"*.{file_ext}\"), recursive=True)\n    return wav_paths\ndef find_feat_files(data_path):\n    feat_paths = glob.glob(os.path.join(data_path, \"**\", \"*.npy\"), recursive=True)\n    return feat_paths\ndef load_wav_data(data_path, eval_split_size, file_ext=\"wav\"):\n    wav_paths = find_wav_files(data_path, file_ext=file_ext)\n    assert len(wav_paths) > 0, f\" [!] {data_path} is empty.\"\n    np.random.seed(0)",
        "detail": "env.src.tts.TTS.vocoder.datasets.preprocess",
        "documentation": {}
    },
    {
        "label": "find_feat_files",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vocoder.datasets.preprocess",
        "description": "env.src.tts.TTS.vocoder.datasets.preprocess",
        "peekOfCode": "def find_feat_files(data_path):\n    feat_paths = glob.glob(os.path.join(data_path, \"**\", \"*.npy\"), recursive=True)\n    return feat_paths\ndef load_wav_data(data_path, eval_split_size, file_ext=\"wav\"):\n    wav_paths = find_wav_files(data_path, file_ext=file_ext)\n    assert len(wav_paths) > 0, f\" [!] {data_path} is empty.\"\n    np.random.seed(0)\n    np.random.shuffle(wav_paths)\n    return wav_paths[:eval_split_size], wav_paths[eval_split_size:]\ndef load_wav_feat_data(data_path, feat_path, eval_split_size):",
        "detail": "env.src.tts.TTS.vocoder.datasets.preprocess",
        "documentation": {}
    },
    {
        "label": "load_wav_data",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vocoder.datasets.preprocess",
        "description": "env.src.tts.TTS.vocoder.datasets.preprocess",
        "peekOfCode": "def load_wav_data(data_path, eval_split_size, file_ext=\"wav\"):\n    wav_paths = find_wav_files(data_path, file_ext=file_ext)\n    assert len(wav_paths) > 0, f\" [!] {data_path} is empty.\"\n    np.random.seed(0)\n    np.random.shuffle(wav_paths)\n    return wav_paths[:eval_split_size], wav_paths[eval_split_size:]\ndef load_wav_feat_data(data_path, feat_path, eval_split_size):\n    wav_paths = find_wav_files(data_path)\n    feat_paths = find_feat_files(feat_path)\n    wav_paths.sort(key=lambda x: Path(x).stem)",
        "detail": "env.src.tts.TTS.vocoder.datasets.preprocess",
        "documentation": {}
    },
    {
        "label": "load_wav_feat_data",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vocoder.datasets.preprocess",
        "description": "env.src.tts.TTS.vocoder.datasets.preprocess",
        "peekOfCode": "def load_wav_feat_data(data_path, feat_path, eval_split_size):\n    wav_paths = find_wav_files(data_path)\n    feat_paths = find_feat_files(feat_path)\n    wav_paths.sort(key=lambda x: Path(x).stem)\n    feat_paths.sort(key=lambda x: Path(x).stem)\n    assert len(wav_paths) == len(feat_paths), f\" [!] {len(wav_paths)} vs {feat_paths}\"\n    for wav, feat in zip(wav_paths, feat_paths):\n        wav_name = Path(wav).stem\n        feat_name = Path(feat).stem\n        assert wav_name == feat_name",
        "detail": "env.src.tts.TTS.vocoder.datasets.preprocess",
        "documentation": {}
    },
    {
        "label": "WaveGradDataset",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.datasets.wavegrad_dataset",
        "description": "env.src.tts.TTS.vocoder.datasets.wavegrad_dataset",
        "peekOfCode": "class WaveGradDataset(Dataset):\n    \"\"\"\n    WaveGrad Dataset searchs for all the wav files under root path\n    and converts them to acoustic features on the fly and returns\n    random segments of (audio, feature) couples.\n    \"\"\"\n    def __init__(\n        self,\n        ap,\n        items,",
        "detail": "env.src.tts.TTS.vocoder.datasets.wavegrad_dataset",
        "documentation": {}
    },
    {
        "label": "WaveRNNDataset",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.datasets.wavernn_dataset",
        "description": "env.src.tts.TTS.vocoder.datasets.wavernn_dataset",
        "peekOfCode": "class WaveRNNDataset(Dataset):\n    \"\"\"\n    WaveRNN Dataset searchs for all the wav files under root path\n    and converts them to acoustic features on the fly.\n    \"\"\"\n    def __init__(\n        self, ap, items, seq_len, hop_len, pad, mode, mulaw, is_training=True, verbose=False, return_segments=True\n    ):\n        super().__init__()\n        self.ap = ap",
        "detail": "env.src.tts.TTS.vocoder.datasets.wavernn_dataset",
        "documentation": {}
    },
    {
        "label": "ResStack",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.hifigan",
        "description": "env.src.tts.TTS.vocoder.layers.hifigan",
        "peekOfCode": "class ResStack(nn.Module):\n    def __init__(self, kernel, channel, padding, dilations=[1, 3, 5]):\n        super().__init__()\n        resstack = []\n        for dilation in dilations:\n            resstack += [\n                nn.LeakyReLU(0.2),\n                nn.ReflectionPad1d(dilation),\n                nn.utils.weight_norm(nn.Conv1d(channel, channel, kernel_size=kernel, dilation=dilation)),\n                nn.LeakyReLU(0.2),",
        "detail": "env.src.tts.TTS.vocoder.layers.hifigan",
        "documentation": {}
    },
    {
        "label": "MRF",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.hifigan",
        "description": "env.src.tts.TTS.vocoder.layers.hifigan",
        "peekOfCode": "class MRF(nn.Module):\n    def __init__(self, kernels, channel, dilations=[1, 3, 5]):  # # pylint: disable=dangerous-default-value\n        super().__init__()\n        self.resblock1 = ResStack(kernels[0], channel, 0, dilations)\n        self.resblock2 = ResStack(kernels[1], channel, 6, dilations)\n        self.resblock3 = ResStack(kernels[2], channel, 12, dilations)\n    def forward(self, x):\n        x1 = self.resblock1(x)\n        x2 = self.resblock2(x)\n        x3 = self.resblock3(x)",
        "detail": "env.src.tts.TTS.vocoder.layers.hifigan",
        "documentation": {}
    },
    {
        "label": "STFTLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.losses",
        "description": "env.src.tts.TTS.vocoder.layers.losses",
        "peekOfCode": "class STFTLoss(nn.Module):\n    \"\"\"STFT loss. Input generate and real waveforms are converted\n    to spectrograms compared with L1 and Spectral convergence losses.\n    It is from ParallelWaveGAN paper https://arxiv.org/pdf/1910.11480.pdf\"\"\"\n    def __init__(self, n_fft, hop_length, win_length):\n        super().__init__()\n        self.n_fft = n_fft\n        self.hop_length = hop_length\n        self.win_length = win_length\n        self.stft = TorchSTFT(n_fft, hop_length, win_length)",
        "detail": "env.src.tts.TTS.vocoder.layers.losses",
        "documentation": {}
    },
    {
        "label": "MultiScaleSTFTLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.losses",
        "description": "env.src.tts.TTS.vocoder.layers.losses",
        "peekOfCode": "class MultiScaleSTFTLoss(torch.nn.Module):\n    \"\"\"Multi-scale STFT loss. Input generate and real waveforms are converted\n    to spectrograms compared with L1 and Spectral convergence losses.\n    It is from ParallelWaveGAN paper https://arxiv.org/pdf/1910.11480.pdf\"\"\"\n    def __init__(self, n_ffts=(1024, 2048, 512), hop_lengths=(120, 240, 50), win_lengths=(600, 1200, 240)):\n        super().__init__()\n        self.loss_funcs = torch.nn.ModuleList()\n        for n_fft, hop_length, win_length in zip(n_ffts, hop_lengths, win_lengths):\n            self.loss_funcs.append(STFTLoss(n_fft, hop_length, win_length))\n    def forward(self, y_hat, y):",
        "detail": "env.src.tts.TTS.vocoder.layers.losses",
        "documentation": {}
    },
    {
        "label": "L1SpecLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.losses",
        "description": "env.src.tts.TTS.vocoder.layers.losses",
        "peekOfCode": "class L1SpecLoss(nn.Module):\n    \"\"\"L1 Loss over Spectrograms as described in HiFiGAN paper https://arxiv.org/pdf/2010.05646.pdf\"\"\"\n    def __init__(\n        self, sample_rate, n_fft, hop_length, win_length, mel_fmin=None, mel_fmax=None, n_mels=None, use_mel=True\n    ):\n        super().__init__()\n        self.use_mel = use_mel\n        self.stft = TorchSTFT(\n            n_fft,\n            hop_length,",
        "detail": "env.src.tts.TTS.vocoder.layers.losses",
        "documentation": {}
    },
    {
        "label": "MultiScaleSubbandSTFTLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.losses",
        "description": "env.src.tts.TTS.vocoder.layers.losses",
        "peekOfCode": "class MultiScaleSubbandSTFTLoss(MultiScaleSTFTLoss):\n    \"\"\"Multiscale STFT loss for multi band model outputs.\n    From MultiBand-MelGAN paper https://arxiv.org/abs/2005.05106\"\"\"\n    # pylint: disable=no-self-use\n    def forward(self, y_hat, y):\n        y_hat = y_hat.view(-1, 1, y_hat.shape[2])\n        y = y.view(-1, 1, y.shape[2])\n        return super().forward(y_hat.squeeze(1), y.squeeze(1))\nclass MSEGLoss(nn.Module):\n    \"\"\"Mean Squared Generator Loss\"\"\"",
        "detail": "env.src.tts.TTS.vocoder.layers.losses",
        "documentation": {}
    },
    {
        "label": "MSEGLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.losses",
        "description": "env.src.tts.TTS.vocoder.layers.losses",
        "peekOfCode": "class MSEGLoss(nn.Module):\n    \"\"\"Mean Squared Generator Loss\"\"\"\n    # pylint: disable=no-self-use\n    def forward(self, score_real):\n        loss_fake = F.mse_loss(score_real, score_real.new_ones(score_real.shape))\n        return loss_fake\nclass HingeGLoss(nn.Module):\n    \"\"\"Hinge Discriminator Loss\"\"\"\n    # pylint: disable=no-self-use\n    def forward(self, score_real):",
        "detail": "env.src.tts.TTS.vocoder.layers.losses",
        "documentation": {}
    },
    {
        "label": "HingeGLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.losses",
        "description": "env.src.tts.TTS.vocoder.layers.losses",
        "peekOfCode": "class HingeGLoss(nn.Module):\n    \"\"\"Hinge Discriminator Loss\"\"\"\n    # pylint: disable=no-self-use\n    def forward(self, score_real):\n        # TODO: this might be wrong\n        loss_fake = torch.mean(F.relu(1.0 - score_real))\n        return loss_fake\n##################################\n# DISCRIMINATOR LOSSES\n##################################",
        "detail": "env.src.tts.TTS.vocoder.layers.losses",
        "documentation": {}
    },
    {
        "label": "MSEDLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.losses",
        "description": "env.src.tts.TTS.vocoder.layers.losses",
        "peekOfCode": "class MSEDLoss(nn.Module):\n    \"\"\"Mean Squared Discriminator Loss\"\"\"\n    def __init__(\n        self,\n    ):\n        super().__init__()\n        self.loss_func = nn.MSELoss()\n    # pylint: disable=no-self-use\n    def forward(self, score_fake, score_real):\n        loss_real = self.loss_func(score_real, score_real.new_ones(score_real.shape))",
        "detail": "env.src.tts.TTS.vocoder.layers.losses",
        "documentation": {}
    },
    {
        "label": "HingeDLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.losses",
        "description": "env.src.tts.TTS.vocoder.layers.losses",
        "peekOfCode": "class HingeDLoss(nn.Module):\n    \"\"\"Hinge Discriminator Loss\"\"\"\n    # pylint: disable=no-self-use\n    def forward(self, score_fake, score_real):\n        loss_real = torch.mean(F.relu(1.0 - score_real))\n        loss_fake = torch.mean(F.relu(1.0 + score_fake))\n        loss_d = loss_real + loss_fake\n        return loss_d, loss_real, loss_fake\nclass MelganFeatureLoss(nn.Module):\n    def __init__(",
        "detail": "env.src.tts.TTS.vocoder.layers.losses",
        "documentation": {}
    },
    {
        "label": "MelganFeatureLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.losses",
        "description": "env.src.tts.TTS.vocoder.layers.losses",
        "peekOfCode": "class MelganFeatureLoss(nn.Module):\n    def __init__(\n        self,\n    ):\n        super().__init__()\n        self.loss_func = nn.L1Loss()\n    # pylint: disable=no-self-use\n    def forward(self, fake_feats, real_feats):\n        loss_feats = 0\n        num_feats = 0",
        "detail": "env.src.tts.TTS.vocoder.layers.losses",
        "documentation": {}
    },
    {
        "label": "GeneratorLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.losses",
        "description": "env.src.tts.TTS.vocoder.layers.losses",
        "peekOfCode": "class GeneratorLoss(nn.Module):\n    \"\"\"Generator Loss Wrapper. Based on model configuration it sets a right set of loss functions and computes\n    losses. It allows to experiment with different combinations of loss functions with different models by just\n    changing configurations.\n    Args:\n        C (AttrDict): model configuration.\n    \"\"\"\n    def __init__(self, C):\n        super().__init__()\n        assert not (",
        "detail": "env.src.tts.TTS.vocoder.layers.losses",
        "documentation": {}
    },
    {
        "label": "DiscriminatorLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.losses",
        "description": "env.src.tts.TTS.vocoder.layers.losses",
        "peekOfCode": "class DiscriminatorLoss(nn.Module):\n    \"\"\"Like ```GeneratorLoss```\"\"\"\n    def __init__(self, C):\n        super().__init__()\n        assert not (\n            C.use_mse_gan_loss and C.use_hinge_gan_loss\n        ), \" [!] Cannot use HingeGANLoss and MSEGANLoss together.\"\n        self.use_mse_gan_loss = C.use_mse_gan_loss\n        self.use_hinge_gan_loss = C.use_hinge_gan_loss\n        if C.use_mse_gan_loss:",
        "detail": "env.src.tts.TTS.vocoder.layers.losses",
        "documentation": {}
    },
    {
        "label": "WaveRNNLoss",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.losses",
        "description": "env.src.tts.TTS.vocoder.layers.losses",
        "peekOfCode": "class WaveRNNLoss(nn.Module):\n    def __init__(self, wave_rnn_mode: Union[str, int]):\n        super().__init__()\n        if wave_rnn_mode == \"mold\":\n            self.loss_func = discretized_mix_logistic_loss\n        elif wave_rnn_mode == \"gauss\":\n            self.loss_func = gaussian_loss\n        elif isinstance(wave_rnn_mode, int):\n            self.loss_func = torch.nn.CrossEntropyLoss()\n        else:",
        "detail": "env.src.tts.TTS.vocoder.layers.losses",
        "documentation": {}
    },
    {
        "label": "KernelPredictor",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.lvc_block",
        "description": "env.src.tts.TTS.vocoder.layers.lvc_block",
        "peekOfCode": "class KernelPredictor(torch.nn.Module):\n    \"\"\"Kernel predictor for the location-variable convolutions\"\"\"\n    def __init__(  # pylint: disable=dangerous-default-value\n        self,\n        cond_channels,\n        conv_in_channels,\n        conv_out_channels,\n        conv_layers,\n        conv_kernel_size=3,\n        kpnet_hidden_channels=64,",
        "detail": "env.src.tts.TTS.vocoder.layers.lvc_block",
        "documentation": {}
    },
    {
        "label": "LVCBlock",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.lvc_block",
        "description": "env.src.tts.TTS.vocoder.layers.lvc_block",
        "peekOfCode": "class LVCBlock(torch.nn.Module):\n    \"\"\"the location-variable convolutions\"\"\"\n    def __init__(\n        self,\n        in_channels,\n        cond_channels,\n        upsample_ratio,\n        conv_layers=4,\n        conv_kernel_size=3,\n        cond_hop_length=256,",
        "detail": "env.src.tts.TTS.vocoder.layers.lvc_block",
        "documentation": {}
    },
    {
        "label": "ResidualStack",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.melgan",
        "description": "env.src.tts.TTS.vocoder.layers.melgan",
        "peekOfCode": "class ResidualStack(nn.Module):\n    def __init__(self, channels, num_res_blocks, kernel_size):\n        super().__init__()\n        assert (kernel_size - 1) % 2 == 0, \" [!] kernel_size has to be odd.\"\n        base_padding = (kernel_size - 1) // 2\n        self.blocks = nn.ModuleList()\n        for idx in range(num_res_blocks):\n            layer_kernel_size = kernel_size\n            layer_dilation = layer_kernel_size**idx\n            layer_padding = base_padding * layer_dilation",
        "detail": "env.src.tts.TTS.vocoder.layers.melgan",
        "documentation": {}
    },
    {
        "label": "ResidualBlock",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.parallel_wavegan",
        "description": "env.src.tts.TTS.vocoder.layers.parallel_wavegan",
        "peekOfCode": "class ResidualBlock(torch.nn.Module):\n    \"\"\"Residual block module in WaveNet.\"\"\"\n    def __init__(\n        self,\n        kernel_size=3,\n        res_channels=64,\n        gate_channels=128,\n        skip_channels=64,\n        aux_channels=80,\n        dropout=0.0,",
        "detail": "env.src.tts.TTS.vocoder.layers.parallel_wavegan",
        "documentation": {}
    },
    {
        "label": "PQMF",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.pqmf",
        "description": "env.src.tts.TTS.vocoder.layers.pqmf",
        "peekOfCode": "class PQMF(torch.nn.Module):\n    def __init__(self, N=4, taps=62, cutoff=0.15, beta=9.0):\n        super().__init__()\n        self.N = N\n        self.taps = taps\n        self.cutoff = cutoff\n        self.beta = beta\n        QMF = sig.firwin(taps + 1, cutoff, window=(\"kaiser\", beta))\n        H = np.zeros((N, len(QMF)))\n        G = np.zeros((N, len(QMF)))",
        "detail": "env.src.tts.TTS.vocoder.layers.pqmf",
        "documentation": {}
    },
    {
        "label": "Stretch2d",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.upsample",
        "description": "env.src.tts.TTS.vocoder.layers.upsample",
        "peekOfCode": "class Stretch2d(torch.nn.Module):\n    def __init__(self, x_scale, y_scale, mode=\"nearest\"):\n        super().__init__()\n        self.x_scale = x_scale\n        self.y_scale = y_scale\n        self.mode = mode\n    def forward(self, x):\n        \"\"\"\n        x (Tensor): Input tensor (B, C, F, T).\n        Tensor: Interpolated tensor (B, C, F * y_scale, T * x_scale),",
        "detail": "env.src.tts.TTS.vocoder.layers.upsample",
        "documentation": {}
    },
    {
        "label": "UpsampleNetwork",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.upsample",
        "description": "env.src.tts.TTS.vocoder.layers.upsample",
        "peekOfCode": "class UpsampleNetwork(torch.nn.Module):\n    # pylint: disable=dangerous-default-value\n    def __init__(\n        self,\n        upsample_factors,\n        nonlinear_activation=None,\n        nonlinear_activation_params={},\n        interpolate_mode=\"nearest\",\n        freq_axis_kernel_size=1,\n        use_causal_conv=False,",
        "detail": "env.src.tts.TTS.vocoder.layers.upsample",
        "documentation": {}
    },
    {
        "label": "ConvUpsample",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.upsample",
        "description": "env.src.tts.TTS.vocoder.layers.upsample",
        "peekOfCode": "class ConvUpsample(torch.nn.Module):\n    # pylint: disable=dangerous-default-value\n    def __init__(\n        self,\n        upsample_factors,\n        nonlinear_activation=None,\n        nonlinear_activation_params={},\n        interpolate_mode=\"nearest\",\n        freq_axis_kernel_size=1,\n        aux_channels=80,",
        "detail": "env.src.tts.TTS.vocoder.layers.upsample",
        "documentation": {}
    },
    {
        "label": "Conv1d",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.wavegrad",
        "description": "env.src.tts.TTS.vocoder.layers.wavegrad",
        "peekOfCode": "class Conv1d(nn.Conv1d):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        nn.init.orthogonal_(self.weight)\n        nn.init.zeros_(self.bias)\nclass PositionalEncoding(nn.Module):\n    \"\"\"Positional encoding with noise level conditioning\"\"\"\n    def __init__(self, n_channels, max_len=10000):\n        super().__init__()\n        self.n_channels = n_channels",
        "detail": "env.src.tts.TTS.vocoder.layers.wavegrad",
        "documentation": {}
    },
    {
        "label": "PositionalEncoding",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.wavegrad",
        "description": "env.src.tts.TTS.vocoder.layers.wavegrad",
        "peekOfCode": "class PositionalEncoding(nn.Module):\n    \"\"\"Positional encoding with noise level conditioning\"\"\"\n    def __init__(self, n_channels, max_len=10000):\n        super().__init__()\n        self.n_channels = n_channels\n        self.max_len = max_len\n        self.C = 5000\n        self.pe = torch.zeros(0, 0)\n    def forward(self, x, noise_level):\n        if x.shape[2] > self.pe.shape[1]:",
        "detail": "env.src.tts.TTS.vocoder.layers.wavegrad",
        "documentation": {}
    },
    {
        "label": "FiLM",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.wavegrad",
        "description": "env.src.tts.TTS.vocoder.layers.wavegrad",
        "peekOfCode": "class FiLM(nn.Module):\n    def __init__(self, input_size, output_size):\n        super().__init__()\n        self.encoding = PositionalEncoding(input_size)\n        self.input_conv = nn.Conv1d(input_size, input_size, 3, padding=1)\n        self.output_conv = nn.Conv1d(input_size, output_size * 2, 3, padding=1)\n        nn.init.xavier_uniform_(self.input_conv.weight)\n        nn.init.xavier_uniform_(self.output_conv.weight)\n        nn.init.zeros_(self.input_conv.bias)\n        nn.init.zeros_(self.output_conv.bias)",
        "detail": "env.src.tts.TTS.vocoder.layers.wavegrad",
        "documentation": {}
    },
    {
        "label": "UBlock",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.wavegrad",
        "description": "env.src.tts.TTS.vocoder.layers.wavegrad",
        "peekOfCode": "class UBlock(nn.Module):\n    def __init__(self, input_size, hidden_size, factor, dilation):\n        super().__init__()\n        assert isinstance(dilation, (list, tuple))\n        assert len(dilation) == 4\n        self.factor = factor\n        self.res_block = Conv1d(input_size, hidden_size, 1)\n        self.main_block = nn.ModuleList(\n            [\n                Conv1d(input_size, hidden_size, 3, dilation=dilation[0], padding=dilation[0]),",
        "detail": "env.src.tts.TTS.vocoder.layers.wavegrad",
        "documentation": {}
    },
    {
        "label": "DBlock",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.layers.wavegrad",
        "description": "env.src.tts.TTS.vocoder.layers.wavegrad",
        "peekOfCode": "class DBlock(nn.Module):\n    def __init__(self, input_size, hidden_size, factor):\n        super().__init__()\n        self.factor = factor\n        self.res_block = Conv1d(input_size, hidden_size, 1)\n        self.main_block = nn.ModuleList(\n            [\n                Conv1d(input_size, hidden_size, 3, dilation=1, padding=1),\n                Conv1d(hidden_size, hidden_size, 3, dilation=2, padding=2),\n                Conv1d(hidden_size, hidden_size, 3, dilation=4, padding=4),",
        "detail": "env.src.tts.TTS.vocoder.layers.wavegrad",
        "documentation": {}
    },
    {
        "label": "shif_and_scale",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vocoder.layers.wavegrad",
        "description": "env.src.tts.TTS.vocoder.layers.wavegrad",
        "peekOfCode": "def shif_and_scale(x, scale, shift):\n    o = shift + scale * x\n    return o\nclass UBlock(nn.Module):\n    def __init__(self, input_size, hidden_size, factor, dilation):\n        super().__init__()\n        assert isinstance(dilation, (list, tuple))\n        assert len(dilation) == 4\n        self.factor = factor\n        self.res_block = Conv1d(input_size, hidden_size, 1)",
        "detail": "env.src.tts.TTS.vocoder.layers.wavegrad",
        "documentation": {}
    },
    {
        "label": "BaseVocoder",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.base_vocoder",
        "description": "env.src.tts.TTS.vocoder.models.base_vocoder",
        "peekOfCode": "class BaseVocoder(BaseTrainerModel):\n    \"\"\"Base `vocoder` class. Every new `vocoder` model must inherit this.\n    It defines `vocoder` specific functions on top of `Model`.\n    Notes on input/output tensor shapes:\n        Any input or output tensor of the model must be shaped as\n        - 3D tensors `batch x time x channels`\n        - 2D tensors `batch x channels`\n        - 1D tensors `batch x 1`\n    \"\"\"\n    MODEL_TYPE = \"vocoder\"",
        "detail": "env.src.tts.TTS.vocoder.models.base_vocoder",
        "documentation": {}
    },
    {
        "label": "FullbandMelganGenerator",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.fullband_melgan_generator",
        "description": "env.src.tts.TTS.vocoder.models.fullband_melgan_generator",
        "peekOfCode": "class FullbandMelganGenerator(MelganGenerator):\n    def __init__(\n        self,\n        in_channels=80,\n        out_channels=1,\n        proj_kernel=7,\n        base_channels=512,\n        upsample_factors=(2, 8, 2, 2),\n        res_kernel=3,\n        num_res_blocks=4,",
        "detail": "env.src.tts.TTS.vocoder.models.fullband_melgan_generator",
        "documentation": {}
    },
    {
        "label": "GAN",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.gan",
        "description": "env.src.tts.TTS.vocoder.models.gan",
        "peekOfCode": "class GAN(BaseVocoder):\n    def __init__(self, config: Coqpit, ap: AudioProcessor = None):\n        \"\"\"Wrap a generator and a discriminator network. It provides a compatible interface for the trainer.\n        It also helps mixing and matching different generator and disciminator networks easily.\n        To implement a new GAN models, you just need to define the generator and the discriminator networks, the rest\n        is handled by the `GAN` class.\n        Args:\n            config (Coqpit): Model configuration.\n            ap (AudioProcessor): TTS AudioProcessor instance. Defaults to None.\n        Examples:",
        "detail": "env.src.tts.TTS.vocoder.models.gan",
        "documentation": {}
    },
    {
        "label": "DiscriminatorP",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.hifigan_discriminator",
        "description": "env.src.tts.TTS.vocoder.models.hifigan_discriminator",
        "peekOfCode": "class DiscriminatorP(torch.nn.Module):\n    \"\"\"HiFiGAN Periodic Discriminator\n    Takes every Pth value from the input waveform and applied a stack of convoluations.\n    Note:\n        if `period` is 2\n        `waveform = [1, 2, 3, 4, 5, 6 ...] --> [1, 3, 5 ... ] --> convs -> score, feat`\n    Args:\n        x (Tensor): input waveform.\n    Returns:\n        [Tensor]: discriminator scores per sample in the batch.",
        "detail": "env.src.tts.TTS.vocoder.models.hifigan_discriminator",
        "documentation": {}
    },
    {
        "label": "MultiPeriodDiscriminator",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.hifigan_discriminator",
        "description": "env.src.tts.TTS.vocoder.models.hifigan_discriminator",
        "peekOfCode": "class MultiPeriodDiscriminator(torch.nn.Module):\n    \"\"\"HiFiGAN Multi-Period Discriminator (MPD)\n    Wrapper for the `PeriodDiscriminator` to apply it in different periods.\n    Periods are suggested to be prime numbers to reduce the overlap between each discriminator.\n    \"\"\"\n    def __init__(self, use_spectral_norm=False):\n        super().__init__()\n        self.discriminators = nn.ModuleList(\n            [\n                DiscriminatorP(2, use_spectral_norm=use_spectral_norm),",
        "detail": "env.src.tts.TTS.vocoder.models.hifigan_discriminator",
        "documentation": {}
    },
    {
        "label": "DiscriminatorS",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.hifigan_discriminator",
        "description": "env.src.tts.TTS.vocoder.models.hifigan_discriminator",
        "peekOfCode": "class DiscriminatorS(torch.nn.Module):\n    \"\"\"HiFiGAN Scale Discriminator.\n    It is similar to `MelganDiscriminator` but with a specific architecture explained in the paper.\n    Args:\n        use_spectral_norm (bool): if `True` swith to spectral norm instead of weight norm.\n    \"\"\"\n    def __init__(self, use_spectral_norm=False):\n        super().__init__()\n        norm_f = nn.utils.spectral_norm if use_spectral_norm else nn.utils.weight_norm\n        self.convs = nn.ModuleList(",
        "detail": "env.src.tts.TTS.vocoder.models.hifigan_discriminator",
        "documentation": {}
    },
    {
        "label": "MultiScaleDiscriminator",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.hifigan_discriminator",
        "description": "env.src.tts.TTS.vocoder.models.hifigan_discriminator",
        "peekOfCode": "class MultiScaleDiscriminator(torch.nn.Module):\n    \"\"\"HiFiGAN Multi-Scale Discriminator.\n    It is similar to `MultiScaleMelganDiscriminator` but specially tailored for HiFiGAN as in the paper.\n    \"\"\"\n    def __init__(self):\n        super().__init__()\n        self.discriminators = nn.ModuleList(\n            [\n                DiscriminatorS(use_spectral_norm=True),\n                DiscriminatorS(),",
        "detail": "env.src.tts.TTS.vocoder.models.hifigan_discriminator",
        "documentation": {}
    },
    {
        "label": "HifiganDiscriminator",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.hifigan_discriminator",
        "description": "env.src.tts.TTS.vocoder.models.hifigan_discriminator",
        "peekOfCode": "class HifiganDiscriminator(nn.Module):\n    \"\"\"HiFiGAN discriminator wrapping MPD and MSD.\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.mpd = MultiPeriodDiscriminator()\n        self.msd = MultiScaleDiscriminator()\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (Tensor): input waveform.",
        "detail": "env.src.tts.TTS.vocoder.models.hifigan_discriminator",
        "documentation": {}
    },
    {
        "label": "LRELU_SLOPE",
        "kind": 5,
        "importPath": "env.src.tts.TTS.vocoder.models.hifigan_discriminator",
        "description": "env.src.tts.TTS.vocoder.models.hifigan_discriminator",
        "peekOfCode": "LRELU_SLOPE = 0.1\nclass DiscriminatorP(torch.nn.Module):\n    \"\"\"HiFiGAN Periodic Discriminator\n    Takes every Pth value from the input waveform and applied a stack of convoluations.\n    Note:\n        if `period` is 2\n        `waveform = [1, 2, 3, 4, 5, 6 ...] --> [1, 3, 5 ... ] --> convs -> score, feat`\n    Args:\n        x (Tensor): input waveform.\n    Returns:",
        "detail": "env.src.tts.TTS.vocoder.models.hifigan_discriminator",
        "documentation": {}
    },
    {
        "label": "ResBlock1",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.hifigan_generator",
        "description": "env.src.tts.TTS.vocoder.models.hifigan_generator",
        "peekOfCode": "class ResBlock1(torch.nn.Module):\n    \"\"\"Residual Block Type 1. It has 3 convolutional layers in each convolutional block.\n    Network::\n        x -> lrelu -> conv1_1 -> conv1_2 -> conv1_3 -> z -> lrelu -> conv2_1 -> conv2_2 -> conv2_3 -> o -> + -> o\n        |--------------------------------------------------------------------------------------------------|\n    Args:\n        channels (int): number of hidden channels for the convolutional layers.\n        kernel_size (int): size of the convolution filter in each layer.\n        dilations (list): list of dilation value for each conv layer in a block.\n    \"\"\"",
        "detail": "env.src.tts.TTS.vocoder.models.hifigan_generator",
        "documentation": {}
    },
    {
        "label": "ResBlock2",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.hifigan_generator",
        "description": "env.src.tts.TTS.vocoder.models.hifigan_generator",
        "peekOfCode": "class ResBlock2(torch.nn.Module):\n    \"\"\"Residual Block Type 2. It has 1 convolutional layers in each convolutional block.\n    Network::\n        x -> lrelu -> conv1-> -> z -> lrelu -> conv2-> o -> + -> o\n        |---------------------------------------------------|\n    Args:\n        channels (int): number of hidden channels for the convolutional layers.\n        kernel_size (int): size of the convolution filter in each layer.\n        dilations (list): list of dilation value for each conv layer in a block.\n    \"\"\"",
        "detail": "env.src.tts.TTS.vocoder.models.hifigan_generator",
        "documentation": {}
    },
    {
        "label": "HifiganGenerator",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.hifigan_generator",
        "description": "env.src.tts.TTS.vocoder.models.hifigan_generator",
        "peekOfCode": "class HifiganGenerator(torch.nn.Module):\n    def __init__(\n        self,\n        in_channels,\n        out_channels,\n        resblock_type,\n        resblock_dilation_sizes,\n        resblock_kernel_sizes,\n        upsample_kernel_sizes,\n        upsample_initial_channel,",
        "detail": "env.src.tts.TTS.vocoder.models.hifigan_generator",
        "documentation": {}
    },
    {
        "label": "get_padding",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vocoder.models.hifigan_generator",
        "description": "env.src.tts.TTS.vocoder.models.hifigan_generator",
        "peekOfCode": "def get_padding(k, d):\n    return int((k * d - d) / 2)\nclass ResBlock1(torch.nn.Module):\n    \"\"\"Residual Block Type 1. It has 3 convolutional layers in each convolutional block.\n    Network::\n        x -> lrelu -> conv1_1 -> conv1_2 -> conv1_3 -> z -> lrelu -> conv2_1 -> conv2_2 -> conv2_3 -> o -> + -> o\n        |--------------------------------------------------------------------------------------------------|\n    Args:\n        channels (int): number of hidden channels for the convolutional layers.\n        kernel_size (int): size of the convolution filter in each layer.",
        "detail": "env.src.tts.TTS.vocoder.models.hifigan_generator",
        "documentation": {}
    },
    {
        "label": "LRELU_SLOPE",
        "kind": 5,
        "importPath": "env.src.tts.TTS.vocoder.models.hifigan_generator",
        "description": "env.src.tts.TTS.vocoder.models.hifigan_generator",
        "peekOfCode": "LRELU_SLOPE = 0.1\ndef get_padding(k, d):\n    return int((k * d - d) / 2)\nclass ResBlock1(torch.nn.Module):\n    \"\"\"Residual Block Type 1. It has 3 convolutional layers in each convolutional block.\n    Network::\n        x -> lrelu -> conv1_1 -> conv1_2 -> conv1_3 -> z -> lrelu -> conv2_1 -> conv2_2 -> conv2_3 -> o -> + -> o\n        |--------------------------------------------------------------------------------------------------|\n    Args:\n        channels (int): number of hidden channels for the convolutional layers.",
        "detail": "env.src.tts.TTS.vocoder.models.hifigan_generator",
        "documentation": {}
    },
    {
        "label": "MelganDiscriminator",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.melgan_discriminator",
        "description": "env.src.tts.TTS.vocoder.models.melgan_discriminator",
        "peekOfCode": "class MelganDiscriminator(nn.Module):\n    def __init__(\n        self,\n        in_channels=1,\n        out_channels=1,\n        kernel_sizes=(5, 3),\n        base_channels=16,\n        max_channels=1024,\n        downsample_factors=(4, 4, 4, 4),\n        groups_denominator=4,",
        "detail": "env.src.tts.TTS.vocoder.models.melgan_discriminator",
        "documentation": {}
    },
    {
        "label": "MelganGenerator",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.melgan_generator",
        "description": "env.src.tts.TTS.vocoder.models.melgan_generator",
        "peekOfCode": "class MelganGenerator(nn.Module):\n    def __init__(\n        self,\n        in_channels=80,\n        out_channels=1,\n        proj_kernel=7,\n        base_channels=512,\n        upsample_factors=(8, 8, 2, 2),\n        res_kernel=3,\n        num_res_blocks=3,",
        "detail": "env.src.tts.TTS.vocoder.models.melgan_generator",
        "documentation": {}
    },
    {
        "label": "MelganMultiscaleDiscriminator",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.melgan_multiscale_discriminator",
        "description": "env.src.tts.TTS.vocoder.models.melgan_multiscale_discriminator",
        "peekOfCode": "class MelganMultiscaleDiscriminator(nn.Module):\n    def __init__(\n        self,\n        in_channels=1,\n        out_channels=1,\n        num_scales=3,\n        kernel_sizes=(5, 3),\n        base_channels=16,\n        max_channels=1024,\n        downsample_factors=(4, 4, 4),",
        "detail": "env.src.tts.TTS.vocoder.models.melgan_multiscale_discriminator",
        "documentation": {}
    },
    {
        "label": "MultibandMelganGenerator",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.multiband_melgan_generator",
        "description": "env.src.tts.TTS.vocoder.models.multiband_melgan_generator",
        "peekOfCode": "class MultibandMelganGenerator(MelganGenerator):\n    def __init__(\n        self,\n        in_channels=80,\n        out_channels=4,\n        proj_kernel=7,\n        base_channels=384,\n        upsample_factors=(2, 8, 2, 2),\n        res_kernel=3,\n        num_res_blocks=3,",
        "detail": "env.src.tts.TTS.vocoder.models.multiband_melgan_generator",
        "documentation": {}
    },
    {
        "label": "ParallelWaveganDiscriminator",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.parallel_wavegan_discriminator",
        "description": "env.src.tts.TTS.vocoder.models.parallel_wavegan_discriminator",
        "peekOfCode": "class ParallelWaveganDiscriminator(nn.Module):\n    \"\"\"PWGAN discriminator as in https://arxiv.org/abs/1910.11480.\n    It classifies each audio window real/fake and returns a sequence\n    of predictions.\n        It is a stack of convolutional blocks with dilation.\n    \"\"\"\n    # pylint: disable=dangerous-default-value\n    def __init__(\n        self,\n        in_channels=1,",
        "detail": "env.src.tts.TTS.vocoder.models.parallel_wavegan_discriminator",
        "documentation": {}
    },
    {
        "label": "ResidualParallelWaveganDiscriminator",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.parallel_wavegan_discriminator",
        "description": "env.src.tts.TTS.vocoder.models.parallel_wavegan_discriminator",
        "peekOfCode": "class ResidualParallelWaveganDiscriminator(nn.Module):\n    # pylint: disable=dangerous-default-value\n    def __init__(\n        self,\n        in_channels=1,\n        out_channels=1,\n        kernel_size=3,\n        num_layers=30,\n        stacks=3,\n        res_channels=64,",
        "detail": "env.src.tts.TTS.vocoder.models.parallel_wavegan_discriminator",
        "documentation": {}
    },
    {
        "label": "ParallelWaveganGenerator",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.parallel_wavegan_generator",
        "description": "env.src.tts.TTS.vocoder.models.parallel_wavegan_generator",
        "peekOfCode": "class ParallelWaveganGenerator(torch.nn.Module):\n    \"\"\"PWGAN generator as in https://arxiv.org/pdf/1910.11480.pdf.\n    It is similar to WaveNet with no causal convolution.\n        It is conditioned on an aux feature (spectrogram) to generate\n    an output waveform from an input noise.\n    \"\"\"\n    # pylint: disable=dangerous-default-value\n    def __init__(\n        self,\n        in_channels=1,",
        "detail": "env.src.tts.TTS.vocoder.models.parallel_wavegan_generator",
        "documentation": {}
    },
    {
        "label": "GBlock",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.random_window_discriminator",
        "description": "env.src.tts.TTS.vocoder.models.random_window_discriminator",
        "peekOfCode": "class GBlock(nn.Module):\n    def __init__(self, in_channels, cond_channels, downsample_factor):\n        super().__init__()\n        self.in_channels = in_channels\n        self.cond_channels = cond_channels\n        self.downsample_factor = downsample_factor\n        self.start = nn.Sequential(\n            nn.AvgPool1d(downsample_factor, stride=downsample_factor),\n            nn.ReLU(),\n            nn.Conv1d(in_channels, in_channels * 2, kernel_size=3, padding=1),",
        "detail": "env.src.tts.TTS.vocoder.models.random_window_discriminator",
        "documentation": {}
    },
    {
        "label": "DBlock",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.random_window_discriminator",
        "description": "env.src.tts.TTS.vocoder.models.random_window_discriminator",
        "peekOfCode": "class DBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, downsample_factor):\n        super().__init__()\n        self.in_channels = in_channels\n        self.downsample_factor = downsample_factor\n        self.out_channels = out_channels\n        self.donwsample_layer = nn.AvgPool1d(downsample_factor, stride=downsample_factor)\n        self.layers = nn.Sequential(\n            nn.ReLU(),\n            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),",
        "detail": "env.src.tts.TTS.vocoder.models.random_window_discriminator",
        "documentation": {}
    },
    {
        "label": "ConditionalDiscriminator",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.random_window_discriminator",
        "description": "env.src.tts.TTS.vocoder.models.random_window_discriminator",
        "peekOfCode": "class ConditionalDiscriminator(nn.Module):\n    def __init__(self, in_channels, cond_channels, downsample_factors=(2, 2, 2), out_channels=(128, 256)):\n        super().__init__()\n        assert len(downsample_factors) == len(out_channels) + 1\n        self.in_channels = in_channels\n        self.cond_channels = cond_channels\n        self.downsample_factors = downsample_factors\n        self.out_channels = out_channels\n        self.pre_cond_layers = nn.ModuleList()\n        self.post_cond_layers = nn.ModuleList()",
        "detail": "env.src.tts.TTS.vocoder.models.random_window_discriminator",
        "documentation": {}
    },
    {
        "label": "UnconditionalDiscriminator",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.random_window_discriminator",
        "description": "env.src.tts.TTS.vocoder.models.random_window_discriminator",
        "peekOfCode": "class UnconditionalDiscriminator(nn.Module):\n    def __init__(self, in_channels, base_channels=64, downsample_factors=(8, 4), out_channels=(128, 256)):\n        super().__init__()\n        self.downsample_factors = downsample_factors\n        self.in_channels = in_channels\n        self.downsample_factors = downsample_factors\n        self.out_channels = out_channels\n        self.layers = nn.ModuleList()\n        self.layers += [DBlock(self.in_channels, base_channels, 1)]\n        in_channels = base_channels",
        "detail": "env.src.tts.TTS.vocoder.models.random_window_discriminator",
        "documentation": {}
    },
    {
        "label": "RandomWindowDiscriminator",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.random_window_discriminator",
        "description": "env.src.tts.TTS.vocoder.models.random_window_discriminator",
        "peekOfCode": "class RandomWindowDiscriminator(nn.Module):\n    \"\"\"Random Window Discriminator as described in\n    http://arxiv.org/abs/1909.11646\"\"\"\n    def __init__(\n        self,\n        cond_channels,\n        hop_length,\n        uncond_disc_donwsample_factors=(8, 4),\n        cond_disc_downsample_factors=((8, 4, 2, 2, 2), (8, 4, 2, 2), (8, 4, 2), (8, 4), (4, 2, 2)),\n        cond_disc_out_channels=((128, 128, 256, 256), (128, 256, 256), (128, 256), (256,), (128, 256)),",
        "detail": "env.src.tts.TTS.vocoder.models.random_window_discriminator",
        "documentation": {}
    },
    {
        "label": "SpecDiscriminator",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.univnet_discriminator",
        "description": "env.src.tts.TTS.vocoder.models.univnet_discriminator",
        "peekOfCode": "class SpecDiscriminator(nn.Module):\n    \"\"\"docstring for Discriminator.\"\"\"\n    def __init__(self, fft_size=1024, hop_length=120, win_length=600, use_spectral_norm=False):\n        super().__init__()\n        norm_f = weight_norm if use_spectral_norm is False else spectral_norm\n        self.fft_size = fft_size\n        self.hop_length = hop_length\n        self.win_length = win_length\n        self.stft = TorchSTFT(fft_size, hop_length, win_length)\n        self.discriminators = nn.ModuleList(",
        "detail": "env.src.tts.TTS.vocoder.models.univnet_discriminator",
        "documentation": {}
    },
    {
        "label": "MultiResSpecDiscriminator",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.univnet_discriminator",
        "description": "env.src.tts.TTS.vocoder.models.univnet_discriminator",
        "peekOfCode": "class MultiResSpecDiscriminator(torch.nn.Module):\n    def __init__(  # pylint: disable=dangerous-default-value\n        self, fft_sizes=[1024, 2048, 512], hop_sizes=[120, 240, 50], win_lengths=[600, 1200, 240], window=\"hann_window\"\n    ):\n        super().__init__()\n        self.discriminators = nn.ModuleList(\n            [\n                SpecDiscriminator(fft_sizes[0], hop_sizes[0], win_lengths[0], window),\n                SpecDiscriminator(fft_sizes[1], hop_sizes[1], win_lengths[1], window),\n                SpecDiscriminator(fft_sizes[2], hop_sizes[2], win_lengths[2], window),",
        "detail": "env.src.tts.TTS.vocoder.models.univnet_discriminator",
        "documentation": {}
    },
    {
        "label": "UnivnetDiscriminator",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.univnet_discriminator",
        "description": "env.src.tts.TTS.vocoder.models.univnet_discriminator",
        "peekOfCode": "class UnivnetDiscriminator(nn.Module):\n    \"\"\"Univnet discriminator wrapping MPD and MSD.\"\"\"\n    def __init__(self):\n        super().__init__()\n        self.mpd = MultiPeriodDiscriminator()\n        self.msd = MultiResSpecDiscriminator()\n    def forward(self, x):\n        \"\"\"\n        Args:\n            x (Tensor): input waveform.",
        "detail": "env.src.tts.TTS.vocoder.models.univnet_discriminator",
        "documentation": {}
    },
    {
        "label": "LRELU_SLOPE",
        "kind": 5,
        "importPath": "env.src.tts.TTS.vocoder.models.univnet_discriminator",
        "description": "env.src.tts.TTS.vocoder.models.univnet_discriminator",
        "peekOfCode": "LRELU_SLOPE = 0.1\nclass SpecDiscriminator(nn.Module):\n    \"\"\"docstring for Discriminator.\"\"\"\n    def __init__(self, fft_size=1024, hop_length=120, win_length=600, use_spectral_norm=False):\n        super().__init__()\n        norm_f = weight_norm if use_spectral_norm is False else spectral_norm\n        self.fft_size = fft_size\n        self.hop_length = hop_length\n        self.win_length = win_length\n        self.stft = TorchSTFT(fft_size, hop_length, win_length)",
        "detail": "env.src.tts.TTS.vocoder.models.univnet_discriminator",
        "documentation": {}
    },
    {
        "label": "UnivnetGenerator",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.univnet_generator",
        "description": "env.src.tts.TTS.vocoder.models.univnet_generator",
        "peekOfCode": "class UnivnetGenerator(torch.nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        hidden_channels: int,\n        cond_channels: int,\n        upsample_factors: List[int],\n        lvc_layers_each_block: int,\n        lvc_kernel_size: int,",
        "detail": "env.src.tts.TTS.vocoder.models.univnet_generator",
        "documentation": {}
    },
    {
        "label": "LRELU_SLOPE",
        "kind": 5,
        "importPath": "env.src.tts.TTS.vocoder.models.univnet_generator",
        "description": "env.src.tts.TTS.vocoder.models.univnet_generator",
        "peekOfCode": "LRELU_SLOPE = 0.1\nclass UnivnetGenerator(torch.nn.Module):\n    def __init__(\n        self,\n        in_channels: int,\n        out_channels: int,\n        hidden_channels: int,\n        cond_channels: int,\n        upsample_factors: List[int],\n        lvc_layers_each_block: int,",
        "detail": "env.src.tts.TTS.vocoder.models.univnet_generator",
        "documentation": {}
    },
    {
        "label": "WavegradArgs",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.wavegrad",
        "description": "env.src.tts.TTS.vocoder.models.wavegrad",
        "peekOfCode": "class WavegradArgs(Coqpit):\n    in_channels: int = 80\n    out_channels: int = 1\n    use_weight_norm: bool = False\n    y_conv_channels: int = 32\n    x_conv_channels: int = 768\n    dblock_out_channels: List[int] = field(default_factory=lambda: [128, 128, 256, 512])\n    ublock_out_channels: List[int] = field(default_factory=lambda: [512, 512, 256, 128, 128])\n    upsample_factors: List[int] = field(default_factory=lambda: [4, 4, 4, 2, 2])\n    upsample_dilations: List[List[int]] = field(",
        "detail": "env.src.tts.TTS.vocoder.models.wavegrad",
        "documentation": {}
    },
    {
        "label": "Wavegrad",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.wavegrad",
        "description": "env.src.tts.TTS.vocoder.models.wavegrad",
        "peekOfCode": "class Wavegrad(BaseVocoder):\n    \"\"\"  WaveGrad  model.\n    Paper - https://arxiv.org/abs/2009.00713\n    Examples:\n        Initializing the model.\n        >>> from TTS.vocoder.configs import WavegradConfig\n        >>> config = WavegradConfig()\n        >>> model = Wavegrad(config)\n    Paper Abstract:\n        This paper introduces WaveGrad, a conditional model for waveform generation which estimates gradients of the",
        "detail": "env.src.tts.TTS.vocoder.models.wavegrad",
        "documentation": {}
    },
    {
        "label": "ResBlock",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.wavernn",
        "description": "env.src.tts.TTS.vocoder.models.wavernn",
        "peekOfCode": "class ResBlock(nn.Module):\n    def __init__(self, dims):\n        super().__init__()\n        self.conv1 = nn.Conv1d(dims, dims, kernel_size=1, bias=False)\n        self.conv2 = nn.Conv1d(dims, dims, kernel_size=1, bias=False)\n        self.batch_norm1 = nn.BatchNorm1d(dims)\n        self.batch_norm2 = nn.BatchNorm1d(dims)\n    def forward(self, x):\n        residual = x\n        x = self.conv1(x)",
        "detail": "env.src.tts.TTS.vocoder.models.wavernn",
        "documentation": {}
    },
    {
        "label": "MelResNet",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.wavernn",
        "description": "env.src.tts.TTS.vocoder.models.wavernn",
        "peekOfCode": "class MelResNet(nn.Module):\n    def __init__(self, num_res_blocks, in_dims, compute_dims, res_out_dims, pad):\n        super().__init__()\n        k_size = pad * 2 + 1\n        self.conv_in = nn.Conv1d(in_dims, compute_dims, kernel_size=k_size, bias=False)\n        self.batch_norm = nn.BatchNorm1d(compute_dims)\n        self.layers = nn.ModuleList()\n        for _ in range(num_res_blocks):\n            self.layers.append(ResBlock(compute_dims))\n        self.conv_out = nn.Conv1d(compute_dims, res_out_dims, kernel_size=1)",
        "detail": "env.src.tts.TTS.vocoder.models.wavernn",
        "documentation": {}
    },
    {
        "label": "Stretch2d",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.wavernn",
        "description": "env.src.tts.TTS.vocoder.models.wavernn",
        "peekOfCode": "class Stretch2d(nn.Module):\n    def __init__(self, x_scale, y_scale):\n        super().__init__()\n        self.x_scale = x_scale\n        self.y_scale = y_scale\n    def forward(self, x):\n        b, c, h, w = x.size()\n        x = x.unsqueeze(-1).unsqueeze(3)\n        x = x.repeat(1, 1, 1, self.y_scale, 1, self.x_scale)\n        return x.view(b, c, h * self.y_scale, w * self.x_scale)",
        "detail": "env.src.tts.TTS.vocoder.models.wavernn",
        "documentation": {}
    },
    {
        "label": "UpsampleNetwork",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.wavernn",
        "description": "env.src.tts.TTS.vocoder.models.wavernn",
        "peekOfCode": "class UpsampleNetwork(nn.Module):\n    def __init__(\n        self,\n        feat_dims,\n        upsample_scales,\n        compute_dims,\n        num_res_blocks,\n        res_out_dims,\n        pad,\n        use_aux_net,",
        "detail": "env.src.tts.TTS.vocoder.models.wavernn",
        "documentation": {}
    },
    {
        "label": "Upsample",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.wavernn",
        "description": "env.src.tts.TTS.vocoder.models.wavernn",
        "peekOfCode": "class Upsample(nn.Module):\n    def __init__(self, scale, pad, num_res_blocks, feat_dims, compute_dims, res_out_dims, use_aux_net):\n        super().__init__()\n        self.scale = scale\n        self.pad = pad\n        self.indent = pad * scale\n        self.use_aux_net = use_aux_net\n        self.resnet = MelResNet(num_res_blocks, feat_dims, compute_dims, res_out_dims, pad)\n    def forward(self, m):\n        if self.use_aux_net:",
        "detail": "env.src.tts.TTS.vocoder.models.wavernn",
        "documentation": {}
    },
    {
        "label": "WavernnArgs",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.wavernn",
        "description": "env.src.tts.TTS.vocoder.models.wavernn",
        "peekOfCode": "class WavernnArgs(Coqpit):\n    \"\"\" WaveRNN model arguments.\n    rnn_dims (int):\n        Number of hidden channels in RNN layers. Defaults to 512.\n    fc_dims (int):\n        Number of hidden channels in fully-conntected layers. Defaults to 512.\n    compute_dims (int):\n        Number of hidden channels in the feature ResNet. Defaults to 128.\n    res_out_dim (int):\n        Number of hidden channels in the feature ResNet output. Defaults to 128.",
        "detail": "env.src.tts.TTS.vocoder.models.wavernn",
        "documentation": {}
    },
    {
        "label": "Wavernn",
        "kind": 6,
        "importPath": "env.src.tts.TTS.vocoder.models.wavernn",
        "description": "env.src.tts.TTS.vocoder.models.wavernn",
        "peekOfCode": "class Wavernn(BaseVocoder):\n    def __init__(self, config: Coqpit):\n        \"\"\" WaveRNN model.\n        Original paper - https://arxiv.org/abs/1802.08435\n        Official implementation - https://github.com/fatchord/WaveRNN\n        Args:\n            config (Coqpit): [description]\n        Raises:\n            RuntimeError: [description]\n        Examples:",
        "detail": "env.src.tts.TTS.vocoder.models.wavernn",
        "documentation": {}
    },
    {
        "label": "stream",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vocoder.models.wavernn",
        "description": "env.src.tts.TTS.vocoder.models.wavernn",
        "peekOfCode": "def stream(string, variables):\n    sys.stdout.write(f\"\\r{string}\" % variables)\n# pylint: disable=abstract-method\n# relates https://github.com/pytorch/pytorch/issues/42305\nclass ResBlock(nn.Module):\n    def __init__(self, dims):\n        super().__init__()\n        self.conv1 = nn.Conv1d(dims, dims, kernel_size=1, bias=False)\n        self.conv2 = nn.Conv1d(dims, dims, kernel_size=1, bias=False)\n        self.batch_norm1 = nn.BatchNorm1d(dims)",
        "detail": "env.src.tts.TTS.vocoder.models.wavernn",
        "documentation": {}
    },
    {
        "label": "gaussian_loss",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vocoder.utils.distribution",
        "description": "env.src.tts.TTS.vocoder.utils.distribution",
        "peekOfCode": "def gaussian_loss(y_hat, y, log_std_min=-7.0):\n    assert y_hat.dim() == 3\n    assert y_hat.size(2) == 2\n    mean = y_hat[:, :, :1]\n    log_std = torch.clamp(y_hat[:, :, 1:], min=log_std_min)\n    # TODO: replace with pytorch dist\n    log_probs = -0.5 * (-math.log(2.0 * math.pi) - 2.0 * log_std - torch.pow(y - mean, 2) * torch.exp((-2.0 * log_std)))\n    return log_probs.squeeze().mean()\ndef sample_from_gaussian(y_hat, log_std_min=-7.0, scale_factor=1.0):\n    assert y_hat.size(2) == 2",
        "detail": "env.src.tts.TTS.vocoder.utils.distribution",
        "documentation": {}
    },
    {
        "label": "sample_from_gaussian",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vocoder.utils.distribution",
        "description": "env.src.tts.TTS.vocoder.utils.distribution",
        "peekOfCode": "def sample_from_gaussian(y_hat, log_std_min=-7.0, scale_factor=1.0):\n    assert y_hat.size(2) == 2\n    mean = y_hat[:, :, :1]\n    log_std = torch.clamp(y_hat[:, :, 1:], min=log_std_min)\n    dist = Normal(\n        mean,\n        torch.exp(log_std),\n    )\n    sample = dist.sample()\n    sample = torch.clamp(torch.clamp(sample, min=-scale_factor), max=scale_factor)",
        "detail": "env.src.tts.TTS.vocoder.utils.distribution",
        "documentation": {}
    },
    {
        "label": "log_sum_exp",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vocoder.utils.distribution",
        "description": "env.src.tts.TTS.vocoder.utils.distribution",
        "peekOfCode": "def log_sum_exp(x):\n    \"\"\"numerically stable log_sum_exp implementation that prevents overflow\"\"\"\n    # TF ordering\n    axis = len(x.size()) - 1\n    m, _ = torch.max(x, dim=axis)\n    m2, _ = torch.max(x, dim=axis, keepdim=True)\n    return m + torch.log(torch.sum(torch.exp(x - m2), dim=axis))\n# It is adapted from https://github.com/r9y9/wavenet_vocoder/blob/master/wavenet_vocoder/mixture.py\ndef discretized_mix_logistic_loss(y_hat, y, num_classes=65536, log_scale_min=None, reduce=True):\n    if log_scale_min is None:",
        "detail": "env.src.tts.TTS.vocoder.utils.distribution",
        "documentation": {}
    },
    {
        "label": "discretized_mix_logistic_loss",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vocoder.utils.distribution",
        "description": "env.src.tts.TTS.vocoder.utils.distribution",
        "peekOfCode": "def discretized_mix_logistic_loss(y_hat, y, num_classes=65536, log_scale_min=None, reduce=True):\n    if log_scale_min is None:\n        log_scale_min = float(np.log(1e-14))\n    y_hat = y_hat.permute(0, 2, 1)\n    assert y_hat.dim() == 3\n    assert y_hat.size(1) % 3 == 0\n    nr_mix = y_hat.size(1) // 3\n    # (B x T x C)\n    y_hat = y_hat.transpose(1, 2)\n    # unpack parameters. (B, T, num_mixtures) x 3",
        "detail": "env.src.tts.TTS.vocoder.utils.distribution",
        "documentation": {}
    },
    {
        "label": "sample_from_discretized_mix_logistic",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vocoder.utils.distribution",
        "description": "env.src.tts.TTS.vocoder.utils.distribution",
        "peekOfCode": "def sample_from_discretized_mix_logistic(y, log_scale_min=None):\n    \"\"\"\n    Sample from discretized mixture of logistic distributions\n    Args:\n        y (Tensor): :math:`[B, C, T]`\n        log_scale_min (float): Log scale minimum value\n    Returns:\n        Tensor: sample in range of [-1, 1].\n    \"\"\"\n    if log_scale_min is None:",
        "detail": "env.src.tts.TTS.vocoder.utils.distribution",
        "documentation": {}
    },
    {
        "label": "to_one_hot",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vocoder.utils.distribution",
        "description": "env.src.tts.TTS.vocoder.utils.distribution",
        "peekOfCode": "def to_one_hot(tensor, n, fill_with=1.0):\n    # we perform one hot encore with respect to the last axis\n    one_hot = torch.FloatTensor(tensor.size() + (n,)).zero_().type_as(tensor)\n    one_hot.scatter_(len(tensor.size()), tensor.unsqueeze(-1), fill_with)\n    return one_hot",
        "detail": "env.src.tts.TTS.vocoder.utils.distribution",
        "documentation": {}
    },
    {
        "label": "interpolate_vocoder_input",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vocoder.utils.generic_utils",
        "description": "env.src.tts.TTS.vocoder.utils.generic_utils",
        "peekOfCode": "def interpolate_vocoder_input(scale_factor, spec):\n    \"\"\"Interpolate spectrogram by the scale factor.\n    It is mainly used to match the sampling rates of\n    the tts and vocoder models.\n    Args:\n        scale_factor (float): scale factor to interpolate the spectrogram\n        spec (np.array): spectrogram to be interpolated\n    Returns:\n        torch.tensor: interpolated spectrogram.\n    \"\"\"",
        "detail": "env.src.tts.TTS.vocoder.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "plot_results",
        "kind": 2,
        "importPath": "env.src.tts.TTS.vocoder.utils.generic_utils",
        "description": "env.src.tts.TTS.vocoder.utils.generic_utils",
        "peekOfCode": "def plot_results(y_hat: torch.tensor, y: torch.tensor, ap: AudioProcessor, name_prefix: str = None) -> Dict:\n    \"\"\"Plot the predicted and the real waveform and their spectrograms.\n    Args:\n        y_hat (torch.tensor): Predicted waveform.\n        y (torch.tensor): Real waveform.\n        ap (AudioProcessor): Audio processor used to process the waveform.\n        name_prefix (str, optional): Name prefix used to name the figures. Defaults to None.\n    Returns:\n        Dict: output figures keyed by the name of the figures.\n    \"\"\" \"\"\"Plot vocoder model results\"\"\"",
        "detail": "env.src.tts.TTS.vocoder.utils.generic_utils",
        "documentation": {}
    },
    {
        "label": "Speaker",
        "kind": 6,
        "importPath": "env.src.tts.TTS.api",
        "description": "env.src.tts.TTS.api",
        "peekOfCode": "class Speaker(object):\n    \"\"\"Convert dict to object.\"\"\"\n    def __init__(self, d, is_voice=False):\n        self.is_voice = is_voice\n        for k, v in d.items():\n            if isinstance(k, (list, tuple)):\n                setattr(self, k, [Speaker(x) if isinstance(x, dict) else x for x in v])\n            else:\n                setattr(self, k, Speaker(v) if isinstance(v, dict) else v)\n    def __repr__(self):",
        "detail": "env.src.tts.TTS.api",
        "documentation": {}
    },
    {
        "label": "CS_API",
        "kind": 6,
        "importPath": "env.src.tts.TTS.api",
        "description": "env.src.tts.TTS.api",
        "peekOfCode": "class CS_API:\n    \"\"\"Coqui Studio API Wrapper.\n    Coqui Studio is the most advanced voice generation platform. You can generate new voices by voice cloning, voice\n    interpolation, or our unique prompt to voice technology. It also provides a set of built-in voices with different\n    characteristics. You can use these voices to generate new audio files or use them in your applications.\n    You can use all the built-in and your own Coqui Studio speakers with this API with an API token.\n    You can signup to Coqui Studio from https://app.coqui.ai/auth/signup and get an API token from\n    https://app.coqui.ai/account. We can either enter the token as an environment variable as\n    `export COQUI_STUDIO_TOKEN=<token>` or pass it as `CS_API(api_token=<toke>)`.\n    Visit https://app.coqui.ai/api for more information.",
        "detail": "env.src.tts.TTS.api",
        "documentation": {}
    },
    {
        "label": "TTS",
        "kind": 6,
        "importPath": "env.src.tts.TTS.api",
        "description": "env.src.tts.TTS.api",
        "peekOfCode": "class TTS:\n    \"\"\"TODO: Add voice conversion and Capacitron support.\"\"\"\n    def __init__(\n        self,\n        model_name: str = None,\n        model_path: str = None,\n        config_path: str = None,\n        vocoder_path: str = None,\n        vocoder_config_path: str = None,\n        progress_bar: bool = True,",
        "detail": "env.src.tts.TTS.api",
        "documentation": {}
    },
    {
        "label": "BaseTrainerModel",
        "kind": 6,
        "importPath": "env.src.tts.TTS.model",
        "description": "env.src.tts.TTS.model",
        "peekOfCode": "class BaseTrainerModel(TrainerModel):\n    \"\"\"BaseTrainerModel model expanding TrainerModel with required functions by TTS.\n    Every new TTS model must inherit it.\n    \"\"\"\n    @staticmethod\n    @abstractmethod\n    def init_from_config(config: Coqpit):\n        \"\"\"Init the model and all its attributes from the given config.\n        Override this depending on your model.\n        \"\"\"",
        "detail": "env.src.tts.TTS.model",
        "documentation": {}
    },
    {
        "label": "autodoc_mock_imports",
        "kind": 5,
        "importPath": "env.src.tts.docs.source.conf",
        "description": "env.src.tts.docs.source.conf",
        "peekOfCode": "autodoc_mock_imports = [\"soundfile\"]\n# -- Project information -----------------------------------------------------\nproject = 'TTS'\ncopyright = \"2021 Coqui GmbH, 2020 TTS authors\"\nauthor = 'Coqui GmbH'\nwith open(\"../../TTS/VERSION\", \"r\") as ver:\n    version = ver.read().strip()\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.",
        "detail": "env.src.tts.docs.source.conf",
        "documentation": {}
    },
    {
        "label": "project",
        "kind": 5,
        "importPath": "env.src.tts.docs.source.conf",
        "description": "env.src.tts.docs.source.conf",
        "peekOfCode": "project = 'TTS'\ncopyright = \"2021 Coqui GmbH, 2020 TTS authors\"\nauthor = 'Coqui GmbH'\nwith open(\"../../TTS/VERSION\", \"r\") as ver:\n    version = ver.read().strip()\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\nrelease = version\n# The main toctree document.",
        "detail": "env.src.tts.docs.source.conf",
        "documentation": {}
    },
    {
        "label": "copyright",
        "kind": 5,
        "importPath": "env.src.tts.docs.source.conf",
        "description": "env.src.tts.docs.source.conf",
        "peekOfCode": "copyright = \"2021 Coqui GmbH, 2020 TTS authors\"\nauthor = 'Coqui GmbH'\nwith open(\"../../TTS/VERSION\", \"r\") as ver:\n    version = ver.read().strip()\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\nrelease = version\n# The main toctree document.\nmaster_doc = \"index\"",
        "detail": "env.src.tts.docs.source.conf",
        "documentation": {}
    },
    {
        "label": "author",
        "kind": 5,
        "importPath": "env.src.tts.docs.source.conf",
        "description": "env.src.tts.docs.source.conf",
        "peekOfCode": "author = 'Coqui GmbH'\nwith open(\"../../TTS/VERSION\", \"r\") as ver:\n    version = ver.read().strip()\n# The version info for the project you're documenting, acts as replacement for\n# |version| and |release|, also used in various other places throughout the\n# built documents.\nrelease = version\n# The main toctree document.\nmaster_doc = \"index\"\n# -- General configuration ---------------------------------------------------",
        "detail": "env.src.tts.docs.source.conf",
        "documentation": {}
    },
    {
        "label": "release",
        "kind": 5,
        "importPath": "env.src.tts.docs.source.conf",
        "description": "env.src.tts.docs.source.conf",
        "peekOfCode": "release = version\n# The main toctree document.\nmaster_doc = \"index\"\n# -- General configuration ---------------------------------------------------\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.autosummary',",
        "detail": "env.src.tts.docs.source.conf",
        "documentation": {}
    },
    {
        "label": "master_doc",
        "kind": 5,
        "importPath": "env.src.tts.docs.source.conf",
        "description": "env.src.tts.docs.source.conf",
        "peekOfCode": "master_doc = \"index\"\n# -- General configuration ---------------------------------------------------\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom\n# ones.\nextensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.autosummary',\n    'sphinx.ext.doctest',\n    'sphinx.ext.intersphinx',",
        "detail": "env.src.tts.docs.source.conf",
        "documentation": {}
    },
    {
        "label": "extensions",
        "kind": 5,
        "importPath": "env.src.tts.docs.source.conf",
        "description": "env.src.tts.docs.source.conf",
        "peekOfCode": "extensions = [\n    'sphinx.ext.autodoc',\n    'sphinx.ext.autosummary',\n    'sphinx.ext.doctest',\n    'sphinx.ext.intersphinx',\n    'sphinx.ext.todo',\n    'sphinx.ext.coverage',\n    'sphinx.ext.napoleon',\n    'sphinx.ext.viewcode',\n    'sphinx.ext.autosectionlabel',",
        "detail": "env.src.tts.docs.source.conf",
        "documentation": {}
    },
    {
        "label": "templates_path",
        "kind": 5,
        "importPath": "env.src.tts.docs.source.conf",
        "description": "env.src.tts.docs.source.conf",
        "peekOfCode": "templates_path = ['_templates']\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This pattern also affects html_static_path and html_extra_path.\nexclude_patterns = ['_build', 'Thumbs.db', '.DS_Store', 'TODO/*']\nsource_suffix = [\".rst\", \".md\"]\nmyst_enable_extensions = ['linkify',]\n# 'sphinxcontrib.katex',\n# 'sphinx.ext.autosectionlabel',\n# autosectionlabel throws warnings if section names are duplicated.",
        "detail": "env.src.tts.docs.source.conf",
        "documentation": {}
    },
    {
        "label": "exclude_patterns",
        "kind": 5,
        "importPath": "env.src.tts.docs.source.conf",
        "description": "env.src.tts.docs.source.conf",
        "peekOfCode": "exclude_patterns = ['_build', 'Thumbs.db', '.DS_Store', 'TODO/*']\nsource_suffix = [\".rst\", \".md\"]\nmyst_enable_extensions = ['linkify',]\n# 'sphinxcontrib.katex',\n# 'sphinx.ext.autosectionlabel',\n# autosectionlabel throws warnings if section names are duplicated.\n# The following tells autosectionlabel to not throw a warning for\n# duplicated section names that are in different documents.\nautosectionlabel_prefix_document = True\nlanguage = None",
        "detail": "env.src.tts.docs.source.conf",
        "documentation": {}
    },
    {
        "label": "source_suffix",
        "kind": 5,
        "importPath": "env.src.tts.docs.source.conf",
        "description": "env.src.tts.docs.source.conf",
        "peekOfCode": "source_suffix = [\".rst\", \".md\"]\nmyst_enable_extensions = ['linkify',]\n# 'sphinxcontrib.katex',\n# 'sphinx.ext.autosectionlabel',\n# autosectionlabel throws warnings if section names are duplicated.\n# The following tells autosectionlabel to not throw a warning for\n# duplicated section names that are in different documents.\nautosectionlabel_prefix_document = True\nlanguage = None\nautodoc_inherit_docstrings = False",
        "detail": "env.src.tts.docs.source.conf",
        "documentation": {}
    },
    {
        "label": "myst_enable_extensions",
        "kind": 5,
        "importPath": "env.src.tts.docs.source.conf",
        "description": "env.src.tts.docs.source.conf",
        "peekOfCode": "myst_enable_extensions = ['linkify',]\n# 'sphinxcontrib.katex',\n# 'sphinx.ext.autosectionlabel',\n# autosectionlabel throws warnings if section names are duplicated.\n# The following tells autosectionlabel to not throw a warning for\n# duplicated section names that are in different documents.\nautosectionlabel_prefix_document = True\nlanguage = None\nautodoc_inherit_docstrings = False\n# Disable displaying type annotations, these can be very verbose",
        "detail": "env.src.tts.docs.source.conf",
        "documentation": {}
    },
    {
        "label": "autosectionlabel_prefix_document",
        "kind": 5,
        "importPath": "env.src.tts.docs.source.conf",
        "description": "env.src.tts.docs.source.conf",
        "peekOfCode": "autosectionlabel_prefix_document = True\nlanguage = None\nautodoc_inherit_docstrings = False\n# Disable displaying type annotations, these can be very verbose\nautodoc_typehints = 'none'\n# Enable overriding of function signatures in the first line of the docstring.\nautodoc_docstring_signature = True\nnapoleon_custom_sections = [('Shapes', 'shape')]\n# -- Options for HTML output -------------------------------------------------\n# The theme to use for HTML and HTML Help pages.  See the documentation for",
        "detail": "env.src.tts.docs.source.conf",
        "documentation": {}
    },
    {
        "label": "language",
        "kind": 5,
        "importPath": "env.src.tts.docs.source.conf",
        "description": "env.src.tts.docs.source.conf",
        "peekOfCode": "language = None\nautodoc_inherit_docstrings = False\n# Disable displaying type annotations, these can be very verbose\nautodoc_typehints = 'none'\n# Enable overriding of function signatures in the first line of the docstring.\nautodoc_docstring_signature = True\nnapoleon_custom_sections = [('Shapes', 'shape')]\n# -- Options for HTML output -------------------------------------------------\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.",
        "detail": "env.src.tts.docs.source.conf",
        "documentation": {}
    },
    {
        "label": "autodoc_inherit_docstrings",
        "kind": 5,
        "importPath": "env.src.tts.docs.source.conf",
        "description": "env.src.tts.docs.source.conf",
        "peekOfCode": "autodoc_inherit_docstrings = False\n# Disable displaying type annotations, these can be very verbose\nautodoc_typehints = 'none'\n# Enable overriding of function signatures in the first line of the docstring.\nautodoc_docstring_signature = True\nnapoleon_custom_sections = [('Shapes', 'shape')]\n# -- Options for HTML output -------------------------------------------------\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#",
        "detail": "env.src.tts.docs.source.conf",
        "documentation": {}
    },
    {
        "label": "autodoc_typehints",
        "kind": 5,
        "importPath": "env.src.tts.docs.source.conf",
        "description": "env.src.tts.docs.source.conf",
        "peekOfCode": "autodoc_typehints = 'none'\n# Enable overriding of function signatures in the first line of the docstring.\nautodoc_docstring_signature = True\nnapoleon_custom_sections = [('Shapes', 'shape')]\n# -- Options for HTML output -------------------------------------------------\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = 'furo'\nhtml_tite = \"TTS\"",
        "detail": "env.src.tts.docs.source.conf",
        "documentation": {}
    },
    {
        "label": "autodoc_docstring_signature",
        "kind": 5,
        "importPath": "env.src.tts.docs.source.conf",
        "description": "env.src.tts.docs.source.conf",
        "peekOfCode": "autodoc_docstring_signature = True\nnapoleon_custom_sections = [('Shapes', 'shape')]\n# -- Options for HTML output -------------------------------------------------\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = 'furo'\nhtml_tite = \"TTS\"\nhtml_theme_options = {\n    \"light_logo\": \"logo.png\",",
        "detail": "env.src.tts.docs.source.conf",
        "documentation": {}
    },
    {
        "label": "napoleon_custom_sections",
        "kind": 5,
        "importPath": "env.src.tts.docs.source.conf",
        "description": "env.src.tts.docs.source.conf",
        "peekOfCode": "napoleon_custom_sections = [('Shapes', 'shape')]\n# -- Options for HTML output -------------------------------------------------\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\nhtml_theme = 'furo'\nhtml_tite = \"TTS\"\nhtml_theme_options = {\n    \"light_logo\": \"logo.png\",\n    \"dark_logo\": \"logo.png\",",
        "detail": "env.src.tts.docs.source.conf",
        "documentation": {}
    },
    {
        "label": "html_theme",
        "kind": 5,
        "importPath": "env.src.tts.docs.source.conf",
        "description": "env.src.tts.docs.source.conf",
        "peekOfCode": "html_theme = 'furo'\nhtml_tite = \"TTS\"\nhtml_theme_options = {\n    \"light_logo\": \"logo.png\",\n    \"dark_logo\": \"logo.png\",\n    \"sidebar_hide_name\": True,\n}\nhtml_sidebars = {\n        '**': [\n               \"sidebar/scroll-start.html\",",
        "detail": "env.src.tts.docs.source.conf",
        "documentation": {}
    },
    {
        "label": "html_tite",
        "kind": 5,
        "importPath": "env.src.tts.docs.source.conf",
        "description": "env.src.tts.docs.source.conf",
        "peekOfCode": "html_tite = \"TTS\"\nhtml_theme_options = {\n    \"light_logo\": \"logo.png\",\n    \"dark_logo\": \"logo.png\",\n    \"sidebar_hide_name\": True,\n}\nhtml_sidebars = {\n        '**': [\n               \"sidebar/scroll-start.html\",\n    \"sidebar/brand.html\",",
        "detail": "env.src.tts.docs.source.conf",
        "documentation": {}
    },
    {
        "label": "html_theme_options",
        "kind": 5,
        "importPath": "env.src.tts.docs.source.conf",
        "description": "env.src.tts.docs.source.conf",
        "peekOfCode": "html_theme_options = {\n    \"light_logo\": \"logo.png\",\n    \"dark_logo\": \"logo.png\",\n    \"sidebar_hide_name\": True,\n}\nhtml_sidebars = {\n        '**': [\n               \"sidebar/scroll-start.html\",\n    \"sidebar/brand.html\",\n    \"sidebar/search.html\",",
        "detail": "env.src.tts.docs.source.conf",
        "documentation": {}
    },
    {
        "label": "html_sidebars",
        "kind": 5,
        "importPath": "env.src.tts.docs.source.conf",
        "description": "env.src.tts.docs.source.conf",
        "peekOfCode": "html_sidebars = {\n        '**': [\n               \"sidebar/scroll-start.html\",\n    \"sidebar/brand.html\",\n    \"sidebar/search.html\",\n    \"sidebar/navigation.html\",\n    \"sidebar/ethical-ads.html\",\n    \"sidebar/scroll-end.html\",\n        ]\n    }",
        "detail": "env.src.tts.docs.source.conf",
        "documentation": {}
    },
    {
        "label": "html_static_path",
        "kind": 5,
        "importPath": "env.src.tts.docs.source.conf",
        "description": "env.src.tts.docs.source.conf",
        "peekOfCode": "html_static_path = ['_static']",
        "detail": "env.src.tts.docs.source.conf",
        "documentation": {}
    },
    {
        "label": "get_audio_seconds",
        "kind": 2,
        "importPath": "env.src.tts.notebooks.dataset_analysis.analyze",
        "description": "env.src.tts.notebooks.dataset_analysis.analyze",
        "peekOfCode": "def get_audio_seconds(frames):\n    return (frames * 12.5) / 1000\ndef append_data_statistics(meta_data):\n    # get data statistics\n    for char_cnt in meta_data:\n        data = meta_data[char_cnt][\"data\"]\n        audio_len_list = [d[\"audio_len\"] for d in data]\n        mean_audio_len = mean(audio_len_list)\n        try:\n            mode_audio_list = [round(d[\"audio_len\"], 2) for d in data]",
        "detail": "env.src.tts.notebooks.dataset_analysis.analyze",
        "documentation": {}
    },
    {
        "label": "append_data_statistics",
        "kind": 2,
        "importPath": "env.src.tts.notebooks.dataset_analysis.analyze",
        "description": "env.src.tts.notebooks.dataset_analysis.analyze",
        "peekOfCode": "def append_data_statistics(meta_data):\n    # get data statistics\n    for char_cnt in meta_data:\n        data = meta_data[char_cnt][\"data\"]\n        audio_len_list = [d[\"audio_len\"] for d in data]\n        mean_audio_len = mean(audio_len_list)\n        try:\n            mode_audio_list = [round(d[\"audio_len\"], 2) for d in data]\n            mode_audio_len = mode(mode_audio_list)\n        except StatisticsError:",
        "detail": "env.src.tts.notebooks.dataset_analysis.analyze",
        "documentation": {}
    },
    {
        "label": "process_meta_data",
        "kind": 2,
        "importPath": "env.src.tts.notebooks.dataset_analysis.analyze",
        "description": "env.src.tts.notebooks.dataset_analysis.analyze",
        "peekOfCode": "def process_meta_data(path):\n    meta_data = {}\n    # load meta data\n    with open(path, \"r\", encoding=\"utf-8\") as f:\n        data = csv.reader(f, delimiter=\"|\")\n        for row in data:\n            frames = int(row[2])\n            utt = row[3]\n            audio_len = get_audio_seconds(frames)\n            char_count = len(utt)",
        "detail": "env.src.tts.notebooks.dataset_analysis.analyze",
        "documentation": {}
    },
    {
        "label": "get_data_points",
        "kind": 2,
        "importPath": "env.src.tts.notebooks.dataset_analysis.analyze",
        "description": "env.src.tts.notebooks.dataset_analysis.analyze",
        "peekOfCode": "def get_data_points(meta_data):\n    x = meta_data\n    y_avg = [meta_data[d][\"mean\"] for d in meta_data]\n    y_mode = [meta_data[d][\"mode\"] for d in meta_data]\n    y_median = [meta_data[d][\"median\"] for d in meta_data]\n    y_std = [meta_data[d][\"std\"] for d in meta_data]\n    y_num_samples = [len(meta_data[d][\"data\"]) for d in meta_data]\n    return {\n        \"x\": x,\n        \"y_avg\": y_avg,",
        "detail": "env.src.tts.notebooks.dataset_analysis.analyze",
        "documentation": {}
    },
    {
        "label": "save_training",
        "kind": 2,
        "importPath": "env.src.tts.notebooks.dataset_analysis.analyze",
        "description": "env.src.tts.notebooks.dataset_analysis.analyze",
        "peekOfCode": "def save_training(file_path, meta_data):\n    rows = []\n    for char_cnt in meta_data:\n        data = meta_data[char_cnt][\"data\"]\n        for d in data:\n            rows.append(d[\"row\"] + \"\\n\")\n    random.shuffle(rows)\n    with open(file_path, \"w+\", encoding=\"utf-8\") as f:\n        for row in rows:\n            f.write(row)",
        "detail": "env.src.tts.notebooks.dataset_analysis.analyze",
        "documentation": {}
    },
    {
        "label": "plot",
        "kind": 2,
        "importPath": "env.src.tts.notebooks.dataset_analysis.analyze",
        "description": "env.src.tts.notebooks.dataset_analysis.analyze",
        "peekOfCode": "def plot(meta_data, save_path=None):\n    save = False\n    if save_path:\n        save = True\n    graph_data = get_data_points(meta_data)\n    x = graph_data[\"x\"]\n    y_avg = graph_data[\"y_avg\"]\n    y_std = graph_data[\"y_std\"]\n    y_mode = graph_data[\"y_mode\"]\n    y_median = graph_data[\"y_median\"]",
        "detail": "env.src.tts.notebooks.dataset_analysis.analyze",
        "documentation": {}
    },
    {
        "label": "plot_phonemes",
        "kind": 2,
        "importPath": "env.src.tts.notebooks.dataset_analysis.analyze",
        "description": "env.src.tts.notebooks.dataset_analysis.analyze",
        "peekOfCode": "def plot_phonemes(train_path, cmu_dict_path, save_path):\n    cmudict = CMUDict(cmu_dict_path)\n    phonemes = {}\n    with open(train_path, \"r\", encoding=\"utf-8\") as f:\n        data = csv.reader(f, delimiter=\"|\")\n        phonemes[\"None\"] = 0\n        for row in data:\n            words = row[3].split()\n            for word in words:\n                pho = cmudict.lookup(word)",
        "detail": "env.src.tts.notebooks.dataset_analysis.analyze",
        "documentation": {}
    },
    {
        "label": "main",
        "kind": 2,
        "importPath": "env.src.tts.notebooks.dataset_analysis.analyze",
        "description": "env.src.tts.notebooks.dataset_analysis.analyze",
        "peekOfCode": "def main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--train_file_path\",\n        required=True,\n        help=\"this is the path to the train.txt file that the preprocess.py script creates\",\n    )\n    parser.add_argument(\"--save_to\", help=\"path to save charts of data to\")\n    parser.add_argument(\"--cmu_dict_path\", help=\"give cmudict-0.7b to see phoneme distribution\")\n    args = parser.parse_args()",
        "detail": "env.src.tts.notebooks.dataset_analysis.analyze",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "description": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\ndata_path = \"/srv/data/\"\n# Using LJSpeech like dataset processing for the blizzard dataset\ndataset_config = BaseDatasetConfig(formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=data_path)\naudio_config = BaseAudioConfig(\n    sample_rate=24000,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=True,\n    mel_fmin=80.0,",
        "detail": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "documentation": {}
    },
    {
        "label": "data_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "description": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "peekOfCode": "data_path = \"/srv/data/\"\n# Using LJSpeech like dataset processing for the blizzard dataset\ndataset_config = BaseDatasetConfig(formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=data_path)\naudio_config = BaseAudioConfig(\n    sample_rate=24000,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=True,\n    mel_fmin=80.0,\n    mel_fmax=12000,",
        "detail": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "description": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "peekOfCode": "dataset_config = BaseDatasetConfig(formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=data_path)\naudio_config = BaseAudioConfig(\n    sample_rate=24000,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=True,\n    mel_fmin=80.0,\n    mel_fmax=12000,\n    spec_gain=20.0,\n    log_func=\"np.log10\",",
        "detail": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "description": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "peekOfCode": "audio_config = BaseAudioConfig(\n    sample_rate=24000,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=True,\n    mel_fmin=80.0,\n    mel_fmax=12000,\n    spec_gain=20.0,\n    log_func=\"np.log10\",\n    ref_level_db=20,",
        "detail": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "documentation": {}
    },
    {
        "label": "capacitron_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "description": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "peekOfCode": "capacitron_config = CapacitronVAEConfig(capacitron_VAE_loss_alpha=1.0)\nconfig = TacotronConfig(\n    run_name=\"Blizzard-Capacitron-T1\",\n    audio=audio_config,\n    capacitron_vae=capacitron_config,\n    use_capacitron_vae=True,\n    batch_size=128,  # Tune this to your gpu\n    max_audio_len=6 * 24000,  # Tune this to your gpu\n    min_audio_len=0.5 * 24000,\n    eval_batch_size=16,",
        "detail": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "description": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "peekOfCode": "config = TacotronConfig(\n    run_name=\"Blizzard-Capacitron-T1\",\n    audio=audio_config,\n    capacitron_vae=capacitron_config,\n    use_capacitron_vae=True,\n    batch_size=128,  # Tune this to your gpu\n    max_audio_len=6 * 24000,  # Tune this to your gpu\n    min_audio_len=0.5 * 24000,\n    eval_batch_size=16,\n    num_loader_workers=12,",
        "detail": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "description": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "peekOfCode": "ap = AudioProcessor(**config.audio.to_dict())\ntokenizer, config = TTSTokenizer.init_from_config(config)\ntrain_samples, eval_samples = load_tts_samples(dataset_config, eval_split=True)\nmodel = Tacotron(config, ap, tokenizer, speaker_manager=None)\ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,",
        "detail": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "description": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "peekOfCode": "model = Tacotron(config, ap, tokenizer, speaker_manager=None)\ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n)\n# ",
        "detail": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "description": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n)\n# \ntrainer.fit()",
        "detail": "env.src.tts.recipes.blizzard2013.tacotron1-Capacitron.train_capacitron_t1",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "description": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\ndata_path = \"/srv/data/blizzard2013/segmented\"\n# Using LJSpeech like dataset processing for the blizzard dataset\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",\n    path=data_path,\n)\naudio_config = BaseAudioConfig(\n    sample_rate=24000,",
        "detail": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "documentation": {}
    },
    {
        "label": "data_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "description": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "peekOfCode": "data_path = \"/srv/data/blizzard2013/segmented\"\n# Using LJSpeech like dataset processing for the blizzard dataset\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",\n    path=data_path,\n)\naudio_config = BaseAudioConfig(\n    sample_rate=24000,\n    do_trim_silence=True,",
        "detail": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "description": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "peekOfCode": "dataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",\n    path=data_path,\n)\naudio_config = BaseAudioConfig(\n    sample_rate=24000,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=True,",
        "detail": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "description": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "peekOfCode": "audio_config = BaseAudioConfig(\n    sample_rate=24000,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=True,\n    mel_fmin=80.0,\n    mel_fmax=12000,\n    spec_gain=25.0,\n    log_func=\"np.log10\",\n    ref_level_db=20,",
        "detail": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "documentation": {}
    },
    {
        "label": "capacitron_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "description": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "peekOfCode": "capacitron_config = CapacitronVAEConfig(capacitron_VAE_loss_alpha=1.0)\nconfig = Tacotron2Config(\n    run_name=\"Blizzard-Capacitron-T2\",\n    audio=audio_config,\n    capacitron_vae=capacitron_config,\n    use_capacitron_vae=True,\n    batch_size=246,  # Tune this to your gpu\n    max_audio_len=6 * 24000,  # Tune this to your gpu\n    min_audio_len=1 * 24000,\n    eval_batch_size=16,",
        "detail": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "description": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "peekOfCode": "config = Tacotron2Config(\n    run_name=\"Blizzard-Capacitron-T2\",\n    audio=audio_config,\n    capacitron_vae=capacitron_config,\n    use_capacitron_vae=True,\n    batch_size=246,  # Tune this to your gpu\n    max_audio_len=6 * 24000,  # Tune this to your gpu\n    min_audio_len=1 * 24000,\n    eval_batch_size=16,\n    num_loader_workers=12,",
        "detail": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "description": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "peekOfCode": "ap = AudioProcessor(**config.audio.to_dict())\ntokenizer, config = TTSTokenizer.init_from_config(config)\ntrain_samples, eval_samples = load_tts_samples(dataset_config, eval_split=True)\nmodel = Tacotron2(config, ap, tokenizer, speaker_manager=None)\ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,",
        "detail": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "description": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "peekOfCode": "model = Tacotron2(config, ap, tokenizer, speaker_manager=None)\ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n    training_assets={\"audio_processor\": ap},\n)",
        "detail": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "description": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n    training_assets={\"audio_processor\": ap},\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.blizzard2013.tacotron2-Capacitron.train_capacitron_t2",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.align_tts.train_aligntts",
        "description": "env.src.tts.recipes.ljspeech.align_tts.train_aligntts",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\n# init configs\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../LJSpeech-1.1/\")\n)\nconfig = AlignTTSConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,",
        "detail": "env.src.tts.recipes.ljspeech.align_tts.train_aligntts",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.align_tts.train_aligntts",
        "description": "env.src.tts.recipes.ljspeech.align_tts.train_aligntts",
        "peekOfCode": "dataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../LJSpeech-1.1/\")\n)\nconfig = AlignTTSConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,",
        "detail": "env.src.tts.recipes.ljspeech.align_tts.train_aligntts",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.align_tts.train_aligntts",
        "description": "env.src.tts.recipes.ljspeech.align_tts.train_aligntts",
        "peekOfCode": "config = AlignTTSConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=False,",
        "detail": "env.src.tts.recipes.ljspeech.align_tts.train_aligntts",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.align_tts.train_aligntts",
        "description": "env.src.tts.recipes.ljspeech.align_tts.train_aligntts",
        "peekOfCode": "ap = AudioProcessor.init_from_config(config)\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.",
        "detail": "env.src.tts.recipes.ljspeech.align_tts.train_aligntts",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.align_tts.train_aligntts",
        "description": "env.src.tts.recipes.ljspeech.align_tts.train_aligntts",
        "peekOfCode": "model = AlignTTS(config, ap, tokenizer)\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.align_tts.train_aligntts",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.align_tts.train_aligntts",
        "description": "env.src.tts.recipes.ljspeech.align_tts.train_aligntts",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.align_tts.train_aligntts",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.fast_pitch.train_fast_pitch",
        "description": "env.src.tts.recipes.ljspeech.fast_pitch.train_fast_pitch",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\n# init configs\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",\n    # meta_file_attn_mask=os.path.join(output_path, \"../LJSpeech-1.1/metadata_attn_mask.txt\"),\n    path=os.path.join(output_path, \"../LJSpeech-1.1/\"),\n)\naudio_config = BaseAudioConfig(\n    sample_rate=22050,",
        "detail": "env.src.tts.recipes.ljspeech.fast_pitch.train_fast_pitch",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.fast_pitch.train_fast_pitch",
        "description": "env.src.tts.recipes.ljspeech.fast_pitch.train_fast_pitch",
        "peekOfCode": "dataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",\n    # meta_file_attn_mask=os.path.join(output_path, \"../LJSpeech-1.1/metadata_attn_mask.txt\"),\n    path=os.path.join(output_path, \"../LJSpeech-1.1/\"),\n)\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,",
        "detail": "env.src.tts.recipes.ljspeech.fast_pitch.train_fast_pitch",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.fast_pitch.train_fast_pitch",
        "description": "env.src.tts.recipes.ljspeech.fast_pitch.train_fast_pitch",
        "peekOfCode": "audio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,",
        "detail": "env.src.tts.recipes.ljspeech.fast_pitch.train_fast_pitch",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.fast_pitch.train_fast_pitch",
        "description": "env.src.tts.recipes.ljspeech.fast_pitch.train_fast_pitch",
        "peekOfCode": "config = FastPitchConfig(\n    run_name=\"fast_pitch_ljspeech\",\n    audio=audio_config,\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=8,\n    num_eval_loader_workers=4,\n    compute_input_seq_cache=True,\n    compute_f0=True,\n    f0_cache_path=os.path.join(output_path, \"f0_cache\"),",
        "detail": "env.src.tts.recipes.ljspeech.fast_pitch.train_fast_pitch",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.fast_pitch.train_fast_pitch",
        "description": "env.src.tts.recipes.ljspeech.fast_pitch.train_fast_pitch",
        "peekOfCode": "ap = AudioProcessor.init_from_config(config)\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.",
        "detail": "env.src.tts.recipes.ljspeech.fast_pitch.train_fast_pitch",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.fast_pitch.train_fast_pitch",
        "description": "env.src.tts.recipes.ljspeech.fast_pitch.train_fast_pitch",
        "peekOfCode": "model = ForwardTTS(config, ap, tokenizer, speaker_manager=None)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.fast_pitch.train_fast_pitch",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.fast_pitch.train_fast_pitch",
        "description": "env.src.tts.recipes.ljspeech.fast_pitch.train_fast_pitch",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.fast_pitch.train_fast_pitch",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.fast_speech.train_fast_speech",
        "description": "env.src.tts.recipes.ljspeech.fast_speech.train_fast_speech",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\n# init configs\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",\n    # meta_file_attn_mask=os.path.join(output_path, \"../LJSpeech-1.1/metadata_attn_mask.txt\"),\n    path=os.path.join(output_path, \"../LJSpeech-1.1/\"),\n)\naudio_config = BaseAudioConfig(\n    sample_rate=22050,",
        "detail": "env.src.tts.recipes.ljspeech.fast_speech.train_fast_speech",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.fast_speech.train_fast_speech",
        "description": "env.src.tts.recipes.ljspeech.fast_speech.train_fast_speech",
        "peekOfCode": "dataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",\n    # meta_file_attn_mask=os.path.join(output_path, \"../LJSpeech-1.1/metadata_attn_mask.txt\"),\n    path=os.path.join(output_path, \"../LJSpeech-1.1/\"),\n)\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,",
        "detail": "env.src.tts.recipes.ljspeech.fast_speech.train_fast_speech",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.fast_speech.train_fast_speech",
        "description": "env.src.tts.recipes.ljspeech.fast_speech.train_fast_speech",
        "peekOfCode": "audio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,",
        "detail": "env.src.tts.recipes.ljspeech.fast_speech.train_fast_speech",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.fast_speech.train_fast_speech",
        "description": "env.src.tts.recipes.ljspeech.fast_speech.train_fast_speech",
        "peekOfCode": "config = FastSpeechConfig(\n    run_name=\"fast_speech_ljspeech\",\n    audio=audio_config,\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=8,\n    num_eval_loader_workers=4,\n    compute_input_seq_cache=True,\n    compute_f0=False,\n    run_eval=True,",
        "detail": "env.src.tts.recipes.ljspeech.fast_speech.train_fast_speech",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.fast_speech.train_fast_speech",
        "description": "env.src.tts.recipes.ljspeech.fast_speech.train_fast_speech",
        "peekOfCode": "ap = AudioProcessor.init_from_config(config)\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.",
        "detail": "env.src.tts.recipes.ljspeech.fast_speech.train_fast_speech",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.fast_speech.train_fast_speech",
        "description": "env.src.tts.recipes.ljspeech.fast_speech.train_fast_speech",
        "peekOfCode": "model = ForwardTTS(config, ap, tokenizer)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.fast_speech.train_fast_speech",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.fast_speech.train_fast_speech",
        "description": "env.src.tts.recipes.ljspeech.fast_speech.train_fast_speech",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.fast_speech.train_fast_speech",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.fastspeech2.train_fastspeech2",
        "description": "env.src.tts.recipes.ljspeech.fastspeech2.train_fastspeech2",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\n# init configs\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",\n    # meta_file_attn_mask=os.path.join(output_path, \"../LJSpeech-1.1/metadata_attn_mask.txt\"),\n    path=os.path.join(output_path, \"../LJSpeech-1.1/\"),\n)\naudio_config = BaseAudioConfig(\n    sample_rate=22050,",
        "detail": "env.src.tts.recipes.ljspeech.fastspeech2.train_fastspeech2",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.fastspeech2.train_fastspeech2",
        "description": "env.src.tts.recipes.ljspeech.fastspeech2.train_fastspeech2",
        "peekOfCode": "dataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",\n    # meta_file_attn_mask=os.path.join(output_path, \"../LJSpeech-1.1/metadata_attn_mask.txt\"),\n    path=os.path.join(output_path, \"../LJSpeech-1.1/\"),\n)\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,",
        "detail": "env.src.tts.recipes.ljspeech.fastspeech2.train_fastspeech2",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.fastspeech2.train_fastspeech2",
        "description": "env.src.tts.recipes.ljspeech.fastspeech2.train_fastspeech2",
        "peekOfCode": "audio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,",
        "detail": "env.src.tts.recipes.ljspeech.fastspeech2.train_fastspeech2",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.fastspeech2.train_fastspeech2",
        "description": "env.src.tts.recipes.ljspeech.fastspeech2.train_fastspeech2",
        "peekOfCode": "config = Fastspeech2Config(\n    run_name=\"fastspeech2_ljspeech\",\n    audio=audio_config,\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=8,\n    num_eval_loader_workers=4,\n    compute_input_seq_cache=True,\n    compute_f0=True,\n    f0_cache_path=os.path.join(output_path, \"f0_cache\"),",
        "detail": "env.src.tts.recipes.ljspeech.fastspeech2.train_fastspeech2",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.fastspeech2.train_fastspeech2",
        "description": "env.src.tts.recipes.ljspeech.fastspeech2.train_fastspeech2",
        "peekOfCode": "ap = AudioProcessor.init_from_config(config)\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.",
        "detail": "env.src.tts.recipes.ljspeech.fastspeech2.train_fastspeech2",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.fastspeech2.train_fastspeech2",
        "description": "env.src.tts.recipes.ljspeech.fastspeech2.train_fastspeech2",
        "peekOfCode": "model = ForwardTTS(config, ap, tokenizer, speaker_manager=None)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.fastspeech2.train_fastspeech2",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.fastspeech2.train_fastspeech2",
        "description": "env.src.tts.recipes.ljspeech.fastspeech2.train_fastspeech2",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.fastspeech2.train_fastspeech2",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.glow_tts.train_glowtts",
        "description": "env.src.tts.recipes.ljspeech.glow_tts.train_glowtts",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\n# DEFINE DATASET CONFIG\n# Set LJSpeech as our target dataset and define its path.\n# You can also use a simple Dict to define the dataset and pass it to your custom formatter.\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../LJSpeech-1.1/\")\n)\n# INITIALIZE THE TRAINING CONFIGURATION\n# Configure the model. Every config class inherits the BaseTTSConfig.\nconfig = GlowTTSConfig(",
        "detail": "env.src.tts.recipes.ljspeech.glow_tts.train_glowtts",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.glow_tts.train_glowtts",
        "description": "env.src.tts.recipes.ljspeech.glow_tts.train_glowtts",
        "peekOfCode": "dataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../LJSpeech-1.1/\")\n)\n# INITIALIZE THE TRAINING CONFIGURATION\n# Configure the model. Every config class inherits the BaseTTSConfig.\nconfig = GlowTTSConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,",
        "detail": "env.src.tts.recipes.ljspeech.glow_tts.train_glowtts",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.glow_tts.train_glowtts",
        "description": "env.src.tts.recipes.ljspeech.glow_tts.train_glowtts",
        "peekOfCode": "config = GlowTTSConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=True,",
        "detail": "env.src.tts.recipes.ljspeech.glow_tts.train_glowtts",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.glow_tts.train_glowtts",
        "description": "env.src.tts.recipes.ljspeech.glow_tts.train_glowtts",
        "peekOfCode": "ap = AudioProcessor.init_from_config(config)\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.",
        "detail": "env.src.tts.recipes.ljspeech.glow_tts.train_glowtts",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.glow_tts.train_glowtts",
        "description": "env.src.tts.recipes.ljspeech.glow_tts.train_glowtts",
        "peekOfCode": "model = GlowTTS(config, ap, tokenizer, speaker_manager=None)\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.glow_tts.train_glowtts",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.glow_tts.train_glowtts",
        "description": "env.src.tts.recipes.ljspeech.glow_tts.train_glowtts",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.glow_tts.train_glowtts",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.hifigan.train_hifigan",
        "description": "env.src.tts.recipes.ljspeech.hifigan.train_hifigan",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\nconfig = HifiganConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=5,\n    epochs=1000,\n    seq_len=8192,",
        "detail": "env.src.tts.recipes.ljspeech.hifigan.train_hifigan",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.hifigan.train_hifigan",
        "description": "env.src.tts.recipes.ljspeech.hifigan.train_hifigan",
        "peekOfCode": "config = HifiganConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=5,\n    epochs=1000,\n    seq_len=8192,\n    pad_short=2000,",
        "detail": "env.src.tts.recipes.ljspeech.hifigan.train_hifigan",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.hifigan.train_hifigan",
        "description": "env.src.tts.recipes.ljspeech.hifigan.train_hifigan",
        "peekOfCode": "ap = AudioProcessor(**config.audio.to_dict())\n# load training samples\neval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n# init model\nmodel = GAN(config, ap)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.hifigan.train_hifigan",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.hifigan.train_hifigan",
        "description": "env.src.tts.recipes.ljspeech.hifigan.train_hifigan",
        "peekOfCode": "model = GAN(config, ap)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.hifigan.train_hifigan",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.hifigan.train_hifigan",
        "description": "env.src.tts.recipes.ljspeech.hifigan.train_hifigan",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.hifigan.train_hifigan",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.multiband_melgan.train_multiband_melgan",
        "description": "env.src.tts.recipes.ljspeech.multiband_melgan.train_multiband_melgan",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\nconfig = MultibandMelganConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=5,\n    epochs=1000,\n    seq_len=8192,",
        "detail": "env.src.tts.recipes.ljspeech.multiband_melgan.train_multiband_melgan",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.multiband_melgan.train_multiband_melgan",
        "description": "env.src.tts.recipes.ljspeech.multiband_melgan.train_multiband_melgan",
        "peekOfCode": "config = MultibandMelganConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=5,\n    epochs=1000,\n    seq_len=8192,\n    pad_short=2000,",
        "detail": "env.src.tts.recipes.ljspeech.multiband_melgan.train_multiband_melgan",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.multiband_melgan.train_multiband_melgan",
        "description": "env.src.tts.recipes.ljspeech.multiband_melgan.train_multiband_melgan",
        "peekOfCode": "ap = AudioProcessor(**config.audio.to_dict())\n# load training samples\neval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n# init model\nmodel = GAN(config, ap)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.multiband_melgan.train_multiband_melgan",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.multiband_melgan.train_multiband_melgan",
        "description": "env.src.tts.recipes.ljspeech.multiband_melgan.train_multiband_melgan",
        "peekOfCode": "model = GAN(config, ap)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.multiband_melgan.train_multiband_melgan",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.multiband_melgan.train_multiband_melgan",
        "description": "env.src.tts.recipes.ljspeech.multiband_melgan.train_multiband_melgan",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.multiband_melgan.train_multiband_melgan",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.neuralhmm_tts.train_neuralhmmtts",
        "description": "env.src.tts.recipes.ljspeech.neuralhmm_tts.train_neuralhmmtts",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\n# init configs\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(\"data\", \"LJSpeech-1.1/\")\n)\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,",
        "detail": "env.src.tts.recipes.ljspeech.neuralhmm_tts.train_neuralhmmtts",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.neuralhmm_tts.train_neuralhmmtts",
        "description": "env.src.tts.recipes.ljspeech.neuralhmm_tts.train_neuralhmmtts",
        "peekOfCode": "dataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(\"data\", \"LJSpeech-1.1/\")\n)\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,",
        "detail": "env.src.tts.recipes.ljspeech.neuralhmm_tts.train_neuralhmmtts",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.neuralhmm_tts.train_neuralhmmtts",
        "description": "env.src.tts.recipes.ljspeech.neuralhmm_tts.train_neuralhmmtts",
        "peekOfCode": "audio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,",
        "detail": "env.src.tts.recipes.ljspeech.neuralhmm_tts.train_neuralhmmtts",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.neuralhmm_tts.train_neuralhmmtts",
        "description": "env.src.tts.recipes.ljspeech.neuralhmm_tts.train_neuralhmmtts",
        "peekOfCode": "config = NeuralhmmTTSConfig(  # This is the config that is saved for the future use\n    run_name=\"neuralhmmtts_ljspeech\",\n    audio=audio_config,\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,",
        "detail": "env.src.tts.recipes.ljspeech.neuralhmm_tts.train_neuralhmmtts",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.neuralhmm_tts.train_neuralhmmtts",
        "description": "env.src.tts.recipes.ljspeech.neuralhmm_tts.train_neuralhmmtts",
        "peekOfCode": "ap = AudioProcessor.init_from_config(config)\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.",
        "detail": "env.src.tts.recipes.ljspeech.neuralhmm_tts.train_neuralhmmtts",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.neuralhmm_tts.train_neuralhmmtts",
        "description": "env.src.tts.recipes.ljspeech.neuralhmm_tts.train_neuralhmmtts",
        "peekOfCode": "model = NeuralhmmTTS(config, ap, tokenizer)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n    gpu=1,",
        "detail": "env.src.tts.recipes.ljspeech.neuralhmm_tts.train_neuralhmmtts",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.neuralhmm_tts.train_neuralhmmtts",
        "description": "env.src.tts.recipes.ljspeech.neuralhmm_tts.train_neuralhmmtts",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n    gpu=1,\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.neuralhmm_tts.train_neuralhmmtts",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.overflow.train_overflow",
        "description": "env.src.tts.recipes.ljspeech.overflow.train_overflow",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\n# init configs\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(\"data\", \"LJSpeech-1.1/\")\n)\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,",
        "detail": "env.src.tts.recipes.ljspeech.overflow.train_overflow",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.overflow.train_overflow",
        "description": "env.src.tts.recipes.ljspeech.overflow.train_overflow",
        "peekOfCode": "dataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(\"data\", \"LJSpeech-1.1/\")\n)\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,",
        "detail": "env.src.tts.recipes.ljspeech.overflow.train_overflow",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.overflow.train_overflow",
        "description": "env.src.tts.recipes.ljspeech.overflow.train_overflow",
        "peekOfCode": "audio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,",
        "detail": "env.src.tts.recipes.ljspeech.overflow.train_overflow",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.overflow.train_overflow",
        "description": "env.src.tts.recipes.ljspeech.overflow.train_overflow",
        "peekOfCode": "config = OverflowConfig(  # This is the config that is saved for the future use\n    run_name=\"overflow_ljspeech\",\n    audio=audio_config,\n    batch_size=30,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,",
        "detail": "env.src.tts.recipes.ljspeech.overflow.train_overflow",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.overflow.train_overflow",
        "description": "env.src.tts.recipes.ljspeech.overflow.train_overflow",
        "peekOfCode": "ap = AudioProcessor.init_from_config(config)\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.",
        "detail": "env.src.tts.recipes.ljspeech.overflow.train_overflow",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.overflow.train_overflow",
        "description": "env.src.tts.recipes.ljspeech.overflow.train_overflow",
        "peekOfCode": "model = Overflow(config, ap, tokenizer)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n    gpu=1,",
        "detail": "env.src.tts.recipes.ljspeech.overflow.train_overflow",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.overflow.train_overflow",
        "description": "env.src.tts.recipes.ljspeech.overflow.train_overflow",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n    gpu=1,\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.overflow.train_overflow",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.speedy_speech.train_speedy_speech",
        "description": "env.src.tts.recipes.ljspeech.speedy_speech.train_speedy_speech",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../LJSpeech-1.1/\")\n)\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,",
        "detail": "env.src.tts.recipes.ljspeech.speedy_speech.train_speedy_speech",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.speedy_speech.train_speedy_speech",
        "description": "env.src.tts.recipes.ljspeech.speedy_speech.train_speedy_speech",
        "peekOfCode": "dataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../LJSpeech-1.1/\")\n)\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,",
        "detail": "env.src.tts.recipes.ljspeech.speedy_speech.train_speedy_speech",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.speedy_speech.train_speedy_speech",
        "description": "env.src.tts.recipes.ljspeech.speedy_speech.train_speedy_speech",
        "peekOfCode": "audio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,",
        "detail": "env.src.tts.recipes.ljspeech.speedy_speech.train_speedy_speech",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.speedy_speech.train_speedy_speech",
        "description": "env.src.tts.recipes.ljspeech.speedy_speech.train_speedy_speech",
        "peekOfCode": "config = SpeedySpeechConfig(\n    run_name=\"speedy_speech_ljspeech\",\n    audio=audio_config,\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    compute_input_seq_cache=True,\n    run_eval=True,\n    test_delay_epochs=-1,",
        "detail": "env.src.tts.recipes.ljspeech.speedy_speech.train_speedy_speech",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.speedy_speech.train_speedy_speech",
        "description": "env.src.tts.recipes.ljspeech.speedy_speech.train_speedy_speech",
        "peekOfCode": "ap = AudioProcessor.init_from_config(config)\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.",
        "detail": "env.src.tts.recipes.ljspeech.speedy_speech.train_speedy_speech",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.speedy_speech.train_speedy_speech",
        "description": "env.src.tts.recipes.ljspeech.speedy_speech.train_speedy_speech",
        "peekOfCode": "model = ForwardTTS(config, ap, tokenizer)\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.speedy_speech.train_speedy_speech",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.speedy_speech.train_speedy_speech",
        "description": "env.src.tts.recipes.ljspeech.speedy_speech.train_speedy_speech",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.speedy_speech.train_speedy_speech",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "description": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\ndata_path = \"/srv/data/\"\n# Using LJSpeech like dataset processing for the blizzard dataset\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",\n    path=data_path,\n)\naudio_config = BaseAudioConfig(\n    sample_rate=22050,",
        "detail": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "documentation": {}
    },
    {
        "label": "data_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "description": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "peekOfCode": "data_path = \"/srv/data/\"\n# Using LJSpeech like dataset processing for the blizzard dataset\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",\n    path=data_path,\n)\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,",
        "detail": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "description": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "peekOfCode": "dataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",\n    path=data_path,\n)\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,",
        "detail": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "description": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "peekOfCode": "audio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=11025,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,",
        "detail": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "documentation": {}
    },
    {
        "label": "capacitron_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "description": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "peekOfCode": "capacitron_config = CapacitronVAEConfig(capacitron_VAE_loss_alpha=1.0, capacitron_capacity=50)\nconfig = Tacotron2Config(\n    run_name=\"Capacitron-Tacotron2\",\n    audio=audio_config,\n    capacitron_vae=capacitron_config,\n    use_capacitron_vae=True,\n    batch_size=128,  # Tune this to your gpu\n    max_audio_len=8 * 22050,  # Tune this to your gpu\n    min_audio_len=1 * 22050,\n    eval_batch_size=16,",
        "detail": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "description": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "peekOfCode": "config = Tacotron2Config(\n    run_name=\"Capacitron-Tacotron2\",\n    audio=audio_config,\n    capacitron_vae=capacitron_config,\n    use_capacitron_vae=True,\n    batch_size=128,  # Tune this to your gpu\n    max_audio_len=8 * 22050,  # Tune this to your gpu\n    min_audio_len=1 * 22050,\n    eval_batch_size=16,\n    num_loader_workers=8,",
        "detail": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "description": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "peekOfCode": "ap = AudioProcessor(**config.audio.to_dict())\ntokenizer, config = TTSTokenizer.init_from_config(config)\ntrain_samples, eval_samples = load_tts_samples(dataset_config, eval_split=True)\nmodel = Tacotron2(config, ap, tokenizer, speaker_manager=None)\ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,",
        "detail": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "description": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "peekOfCode": "model = Tacotron2(config, ap, tokenizer, speaker_manager=None)\ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n    training_assets={\"audio_processor\": ap},\n)",
        "detail": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "description": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n    training_assets={\"audio_processor\": ap},\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.tacotron2-Capacitron.train_capacitron_t2",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.tacotron2-DCA.train_tacotron_dca",
        "description": "env.src.tts.recipes.ljspeech.tacotron2-DCA.train_tacotron_dca",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\n# init configs\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../LJSpeech-1.1/\")\n)\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,",
        "detail": "env.src.tts.recipes.ljspeech.tacotron2-DCA.train_tacotron_dca",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.tacotron2-DCA.train_tacotron_dca",
        "description": "env.src.tts.recipes.ljspeech.tacotron2-DCA.train_tacotron_dca",
        "peekOfCode": "dataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../LJSpeech-1.1/\")\n)\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,",
        "detail": "env.src.tts.recipes.ljspeech.tacotron2-DCA.train_tacotron_dca",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.tacotron2-DCA.train_tacotron_dca",
        "description": "env.src.tts.recipes.ljspeech.tacotron2-DCA.train_tacotron_dca",
        "peekOfCode": "audio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,",
        "detail": "env.src.tts.recipes.ljspeech.tacotron2-DCA.train_tacotron_dca",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.tacotron2-DCA.train_tacotron_dca",
        "description": "env.src.tts.recipes.ljspeech.tacotron2-DCA.train_tacotron_dca",
        "peekOfCode": "config = Tacotron2Config(  # This is the config that is saved for the future use\n    audio=audio_config,\n    batch_size=64,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    ga_alpha=0.0,\n    decoder_loss_alpha=0.25,",
        "detail": "env.src.tts.recipes.ljspeech.tacotron2-DCA.train_tacotron_dca",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.tacotron2-DCA.train_tacotron_dca",
        "description": "env.src.tts.recipes.ljspeech.tacotron2-DCA.train_tacotron_dca",
        "peekOfCode": "ap = AudioProcessor.init_from_config(config)\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.",
        "detail": "env.src.tts.recipes.ljspeech.tacotron2-DCA.train_tacotron_dca",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.tacotron2-DCA.train_tacotron_dca",
        "description": "env.src.tts.recipes.ljspeech.tacotron2-DCA.train_tacotron_dca",
        "peekOfCode": "model = Tacotron2(config, ap, tokenizer)\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.tacotron2-DCA.train_tacotron_dca",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.tacotron2-DCA.train_tacotron_dca",
        "description": "env.src.tts.recipes.ljspeech.tacotron2-DCA.train_tacotron_dca",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.tacotron2-DCA.train_tacotron_dca",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.tacotron2-DDC.train_tacotron_ddc",
        "description": "env.src.tts.recipes.ljspeech.tacotron2-DDC.train_tacotron_ddc",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\n# init configs\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../LJSpeech-1.1/\")\n)\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,",
        "detail": "env.src.tts.recipes.ljspeech.tacotron2-DDC.train_tacotron_ddc",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.tacotron2-DDC.train_tacotron_ddc",
        "description": "env.src.tts.recipes.ljspeech.tacotron2-DDC.train_tacotron_ddc",
        "peekOfCode": "dataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../LJSpeech-1.1/\")\n)\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,",
        "detail": "env.src.tts.recipes.ljspeech.tacotron2-DDC.train_tacotron_ddc",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.tacotron2-DDC.train_tacotron_ddc",
        "description": "env.src.tts.recipes.ljspeech.tacotron2-DDC.train_tacotron_ddc",
        "peekOfCode": "audio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,",
        "detail": "env.src.tts.recipes.ljspeech.tacotron2-DDC.train_tacotron_ddc",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.tacotron2-DDC.train_tacotron_ddc",
        "description": "env.src.tts.recipes.ljspeech.tacotron2-DDC.train_tacotron_ddc",
        "peekOfCode": "config = Tacotron2Config(  # This is the config that is saved for the future use\n    audio=audio_config,\n    batch_size=64,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    r=6,\n    gradual_training=[[0, 6, 64], [10000, 4, 32], [50000, 3, 32], [100000, 2, 32]],",
        "detail": "env.src.tts.recipes.ljspeech.tacotron2-DDC.train_tacotron_ddc",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.tacotron2-DDC.train_tacotron_ddc",
        "description": "env.src.tts.recipes.ljspeech.tacotron2-DDC.train_tacotron_ddc",
        "peekOfCode": "ap = AudioProcessor(**config.audio.to_dict())\n# INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# LOAD DATA SAMPLES",
        "detail": "env.src.tts.recipes.ljspeech.tacotron2-DDC.train_tacotron_ddc",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.tacotron2-DDC.train_tacotron_ddc",
        "description": "env.src.tts.recipes.ljspeech.tacotron2-DDC.train_tacotron_ddc",
        "peekOfCode": "ap = AudioProcessor.init_from_config(config)\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.",
        "detail": "env.src.tts.recipes.ljspeech.tacotron2-DDC.train_tacotron_ddc",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.tacotron2-DDC.train_tacotron_ddc",
        "description": "env.src.tts.recipes.ljspeech.tacotron2-DDC.train_tacotron_ddc",
        "peekOfCode": "model = Tacotron2(config, ap, tokenizer, speaker_manager=None)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.tacotron2-DDC.train_tacotron_ddc",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.tacotron2-DDC.train_tacotron_ddc",
        "description": "env.src.tts.recipes.ljspeech.tacotron2-DDC.train_tacotron_ddc",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.tacotron2-DDC.train_tacotron_ddc",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.univnet.train",
        "description": "env.src.tts.recipes.ljspeech.univnet.train",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\nconfig = UnivnetConfig(\n    batch_size=64,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    seq_len=8192,",
        "detail": "env.src.tts.recipes.ljspeech.univnet.train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.univnet.train",
        "description": "env.src.tts.recipes.ljspeech.univnet.train",
        "peekOfCode": "config = UnivnetConfig(\n    batch_size=64,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    seq_len=8192,\n    pad_short=2000,",
        "detail": "env.src.tts.recipes.ljspeech.univnet.train",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.univnet.train",
        "description": "env.src.tts.recipes.ljspeech.univnet.train",
        "peekOfCode": "ap = AudioProcessor(**config.audio.to_dict())\n# load training samples\neval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n# init model\nmodel = GAN(config, ap)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.univnet.train",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.univnet.train",
        "description": "env.src.tts.recipes.ljspeech.univnet.train",
        "peekOfCode": "model = GAN(config, ap)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.univnet.train",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.univnet.train",
        "description": "env.src.tts.recipes.ljspeech.univnet.train",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.univnet.train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.vits_tts.train_vits",
        "description": "env.src.tts.recipes.ljspeech.vits_tts.train_vits",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../LJSpeech-1.1/\")\n)\naudio_config = VitsAudioConfig(\n    sample_rate=22050, win_length=1024, hop_length=256, num_mels=80, mel_fmin=0, mel_fmax=None\n)\nconfig = VitsConfig(\n    audio=audio_config,\n    run_name=\"vits_ljspeech\",",
        "detail": "env.src.tts.recipes.ljspeech.vits_tts.train_vits",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.vits_tts.train_vits",
        "description": "env.src.tts.recipes.ljspeech.vits_tts.train_vits",
        "peekOfCode": "dataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../LJSpeech-1.1/\")\n)\naudio_config = VitsAudioConfig(\n    sample_rate=22050, win_length=1024, hop_length=256, num_mels=80, mel_fmin=0, mel_fmax=None\n)\nconfig = VitsConfig(\n    audio=audio_config,\n    run_name=\"vits_ljspeech\",\n    batch_size=32,",
        "detail": "env.src.tts.recipes.ljspeech.vits_tts.train_vits",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.vits_tts.train_vits",
        "description": "env.src.tts.recipes.ljspeech.vits_tts.train_vits",
        "peekOfCode": "audio_config = VitsAudioConfig(\n    sample_rate=22050, win_length=1024, hop_length=256, num_mels=80, mel_fmin=0, mel_fmax=None\n)\nconfig = VitsConfig(\n    audio=audio_config,\n    run_name=\"vits_ljspeech\",\n    batch_size=32,\n    eval_batch_size=16,\n    batch_group_size=5,\n    num_loader_workers=8,",
        "detail": "env.src.tts.recipes.ljspeech.vits_tts.train_vits",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.vits_tts.train_vits",
        "description": "env.src.tts.recipes.ljspeech.vits_tts.train_vits",
        "peekOfCode": "config = VitsConfig(\n    audio=audio_config,\n    run_name=\"vits_ljspeech\",\n    batch_size=32,\n    eval_batch_size=16,\n    batch_group_size=5,\n    num_loader_workers=8,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,",
        "detail": "env.src.tts.recipes.ljspeech.vits_tts.train_vits",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.vits_tts.train_vits",
        "description": "env.src.tts.recipes.ljspeech.vits_tts.train_vits",
        "peekOfCode": "ap = AudioProcessor.init_from_config(config)\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# config is updated with the default characters if not defined in the config.\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.",
        "detail": "env.src.tts.recipes.ljspeech.vits_tts.train_vits",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.vits_tts.train_vits",
        "description": "env.src.tts.recipes.ljspeech.vits_tts.train_vits",
        "peekOfCode": "model = Vits(config, ap, tokenizer, speaker_manager=None)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n)",
        "detail": "env.src.tts.recipes.ljspeech.vits_tts.train_vits",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.vits_tts.train_vits",
        "description": "env.src.tts.recipes.ljspeech.vits_tts.train_vits",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.vits_tts.train_vits",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.wavegrad.train_wavegrad",
        "description": "env.src.tts.recipes.ljspeech.wavegrad.train_wavegrad",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\nconfig = WavegradConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    seq_len=6144,",
        "detail": "env.src.tts.recipes.ljspeech.wavegrad.train_wavegrad",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.wavegrad.train_wavegrad",
        "description": "env.src.tts.recipes.ljspeech.wavegrad.train_wavegrad",
        "peekOfCode": "config = WavegradConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    seq_len=6144,\n    pad_short=2000,",
        "detail": "env.src.tts.recipes.ljspeech.wavegrad.train_wavegrad",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.wavegrad.train_wavegrad",
        "description": "env.src.tts.recipes.ljspeech.wavegrad.train_wavegrad",
        "peekOfCode": "ap = AudioProcessor(**config.audio.to_dict())\n# load training samples\neval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n# init model\nmodel = Wavegrad(config)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,",
        "detail": "env.src.tts.recipes.ljspeech.wavegrad.train_wavegrad",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.wavegrad.train_wavegrad",
        "description": "env.src.tts.recipes.ljspeech.wavegrad.train_wavegrad",
        "peekOfCode": "model = Wavegrad(config)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n    training_assets={\"audio_processor\": ap},",
        "detail": "env.src.tts.recipes.ljspeech.wavegrad.train_wavegrad",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.wavegrad.train_wavegrad",
        "description": "env.src.tts.recipes.ljspeech.wavegrad.train_wavegrad",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n    training_assets={\"audio_processor\": ap},\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.wavegrad.train_wavegrad",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.wavernn.train_wavernn",
        "description": "env.src.tts.recipes.ljspeech.wavernn.train_wavernn",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\nconfig = WavernnConfig(\n    batch_size=64,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=10000,\n    seq_len=1280,",
        "detail": "env.src.tts.recipes.ljspeech.wavernn.train_wavernn",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.wavernn.train_wavernn",
        "description": "env.src.tts.recipes.ljspeech.wavernn.train_wavernn",
        "peekOfCode": "config = WavernnConfig(\n    batch_size=64,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=10000,\n    seq_len=1280,\n    pad_short=2000,",
        "detail": "env.src.tts.recipes.ljspeech.wavernn.train_wavernn",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.wavernn.train_wavernn",
        "description": "env.src.tts.recipes.ljspeech.wavernn.train_wavernn",
        "peekOfCode": "ap = AudioProcessor(**config.audio.to_dict())\n# load training samples\neval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n# init model\nmodel = Wavernn(config)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,",
        "detail": "env.src.tts.recipes.ljspeech.wavernn.train_wavernn",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.wavernn.train_wavernn",
        "description": "env.src.tts.recipes.ljspeech.wavernn.train_wavernn",
        "peekOfCode": "model = Wavernn(config)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n    training_assets={\"audio_processor\": ap},",
        "detail": "env.src.tts.recipes.ljspeech.wavernn.train_wavernn",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.ljspeech.wavernn.train_wavernn",
        "description": "env.src.tts.recipes.ljspeech.wavernn.train_wavernn",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n    training_assets={\"audio_processor\": ap},\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.ljspeech.wavernn.train_wavernn",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\nmailabs_path = \"/home/julian/workspace/mailabs/**\"\ndataset_paths = glob(mailabs_path)\ndataset_config = [\n    BaseDatasetConfig(formatter=\"mailabs\", meta_file_train=None, path=path, language=path.split(\"/\")[-1])\n    for path in dataset_paths\n]\naudio_config = VitsAudioConfig(\n    sample_rate=16000,\n    win_length=1024,",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "documentation": {}
    },
    {
        "label": "mailabs_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "peekOfCode": "mailabs_path = \"/home/julian/workspace/mailabs/**\"\ndataset_paths = glob(mailabs_path)\ndataset_config = [\n    BaseDatasetConfig(formatter=\"mailabs\", meta_file_train=None, path=path, language=path.split(\"/\")[-1])\n    for path in dataset_paths\n]\naudio_config = VitsAudioConfig(\n    sample_rate=16000,\n    win_length=1024,\n    hop_length=256,",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "documentation": {}
    },
    {
        "label": "dataset_paths",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "peekOfCode": "dataset_paths = glob(mailabs_path)\ndataset_config = [\n    BaseDatasetConfig(formatter=\"mailabs\", meta_file_train=None, path=path, language=path.split(\"/\")[-1])\n    for path in dataset_paths\n]\naudio_config = VitsAudioConfig(\n    sample_rate=16000,\n    win_length=1024,\n    hop_length=256,\n    num_mels=80,",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "peekOfCode": "dataset_config = [\n    BaseDatasetConfig(formatter=\"mailabs\", meta_file_train=None, path=path, language=path.split(\"/\")[-1])\n    for path in dataset_paths\n]\naudio_config = VitsAudioConfig(\n    sample_rate=16000,\n    win_length=1024,\n    hop_length=256,\n    num_mels=80,\n    mel_fmin=0,",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "peekOfCode": "audio_config = VitsAudioConfig(\n    sample_rate=16000,\n    win_length=1024,\n    hop_length=256,\n    num_mels=80,\n    mel_fmin=0,\n    mel_fmax=None,\n)\nvitsArgs = VitsArgs(\n    use_language_embedding=True,",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "documentation": {}
    },
    {
        "label": "vitsArgs",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "peekOfCode": "vitsArgs = VitsArgs(\n    use_language_embedding=True,\n    embedded_language_dim=4,\n    use_speaker_embedding=True,\n    use_sdp=False,\n)\nconfig = VitsConfig(\n    model_args=vitsArgs,\n    audio=audio_config,\n    run_name=\"vits_vctk\",",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "peekOfCode": "config = VitsConfig(\n    model_args=vitsArgs,\n    audio=audio_config,\n    run_name=\"vits_vctk\",\n    use_speaker_embedding=True,\n    batch_size=32,\n    eval_batch_size=16,\n    batch_group_size=0,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "peekOfCode": "ap = AudioProcessor(**config.audio.to_dict())\n# load training samples\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n# init speaker manager for multi-speaker training\n# it maps speaker-id to speaker-name in the model and data-loader",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "documentation": {}
    },
    {
        "label": "speaker_manager",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "peekOfCode": "speaker_manager = SpeakerManager()\nspeaker_manager.set_ids_from_data(train_samples + eval_samples, parse_key=\"speaker_name\")\nconfig.model_args.num_speakers = speaker_manager.num_speakers\nlanguage_manager = LanguageManager(config=config)\nconfig.model_args.num_languages = language_manager.num_languages\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# config is updated with the default characters if not defined in the config.\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# init model",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "documentation": {}
    },
    {
        "label": "config.model_args.num_speakers",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "peekOfCode": "config.model_args.num_speakers = speaker_manager.num_speakers\nlanguage_manager = LanguageManager(config=config)\nconfig.model_args.num_languages = language_manager.num_languages\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# config is updated with the default characters if not defined in the config.\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# init model\nmodel = Vits(config, ap, tokenizer, speaker_manager, language_manager)\n# init the trainer and ",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "documentation": {}
    },
    {
        "label": "language_manager",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "peekOfCode": "language_manager = LanguageManager(config=config)\nconfig.model_args.num_languages = language_manager.num_languages\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# config is updated with the default characters if not defined in the config.\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# init model\nmodel = Vits(config, ap, tokenizer, speaker_manager, language_manager)\n# init the trainer and \ntrainer = Trainer(",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "documentation": {}
    },
    {
        "label": "config.model_args.num_languages",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "peekOfCode": "config.model_args.num_languages = language_manager.num_languages\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# config is updated with the default characters if not defined in the config.\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# init model\nmodel = Vits(config, ap, tokenizer, speaker_manager, language_manager)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "peekOfCode": "model = Vits(config, ap, tokenizer, speaker_manager, language_manager)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "peekOfCode": "output_path = \"/media/julian/Workdisk/train\"\nmailabs_path = \"/home/julian/workspace/mailabs/**\"\ndataset_paths = glob(mailabs_path)\ndataset_config = [\n    BaseDatasetConfig(\n        formatter=\"mailabs\",\n        meta_file_train=None,\n        path=path,\n        language=path.split(\"/\")[-1],  # language code is the folder name\n    )",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "documentation": {}
    },
    {
        "label": "mailabs_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "peekOfCode": "mailabs_path = \"/home/julian/workspace/mailabs/**\"\ndataset_paths = glob(mailabs_path)\ndataset_config = [\n    BaseDatasetConfig(\n        formatter=\"mailabs\",\n        meta_file_train=None,\n        path=path,\n        language=path.split(\"/\")[-1],  # language code is the folder name\n    )\n    for path in dataset_paths",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "documentation": {}
    },
    {
        "label": "dataset_paths",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "peekOfCode": "dataset_paths = glob(mailabs_path)\ndataset_config = [\n    BaseDatasetConfig(\n        formatter=\"mailabs\",\n        meta_file_train=None,\n        path=path,\n        language=path.split(\"/\")[-1],  # language code is the folder name\n    )\n    for path in dataset_paths\n]",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "peekOfCode": "dataset_config = [\n    BaseDatasetConfig(\n        formatter=\"mailabs\",\n        meta_file_train=None,\n        path=path,\n        language=path.split(\"/\")[-1],  # language code is the folder name\n    )\n    for path in dataset_paths\n]\naudio_config = VitsAudioConfig(",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "peekOfCode": "audio_config = VitsAudioConfig(\n    sample_rate=16000,\n    win_length=1024,\n    hop_length=256,\n    num_mels=80,\n    mel_fmin=0,\n    mel_fmax=None,\n)\nvitsArgs = VitsArgs(\n    use_language_embedding=True,",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "documentation": {}
    },
    {
        "label": "vitsArgs",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "peekOfCode": "vitsArgs = VitsArgs(\n    use_language_embedding=True,\n    embedded_language_dim=4,\n    use_speaker_embedding=True,\n    use_sdp=False,\n)\nconfig = VitsConfig(\n    model_args=vitsArgs,\n    audio=audio_config,\n    run_name=\"vits_vctk\",",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "peekOfCode": "config = VitsConfig(\n    model_args=vitsArgs,\n    audio=audio_config,\n    run_name=\"vits_vctk\",\n    use_speaker_embedding=True,\n    batch_size=32,\n    eval_batch_size=16,\n    batch_group_size=0,\n    num_loader_workers=12,\n    num_eval_loader_workers=12,",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "peekOfCode": "ap = AudioProcessor(**config.audio.to_dict())\n# load training samples\ntrain_samples, eval_samples = load_tts_samples(\n    dataset_config,\n    eval_split=True,\n    eval_split_max_size=config.eval_split_max_size,\n    eval_split_size=config.eval_split_size,\n)\n# init speaker manager for multi-speaker training\n# it maps speaker-id to speaker-name in the model and data-loader",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "documentation": {}
    },
    {
        "label": "speaker_manager",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "peekOfCode": "speaker_manager = SpeakerManager()\nspeaker_manager.set_ids_from_data(train_samples + eval_samples, parse_key=\"speaker_name\")\nconfig.model_args.num_speakers = speaker_manager.num_speakers\nlanguage_manager = LanguageManager(config=config)\nconfig.model_args.num_languages = language_manager.num_languages\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# config is updated with the default characters if not defined in the config.\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# init model",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "documentation": {}
    },
    {
        "label": "config.model_args.num_speakers",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "peekOfCode": "config.model_args.num_speakers = speaker_manager.num_speakers\nlanguage_manager = LanguageManager(config=config)\nconfig.model_args.num_languages = language_manager.num_languages\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# config is updated with the default characters if not defined in the config.\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# init model\nmodel = Vits(config, ap, tokenizer, speaker_manager, language_manager)\n# init the trainer and ",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "documentation": {}
    },
    {
        "label": "language_manager",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "peekOfCode": "language_manager = LanguageManager(config=config)\nconfig.model_args.num_languages = language_manager.num_languages\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# config is updated with the default characters if not defined in the config.\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# init model\nmodel = Vits(config, ap, tokenizer, speaker_manager, language_manager)\n# init the trainer and \ntrainer = Trainer(",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "documentation": {}
    },
    {
        "label": "config.model_args.num_languages",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "peekOfCode": "config.model_args.num_languages = language_manager.num_languages\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# config is updated with the default characters if not defined in the config.\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# init model\nmodel = Vits(config, ap, tokenizer, speaker_manager, language_manager)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "peekOfCode": "model = Vits(config, ap, tokenizer, speaker_manager, language_manager)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "description": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.multilingual.vits_tts.train_vits_tts_phonemes",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.align_tts.train_aligntts",
        "description": "env.src.tts.recipes.thorsten_DE.align_tts.train_aligntts",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\n# init configs\ndataset_config = BaseDatasetConfig(\n    formatter=\"thorsten\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../thorsten-de/\")\n)\n# download dataset if not already present\nif not os.path.exists(dataset_config.path):\n    print(\"Downloading dataset\")\n    download_thorsten_de(os.path.split(os.path.abspath(dataset_config.path))[0])\nconfig = AlignTTSConfig(",
        "detail": "env.src.tts.recipes.thorsten_DE.align_tts.train_aligntts",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.align_tts.train_aligntts",
        "description": "env.src.tts.recipes.thorsten_DE.align_tts.train_aligntts",
        "peekOfCode": "dataset_config = BaseDatasetConfig(\n    formatter=\"thorsten\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../thorsten-de/\")\n)\n# download dataset if not already present\nif not os.path.exists(dataset_config.path):\n    print(\"Downloading dataset\")\n    download_thorsten_de(os.path.split(os.path.abspath(dataset_config.path))[0])\nconfig = AlignTTSConfig(\n    batch_size=32,\n    eval_batch_size=16,",
        "detail": "env.src.tts.recipes.thorsten_DE.align_tts.train_aligntts",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.align_tts.train_aligntts",
        "description": "env.src.tts.recipes.thorsten_DE.align_tts.train_aligntts",
        "peekOfCode": "config = AlignTTSConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=False,",
        "detail": "env.src.tts.recipes.thorsten_DE.align_tts.train_aligntts",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.align_tts.train_aligntts",
        "description": "env.src.tts.recipes.thorsten_DE.align_tts.train_aligntts",
        "peekOfCode": "ap = AudioProcessor.init_from_config(config)\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.",
        "detail": "env.src.tts.recipes.thorsten_DE.align_tts.train_aligntts",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.align_tts.train_aligntts",
        "description": "env.src.tts.recipes.thorsten_DE.align_tts.train_aligntts",
        "peekOfCode": "model = AlignTTS(config, ap, tokenizer)\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.thorsten_DE.align_tts.train_aligntts",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.align_tts.train_aligntts",
        "description": "env.src.tts.recipes.thorsten_DE.align_tts.train_aligntts",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.thorsten_DE.align_tts.train_aligntts",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.glow_tts.train_glowtts",
        "description": "env.src.tts.recipes.thorsten_DE.glow_tts.train_glowtts",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\n# DEFINE DATASET CONFIG\n# Set LJSpeech as our target dataset and define its path.\n# You can also use a simple Dict to define the dataset and pass it to your custom formatter.\ndataset_config = BaseDatasetConfig(\n    formatter=\"thorsten\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../thorsten-de/\")\n)\n# download dataset if not already present\nif not os.path.exists(dataset_config.path):\n    print(\"Downloading dataset\")",
        "detail": "env.src.tts.recipes.thorsten_DE.glow_tts.train_glowtts",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.glow_tts.train_glowtts",
        "description": "env.src.tts.recipes.thorsten_DE.glow_tts.train_glowtts",
        "peekOfCode": "dataset_config = BaseDatasetConfig(\n    formatter=\"thorsten\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../thorsten-de/\")\n)\n# download dataset if not already present\nif not os.path.exists(dataset_config.path):\n    print(\"Downloading dataset\")\n    download_thorsten_de(os.path.split(os.path.abspath(dataset_config.path))[0])\n# INITIALIZE THE TRAINING CONFIGURATION\n# Configure the model. Every config class inherits the BaseTTSConfig.\nconfig = GlowTTSConfig(",
        "detail": "env.src.tts.recipes.thorsten_DE.glow_tts.train_glowtts",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.glow_tts.train_glowtts",
        "description": "env.src.tts.recipes.thorsten_DE.glow_tts.train_glowtts",
        "peekOfCode": "config = GlowTTSConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=True,",
        "detail": "env.src.tts.recipes.thorsten_DE.glow_tts.train_glowtts",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.glow_tts.train_glowtts",
        "description": "env.src.tts.recipes.thorsten_DE.glow_tts.train_glowtts",
        "peekOfCode": "ap = AudioProcessor.init_from_config(config)\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.",
        "detail": "env.src.tts.recipes.thorsten_DE.glow_tts.train_glowtts",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.glow_tts.train_glowtts",
        "description": "env.src.tts.recipes.thorsten_DE.glow_tts.train_glowtts",
        "peekOfCode": "model = GlowTTS(config, ap, tokenizer, speaker_manager=None)\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.thorsten_DE.glow_tts.train_glowtts",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.glow_tts.train_glowtts",
        "description": "env.src.tts.recipes.thorsten_DE.glow_tts.train_glowtts",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.thorsten_DE.glow_tts.train_glowtts",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.hifigan.train_hifigan",
        "description": "env.src.tts.recipes.thorsten_DE.hifigan.train_hifigan",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\nconfig = HifiganConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=5,\n    epochs=1000,\n    seq_len=8192,",
        "detail": "env.src.tts.recipes.thorsten_DE.hifigan.train_hifigan",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.hifigan.train_hifigan",
        "description": "env.src.tts.recipes.thorsten_DE.hifigan.train_hifigan",
        "peekOfCode": "config = HifiganConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=5,\n    epochs=1000,\n    seq_len=8192,\n    pad_short=2000,",
        "detail": "env.src.tts.recipes.thorsten_DE.hifigan.train_hifigan",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.hifigan.train_hifigan",
        "description": "env.src.tts.recipes.thorsten_DE.hifigan.train_hifigan",
        "peekOfCode": "ap = AudioProcessor(**config.audio.to_dict())\n# load training samples\neval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n# init model\nmodel = GAN(config, ap)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.thorsten_DE.hifigan.train_hifigan",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.hifigan.train_hifigan",
        "description": "env.src.tts.recipes.thorsten_DE.hifigan.train_hifigan",
        "peekOfCode": "model = GAN(config, ap)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.thorsten_DE.hifigan.train_hifigan",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.hifigan.train_hifigan",
        "description": "env.src.tts.recipes.thorsten_DE.hifigan.train_hifigan",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.thorsten_DE.hifigan.train_hifigan",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.multiband_melgan.train_multiband_melgan",
        "description": "env.src.tts.recipes.thorsten_DE.multiband_melgan.train_multiband_melgan",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\nconfig = MultibandMelganConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=5,\n    epochs=1000,\n    seq_len=8192,",
        "detail": "env.src.tts.recipes.thorsten_DE.multiband_melgan.train_multiband_melgan",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.multiband_melgan.train_multiband_melgan",
        "description": "env.src.tts.recipes.thorsten_DE.multiband_melgan.train_multiband_melgan",
        "peekOfCode": "config = MultibandMelganConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=5,\n    epochs=1000,\n    seq_len=8192,\n    pad_short=2000,",
        "detail": "env.src.tts.recipes.thorsten_DE.multiband_melgan.train_multiband_melgan",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.multiband_melgan.train_multiband_melgan",
        "description": "env.src.tts.recipes.thorsten_DE.multiband_melgan.train_multiband_melgan",
        "peekOfCode": "ap = AudioProcessor(**config.audio.to_dict())\n# load training samples\neval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n# init model\nmodel = GAN(config, ap)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.thorsten_DE.multiband_melgan.train_multiband_melgan",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.multiband_melgan.train_multiband_melgan",
        "description": "env.src.tts.recipes.thorsten_DE.multiband_melgan.train_multiband_melgan",
        "peekOfCode": "model = GAN(config, ap)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.thorsten_DE.multiband_melgan.train_multiband_melgan",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.multiband_melgan.train_multiband_melgan",
        "description": "env.src.tts.recipes.thorsten_DE.multiband_melgan.train_multiband_melgan",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.thorsten_DE.multiband_melgan.train_multiband_melgan",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.speedy_speech.train_speedy_speech",
        "description": "env.src.tts.recipes.thorsten_DE.speedy_speech.train_speedy_speech",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\ndataset_config = BaseDatasetConfig(\n    formatter=\"thorsten\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../thorsten-de/\")\n)\n# download dataset if not already present\nif not os.path.exists(dataset_config.path):\n    print(\"Downloading dataset\")\n    download_thorsten_de(os.path.split(os.path.abspath(dataset_config.path))[0])\naudio_config = BaseAudioConfig(\n    sample_rate=22050,",
        "detail": "env.src.tts.recipes.thorsten_DE.speedy_speech.train_speedy_speech",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.speedy_speech.train_speedy_speech",
        "description": "env.src.tts.recipes.thorsten_DE.speedy_speech.train_speedy_speech",
        "peekOfCode": "dataset_config = BaseDatasetConfig(\n    formatter=\"thorsten\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../thorsten-de/\")\n)\n# download dataset if not already present\nif not os.path.exists(dataset_config.path):\n    print(\"Downloading dataset\")\n    download_thorsten_de(os.path.split(os.path.abspath(dataset_config.path))[0])\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,",
        "detail": "env.src.tts.recipes.thorsten_DE.speedy_speech.train_speedy_speech",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.speedy_speech.train_speedy_speech",
        "description": "env.src.tts.recipes.thorsten_DE.speedy_speech.train_speedy_speech",
        "peekOfCode": "audio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,",
        "detail": "env.src.tts.recipes.thorsten_DE.speedy_speech.train_speedy_speech",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.speedy_speech.train_speedy_speech",
        "description": "env.src.tts.recipes.thorsten_DE.speedy_speech.train_speedy_speech",
        "peekOfCode": "config = SpeedySpeechConfig(\n    run_name=\"speedy_speech_thorsten-de\",\n    audio=audio_config,\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    compute_input_seq_cache=True,\n    run_eval=True,\n    test_delay_epochs=-1,",
        "detail": "env.src.tts.recipes.thorsten_DE.speedy_speech.train_speedy_speech",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.speedy_speech.train_speedy_speech",
        "description": "env.src.tts.recipes.thorsten_DE.speedy_speech.train_speedy_speech",
        "peekOfCode": "ap = AudioProcessor.init_from_config(config)\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.",
        "detail": "env.src.tts.recipes.thorsten_DE.speedy_speech.train_speedy_speech",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.speedy_speech.train_speedy_speech",
        "description": "env.src.tts.recipes.thorsten_DE.speedy_speech.train_speedy_speech",
        "peekOfCode": "model = ForwardTTS(config, ap, tokenizer)\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.thorsten_DE.speedy_speech.train_speedy_speech",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.speedy_speech.train_speedy_speech",
        "description": "env.src.tts.recipes.thorsten_DE.speedy_speech.train_speedy_speech",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.thorsten_DE.speedy_speech.train_speedy_speech",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.tacotron2-DDC.train_tacotron_ddc",
        "description": "env.src.tts.recipes.thorsten_DE.tacotron2-DDC.train_tacotron_ddc",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\n# init configs\ndataset_config = BaseDatasetConfig(\n    formatter=\"thorsten\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../thorsten-de/\")\n)\n# download dataset if not already present\nif not os.path.exists(dataset_config.path):\n    print(\"Downloading dataset\")\n    download_thorsten_de(os.path.split(os.path.abspath(dataset_config.path))[0])\naudio_config = BaseAudioConfig(",
        "detail": "env.src.tts.recipes.thorsten_DE.tacotron2-DDC.train_tacotron_ddc",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.tacotron2-DDC.train_tacotron_ddc",
        "description": "env.src.tts.recipes.thorsten_DE.tacotron2-DDC.train_tacotron_ddc",
        "peekOfCode": "dataset_config = BaseDatasetConfig(\n    formatter=\"thorsten\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../thorsten-de/\")\n)\n# download dataset if not already present\nif not os.path.exists(dataset_config.path):\n    print(\"Downloading dataset\")\n    download_thorsten_de(os.path.split(os.path.abspath(dataset_config.path))[0])\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,",
        "detail": "env.src.tts.recipes.thorsten_DE.tacotron2-DDC.train_tacotron_ddc",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.tacotron2-DDC.train_tacotron_ddc",
        "description": "env.src.tts.recipes.thorsten_DE.tacotron2-DDC.train_tacotron_ddc",
        "peekOfCode": "audio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,",
        "detail": "env.src.tts.recipes.thorsten_DE.tacotron2-DDC.train_tacotron_ddc",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.tacotron2-DDC.train_tacotron_ddc",
        "description": "env.src.tts.recipes.thorsten_DE.tacotron2-DDC.train_tacotron_ddc",
        "peekOfCode": "config = Tacotron2Config(  # This is the config that is saved for the future use\n    audio=audio_config,\n    batch_size=40,  # BS of 40 and max length of 10s will use about 20GB of GPU memory\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    r=6,\n    gradual_training=[[0, 6, 64], [10000, 4, 32], [50000, 3, 32], [100000, 2, 32]],",
        "detail": "env.src.tts.recipes.thorsten_DE.tacotron2-DDC.train_tacotron_ddc",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.tacotron2-DDC.train_tacotron_ddc",
        "description": "env.src.tts.recipes.thorsten_DE.tacotron2-DDC.train_tacotron_ddc",
        "peekOfCode": "ap = AudioProcessor(**config.audio.to_dict())\n# INITIALIZE THE AUDIO PROCESSOR\n# Audio processor is used for feature extraction and audio I/O.\n# It mainly serves to the dataloader and the training loggers.\nap = AudioProcessor.init_from_config(config)\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# LOAD DATA SAMPLES",
        "detail": "env.src.tts.recipes.thorsten_DE.tacotron2-DDC.train_tacotron_ddc",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.tacotron2-DDC.train_tacotron_ddc",
        "description": "env.src.tts.recipes.thorsten_DE.tacotron2-DDC.train_tacotron_ddc",
        "peekOfCode": "ap = AudioProcessor.init_from_config(config)\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.",
        "detail": "env.src.tts.recipes.thorsten_DE.tacotron2-DDC.train_tacotron_ddc",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.tacotron2-DDC.train_tacotron_ddc",
        "description": "env.src.tts.recipes.thorsten_DE.tacotron2-DDC.train_tacotron_ddc",
        "peekOfCode": "model = Tacotron2(config, ap, tokenizer, speaker_manager=None)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.thorsten_DE.tacotron2-DDC.train_tacotron_ddc",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.tacotron2-DDC.train_tacotron_ddc",
        "description": "env.src.tts.recipes.thorsten_DE.tacotron2-DDC.train_tacotron_ddc",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.thorsten_DE.tacotron2-DDC.train_tacotron_ddc",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.univnet.train_univnet",
        "description": "env.src.tts.recipes.thorsten_DE.univnet.train_univnet",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\nconfig = UnivnetConfig(\n    batch_size=64,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    seq_len=8192,",
        "detail": "env.src.tts.recipes.thorsten_DE.univnet.train_univnet",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.univnet.train_univnet",
        "description": "env.src.tts.recipes.thorsten_DE.univnet.train_univnet",
        "peekOfCode": "config = UnivnetConfig(\n    batch_size=64,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    seq_len=8192,\n    pad_short=2000,",
        "detail": "env.src.tts.recipes.thorsten_DE.univnet.train_univnet",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.univnet.train_univnet",
        "description": "env.src.tts.recipes.thorsten_DE.univnet.train_univnet",
        "peekOfCode": "ap = AudioProcessor(**config.audio.to_dict())\n# load training samples\neval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n# init model\nmodel = GAN(config, ap)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.thorsten_DE.univnet.train_univnet",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.univnet.train_univnet",
        "description": "env.src.tts.recipes.thorsten_DE.univnet.train_univnet",
        "peekOfCode": "model = GAN(config, ap)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.thorsten_DE.univnet.train_univnet",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.univnet.train_univnet",
        "description": "env.src.tts.recipes.thorsten_DE.univnet.train_univnet",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.thorsten_DE.univnet.train_univnet",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.vits_tts.train_vits",
        "description": "env.src.tts.recipes.thorsten_DE.vits_tts.train_vits",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\ndataset_config = BaseDatasetConfig(\n    formatter=\"thorsten\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../thorsten-de/\")\n)\n# download dataset if not already present\nif not os.path.exists(dataset_config.path):\n    print(\"Downloading dataset\")\n    download_thorsten_de(os.path.split(os.path.abspath(dataset_config.path))[0])\naudio_config = VitsAudioConfig(\n    sample_rate=22050,",
        "detail": "env.src.tts.recipes.thorsten_DE.vits_tts.train_vits",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.vits_tts.train_vits",
        "description": "env.src.tts.recipes.thorsten_DE.vits_tts.train_vits",
        "peekOfCode": "dataset_config = BaseDatasetConfig(\n    formatter=\"thorsten\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"../thorsten-de/\")\n)\n# download dataset if not already present\nif not os.path.exists(dataset_config.path):\n    print(\"Downloading dataset\")\n    download_thorsten_de(os.path.split(os.path.abspath(dataset_config.path))[0])\naudio_config = VitsAudioConfig(\n    sample_rate=22050,\n    win_length=1024,",
        "detail": "env.src.tts.recipes.thorsten_DE.vits_tts.train_vits",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.vits_tts.train_vits",
        "description": "env.src.tts.recipes.thorsten_DE.vits_tts.train_vits",
        "peekOfCode": "audio_config = VitsAudioConfig(\n    sample_rate=22050,\n    win_length=1024,\n    hop_length=256,\n    num_mels=80,\n    mel_fmin=0,\n    mel_fmax=None,\n)\nconfig = VitsConfig(\n    audio=audio_config,",
        "detail": "env.src.tts.recipes.thorsten_DE.vits_tts.train_vits",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.vits_tts.train_vits",
        "description": "env.src.tts.recipes.thorsten_DE.vits_tts.train_vits",
        "peekOfCode": "config = VitsConfig(\n    audio=audio_config,\n    run_name=\"vits_thorsten-de\",\n    batch_size=32,\n    eval_batch_size=16,\n    batch_group_size=5,\n    num_loader_workers=0,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,",
        "detail": "env.src.tts.recipes.thorsten_DE.vits_tts.train_vits",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.vits_tts.train_vits",
        "description": "env.src.tts.recipes.thorsten_DE.vits_tts.train_vits",
        "peekOfCode": "ap = AudioProcessor.init_from_config(config)\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# config is updated with the default characters if not defined in the config.\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.",
        "detail": "env.src.tts.recipes.thorsten_DE.vits_tts.train_vits",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.vits_tts.train_vits",
        "description": "env.src.tts.recipes.thorsten_DE.vits_tts.train_vits",
        "peekOfCode": "model = Vits(config, ap, tokenizer, speaker_manager=None)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n)",
        "detail": "env.src.tts.recipes.thorsten_DE.vits_tts.train_vits",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.vits_tts.train_vits",
        "description": "env.src.tts.recipes.thorsten_DE.vits_tts.train_vits",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.thorsten_DE.vits_tts.train_vits",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.wavegrad.train_wavegrad",
        "description": "env.src.tts.recipes.thorsten_DE.wavegrad.train_wavegrad",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\nconfig = WavegradConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    seq_len=6144,",
        "detail": "env.src.tts.recipes.thorsten_DE.wavegrad.train_wavegrad",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.wavegrad.train_wavegrad",
        "description": "env.src.tts.recipes.thorsten_DE.wavegrad.train_wavegrad",
        "peekOfCode": "config = WavegradConfig(\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    seq_len=6144,\n    pad_short=2000,",
        "detail": "env.src.tts.recipes.thorsten_DE.wavegrad.train_wavegrad",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.wavegrad.train_wavegrad",
        "description": "env.src.tts.recipes.thorsten_DE.wavegrad.train_wavegrad",
        "peekOfCode": "ap = AudioProcessor(**config.audio.to_dict())\n# load training samples\neval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n# init model\nmodel = Wavegrad(config)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,",
        "detail": "env.src.tts.recipes.thorsten_DE.wavegrad.train_wavegrad",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.wavegrad.train_wavegrad",
        "description": "env.src.tts.recipes.thorsten_DE.wavegrad.train_wavegrad",
        "peekOfCode": "model = Wavegrad(config)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n    training_assets={\"audio_processor\": ap},",
        "detail": "env.src.tts.recipes.thorsten_DE.wavegrad.train_wavegrad",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.wavegrad.train_wavegrad",
        "description": "env.src.tts.recipes.thorsten_DE.wavegrad.train_wavegrad",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n    training_assets={\"audio_processor\": ap},\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.thorsten_DE.wavegrad.train_wavegrad",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.wavernn.train_wavernn",
        "description": "env.src.tts.recipes.thorsten_DE.wavernn.train_wavernn",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\nconfig = WavernnConfig(\n    batch_size=64,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=10000,\n    seq_len=1280,",
        "detail": "env.src.tts.recipes.thorsten_DE.wavernn.train_wavernn",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.wavernn.train_wavernn",
        "description": "env.src.tts.recipes.thorsten_DE.wavernn.train_wavernn",
        "peekOfCode": "config = WavernnConfig(\n    batch_size=64,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=10000,\n    seq_len=1280,\n    pad_short=2000,",
        "detail": "env.src.tts.recipes.thorsten_DE.wavernn.train_wavernn",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.wavernn.train_wavernn",
        "description": "env.src.tts.recipes.thorsten_DE.wavernn.train_wavernn",
        "peekOfCode": "ap = AudioProcessor(**config.audio.to_dict())\n# load training samples\neval_samples, train_samples = load_wav_data(config.data_path, config.eval_split_size)\n# init model\nmodel = Wavernn(config)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,",
        "detail": "env.src.tts.recipes.thorsten_DE.wavernn.train_wavernn",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.wavernn.train_wavernn",
        "description": "env.src.tts.recipes.thorsten_DE.wavernn.train_wavernn",
        "peekOfCode": "model = Wavernn(config)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n    training_assets={\"audio_processor\": ap},",
        "detail": "env.src.tts.recipes.thorsten_DE.wavernn.train_wavernn",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.thorsten_DE.wavernn.train_wavernn",
        "description": "env.src.tts.recipes.thorsten_DE.wavernn.train_wavernn",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n    training_assets={\"audio_processor\": ap},\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.thorsten_DE.wavernn.train_wavernn",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "description": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\ndataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", path=os.path.join(output_path, \"../VCTK/\"))\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=23.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,",
        "detail": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "description": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "peekOfCode": "dataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", path=os.path.join(output_path, \"../VCTK/\"))\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=23.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",",
        "detail": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "description": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "peekOfCode": "audio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=23.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,",
        "detail": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "description": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "peekOfCode": "config = FastPitchConfig(\n    run_name=\"fast_pitch_ljspeech\",\n    audio=audio_config,\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=8,\n    num_eval_loader_workers=4,\n    compute_input_seq_cache=True,\n    precompute_num_workers=4,\n    compute_f0=True,",
        "detail": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "description": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "peekOfCode": "ap = AudioProcessor.init_from_config(config)\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.",
        "detail": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "documentation": {}
    },
    {
        "label": "speaker_manager",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "description": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "peekOfCode": "speaker_manager = SpeakerManager()\nspeaker_manager.set_ids_from_data(train_samples + eval_samples, parse_key=\"speaker_name\")\nconfig.model_args.num_speakers = speaker_manager.num_speakers\n# init model\nmodel = ForwardTTS(config, ap, tokenizer, speaker_manager=speaker_manager)\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples",
        "detail": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "documentation": {}
    },
    {
        "label": "config.model_args.num_speakers",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "description": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "peekOfCode": "config.model_args.num_speakers = speaker_manager.num_speakers\n# init model\nmodel = ForwardTTS(config, ap, tokenizer, speaker_manager=speaker_manager)\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... ",
        "detail": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "description": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "peekOfCode": "model = ForwardTTS(config, ap, tokenizer, speaker_manager=speaker_manager)\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "description": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.vctk.fast_pitch.train_fast_pitch",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "description": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\ndataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", path=os.path.join(output_path, \"../VCTK/\"))\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=23.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,",
        "detail": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "description": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "peekOfCode": "dataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", path=os.path.join(output_path, \"../VCTK/\"))\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=23.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",",
        "detail": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "description": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "peekOfCode": "audio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=23.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,",
        "detail": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "description": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "peekOfCode": "config = FastSpeechConfig(\n    run_name=\"fast_speech_vctk\",\n    audio=audio_config,\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=8,\n    num_eval_loader_workers=4,\n    compute_input_seq_cache=True,\n    precompute_num_workers=4,\n    run_eval=True,",
        "detail": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "description": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "peekOfCode": "ap = AudioProcessor.init_from_config(config)\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.",
        "detail": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "documentation": {}
    },
    {
        "label": "speaker_manager",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "description": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "peekOfCode": "speaker_manager = SpeakerManager()\nspeaker_manager.set_ids_from_data(train_samples + eval_samples, parse_key=\"speaker_name\")\nconfig.model_args.num_speakers = speaker_manager.num_speakers\n# init model\nmodel = ForwardTTS(config, ap, tokenizer, speaker_manager=speaker_manager)\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples",
        "detail": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "documentation": {}
    },
    {
        "label": "config.model_args.num_speakers",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "description": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "peekOfCode": "config.model_args.num_speakers = speaker_manager.num_speakers\n# init model\nmodel = ForwardTTS(config, ap, tokenizer, speaker_manager=speaker_manager)\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... ",
        "detail": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "description": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "peekOfCode": "model = ForwardTTS(config, ap, tokenizer, speaker_manager=speaker_manager)\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "description": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.vctk.fast_speech.train_fast_speech",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "description": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\ndataset_path = os.path.join(output_path, \"../VCTK/\")\n# download the dataset if not downloaded\nif not os.path.exists(dataset_path):\n    from TTS.utils.downloaders import download_vctk\n    download_vctk(dataset_path)\n# define dataset config\ndataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", path=dataset_path)\n# define audio config\n#  resample the dataset externally using `TTS/bin/resample.py` and set `resample=False` for faster training",
        "detail": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "documentation": {}
    },
    {
        "label": "dataset_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "description": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "peekOfCode": "dataset_path = os.path.join(output_path, \"../VCTK/\")\n# download the dataset if not downloaded\nif not os.path.exists(dataset_path):\n    from TTS.utils.downloaders import download_vctk\n    download_vctk(dataset_path)\n# define dataset config\ndataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", path=dataset_path)\n# define audio config\n#  resample the dataset externally using `TTS/bin/resample.py` and set `resample=False` for faster training\naudio_config = BaseAudioConfig(sample_rate=22050, resample=True, do_trim_silence=True, trim_db=23.0)",
        "detail": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "description": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "peekOfCode": "dataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", path=dataset_path)\n# define audio config\n#  resample the dataset externally using `TTS/bin/resample.py` and set `resample=False` for faster training\naudio_config = BaseAudioConfig(sample_rate=22050, resample=True, do_trim_silence=True, trim_db=23.0)\n# define model config\nconfig = GlowTTSConfig(\n    batch_size=64,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,",
        "detail": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "description": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "peekOfCode": "audio_config = BaseAudioConfig(sample_rate=22050, resample=True, do_trim_silence=True, trim_db=23.0)\n# define model config\nconfig = GlowTTSConfig(\n    batch_size=64,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    precompute_num_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,",
        "detail": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "description": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "peekOfCode": "config = GlowTTSConfig(\n    batch_size=64,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    precompute_num_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1000,\n    text_cleaner=\"phoneme_cleaners\",",
        "detail": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "description": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "peekOfCode": "ap = AudioProcessor.init_from_config(config)\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.",
        "detail": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "documentation": {}
    },
    {
        "label": "speaker_manager",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "description": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "peekOfCode": "speaker_manager = SpeakerManager()\nspeaker_manager.set_ids_from_data(train_samples + eval_samples, parse_key=\"speaker_name\")\nconfig.num_speakers = speaker_manager.num_speakers\n# init model\nmodel = GlowTTS(config, ap, tokenizer, speaker_manager=speaker_manager)\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples",
        "detail": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "documentation": {}
    },
    {
        "label": "config.num_speakers",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "description": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "peekOfCode": "config.num_speakers = speaker_manager.num_speakers\n# init model\nmodel = GlowTTS(config, ap, tokenizer, speaker_manager=speaker_manager)\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... ",
        "detail": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "description": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "peekOfCode": "model = GlowTTS(config, ap, tokenizer, speaker_manager=speaker_manager)\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "description": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.vctk.glow_tts.train_glow_tts",
        "documentation": {}
    },
    {
        "label": "CURRENT_PATH",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "description": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "peekOfCode": "CURRENT_PATH = os.getcwd()\n# change the root path to the TTS root path\nos.chdir(\"../../../\")\n### Definitions ###\n# dataset\nVCTK_PATH = \"/raid/datasets/VCTK_NEW_16khz_removed_silence_silero_vad/\"  # download:  https://datashare.ed.ac.uk/bitstream/handle/10283/3443/VCTK-Corpus-0.92.zipdddddddddd\nRIR_SIMULATED_PATH = \"/raid/datasets/DA/RIRS_NOISES/simulated_rirs/\"  # download: https://www.openslr.org/17/\nMUSAN_PATH = \"/raid/datasets/DA/musan/\"  # download: https://www.openslr.org/17/\n# training\nOUTPUT_PATH = os.path.join(",
        "detail": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "documentation": {}
    },
    {
        "label": "VCTK_PATH",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "description": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "peekOfCode": "VCTK_PATH = \"/raid/datasets/VCTK_NEW_16khz_removed_silence_silero_vad/\"  # download:  https://datashare.ed.ac.uk/bitstream/handle/10283/3443/VCTK-Corpus-0.92.zipdddddddddd\nRIR_SIMULATED_PATH = \"/raid/datasets/DA/RIRS_NOISES/simulated_rirs/\"  # download: https://www.openslr.org/17/\nMUSAN_PATH = \"/raid/datasets/DA/musan/\"  # download: https://www.openslr.org/17/\n# training\nOUTPUT_PATH = os.path.join(\n    CURRENT_PATH, \"resnet_speaker_encoder_training_output/\"\n)  # path to save the train logs and checkpoint\nCONFIG_OUT_PATH = os.path.join(OUTPUT_PATH, \"config_se.json\")\nRESTORE_PATH = None  # Checkpoint to use for transfer learning if None ignore\n# instance the config",
        "detail": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "documentation": {}
    },
    {
        "label": "RIR_SIMULATED_PATH",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "description": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "peekOfCode": "RIR_SIMULATED_PATH = \"/raid/datasets/DA/RIRS_NOISES/simulated_rirs/\"  # download: https://www.openslr.org/17/\nMUSAN_PATH = \"/raid/datasets/DA/musan/\"  # download: https://www.openslr.org/17/\n# training\nOUTPUT_PATH = os.path.join(\n    CURRENT_PATH, \"resnet_speaker_encoder_training_output/\"\n)  # path to save the train logs and checkpoint\nCONFIG_OUT_PATH = os.path.join(OUTPUT_PATH, \"config_se.json\")\nRESTORE_PATH = None  # Checkpoint to use for transfer learning if None ignore\n# instance the config\n# to speaker encoder",
        "detail": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "documentation": {}
    },
    {
        "label": "MUSAN_PATH",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "description": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "peekOfCode": "MUSAN_PATH = \"/raid/datasets/DA/musan/\"  # download: https://www.openslr.org/17/\n# training\nOUTPUT_PATH = os.path.join(\n    CURRENT_PATH, \"resnet_speaker_encoder_training_output/\"\n)  # path to save the train logs and checkpoint\nCONFIG_OUT_PATH = os.path.join(OUTPUT_PATH, \"config_se.json\")\nRESTORE_PATH = None  # Checkpoint to use for transfer learning if None ignore\n# instance the config\n# to speaker encoder\nconfig = SpeakerEncoderConfig()",
        "detail": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "documentation": {}
    },
    {
        "label": "OUTPUT_PATH",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "description": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "peekOfCode": "OUTPUT_PATH = os.path.join(\n    CURRENT_PATH, \"resnet_speaker_encoder_training_output/\"\n)  # path to save the train logs and checkpoint\nCONFIG_OUT_PATH = os.path.join(OUTPUT_PATH, \"config_se.json\")\nRESTORE_PATH = None  # Checkpoint to use for transfer learning if None ignore\n# instance the config\n# to speaker encoder\nconfig = SpeakerEncoderConfig()\n# to emotion encoder\n# config = EmotionEncoderConfig()",
        "detail": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "documentation": {}
    },
    {
        "label": "CONFIG_OUT_PATH",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "description": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "peekOfCode": "CONFIG_OUT_PATH = os.path.join(OUTPUT_PATH, \"config_se.json\")\nRESTORE_PATH = None  # Checkpoint to use for transfer learning if None ignore\n# instance the config\n# to speaker encoder\nconfig = SpeakerEncoderConfig()\n# to emotion encoder\n# config = EmotionEncoderConfig()\n#### DATASET CONFIG ####\n# The formatter need to return the key \"speaker_name\"  for the speaker encoder and the \"emotion_name\" for the emotion encoder\ndataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", language=\"en-us\", path=VCTK_PATH)",
        "detail": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "documentation": {}
    },
    {
        "label": "RESTORE_PATH",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "description": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "peekOfCode": "RESTORE_PATH = None  # Checkpoint to use for transfer learning if None ignore\n# instance the config\n# to speaker encoder\nconfig = SpeakerEncoderConfig()\n# to emotion encoder\n# config = EmotionEncoderConfig()\n#### DATASET CONFIG ####\n# The formatter need to return the key \"speaker_name\"  for the speaker encoder and the \"emotion_name\" for the emotion encoder\ndataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", language=\"en-us\", path=VCTK_PATH)\n# add the dataset to the config",
        "detail": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "description": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "peekOfCode": "config = SpeakerEncoderConfig()\n# to emotion encoder\n# config = EmotionEncoderConfig()\n#### DATASET CONFIG ####\n# The formatter need to return the key \"speaker_name\"  for the speaker encoder and the \"emotion_name\" for the emotion encoder\ndataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", language=\"en-us\", path=VCTK_PATH)\n# add the dataset to the config\nconfig.datasets = [dataset_config]\n#### TRAINING CONFIG ####\n# The encoder data loader balancer the dataset item equally to guarantee better training and to attend the losses requirements",
        "detail": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "description": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "peekOfCode": "dataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", language=\"en-us\", path=VCTK_PATH)\n# add the dataset to the config\nconfig.datasets = [dataset_config]\n#### TRAINING CONFIG ####\n# The encoder data loader balancer the dataset item equally to guarantee better training and to attend the losses requirements\n# It have two parameters to control the final batch size the number total of speaker used in each batch and the number of samples for each speaker\n# number total of speaker in batch in training\nconfig.num_classes_in_batch = 100\n# number of utterance per class/speaker in the batch in training\nconfig.num_utter_per_class = 4",
        "detail": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "documentation": {}
    },
    {
        "label": "config.datasets",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "description": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "peekOfCode": "config.datasets = [dataset_config]\n#### TRAINING CONFIG ####\n# The encoder data loader balancer the dataset item equally to guarantee better training and to attend the losses requirements\n# It have two parameters to control the final batch size the number total of speaker used in each batch and the number of samples for each speaker\n# number total of speaker in batch in training\nconfig.num_classes_in_batch = 100\n# number of utterance per class/speaker in the batch in training\nconfig.num_utter_per_class = 4\n# final batch size = config.num_classes_in_batch * config.num_utter_per_class\n# number total of speaker in batch in evaluation",
        "detail": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "documentation": {}
    },
    {
        "label": "config.num_classes_in_batch",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "description": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "peekOfCode": "config.num_classes_in_batch = 100\n# number of utterance per class/speaker in the batch in training\nconfig.num_utter_per_class = 4\n# final batch size = config.num_classes_in_batch * config.num_utter_per_class\n# number total of speaker in batch in evaluation\nconfig.eval_num_classes_in_batch = 100\n# number of utterance per class/speaker in the batch in evaluation\nconfig.eval_num_utter_per_class = 4\n# number of data loader workers\nconfig.num_loader_workers = 8",
        "detail": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "documentation": {}
    },
    {
        "label": "config.num_utter_per_class",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "description": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "peekOfCode": "config.num_utter_per_class = 4\n# final batch size = config.num_classes_in_batch * config.num_utter_per_class\n# number total of speaker in batch in evaluation\nconfig.eval_num_classes_in_batch = 100\n# number of utterance per class/speaker in the batch in evaluation\nconfig.eval_num_utter_per_class = 4\n# number of data loader workers\nconfig.num_loader_workers = 8\nconfig.num_val_loader_workers = 8\n# number of epochs",
        "detail": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "documentation": {}
    },
    {
        "label": "config.eval_num_classes_in_batch",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "description": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "peekOfCode": "config.eval_num_classes_in_batch = 100\n# number of utterance per class/speaker in the batch in evaluation\nconfig.eval_num_utter_per_class = 4\n# number of data loader workers\nconfig.num_loader_workers = 8\nconfig.num_val_loader_workers = 8\n# number of epochs\nconfig.epochs = 10000\n# loss to be used in training\nconfig.loss = \"softmaxproto\"",
        "detail": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "documentation": {}
    },
    {
        "label": "config.eval_num_utter_per_class",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "description": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "peekOfCode": "config.eval_num_utter_per_class = 4\n# number of data loader workers\nconfig.num_loader_workers = 8\nconfig.num_val_loader_workers = 8\n# number of epochs\nconfig.epochs = 10000\n# loss to be used in training\nconfig.loss = \"softmaxproto\"\n# run eval\nconfig.run_eval = False",
        "detail": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "documentation": {}
    },
    {
        "label": "config.num_loader_workers",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "description": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "peekOfCode": "config.num_loader_workers = 8\nconfig.num_val_loader_workers = 8\n# number of epochs\nconfig.epochs = 10000\n# loss to be used in training\nconfig.loss = \"softmaxproto\"\n# run eval\nconfig.run_eval = False\n# output path for the checkpoints\nconfig.output_path = OUTPUT_PATH",
        "detail": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "documentation": {}
    },
    {
        "label": "config.num_val_loader_workers",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "description": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "peekOfCode": "config.num_val_loader_workers = 8\n# number of epochs\nconfig.epochs = 10000\n# loss to be used in training\nconfig.loss = \"softmaxproto\"\n# run eval\nconfig.run_eval = False\n# output path for the checkpoints\nconfig.output_path = OUTPUT_PATH\n# Save local checkpoint every save_step steps",
        "detail": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "documentation": {}
    },
    {
        "label": "config.epochs",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "description": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "peekOfCode": "config.epochs = 10000\n# loss to be used in training\nconfig.loss = \"softmaxproto\"\n# run eval\nconfig.run_eval = False\n# output path for the checkpoints\nconfig.output_path = OUTPUT_PATH\n# Save local checkpoint every save_step steps\nconfig.save_step = 2000\n### Model Config ###",
        "detail": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "documentation": {}
    },
    {
        "label": "config.loss",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "description": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "peekOfCode": "config.loss = \"softmaxproto\"\n# run eval\nconfig.run_eval = False\n# output path for the checkpoints\nconfig.output_path = OUTPUT_PATH\n# Save local checkpoint every save_step steps\nconfig.save_step = 2000\n### Model Config ###\nconfig.model_params = {\n    \"model_name\": \"resnet\",  # supported \"lstm\" and \"resnet\"",
        "detail": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "documentation": {}
    },
    {
        "label": "config.run_eval",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "description": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "peekOfCode": "config.run_eval = False\n# output path for the checkpoints\nconfig.output_path = OUTPUT_PATH\n# Save local checkpoint every save_step steps\nconfig.save_step = 2000\n### Model Config ###\nconfig.model_params = {\n    \"model_name\": \"resnet\",  # supported \"lstm\" and \"resnet\"\n    \"input_dim\": 64,\n    \"use_torch_spec\": True,",
        "detail": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "documentation": {}
    },
    {
        "label": "config.output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "description": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "peekOfCode": "config.output_path = OUTPUT_PATH\n# Save local checkpoint every save_step steps\nconfig.save_step = 2000\n### Model Config ###\nconfig.model_params = {\n    \"model_name\": \"resnet\",  # supported \"lstm\" and \"resnet\"\n    \"input_dim\": 64,\n    \"use_torch_spec\": True,\n    \"log_input\": True,\n    \"proj_dim\": 512,  # embedding dim",
        "detail": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "documentation": {}
    },
    {
        "label": "config.save_step",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "description": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "peekOfCode": "config.save_step = 2000\n### Model Config ###\nconfig.model_params = {\n    \"model_name\": \"resnet\",  # supported \"lstm\" and \"resnet\"\n    \"input_dim\": 64,\n    \"use_torch_spec\": True,\n    \"log_input\": True,\n    \"proj_dim\": 512,  # embedding dim\n}\n### Audio Config ###",
        "detail": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "documentation": {}
    },
    {
        "label": "config.model_params",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "description": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "peekOfCode": "config.model_params = {\n    \"model_name\": \"resnet\",  # supported \"lstm\" and \"resnet\"\n    \"input_dim\": 64,\n    \"use_torch_spec\": True,\n    \"log_input\": True,\n    \"proj_dim\": 512,  # embedding dim\n}\n### Audio Config ###\n# To fast train the model divides the audio in small parts. it parameter defines the length in seconds of these \"parts\"\nconfig.voice_len = 2.0",
        "detail": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "documentation": {}
    },
    {
        "label": "config.voice_len",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "description": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "peekOfCode": "config.voice_len = 2.0\n# all others configs\nconfig.audio = {\n    \"fft_size\": 512,\n    \"win_length\": 400,\n    \"hop_length\": 160,\n    \"frame_shift_ms\": None,\n    \"frame_length_ms\": None,\n    \"stft_pad_mode\": \"reflect\",\n    \"sample_rate\": 16000,",
        "detail": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "documentation": {}
    },
    {
        "label": "config.audio",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "description": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "peekOfCode": "config.audio = {\n    \"fft_size\": 512,\n    \"win_length\": 400,\n    \"hop_length\": 160,\n    \"frame_shift_ms\": None,\n    \"frame_length_ms\": None,\n    \"stft_pad_mode\": \"reflect\",\n    \"sample_rate\": 16000,\n    \"resample\": False,\n    \"preemphasis\": 0.97,",
        "detail": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "documentation": {}
    },
    {
        "label": "config.audio_augmentation",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "description": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "peekOfCode": "config.audio_augmentation = {\n    # additive noise and room impulse response (RIR) simulation similar to: https://arxiv.org/pdf/2009.14153.pdf\n    \"p\": 0.5,  # probability to the use of one of the augmentation - 0 means disabled\n    \"rir\": {\"rir_path\": RIR_SIMULATED_PATH, \"conv_mode\": \"full\"},  # download: https://www.openslr.org/17/\n    \"additive\": {\n        \"sounds_path\": MUSAN_PATH,\n        \"speech\": {\"min_snr_in_db\": 13, \"max_snr_in_db\": 20, \"min_num_noises\": 1, \"max_num_noises\": 1},\n        \"noise\": {\"min_snr_in_db\": 0, \"max_snr_in_db\": 15, \"min_num_noises\": 1, \"max_num_noises\": 1},\n        \"music\": {\"min_snr_in_db\": 5, \"max_snr_in_db\": 15, \"min_num_noises\": 1, \"max_num_noises\": 1},\n    },",
        "detail": "env.src.tts.recipes.vctk.resnet_speaker_encoder.train_encoder",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "description": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\ndataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", path=os.path.join(output_path, \"../VCTK/\"))\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=23.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,",
        "detail": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "description": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "peekOfCode": "dataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", path=os.path.join(output_path, \"../VCTK/\"))\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=23.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",",
        "detail": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "description": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "peekOfCode": "audio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=23.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,",
        "detail": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "description": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "peekOfCode": "config = SpeedySpeechConfig(\n    run_name=\"fast_pitch_ljspeech\",\n    audio=audio_config,\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=8,\n    num_eval_loader_workers=4,\n    compute_input_seq_cache=True,\n    precompute_num_workers=4,\n    run_eval=True,",
        "detail": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "description": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "peekOfCode": "ap = AudioProcessor.init_from_config(config)\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.",
        "detail": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "documentation": {}
    },
    {
        "label": "speaker_manager",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "description": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "peekOfCode": "speaker_manager = SpeakerManager()\nspeaker_manager.set_ids_from_data(train_samples + eval_samples, parse_key=\"speaker_name\")\nconfig.model_args.num_speakers = speaker_manager.num_speakers\n# init model\nmodel = ForwardTTS(config, ap, tokenizer, speaker_manager)\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples",
        "detail": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "documentation": {}
    },
    {
        "label": "config.model_args.num_speakers",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "description": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "peekOfCode": "config.model_args.num_speakers = speaker_manager.num_speakers\n# init model\nmodel = ForwardTTS(config, ap, tokenizer, speaker_manager)\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... ",
        "detail": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "description": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "peekOfCode": "model = ForwardTTS(config, ap, tokenizer, speaker_manager)\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "description": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.vctk.speedy_speech.train_speedy_speech",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.tacotron-DDC.train_tacotron-DDC",
        "description": "env.src.tts.recipes.vctk.tacotron-DDC.train_tacotron-DDC",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\ndataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", path=os.path.join(output_path, \"../VCTK/\"))\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    resample=True,  # Resample to 22050 Hz. It slows down training. Use `TTS/bin/resample.py` to pre-resample and set this False for faster training.\n    do_trim_silence=True,\n    trim_db=23.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,",
        "detail": "env.src.tts.recipes.vctk.tacotron-DDC.train_tacotron-DDC",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.tacotron-DDC.train_tacotron-DDC",
        "description": "env.src.tts.recipes.vctk.tacotron-DDC.train_tacotron-DDC",
        "peekOfCode": "dataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", path=os.path.join(output_path, \"../VCTK/\"))\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    resample=True,  # Resample to 22050 Hz. It slows down training. Use `TTS/bin/resample.py` to pre-resample and set this False for faster training.\n    do_trim_silence=True,\n    trim_db=23.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,",
        "detail": "env.src.tts.recipes.vctk.tacotron-DDC.train_tacotron-DDC",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.tacotron-DDC.train_tacotron-DDC",
        "description": "env.src.tts.recipes.vctk.tacotron-DDC.train_tacotron-DDC",
        "peekOfCode": "audio_config = BaseAudioConfig(\n    sample_rate=22050,\n    resample=True,  # Resample to 22050 Hz. It slows down training. Use `TTS/bin/resample.py` to pre-resample and set this False for faster training.\n    do_trim_silence=True,\n    trim_db=23.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",",
        "detail": "env.src.tts.recipes.vctk.tacotron-DDC.train_tacotron-DDC",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.tacotron-DDC.train_tacotron-DDC",
        "description": "env.src.tts.recipes.vctk.tacotron-DDC.train_tacotron-DDC",
        "peekOfCode": "config = TacotronConfig(  # This is the config that is saved for the future use\n    audio=audio_config,\n    batch_size=48,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    precompute_num_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    r=6,",
        "detail": "env.src.tts.recipes.vctk.tacotron-DDC.train_tacotron-DDC",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.tacotron-DDC.train_tacotron-DDC",
        "description": "env.src.tts.recipes.vctk.tacotron-DDC.train_tacotron-DDC",
        "peekOfCode": "ap = AudioProcessor.init_from_config(config)\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.",
        "detail": "env.src.tts.recipes.vctk.tacotron-DDC.train_tacotron-DDC",
        "documentation": {}
    },
    {
        "label": "speaker_manager",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.tacotron-DDC.train_tacotron-DDC",
        "description": "env.src.tts.recipes.vctk.tacotron-DDC.train_tacotron-DDC",
        "peekOfCode": "speaker_manager = SpeakerManager()\nspeaker_manager.set_ids_from_data(train_samples + eval_samples, parse_key=\"speaker_name\")\n# init model\nmodel = Tacotron(config, ap, tokenizer, speaker_manager)\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)",
        "detail": "env.src.tts.recipes.vctk.tacotron-DDC.train_tacotron-DDC",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.tacotron-DDC.train_tacotron-DDC",
        "description": "env.src.tts.recipes.vctk.tacotron-DDC.train_tacotron-DDC",
        "peekOfCode": "model = Tacotron(config, ap, tokenizer, speaker_manager)\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.vctk.tacotron-DDC.train_tacotron-DDC",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.tacotron-DDC.train_tacotron-DDC",
        "description": "env.src.tts.recipes.vctk.tacotron-DDC.train_tacotron-DDC",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.vctk.tacotron-DDC.train_tacotron-DDC",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.tacotron2.train_tacotron2",
        "description": "env.src.tts.recipes.vctk.tacotron2.train_tacotron2",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\ndataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", path=os.path.join(output_path, \"../VCTK/\"))\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    resample=False,  # Resample to 22050 Hz. It slows down training. Use `TTS/bin/resample.py` to pre-resample and set this False for faster training.\n    do_trim_silence=True,\n    trim_db=23.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,",
        "detail": "env.src.tts.recipes.vctk.tacotron2.train_tacotron2",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.tacotron2.train_tacotron2",
        "description": "env.src.tts.recipes.vctk.tacotron2.train_tacotron2",
        "peekOfCode": "dataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", path=os.path.join(output_path, \"../VCTK/\"))\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    resample=False,  # Resample to 22050 Hz. It slows down training. Use `TTS/bin/resample.py` to pre-resample and set this False for faster training.\n    do_trim_silence=True,\n    trim_db=23.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,",
        "detail": "env.src.tts.recipes.vctk.tacotron2.train_tacotron2",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.tacotron2.train_tacotron2",
        "description": "env.src.tts.recipes.vctk.tacotron2.train_tacotron2",
        "peekOfCode": "audio_config = BaseAudioConfig(\n    sample_rate=22050,\n    resample=False,  # Resample to 22050 Hz. It slows down training. Use `TTS/bin/resample.py` to pre-resample and set this False for faster training.\n    do_trim_silence=True,\n    trim_db=23.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",",
        "detail": "env.src.tts.recipes.vctk.tacotron2.train_tacotron2",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.tacotron2.train_tacotron2",
        "description": "env.src.tts.recipes.vctk.tacotron2.train_tacotron2",
        "peekOfCode": "config = Tacotron2Config(  # This is the config that is saved for the future use\n    audio=audio_config,\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    r=2,\n    # gradual_training=[[0, 6, 48], [10000, 4, 32], [50000, 3, 32], [100000, 2, 32]],",
        "detail": "env.src.tts.recipes.vctk.tacotron2.train_tacotron2",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.tacotron2.train_tacotron2",
        "description": "env.src.tts.recipes.vctk.tacotron2.train_tacotron2",
        "peekOfCode": "ap = AudioProcessor.init_from_config(config)\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.",
        "detail": "env.src.tts.recipes.vctk.tacotron2.train_tacotron2",
        "documentation": {}
    },
    {
        "label": "speaker_manager",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.tacotron2.train_tacotron2",
        "description": "env.src.tts.recipes.vctk.tacotron2.train_tacotron2",
        "peekOfCode": "speaker_manager = SpeakerManager()\nspeaker_manager.set_ids_from_data(train_samples + eval_samples, parse_key=\"speaker_name\")\n# init model\nmodel = Tacotron2(config, ap, tokenizer, speaker_manager)\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)",
        "detail": "env.src.tts.recipes.vctk.tacotron2.train_tacotron2",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.tacotron2.train_tacotron2",
        "description": "env.src.tts.recipes.vctk.tacotron2.train_tacotron2",
        "peekOfCode": "model = Tacotron2(config, ap, tokenizer, speaker_manager)\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.vctk.tacotron2.train_tacotron2",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.tacotron2.train_tacotron2",
        "description": "env.src.tts.recipes.vctk.tacotron2.train_tacotron2",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.vctk.tacotron2.train_tacotron2",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.tacotron2-DDC.train_tacotron2-ddc",
        "description": "env.src.tts.recipes.vctk.tacotron2-DDC.train_tacotron2-ddc",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\ndataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", path=os.path.join(output_path, \"../VCTK/\"))\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    resample=False,  # Resample to 22050 Hz. It slows down training. Use `TTS/bin/resample.py` to pre-resample and set this False for faster training.\n    do_trim_silence=True,\n    trim_db=23.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,",
        "detail": "env.src.tts.recipes.vctk.tacotron2-DDC.train_tacotron2-ddc",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.tacotron2-DDC.train_tacotron2-ddc",
        "description": "env.src.tts.recipes.vctk.tacotron2-DDC.train_tacotron2-ddc",
        "peekOfCode": "dataset_config = BaseDatasetConfig(formatter=\"vctk\", meta_file_train=\"\", path=os.path.join(output_path, \"../VCTK/\"))\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    resample=False,  # Resample to 22050 Hz. It slows down training. Use `TTS/bin/resample.py` to pre-resample and set this False for faster training.\n    do_trim_silence=True,\n    trim_db=23.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,",
        "detail": "env.src.tts.recipes.vctk.tacotron2-DDC.train_tacotron2-ddc",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.tacotron2-DDC.train_tacotron2-ddc",
        "description": "env.src.tts.recipes.vctk.tacotron2-DDC.train_tacotron2-ddc",
        "peekOfCode": "audio_config = BaseAudioConfig(\n    sample_rate=22050,\n    resample=False,  # Resample to 22050 Hz. It slows down training. Use `TTS/bin/resample.py` to pre-resample and set this False for faster training.\n    do_trim_silence=True,\n    trim_db=23.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",",
        "detail": "env.src.tts.recipes.vctk.tacotron2-DDC.train_tacotron2-ddc",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.tacotron2-DDC.train_tacotron2-ddc",
        "description": "env.src.tts.recipes.vctk.tacotron2-DDC.train_tacotron2-ddc",
        "peekOfCode": "config = Tacotron2Config(  # This is the config that is saved for the future use\n    audio=audio_config,\n    batch_size=32,\n    eval_batch_size=16,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,\n    test_delay_epochs=-1,\n    r=2,\n    # gradual_training=[[0, 6, 48], [10000, 4, 32], [50000, 3, 32], [100000, 2, 32]],",
        "detail": "env.src.tts.recipes.vctk.tacotron2-DDC.train_tacotron2-ddc",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.tacotron2-DDC.train_tacotron2-ddc",
        "description": "env.src.tts.recipes.vctk.tacotron2-DDC.train_tacotron2-ddc",
        "peekOfCode": "ap = AudioProcessor.init_from_config(config)\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# If characters are not defined in the config, default characters are passed to the config\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.",
        "detail": "env.src.tts.recipes.vctk.tacotron2-DDC.train_tacotron2-ddc",
        "documentation": {}
    },
    {
        "label": "speaker_manager",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.tacotron2-DDC.train_tacotron2-ddc",
        "description": "env.src.tts.recipes.vctk.tacotron2-DDC.train_tacotron2-ddc",
        "peekOfCode": "speaker_manager = SpeakerManager()\nspeaker_manager.set_ids_from_data(train_samples + eval_samples, parse_key=\"speaker_name\")\n# init model\nmodel = Tacotron2(config, ap, tokenizer, speaker_manager)\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)",
        "detail": "env.src.tts.recipes.vctk.tacotron2-DDC.train_tacotron2-ddc",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.tacotron2-DDC.train_tacotron2-ddc",
        "description": "env.src.tts.recipes.vctk.tacotron2-DDC.train_tacotron2-ddc",
        "peekOfCode": "model = Tacotron2(config, ap, tokenizer, speaker_manager)\n# INITIALIZE THE TRAINER\n# Trainer provides a generic API to train all the TTS models with all its perks like mixed-precision training,\n# distributed training, etc.\ntrainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.vctk.tacotron2-DDC.train_tacotron2-ddc",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.tacotron2-DDC.train_tacotron2-ddc",
        "description": "env.src.tts.recipes.vctk.tacotron2-DDC.train_tacotron2-ddc",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(), config, output_path, model=model, train_samples=train_samples, eval_samples=eval_samples\n)\n# AND... 3,2,1... \ntrainer.fit()",
        "detail": "env.src.tts.recipes.vctk.tacotron2-DDC.train_tacotron2-ddc",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.vits.train_vits",
        "description": "env.src.tts.recipes.vctk.vits.train_vits",
        "peekOfCode": "output_path = os.path.dirname(os.path.abspath(__file__))\ndataset_config = BaseDatasetConfig(\n    formatter=\"vctk\", meta_file_train=\"\", language=\"en-us\", path=os.path.join(output_path, \"../VCTK/\")\n)\naudio_config = VitsAudioConfig(\n    sample_rate=22050, win_length=1024, hop_length=256, num_mels=80, mel_fmin=0, mel_fmax=None\n)\nvitsArgs = VitsArgs(\n    use_speaker_embedding=True,\n)",
        "detail": "env.src.tts.recipes.vctk.vits.train_vits",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.vits.train_vits",
        "description": "env.src.tts.recipes.vctk.vits.train_vits",
        "peekOfCode": "dataset_config = BaseDatasetConfig(\n    formatter=\"vctk\", meta_file_train=\"\", language=\"en-us\", path=os.path.join(output_path, \"../VCTK/\")\n)\naudio_config = VitsAudioConfig(\n    sample_rate=22050, win_length=1024, hop_length=256, num_mels=80, mel_fmin=0, mel_fmax=None\n)\nvitsArgs = VitsArgs(\n    use_speaker_embedding=True,\n)\nconfig = VitsConfig(",
        "detail": "env.src.tts.recipes.vctk.vits.train_vits",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.vits.train_vits",
        "description": "env.src.tts.recipes.vctk.vits.train_vits",
        "peekOfCode": "audio_config = VitsAudioConfig(\n    sample_rate=22050, win_length=1024, hop_length=256, num_mels=80, mel_fmin=0, mel_fmax=None\n)\nvitsArgs = VitsArgs(\n    use_speaker_embedding=True,\n)\nconfig = VitsConfig(\n    model_args=vitsArgs,\n    audio=audio_config,\n    run_name=\"vits_vctk\",",
        "detail": "env.src.tts.recipes.vctk.vits.train_vits",
        "documentation": {}
    },
    {
        "label": "vitsArgs",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.vits.train_vits",
        "description": "env.src.tts.recipes.vctk.vits.train_vits",
        "peekOfCode": "vitsArgs = VitsArgs(\n    use_speaker_embedding=True,\n)\nconfig = VitsConfig(\n    model_args=vitsArgs,\n    audio=audio_config,\n    run_name=\"vits_vctk\",\n    batch_size=32,\n    eval_batch_size=16,\n    batch_group_size=5,",
        "detail": "env.src.tts.recipes.vctk.vits.train_vits",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.vits.train_vits",
        "description": "env.src.tts.recipes.vctk.vits.train_vits",
        "peekOfCode": "config = VitsConfig(\n    model_args=vitsArgs,\n    audio=audio_config,\n    run_name=\"vits_vctk\",\n    batch_size=32,\n    eval_batch_size=16,\n    batch_group_size=5,\n    num_loader_workers=4,\n    num_eval_loader_workers=4,\n    run_eval=True,",
        "detail": "env.src.tts.recipes.vctk.vits.train_vits",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.vits.train_vits",
        "description": "env.src.tts.recipes.vctk.vits.train_vits",
        "peekOfCode": "ap = AudioProcessor.init_from_config(config)\n# INITIALIZE THE TOKENIZER\n# Tokenizer is used to convert text to sequences of token IDs.\n# config is updated with the default characters if not defined in the config.\ntokenizer, config = TTSTokenizer.init_from_config(config)\n# LOAD DATA SAMPLES\n# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n# You can define your custom sample loader returning the list of samples.\n# Or define your custom formatter and pass it to the `load_tts_samples`.\n# Check `TTS.tts.datasets.load_tts_samples` for more details.",
        "detail": "env.src.tts.recipes.vctk.vits.train_vits",
        "documentation": {}
    },
    {
        "label": "speaker_manager",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.vits.train_vits",
        "description": "env.src.tts.recipes.vctk.vits.train_vits",
        "peekOfCode": "speaker_manager = SpeakerManager()\nspeaker_manager.set_ids_from_data(train_samples + eval_samples, parse_key=\"speaker_name\")\nconfig.model_args.num_speakers = speaker_manager.num_speakers\n# init model\nmodel = Vits(config, ap, tokenizer, speaker_manager)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,",
        "detail": "env.src.tts.recipes.vctk.vits.train_vits",
        "documentation": {}
    },
    {
        "label": "config.model_args.num_speakers",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.vits.train_vits",
        "description": "env.src.tts.recipes.vctk.vits.train_vits",
        "peekOfCode": "config.model_args.num_speakers = speaker_manager.num_speakers\n# init model\nmodel = Vits(config, ap, tokenizer, speaker_manager)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,",
        "detail": "env.src.tts.recipes.vctk.vits.train_vits",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.vits.train_vits",
        "description": "env.src.tts.recipes.vctk.vits.train_vits",
        "peekOfCode": "model = Vits(config, ap, tokenizer, speaker_manager)\n# init the trainer and \ntrainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n)",
        "detail": "env.src.tts.recipes.vctk.vits.train_vits",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.vits.train_vits",
        "description": "env.src.tts.recipes.vctk.vits.train_vits",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(),\n    config,\n    output_path,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.vctk.vits.train_vits",
        "documentation": {}
    },
    {
        "label": "CURRENT_PATH",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "description": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "peekOfCode": "CURRENT_PATH = os.path.dirname(os.path.abspath(__file__))\n# Name of the run for the Trainer\nRUN_NAME = \"YourTTS-EN-VCTK\"\n# Path where you want to save the models outputs (configs, checkpoints and tensorboard logs)\nOUT_PATH = os.path.dirname(os.path.abspath(__file__))  # \"/raid/coqui/Checkpoints/original-YourTTS/\"\n# If you want to do transfer learning and speedup your training you can set here the path to the original YourTTS model\nRESTORE_PATH = None  # \"/root/.local/share/tts/tts_models--multilingual--multi-dataset--your_tts/model_file.pth\"\n# This paramter is useful to debug, it skips the training epochs and just do the evaluation  and produce the test sentences\nSKIP_TRAIN_EPOCH = False\n# Set here the batch size to be used in training and evaluation",
        "detail": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "documentation": {}
    },
    {
        "label": "RUN_NAME",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "description": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "peekOfCode": "RUN_NAME = \"YourTTS-EN-VCTK\"\n# Path where you want to save the models outputs (configs, checkpoints and tensorboard logs)\nOUT_PATH = os.path.dirname(os.path.abspath(__file__))  # \"/raid/coqui/Checkpoints/original-YourTTS/\"\n# If you want to do transfer learning and speedup your training you can set here the path to the original YourTTS model\nRESTORE_PATH = None  # \"/root/.local/share/tts/tts_models--multilingual--multi-dataset--your_tts/model_file.pth\"\n# This paramter is useful to debug, it skips the training epochs and just do the evaluation  and produce the test sentences\nSKIP_TRAIN_EPOCH = False\n# Set here the batch size to be used in training and evaluation\nBATCH_SIZE = 32\n# Training Sampling rate and the target sampling rate for resampling the downloaded dataset (Note: If you change this you might need to redownload the dataset !!)",
        "detail": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "documentation": {}
    },
    {
        "label": "OUT_PATH",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "description": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "peekOfCode": "OUT_PATH = os.path.dirname(os.path.abspath(__file__))  # \"/raid/coqui/Checkpoints/original-YourTTS/\"\n# If you want to do transfer learning and speedup your training you can set here the path to the original YourTTS model\nRESTORE_PATH = None  # \"/root/.local/share/tts/tts_models--multilingual--multi-dataset--your_tts/model_file.pth\"\n# This paramter is useful to debug, it skips the training epochs and just do the evaluation  and produce the test sentences\nSKIP_TRAIN_EPOCH = False\n# Set here the batch size to be used in training and evaluation\nBATCH_SIZE = 32\n# Training Sampling rate and the target sampling rate for resampling the downloaded dataset (Note: If you change this you might need to redownload the dataset !!)\n# Note: If you add new datasets, please make sure that the dataset sampling rate and this parameter are matching, otherwise resample your audios\nSAMPLE_RATE = 16000",
        "detail": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "documentation": {}
    },
    {
        "label": "RESTORE_PATH",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "description": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "peekOfCode": "RESTORE_PATH = None  # \"/root/.local/share/tts/tts_models--multilingual--multi-dataset--your_tts/model_file.pth\"\n# This paramter is useful to debug, it skips the training epochs and just do the evaluation  and produce the test sentences\nSKIP_TRAIN_EPOCH = False\n# Set here the batch size to be used in training and evaluation\nBATCH_SIZE = 32\n# Training Sampling rate and the target sampling rate for resampling the downloaded dataset (Note: If you change this you might need to redownload the dataset !!)\n# Note: If you add new datasets, please make sure that the dataset sampling rate and this parameter are matching, otherwise resample your audios\nSAMPLE_RATE = 16000\n# Max audio length in seconds to be used in training (every audio bigger than it will be ignored)\nMAX_AUDIO_LEN_IN_SECONDS = 10",
        "detail": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "documentation": {}
    },
    {
        "label": "SKIP_TRAIN_EPOCH",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "description": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "peekOfCode": "SKIP_TRAIN_EPOCH = False\n# Set here the batch size to be used in training and evaluation\nBATCH_SIZE = 32\n# Training Sampling rate and the target sampling rate for resampling the downloaded dataset (Note: If you change this you might need to redownload the dataset !!)\n# Note: If you add new datasets, please make sure that the dataset sampling rate and this parameter are matching, otherwise resample your audios\nSAMPLE_RATE = 16000\n# Max audio length in seconds to be used in training (every audio bigger than it will be ignored)\nMAX_AUDIO_LEN_IN_SECONDS = 10\n### Download VCTK dataset\nVCTK_DOWNLOAD_PATH = os.path.join(CURRENT_PATH, \"VCTK\")",
        "detail": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "documentation": {}
    },
    {
        "label": "BATCH_SIZE",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "description": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "peekOfCode": "BATCH_SIZE = 32\n# Training Sampling rate and the target sampling rate for resampling the downloaded dataset (Note: If you change this you might need to redownload the dataset !!)\n# Note: If you add new datasets, please make sure that the dataset sampling rate and this parameter are matching, otherwise resample your audios\nSAMPLE_RATE = 16000\n# Max audio length in seconds to be used in training (every audio bigger than it will be ignored)\nMAX_AUDIO_LEN_IN_SECONDS = 10\n### Download VCTK dataset\nVCTK_DOWNLOAD_PATH = os.path.join(CURRENT_PATH, \"VCTK\")\n# Define the number of threads used during the audio resampling\nNUM_RESAMPLE_THREADS = 10",
        "detail": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "documentation": {}
    },
    {
        "label": "SAMPLE_RATE",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "description": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "peekOfCode": "SAMPLE_RATE = 16000\n# Max audio length in seconds to be used in training (every audio bigger than it will be ignored)\nMAX_AUDIO_LEN_IN_SECONDS = 10\n### Download VCTK dataset\nVCTK_DOWNLOAD_PATH = os.path.join(CURRENT_PATH, \"VCTK\")\n# Define the number of threads used during the audio resampling\nNUM_RESAMPLE_THREADS = 10\n# Check if VCTK dataset is not already downloaded, if not download it\nif not os.path.exists(VCTK_DOWNLOAD_PATH):\n    print(\">>> Downloading VCTK dataset:\")",
        "detail": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "documentation": {}
    },
    {
        "label": "MAX_AUDIO_LEN_IN_SECONDS",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "description": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "peekOfCode": "MAX_AUDIO_LEN_IN_SECONDS = 10\n### Download VCTK dataset\nVCTK_DOWNLOAD_PATH = os.path.join(CURRENT_PATH, \"VCTK\")\n# Define the number of threads used during the audio resampling\nNUM_RESAMPLE_THREADS = 10\n# Check if VCTK dataset is not already downloaded, if not download it\nif not os.path.exists(VCTK_DOWNLOAD_PATH):\n    print(\">>> Downloading VCTK dataset:\")\n    download_vctk(VCTK_DOWNLOAD_PATH)\n    resample_files(VCTK_DOWNLOAD_PATH, SAMPLE_RATE, file_ext=\"flac\", n_jobs=NUM_RESAMPLE_THREADS)",
        "detail": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "documentation": {}
    },
    {
        "label": "VCTK_DOWNLOAD_PATH",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "description": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "peekOfCode": "VCTK_DOWNLOAD_PATH = os.path.join(CURRENT_PATH, \"VCTK\")\n# Define the number of threads used during the audio resampling\nNUM_RESAMPLE_THREADS = 10\n# Check if VCTK dataset is not already downloaded, if not download it\nif not os.path.exists(VCTK_DOWNLOAD_PATH):\n    print(\">>> Downloading VCTK dataset:\")\n    download_vctk(VCTK_DOWNLOAD_PATH)\n    resample_files(VCTK_DOWNLOAD_PATH, SAMPLE_RATE, file_ext=\"flac\", n_jobs=NUM_RESAMPLE_THREADS)\n# init configs\nvctk_config = BaseDatasetConfig(",
        "detail": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "documentation": {}
    },
    {
        "label": "NUM_RESAMPLE_THREADS",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "description": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "peekOfCode": "NUM_RESAMPLE_THREADS = 10\n# Check if VCTK dataset is not already downloaded, if not download it\nif not os.path.exists(VCTK_DOWNLOAD_PATH):\n    print(\">>> Downloading VCTK dataset:\")\n    download_vctk(VCTK_DOWNLOAD_PATH)\n    resample_files(VCTK_DOWNLOAD_PATH, SAMPLE_RATE, file_ext=\"flac\", n_jobs=NUM_RESAMPLE_THREADS)\n# init configs\nvctk_config = BaseDatasetConfig(\n    formatter=\"vctk\",\n    dataset_name=\"vctk\",",
        "detail": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "documentation": {}
    },
    {
        "label": "vctk_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "description": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "peekOfCode": "vctk_config = BaseDatasetConfig(\n    formatter=\"vctk\",\n    dataset_name=\"vctk\",\n    meta_file_train=\"\",\n    meta_file_val=\"\",\n    path=VCTK_DOWNLOAD_PATH,\n    language=\"en\",\n    ignored_speakers=[\n        \"p261\",\n        \"p225\",",
        "detail": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "documentation": {}
    },
    {
        "label": "DATASETS_CONFIG_LIST",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "description": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "peekOfCode": "DATASETS_CONFIG_LIST = [vctk_config]\n### Extract speaker embeddings\nSPEAKER_ENCODER_CHECKPOINT_PATH = (\n    \"https://github.com/coqui-ai/TTS/releases/download/speaker_encoder_model/model_se.pth.tar\"\n)\nSPEAKER_ENCODER_CONFIG_PATH = \"https://github.com/coqui-ai/TTS/releases/download/speaker_encoder_model/config_se.json\"\nD_VECTOR_FILES = []  # List of speaker embeddings/d-vectors to be used during the training\n# Iterates all the dataset configs checking if the speakers embeddings are already computated, if not compute it\nfor dataset_conf in DATASETS_CONFIG_LIST:\n    # Check if the embeddings weren't already computed, if not compute it",
        "detail": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "documentation": {}
    },
    {
        "label": "SPEAKER_ENCODER_CHECKPOINT_PATH",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "description": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "peekOfCode": "SPEAKER_ENCODER_CHECKPOINT_PATH = (\n    \"https://github.com/coqui-ai/TTS/releases/download/speaker_encoder_model/model_se.pth.tar\"\n)\nSPEAKER_ENCODER_CONFIG_PATH = \"https://github.com/coqui-ai/TTS/releases/download/speaker_encoder_model/config_se.json\"\nD_VECTOR_FILES = []  # List of speaker embeddings/d-vectors to be used during the training\n# Iterates all the dataset configs checking if the speakers embeddings are already computated, if not compute it\nfor dataset_conf in DATASETS_CONFIG_LIST:\n    # Check if the embeddings weren't already computed, if not compute it\n    embeddings_file = os.path.join(dataset_conf.path, \"speakers.pth\")\n    if not os.path.isfile(embeddings_file):",
        "detail": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "documentation": {}
    },
    {
        "label": "SPEAKER_ENCODER_CONFIG_PATH",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "description": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "peekOfCode": "SPEAKER_ENCODER_CONFIG_PATH = \"https://github.com/coqui-ai/TTS/releases/download/speaker_encoder_model/config_se.json\"\nD_VECTOR_FILES = []  # List of speaker embeddings/d-vectors to be used during the training\n# Iterates all the dataset configs checking if the speakers embeddings are already computated, if not compute it\nfor dataset_conf in DATASETS_CONFIG_LIST:\n    # Check if the embeddings weren't already computed, if not compute it\n    embeddings_file = os.path.join(dataset_conf.path, \"speakers.pth\")\n    if not os.path.isfile(embeddings_file):\n        print(f\">>> Computing the speaker embeddings for the {dataset_conf.dataset_name} dataset\")\n        compute_embeddings(\n            SPEAKER_ENCODER_CHECKPOINT_PATH,",
        "detail": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "documentation": {}
    },
    {
        "label": "D_VECTOR_FILES",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "description": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "peekOfCode": "D_VECTOR_FILES = []  # List of speaker embeddings/d-vectors to be used during the training\n# Iterates all the dataset configs checking if the speakers embeddings are already computated, if not compute it\nfor dataset_conf in DATASETS_CONFIG_LIST:\n    # Check if the embeddings weren't already computed, if not compute it\n    embeddings_file = os.path.join(dataset_conf.path, \"speakers.pth\")\n    if not os.path.isfile(embeddings_file):\n        print(f\">>> Computing the speaker embeddings for the {dataset_conf.dataset_name} dataset\")\n        compute_embeddings(\n            SPEAKER_ENCODER_CHECKPOINT_PATH,\n            SPEAKER_ENCODER_CONFIG_PATH,",
        "detail": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "description": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "peekOfCode": "audio_config = VitsAudioConfig(\n    sample_rate=SAMPLE_RATE,\n    hop_length=256,\n    win_length=1024,\n    fft_size=1024,\n    mel_fmin=0.0,\n    mel_fmax=None,\n    num_mels=80,\n)\n# Init VITSArgs setting the arguments that are needed for the YourTTS model",
        "detail": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "documentation": {}
    },
    {
        "label": "model_args",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "description": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "peekOfCode": "model_args = VitsArgs(\n    d_vector_file=D_VECTOR_FILES,\n    use_d_vector_file=True,\n    d_vector_dim=512,\n    num_layers_text_encoder=10,\n    speaker_encoder_model_path=SPEAKER_ENCODER_CHECKPOINT_PATH,\n    speaker_encoder_config_path=SPEAKER_ENCODER_CONFIG_PATH,\n    resblock_type_decoder=\"2\",  # In the paper, we accidentally trained the YourTTS using ResNet blocks type 2, if you like you can use the ResNet blocks type 1 like the VITS model\n    # Useful parameters to enable the Speaker Consistency Loss (SCL) described in the paper\n    # use_speaker_encoder_as_loss=True,",
        "detail": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "description": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "peekOfCode": "config = VitsConfig(\n    output_path=OUT_PATH,\n    model_args=model_args,\n    run_name=RUN_NAME,\n    project_name=\"YourTTS\",\n    run_description=\"\"\"\n            - Original YourTTS trained using VCTK dataset\n        \"\"\",\n    dashboard_logger=\"tensorboard\",\n    logger_uri=None,",
        "detail": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "documentation": {}
    },
    {
        "label": "model",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "description": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "peekOfCode": "model = Vits.init_from_config(config)\n# Init the trainer and \ntrainer = Trainer(\n    TrainerArgs(restore_path=RESTORE_PATH, skip_train_epoch=SKIP_TRAIN_EPOCH),\n    config,\n    output_path=OUT_PATH,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n)",
        "detail": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "documentation": {}
    },
    {
        "label": "trainer",
        "kind": 5,
        "importPath": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "description": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "peekOfCode": "trainer = Trainer(\n    TrainerArgs(restore_path=RESTORE_PATH, skip_train_epoch=SKIP_TRAIN_EPOCH),\n    config,\n    output_path=OUT_PATH,\n    model=model,\n    train_samples=train_samples,\n    eval_samples=eval_samples,\n)\ntrainer.fit()",
        "detail": "env.src.tts.recipes.vctk.yourtts.train_yourtts",
        "documentation": {}
    },
    {
        "label": "TestAudio",
        "kind": 6,
        "importPath": "env.src.tts.tests.aux_tests.test_audio_processor",
        "description": "env.src.tts.tests.aux_tests.test_audio_processor",
        "peekOfCode": "class TestAudio(unittest.TestCase):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.ap = AudioProcessor(**conf)\n    def test_audio_synthesis(self):\n        \"\"\"1. load wav\n        2. set normalization parameters\n        3. extract mel-spec\n        4. invert to wav and save the output\n        \"\"\"",
        "detail": "env.src.tts.tests.aux_tests.test_audio_processor",
        "documentation": {}
    },
    {
        "label": "TESTS_PATH",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_audio_processor",
        "description": "env.src.tts.tests.aux_tests.test_audio_processor",
        "peekOfCode": "TESTS_PATH = get_tests_path()\nOUT_PATH = os.path.join(get_tests_output_path(), \"audio_tests\")\nWAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\nos.makedirs(OUT_PATH, exist_ok=True)\nconf = BaseAudioConfig(mel_fmax=8000, pitch_fmax=640, pitch_fmin=1)\n# pylint: disable=protected-access\nclass TestAudio(unittest.TestCase):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.ap = AudioProcessor(**conf)",
        "detail": "env.src.tts.tests.aux_tests.test_audio_processor",
        "documentation": {}
    },
    {
        "label": "OUT_PATH",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_audio_processor",
        "description": "env.src.tts.tests.aux_tests.test_audio_processor",
        "peekOfCode": "OUT_PATH = os.path.join(get_tests_output_path(), \"audio_tests\")\nWAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\nos.makedirs(OUT_PATH, exist_ok=True)\nconf = BaseAudioConfig(mel_fmax=8000, pitch_fmax=640, pitch_fmin=1)\n# pylint: disable=protected-access\nclass TestAudio(unittest.TestCase):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.ap = AudioProcessor(**conf)\n    def test_audio_synthesis(self):",
        "detail": "env.src.tts.tests.aux_tests.test_audio_processor",
        "documentation": {}
    },
    {
        "label": "WAV_FILE",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_audio_processor",
        "description": "env.src.tts.tests.aux_tests.test_audio_processor",
        "peekOfCode": "WAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\nos.makedirs(OUT_PATH, exist_ok=True)\nconf = BaseAudioConfig(mel_fmax=8000, pitch_fmax=640, pitch_fmin=1)\n# pylint: disable=protected-access\nclass TestAudio(unittest.TestCase):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.ap = AudioProcessor(**conf)\n    def test_audio_synthesis(self):\n        \"\"\"1. load wav",
        "detail": "env.src.tts.tests.aux_tests.test_audio_processor",
        "documentation": {}
    },
    {
        "label": "conf",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_audio_processor",
        "description": "env.src.tts.tests.aux_tests.test_audio_processor",
        "peekOfCode": "conf = BaseAudioConfig(mel_fmax=8000, pitch_fmax=640, pitch_fmin=1)\n# pylint: disable=protected-access\nclass TestAudio(unittest.TestCase):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.ap = AudioProcessor(**conf)\n    def test_audio_synthesis(self):\n        \"\"\"1. load wav\n        2. set normalization parameters\n        3. extract mel-spec",
        "detail": "env.src.tts.tests.aux_tests.test_audio_processor",
        "documentation": {}
    },
    {
        "label": "EmbeddingManagerTest",
        "kind": 6,
        "importPath": "env.src.tts.tests.aux_tests.test_embedding_manager",
        "description": "env.src.tts.tests.aux_tests.test_embedding_manager",
        "peekOfCode": "class EmbeddingManagerTest(unittest.TestCase):\n    \"\"\"Test emEeddingManager for loading embedding files and computing embeddings from waveforms\"\"\"\n    @staticmethod\n    def test_speaker_embedding():\n        # load config\n        config = load_config(encoder_config_path)\n        config.audio.resample = True\n        # create a dummy speaker encoder\n        model = setup_encoder_model(config)\n        save_checkpoint(model, None, None, get_tests_input_path(), 0)",
        "detail": "env.src.tts.tests.aux_tests.test_embedding_manager",
        "documentation": {}
    },
    {
        "label": "encoder_config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_embedding_manager",
        "description": "env.src.tts.tests.aux_tests.test_embedding_manager",
        "peekOfCode": "encoder_config_path = os.path.join(get_tests_input_path(), \"test_speaker_encoder_config.json\")\nencoder_model_path = os.path.join(get_tests_input_path(), \"checkpoint_0.pth\")\nsample_wav_path = os.path.join(get_tests_input_path(), \"../data/ljspeech/wavs/LJ001-0001.wav\")\nsample_wav_path2 = os.path.join(get_tests_input_path(), \"../data/ljspeech/wavs/LJ001-0002.wav\")\nembedding_file_path = os.path.join(get_tests_input_path(), \"../data/dummy_speakers.json\")\nembeddings_file_path2 = os.path.join(get_tests_input_path(), \"../data/dummy_speakers2.json\")\nembeddings_file_pth_path = os.path.join(get_tests_input_path(), \"../data/dummy_speakers.pth\")\nclass EmbeddingManagerTest(unittest.TestCase):\n    \"\"\"Test emEeddingManager for loading embedding files and computing embeddings from waveforms\"\"\"\n    @staticmethod",
        "detail": "env.src.tts.tests.aux_tests.test_embedding_manager",
        "documentation": {}
    },
    {
        "label": "encoder_model_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_embedding_manager",
        "description": "env.src.tts.tests.aux_tests.test_embedding_manager",
        "peekOfCode": "encoder_model_path = os.path.join(get_tests_input_path(), \"checkpoint_0.pth\")\nsample_wav_path = os.path.join(get_tests_input_path(), \"../data/ljspeech/wavs/LJ001-0001.wav\")\nsample_wav_path2 = os.path.join(get_tests_input_path(), \"../data/ljspeech/wavs/LJ001-0002.wav\")\nembedding_file_path = os.path.join(get_tests_input_path(), \"../data/dummy_speakers.json\")\nembeddings_file_path2 = os.path.join(get_tests_input_path(), \"../data/dummy_speakers2.json\")\nembeddings_file_pth_path = os.path.join(get_tests_input_path(), \"../data/dummy_speakers.pth\")\nclass EmbeddingManagerTest(unittest.TestCase):\n    \"\"\"Test emEeddingManager for loading embedding files and computing embeddings from waveforms\"\"\"\n    @staticmethod\n    def test_speaker_embedding():",
        "detail": "env.src.tts.tests.aux_tests.test_embedding_manager",
        "documentation": {}
    },
    {
        "label": "sample_wav_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_embedding_manager",
        "description": "env.src.tts.tests.aux_tests.test_embedding_manager",
        "peekOfCode": "sample_wav_path = os.path.join(get_tests_input_path(), \"../data/ljspeech/wavs/LJ001-0001.wav\")\nsample_wav_path2 = os.path.join(get_tests_input_path(), \"../data/ljspeech/wavs/LJ001-0002.wav\")\nembedding_file_path = os.path.join(get_tests_input_path(), \"../data/dummy_speakers.json\")\nembeddings_file_path2 = os.path.join(get_tests_input_path(), \"../data/dummy_speakers2.json\")\nembeddings_file_pth_path = os.path.join(get_tests_input_path(), \"../data/dummy_speakers.pth\")\nclass EmbeddingManagerTest(unittest.TestCase):\n    \"\"\"Test emEeddingManager for loading embedding files and computing embeddings from waveforms\"\"\"\n    @staticmethod\n    def test_speaker_embedding():\n        # load config",
        "detail": "env.src.tts.tests.aux_tests.test_embedding_manager",
        "documentation": {}
    },
    {
        "label": "sample_wav_path2",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_embedding_manager",
        "description": "env.src.tts.tests.aux_tests.test_embedding_manager",
        "peekOfCode": "sample_wav_path2 = os.path.join(get_tests_input_path(), \"../data/ljspeech/wavs/LJ001-0002.wav\")\nembedding_file_path = os.path.join(get_tests_input_path(), \"../data/dummy_speakers.json\")\nembeddings_file_path2 = os.path.join(get_tests_input_path(), \"../data/dummy_speakers2.json\")\nembeddings_file_pth_path = os.path.join(get_tests_input_path(), \"../data/dummy_speakers.pth\")\nclass EmbeddingManagerTest(unittest.TestCase):\n    \"\"\"Test emEeddingManager for loading embedding files and computing embeddings from waveforms\"\"\"\n    @staticmethod\n    def test_speaker_embedding():\n        # load config\n        config = load_config(encoder_config_path)",
        "detail": "env.src.tts.tests.aux_tests.test_embedding_manager",
        "documentation": {}
    },
    {
        "label": "embedding_file_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_embedding_manager",
        "description": "env.src.tts.tests.aux_tests.test_embedding_manager",
        "peekOfCode": "embedding_file_path = os.path.join(get_tests_input_path(), \"../data/dummy_speakers.json\")\nembeddings_file_path2 = os.path.join(get_tests_input_path(), \"../data/dummy_speakers2.json\")\nembeddings_file_pth_path = os.path.join(get_tests_input_path(), \"../data/dummy_speakers.pth\")\nclass EmbeddingManagerTest(unittest.TestCase):\n    \"\"\"Test emEeddingManager for loading embedding files and computing embeddings from waveforms\"\"\"\n    @staticmethod\n    def test_speaker_embedding():\n        # load config\n        config = load_config(encoder_config_path)\n        config.audio.resample = True",
        "detail": "env.src.tts.tests.aux_tests.test_embedding_manager",
        "documentation": {}
    },
    {
        "label": "embeddings_file_path2",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_embedding_manager",
        "description": "env.src.tts.tests.aux_tests.test_embedding_manager",
        "peekOfCode": "embeddings_file_path2 = os.path.join(get_tests_input_path(), \"../data/dummy_speakers2.json\")\nembeddings_file_pth_path = os.path.join(get_tests_input_path(), \"../data/dummy_speakers.pth\")\nclass EmbeddingManagerTest(unittest.TestCase):\n    \"\"\"Test emEeddingManager for loading embedding files and computing embeddings from waveforms\"\"\"\n    @staticmethod\n    def test_speaker_embedding():\n        # load config\n        config = load_config(encoder_config_path)\n        config.audio.resample = True\n        # create a dummy speaker encoder",
        "detail": "env.src.tts.tests.aux_tests.test_embedding_manager",
        "documentation": {}
    },
    {
        "label": "embeddings_file_pth_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_embedding_manager",
        "description": "env.src.tts.tests.aux_tests.test_embedding_manager",
        "peekOfCode": "embeddings_file_pth_path = os.path.join(get_tests_input_path(), \"../data/dummy_speakers.pth\")\nclass EmbeddingManagerTest(unittest.TestCase):\n    \"\"\"Test emEeddingManager for loading embedding files and computing embeddings from waveforms\"\"\"\n    @staticmethod\n    def test_speaker_embedding():\n        # load config\n        config = load_config(encoder_config_path)\n        config.audio.resample = True\n        # create a dummy speaker encoder\n        model = setup_encoder_model(config)",
        "detail": "env.src.tts.tests.aux_tests.test_embedding_manager",
        "documentation": {}
    },
    {
        "label": "TestExtractTTSSpectrograms",
        "kind": 6,
        "importPath": "env.src.tts.tests.aux_tests.test_extract_tts_spectrograms",
        "description": "env.src.tts.tests.aux_tests.test_extract_tts_spectrograms",
        "peekOfCode": "class TestExtractTTSSpectrograms(unittest.TestCase):\n    @staticmethod\n    def test_GlowTTS():\n        # set paths\n        config_path = os.path.join(get_tests_input_path(), \"test_glow_tts.json\")\n        checkpoint_path = os.path.join(get_tests_output_path(), \"glowtts.pth\")\n        output_path = os.path.join(get_tests_output_path(), \"output_extract_tts_spectrograms/\")\n        # load config\n        c = load_config(config_path)\n        # create model",
        "detail": "env.src.tts.tests.aux_tests.test_extract_tts_spectrograms",
        "documentation": {}
    },
    {
        "label": "TestFindUniquePhonemes",
        "kind": 6,
        "importPath": "env.src.tts.tests.aux_tests.test_find_unique_phonemes",
        "description": "env.src.tts.tests.aux_tests.test_find_unique_phonemes",
        "peekOfCode": "class TestFindUniquePhonemes(unittest.TestCase):\n    @staticmethod\n    def test_espeak_phonemes():\n        # prepare the config\n        config = VitsConfig(\n            batch_size=2,\n            eval_batch_size=2,\n            num_loader_workers=0,\n            num_eval_loader_workers=0,\n            text_cleaner=\"english_cleaners\",",
        "detail": "env.src.tts.tests.aux_tests.test_find_unique_phonemes",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_find_unique_phonemes",
        "description": "env.src.tts.tests.aux_tests.test_find_unique_phonemes",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_model_config.json\")\ndataset_config_en = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",\n    meta_file_val=\"metadata.csv\",\n    path=\"tests/data/ljspeech\",\n    language=\"en\",\n)\n\"\"\"\ndataset_config_pt = BaseDatasetConfig(",
        "detail": "env.src.tts.tests.aux_tests.test_find_unique_phonemes",
        "documentation": {}
    },
    {
        "label": "dataset_config_en",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_find_unique_phonemes",
        "description": "env.src.tts.tests.aux_tests.test_find_unique_phonemes",
        "peekOfCode": "dataset_config_en = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",\n    meta_file_val=\"metadata.csv\",\n    path=\"tests/data/ljspeech\",\n    language=\"en\",\n)\n\"\"\"\ndataset_config_pt = BaseDatasetConfig(\n    formatter=\"ljspeech\",",
        "detail": "env.src.tts.tests.aux_tests.test_find_unique_phonemes",
        "documentation": {}
    },
    {
        "label": "dataset_config_pt",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_find_unique_phonemes",
        "description": "env.src.tts.tests.aux_tests.test_find_unique_phonemes",
        "peekOfCode": "dataset_config_pt = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",\n    meta_file_val=\"metadata.csv\",\n    path=\"tests/data/ljspeech\",\n    language=\"pt-br\",\n)\n\"\"\"\n# pylint: disable=protected-access\nclass TestFindUniquePhonemes(unittest.TestCase):",
        "detail": "env.src.tts.tests.aux_tests.test_find_unique_phonemes",
        "documentation": {}
    },
    {
        "label": "TestNumpyTransforms",
        "kind": 6,
        "importPath": "env.src.tts.tests.aux_tests.test_numpy_transforms",
        "description": "env.src.tts.tests.aux_tests.test_numpy_transforms",
        "peekOfCode": "class TestNumpyTransforms(unittest.TestCase):\n    def setUp(self) -> None:\n        @dataclass\n        class AudioConfig(Coqpit):\n            sample_rate: int = 22050\n            fft_size: int = 1024\n            num_mels: int = 256\n            mel_fmax: int = 1800\n            mel_fmin: int = 0\n            hop_length: int = 256",
        "detail": "env.src.tts.tests.aux_tests.test_numpy_transforms",
        "documentation": {}
    },
    {
        "label": "TESTS_PATH",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_numpy_transforms",
        "description": "env.src.tts.tests.aux_tests.test_numpy_transforms",
        "peekOfCode": "TESTS_PATH = get_tests_path()\nOUT_PATH = os.path.join(get_tests_output_path(), \"audio_tests\")\nWAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\nos.makedirs(OUT_PATH, exist_ok=True)\n# pylint: disable=no-self-use\nclass TestNumpyTransforms(unittest.TestCase):\n    def setUp(self) -> None:\n        @dataclass\n        class AudioConfig(Coqpit):\n            sample_rate: int = 22050",
        "detail": "env.src.tts.tests.aux_tests.test_numpy_transforms",
        "documentation": {}
    },
    {
        "label": "OUT_PATH",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_numpy_transforms",
        "description": "env.src.tts.tests.aux_tests.test_numpy_transforms",
        "peekOfCode": "OUT_PATH = os.path.join(get_tests_output_path(), \"audio_tests\")\nWAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\nos.makedirs(OUT_PATH, exist_ok=True)\n# pylint: disable=no-self-use\nclass TestNumpyTransforms(unittest.TestCase):\n    def setUp(self) -> None:\n        @dataclass\n        class AudioConfig(Coqpit):\n            sample_rate: int = 22050\n            fft_size: int = 1024",
        "detail": "env.src.tts.tests.aux_tests.test_numpy_transforms",
        "documentation": {}
    },
    {
        "label": "WAV_FILE",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_numpy_transforms",
        "description": "env.src.tts.tests.aux_tests.test_numpy_transforms",
        "peekOfCode": "WAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\nos.makedirs(OUT_PATH, exist_ok=True)\n# pylint: disable=no-self-use\nclass TestNumpyTransforms(unittest.TestCase):\n    def setUp(self) -> None:\n        @dataclass\n        class AudioConfig(Coqpit):\n            sample_rate: int = 22050\n            fft_size: int = 1024\n            num_mels: int = 256",
        "detail": "env.src.tts.tests.aux_tests.test_numpy_transforms",
        "documentation": {}
    },
    {
        "label": "LSTMSpeakerEncoderTests",
        "kind": 6,
        "importPath": "env.src.tts.tests.aux_tests.test_speaker_encoder",
        "description": "env.src.tts.tests.aux_tests.test_speaker_encoder",
        "peekOfCode": "class LSTMSpeakerEncoderTests(unittest.TestCase):\n    # pylint: disable=R0201\n    def test_in_out(self):\n        dummy_input = T.rand(4, 80, 20)  # B x D x T\n        dummy_hidden = [T.rand(2, 4, 128), T.rand(2, 4, 128)]\n        model = LSTMSpeakerEncoder(input_dim=80, proj_dim=256, lstm_dim=768, num_lstm_layers=3)\n        # computing d vectors\n        output = model.forward(dummy_input)\n        assert output.shape[0] == 4\n        assert output.shape[1] == 256",
        "detail": "env.src.tts.tests.aux_tests.test_speaker_encoder",
        "documentation": {}
    },
    {
        "label": "ResNetSpeakerEncoderTests",
        "kind": 6,
        "importPath": "env.src.tts.tests.aux_tests.test_speaker_encoder",
        "description": "env.src.tts.tests.aux_tests.test_speaker_encoder",
        "peekOfCode": "class ResNetSpeakerEncoderTests(unittest.TestCase):\n    # pylint: disable=R0201\n    def test_in_out(self):\n        dummy_input = T.rand(4, 80, 20)  # B x D x T\n        dummy_hidden = [T.rand(2, 4, 128), T.rand(2, 4, 128)]\n        model = ResNetSpeakerEncoder(input_dim=80, proj_dim=256)\n        # computing d vectors\n        output = model.forward(dummy_input)\n        assert output.shape[0] == 4\n        assert output.shape[1] == 256",
        "detail": "env.src.tts.tests.aux_tests.test_speaker_encoder",
        "documentation": {}
    },
    {
        "label": "GE2ELossTests",
        "kind": 6,
        "importPath": "env.src.tts.tests.aux_tests.test_speaker_encoder",
        "description": "env.src.tts.tests.aux_tests.test_speaker_encoder",
        "peekOfCode": "class GE2ELossTests(unittest.TestCase):\n    # pylint: disable=R0201\n    def test_in_out(self):\n        # check random input\n        dummy_input = T.rand(4, 5, 64)  # num_speaker x num_utterance x dim\n        loss = GE2ELoss(loss_method=\"softmax\")\n        output = loss.forward(dummy_input)\n        assert output.item() >= 0.0\n        # check all zeros\n        dummy_input = T.ones(4, 5, 64)  # num_speaker x num_utterance x dim",
        "detail": "env.src.tts.tests.aux_tests.test_speaker_encoder",
        "documentation": {}
    },
    {
        "label": "AngleProtoLossTests",
        "kind": 6,
        "importPath": "env.src.tts.tests.aux_tests.test_speaker_encoder",
        "description": "env.src.tts.tests.aux_tests.test_speaker_encoder",
        "peekOfCode": "class AngleProtoLossTests(unittest.TestCase):\n    # pylint: disable=R0201\n    def test_in_out(self):\n        # check random input\n        dummy_input = T.rand(4, 5, 64)  # num_speaker x num_utterance x dim\n        loss = AngleProtoLoss()\n        output = loss.forward(dummy_input)\n        assert output.item() >= 0.0\n        # check all zeros\n        dummy_input = T.ones(4, 5, 64)  # num_speaker x num_utterance x dim",
        "detail": "env.src.tts.tests.aux_tests.test_speaker_encoder",
        "documentation": {}
    },
    {
        "label": "SoftmaxAngleProtoLossTests",
        "kind": 6,
        "importPath": "env.src.tts.tests.aux_tests.test_speaker_encoder",
        "description": "env.src.tts.tests.aux_tests.test_speaker_encoder",
        "peekOfCode": "class SoftmaxAngleProtoLossTests(unittest.TestCase):\n    # pylint: disable=R0201\n    def test_in_out(self):\n        embedding_dim = 64\n        num_speakers = 5\n        batch_size = 4\n        dummy_label = T.randint(low=0, high=num_speakers, size=(batch_size, num_speakers))\n        # check random input\n        dummy_input = T.rand(batch_size, num_speakers, embedding_dim)  # num_speaker x num_utterance x dim\n        loss = SoftmaxAngleProtoLoss(embedding_dim=embedding_dim, n_speakers=num_speakers)",
        "detail": "env.src.tts.tests.aux_tests.test_speaker_encoder",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_speaker_encoder",
        "description": "env.src.tts.tests.aux_tests.test_speaker_encoder",
        "peekOfCode": "file_path = get_tests_input_path()\nclass LSTMSpeakerEncoderTests(unittest.TestCase):\n    # pylint: disable=R0201\n    def test_in_out(self):\n        dummy_input = T.rand(4, 80, 20)  # B x D x T\n        dummy_hidden = [T.rand(2, 4, 128), T.rand(2, 4, 128)]\n        model = LSTMSpeakerEncoder(input_dim=80, proj_dim=256, lstm_dim=768, num_lstm_layers=3)\n        # computing d vectors\n        output = model.forward(dummy_input)\n        assert output.shape[0] == 4",
        "detail": "env.src.tts.tests.aux_tests.test_speaker_encoder",
        "documentation": {}
    },
    {
        "label": "run_test_train",
        "kind": 2,
        "importPath": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "description": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "peekOfCode": "def run_test_train():\n    command = (\n        f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_encoder.py --config_path {config_path} \"\n        f\"--coqpit.output_path {output_path} \"\n        \"--coqpit.datasets.0.formatter ljspeech_test \"\n        \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n        \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n        \"--coqpit.datasets.0.path tests/data/ljspeech \"\n    )\n    run_cli(command)",
        "detail": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "description": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_speaker_encoder_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = SpeakerEncoderConfig(\n    batch_size=4,\n    num_classes_in_batch=4,\n    num_utter_per_class=2,\n    eval_num_classes_in_batch=4,\n    eval_num_utter_per_class=2,\n    num_loader_workers=1,\n    epochs=1,",
        "detail": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "description": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = SpeakerEncoderConfig(\n    batch_size=4,\n    num_classes_in_batch=4,\n    num_utter_per_class=2,\n    eval_num_classes_in_batch=4,\n    eval_num_utter_per_class=2,\n    num_loader_workers=1,\n    epochs=1,\n    print_step=1,",
        "detail": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "description": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "peekOfCode": "config = SpeakerEncoderConfig(\n    batch_size=4,\n    num_classes_in_batch=4,\n    num_utter_per_class=2,\n    eval_num_classes_in_batch=4,\n    eval_num_utter_per_class=2,\n    num_loader_workers=1,\n    epochs=1,\n    print_step=1,\n    save_step=2,",
        "detail": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "description": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.audio.trim_db = 60\nconfig.loss = \"ge2e\"\nconfig.save_json(config_path)\nprint(config)\n# train the model for one epoch\nrun_test_train()\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch",
        "detail": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "description": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "peekOfCode": "config.audio.trim_db = 60\nconfig.loss = \"ge2e\"\nconfig.save_json(config_path)\nprint(config)\n# train the model for one epoch\nrun_test_train()\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (",
        "detail": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "documentation": {}
    },
    {
        "label": "config.loss",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "description": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "peekOfCode": "config.loss = \"ge2e\"\nconfig.save_json(config_path)\nprint(config)\n# train the model for one epoch\nrun_test_train()\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_encoder.py --continue_path {continue_path} \"",
        "detail": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "description": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_encoder.py --continue_path {continue_path} \"\n)\nrun_cli(command_train)\nshutil.rmtree(continue_path)\n# test resnet speaker encoder\nconfig.model_params[\"model_name\"] = \"resnet\"\nconfig.save_json(config_path)",
        "detail": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "description": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_encoder.py --continue_path {continue_path} \"\n)\nrun_cli(command_train)\nshutil.rmtree(continue_path)\n# test resnet speaker encoder\nconfig.model_params[\"model_name\"] = \"resnet\"\nconfig.save_json(config_path)\n# train the model for one epoch\nrun_test_train()",
        "detail": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "documentation": {}
    },
    {
        "label": "config.model_params[\"model_name\"]",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "description": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "peekOfCode": "config.model_params[\"model_name\"] = \"resnet\"\nconfig.save_json(config_path)\n# train the model for one epoch\nrun_test_train()\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_encoder.py --continue_path {continue_path} \"\n)",
        "detail": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "description": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_encoder.py --continue_path {continue_path} \"\n)\nrun_cli(command_train)\nshutil.rmtree(continue_path)\n# test model with ge2e loss function\n# config.loss = \"ge2e\"\n# config.save_json(config_path)",
        "detail": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "description": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_encoder.py --continue_path {continue_path} \"\n)\nrun_cli(command_train)\nshutil.rmtree(continue_path)\n# test model with ge2e loss function\n# config.loss = \"ge2e\"\n# config.save_json(config_path)\n# run_test_train()\n# test model with angleproto loss function",
        "detail": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "documentation": {}
    },
    {
        "label": "config.loss",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "description": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "peekOfCode": "config.loss = \"softmaxproto\"\nconfig.save_json(config_path)\nrun_test_train()",
        "detail": "env.src.tts.tests.aux_tests.test_speaker_encoder_train",
        "documentation": {}
    },
    {
        "label": "SpeakerManagerTest",
        "kind": 6,
        "importPath": "env.src.tts.tests.aux_tests.test_speaker_manager",
        "description": "env.src.tts.tests.aux_tests.test_speaker_manager",
        "peekOfCode": "class SpeakerManagerTest(unittest.TestCase):\n    \"\"\"Test SpeakerManager for loading embedding files and computing d_vectors from waveforms\"\"\"\n    @staticmethod\n    def test_speaker_embedding():\n        # load config\n        config = load_config(encoder_config_path)\n        config.audio.resample = True\n        # create a dummy speaker encoder\n        model = setup_encoder_model(config)\n        save_checkpoint(model, None, None, get_tests_input_path(), 0)",
        "detail": "env.src.tts.tests.aux_tests.test_speaker_manager",
        "documentation": {}
    },
    {
        "label": "encoder_config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_speaker_manager",
        "description": "env.src.tts.tests.aux_tests.test_speaker_manager",
        "peekOfCode": "encoder_config_path = os.path.join(get_tests_input_path(), \"test_speaker_encoder_config.json\")\nencoder_model_path = os.path.join(get_tests_input_path(), \"checkpoint_0.pth\")\nsample_wav_path = os.path.join(get_tests_input_path(), \"../data/ljspeech/wavs/LJ001-0001.wav\")\nsample_wav_path2 = os.path.join(get_tests_input_path(), \"../data/ljspeech/wavs/LJ001-0002.wav\")\nd_vectors_file_path = os.path.join(get_tests_input_path(), \"../data/dummy_speakers.json\")\nd_vectors_file_pth_path = os.path.join(get_tests_input_path(), \"../data/dummy_speakers.pth\")\nclass SpeakerManagerTest(unittest.TestCase):\n    \"\"\"Test SpeakerManager for loading embedding files and computing d_vectors from waveforms\"\"\"\n    @staticmethod\n    def test_speaker_embedding():",
        "detail": "env.src.tts.tests.aux_tests.test_speaker_manager",
        "documentation": {}
    },
    {
        "label": "encoder_model_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_speaker_manager",
        "description": "env.src.tts.tests.aux_tests.test_speaker_manager",
        "peekOfCode": "encoder_model_path = os.path.join(get_tests_input_path(), \"checkpoint_0.pth\")\nsample_wav_path = os.path.join(get_tests_input_path(), \"../data/ljspeech/wavs/LJ001-0001.wav\")\nsample_wav_path2 = os.path.join(get_tests_input_path(), \"../data/ljspeech/wavs/LJ001-0002.wav\")\nd_vectors_file_path = os.path.join(get_tests_input_path(), \"../data/dummy_speakers.json\")\nd_vectors_file_pth_path = os.path.join(get_tests_input_path(), \"../data/dummy_speakers.pth\")\nclass SpeakerManagerTest(unittest.TestCase):\n    \"\"\"Test SpeakerManager for loading embedding files and computing d_vectors from waveforms\"\"\"\n    @staticmethod\n    def test_speaker_embedding():\n        # load config",
        "detail": "env.src.tts.tests.aux_tests.test_speaker_manager",
        "documentation": {}
    },
    {
        "label": "sample_wav_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_speaker_manager",
        "description": "env.src.tts.tests.aux_tests.test_speaker_manager",
        "peekOfCode": "sample_wav_path = os.path.join(get_tests_input_path(), \"../data/ljspeech/wavs/LJ001-0001.wav\")\nsample_wav_path2 = os.path.join(get_tests_input_path(), \"../data/ljspeech/wavs/LJ001-0002.wav\")\nd_vectors_file_path = os.path.join(get_tests_input_path(), \"../data/dummy_speakers.json\")\nd_vectors_file_pth_path = os.path.join(get_tests_input_path(), \"../data/dummy_speakers.pth\")\nclass SpeakerManagerTest(unittest.TestCase):\n    \"\"\"Test SpeakerManager for loading embedding files and computing d_vectors from waveforms\"\"\"\n    @staticmethod\n    def test_speaker_embedding():\n        # load config\n        config = load_config(encoder_config_path)",
        "detail": "env.src.tts.tests.aux_tests.test_speaker_manager",
        "documentation": {}
    },
    {
        "label": "sample_wav_path2",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_speaker_manager",
        "description": "env.src.tts.tests.aux_tests.test_speaker_manager",
        "peekOfCode": "sample_wav_path2 = os.path.join(get_tests_input_path(), \"../data/ljspeech/wavs/LJ001-0002.wav\")\nd_vectors_file_path = os.path.join(get_tests_input_path(), \"../data/dummy_speakers.json\")\nd_vectors_file_pth_path = os.path.join(get_tests_input_path(), \"../data/dummy_speakers.pth\")\nclass SpeakerManagerTest(unittest.TestCase):\n    \"\"\"Test SpeakerManager for loading embedding files and computing d_vectors from waveforms\"\"\"\n    @staticmethod\n    def test_speaker_embedding():\n        # load config\n        config = load_config(encoder_config_path)\n        config.audio.resample = True",
        "detail": "env.src.tts.tests.aux_tests.test_speaker_manager",
        "documentation": {}
    },
    {
        "label": "d_vectors_file_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_speaker_manager",
        "description": "env.src.tts.tests.aux_tests.test_speaker_manager",
        "peekOfCode": "d_vectors_file_path = os.path.join(get_tests_input_path(), \"../data/dummy_speakers.json\")\nd_vectors_file_pth_path = os.path.join(get_tests_input_path(), \"../data/dummy_speakers.pth\")\nclass SpeakerManagerTest(unittest.TestCase):\n    \"\"\"Test SpeakerManager for loading embedding files and computing d_vectors from waveforms\"\"\"\n    @staticmethod\n    def test_speaker_embedding():\n        # load config\n        config = load_config(encoder_config_path)\n        config.audio.resample = True\n        # create a dummy speaker encoder",
        "detail": "env.src.tts.tests.aux_tests.test_speaker_manager",
        "documentation": {}
    },
    {
        "label": "d_vectors_file_pth_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.aux_tests.test_speaker_manager",
        "description": "env.src.tts.tests.aux_tests.test_speaker_manager",
        "peekOfCode": "d_vectors_file_pth_path = os.path.join(get_tests_input_path(), \"../data/dummy_speakers.pth\")\nclass SpeakerManagerTest(unittest.TestCase):\n    \"\"\"Test SpeakerManager for loading embedding files and computing d_vectors from waveforms\"\"\"\n    @staticmethod\n    def test_speaker_embedding():\n        # load config\n        config = load_config(encoder_config_path)\n        config.audio.resample = True\n        # create a dummy speaker encoder\n        model = setup_encoder_model(config)",
        "detail": "env.src.tts.tests.aux_tests.test_speaker_manager",
        "documentation": {}
    },
    {
        "label": "TestTTSFormatters",
        "kind": 6,
        "importPath": "env.src.tts.tests.data_tests.test_dataset_formatters",
        "description": "env.src.tts.tests.data_tests.test_dataset_formatters",
        "peekOfCode": "class TestTTSFormatters(unittest.TestCase):\n    def test_common_voice_preprocessor(self):  # pylint: disable=no-self-use\n        root_path = get_tests_input_path()\n        meta_file = \"common_voice.tsv\"\n        items = common_voice(root_path, meta_file)\n        assert items[0][\"text\"] == \"The applicants are invited for coffee and visa is given immediately.\"\n        assert items[0][\"audio_file\"] == os.path.join(get_tests_input_path(), \"clips\", \"common_voice_en_20005954.wav\")\n        assert items[-1][\"text\"] == \"Competition for limited resources has also resulted in some local conflicts.\"\n        assert items[-1][\"audio_file\"] == os.path.join(get_tests_input_path(), \"clips\", \"common_voice_en_19737074.wav\")",
        "detail": "env.src.tts.tests.data_tests.test_dataset_formatters",
        "documentation": {}
    },
    {
        "label": "TestTTSDataset",
        "kind": 6,
        "importPath": "env.src.tts.tests.data_tests.test_loader",
        "description": "env.src.tts.tests.data_tests.test_loader",
        "peekOfCode": "class TestTTSDataset(unittest.TestCase):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.max_loader_iter = 4\n        self.ap = AudioProcessor(**c.audio)\n    def _create_dataloader(self, batch_size, r, bgs, start_by_longest=False):\n        # load dataset\n        meta_data_train, meta_data_eval = load_tts_samples(dataset_config, eval_split=True, eval_split_size=0.2)\n        items = meta_data_train + meta_data_eval\n        tokenizer, _ = TTSTokenizer.init_from_config(c)",
        "detail": "env.src.tts.tests.data_tests.test_loader",
        "documentation": {}
    },
    {
        "label": "OUTPATH",
        "kind": 5,
        "importPath": "env.src.tts.tests.data_tests.test_loader",
        "description": "env.src.tts.tests.data_tests.test_loader",
        "peekOfCode": "OUTPATH = os.path.join(get_tests_output_path(), \"loader_tests/\")\nos.makedirs(OUTPATH, exist_ok=True)\n# create a dummy config for testing data loaders.\nc = BaseTTSConfig(text_cleaner=\"english_cleaners\", num_loader_workers=0, batch_size=2, use_noise_augment=False)\nc.r = 5\nc.data_path = os.path.join(get_tests_data_path(), \"ljspeech/\")\nok_ljspeech = os.path.exists(c.data_path)\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech_test\",  # ljspeech_test to multi-speaker\n    meta_file_train=\"metadata.csv\",",
        "detail": "env.src.tts.tests.data_tests.test_loader",
        "documentation": {}
    },
    {
        "label": "c",
        "kind": 5,
        "importPath": "env.src.tts.tests.data_tests.test_loader",
        "description": "env.src.tts.tests.data_tests.test_loader",
        "peekOfCode": "c = BaseTTSConfig(text_cleaner=\"english_cleaners\", num_loader_workers=0, batch_size=2, use_noise_augment=False)\nc.r = 5\nc.data_path = os.path.join(get_tests_data_path(), \"ljspeech/\")\nok_ljspeech = os.path.exists(c.data_path)\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech_test\",  # ljspeech_test to multi-speaker\n    meta_file_train=\"metadata.csv\",\n    meta_file_val=None,\n    path=c.data_path,\n    language=\"en\",",
        "detail": "env.src.tts.tests.data_tests.test_loader",
        "documentation": {}
    },
    {
        "label": "c.r",
        "kind": 5,
        "importPath": "env.src.tts.tests.data_tests.test_loader",
        "description": "env.src.tts.tests.data_tests.test_loader",
        "peekOfCode": "c.r = 5\nc.data_path = os.path.join(get_tests_data_path(), \"ljspeech/\")\nok_ljspeech = os.path.exists(c.data_path)\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech_test\",  # ljspeech_test to multi-speaker\n    meta_file_train=\"metadata.csv\",\n    meta_file_val=None,\n    path=c.data_path,\n    language=\"en\",\n)",
        "detail": "env.src.tts.tests.data_tests.test_loader",
        "documentation": {}
    },
    {
        "label": "c.data_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.data_tests.test_loader",
        "description": "env.src.tts.tests.data_tests.test_loader",
        "peekOfCode": "c.data_path = os.path.join(get_tests_data_path(), \"ljspeech/\")\nok_ljspeech = os.path.exists(c.data_path)\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech_test\",  # ljspeech_test to multi-speaker\n    meta_file_train=\"metadata.csv\",\n    meta_file_val=None,\n    path=c.data_path,\n    language=\"en\",\n)\nDATA_EXIST = True",
        "detail": "env.src.tts.tests.data_tests.test_loader",
        "documentation": {}
    },
    {
        "label": "ok_ljspeech",
        "kind": 5,
        "importPath": "env.src.tts.tests.data_tests.test_loader",
        "description": "env.src.tts.tests.data_tests.test_loader",
        "peekOfCode": "ok_ljspeech = os.path.exists(c.data_path)\ndataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech_test\",  # ljspeech_test to multi-speaker\n    meta_file_train=\"metadata.csv\",\n    meta_file_val=None,\n    path=c.data_path,\n    language=\"en\",\n)\nDATA_EXIST = True\nif not os.path.exists(c.data_path):",
        "detail": "env.src.tts.tests.data_tests.test_loader",
        "documentation": {}
    },
    {
        "label": "dataset_config",
        "kind": 5,
        "importPath": "env.src.tts.tests.data_tests.test_loader",
        "description": "env.src.tts.tests.data_tests.test_loader",
        "peekOfCode": "dataset_config = BaseDatasetConfig(\n    formatter=\"ljspeech_test\",  # ljspeech_test to multi-speaker\n    meta_file_train=\"metadata.csv\",\n    meta_file_val=None,\n    path=c.data_path,\n    language=\"en\",\n)\nDATA_EXIST = True\nif not os.path.exists(c.data_path):\n    DATA_EXIST = False",
        "detail": "env.src.tts.tests.data_tests.test_loader",
        "documentation": {}
    },
    {
        "label": "DATA_EXIST",
        "kind": 5,
        "importPath": "env.src.tts.tests.data_tests.test_loader",
        "description": "env.src.tts.tests.data_tests.test_loader",
        "peekOfCode": "DATA_EXIST = True\nif not os.path.exists(c.data_path):\n    DATA_EXIST = False\nprint(\" > Dynamic data loader test: {}\".format(DATA_EXIST))\nclass TestTTSDataset(unittest.TestCase):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.max_loader_iter = 4\n        self.ap = AudioProcessor(**c.audio)\n    def _create_dataloader(self, batch_size, r, bgs, start_by_longest=False):",
        "detail": "env.src.tts.tests.data_tests.test_loader",
        "documentation": {}
    },
    {
        "label": "TestSamplers",
        "kind": 6,
        "importPath": "env.src.tts.tests.data_tests.test_samplers",
        "description": "env.src.tts.tests.data_tests.test_samplers",
        "peekOfCode": "class TestSamplers(unittest.TestCase):\n    def test_language_random_sampler(self):  # pylint: disable=no-self-use\n        random_sampler = torch.utils.data.RandomSampler(train_samples)\n        ids = functools.reduce(lambda a, b: a + b, [list(random_sampler) for i in range(100)])\n        en, pt = 0, 0\n        for index in ids:\n            if train_samples[index][\"language\"] == \"en\":\n                en += 1\n            else:\n                pt += 1",
        "detail": "env.src.tts.tests.data_tests.test_samplers",
        "documentation": {}
    },
    {
        "label": "is_balanced",
        "kind": 2,
        "importPath": "env.src.tts.tests.data_tests.test_samplers",
        "description": "env.src.tts.tests.data_tests.test_samplers",
        "peekOfCode": "def is_balanced(lang_1, lang_2):\n    return 0.85 < lang_1 / lang_2 < 1.2\nclass TestSamplers(unittest.TestCase):\n    def test_language_random_sampler(self):  # pylint: disable=no-self-use\n        random_sampler = torch.utils.data.RandomSampler(train_samples)\n        ids = functools.reduce(lambda a, b: a + b, [list(random_sampler) for i in range(100)])\n        en, pt = 0, 0\n        for index in ids:\n            if train_samples[index][\"language\"] == \"en\":\n                en += 1",
        "detail": "env.src.tts.tests.data_tests.test_samplers",
        "documentation": {}
    },
    {
        "label": "dataset_config_en",
        "kind": 5,
        "importPath": "env.src.tts.tests.data_tests.test_samplers",
        "description": "env.src.tts.tests.data_tests.test_samplers",
        "peekOfCode": "dataset_config_en = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",\n    meta_file_val=\"metadata.csv\",\n    path=\"tests/data/ljspeech\",\n    language=\"en\",\n)\ndataset_config_pt = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",",
        "detail": "env.src.tts.tests.data_tests.test_samplers",
        "documentation": {}
    },
    {
        "label": "dataset_config_pt",
        "kind": 5,
        "importPath": "env.src.tts.tests.data_tests.test_samplers",
        "description": "env.src.tts.tests.data_tests.test_samplers",
        "peekOfCode": "dataset_config_pt = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",\n    meta_file_val=\"metadata.csv\",\n    path=\"tests/data/ljspeech\",\n    language=\"pt-br\",\n)\n# Adding the EN samples twice to create a language unbalanced dataset\ntrain_samples, eval_samples = load_tts_samples(\n    [dataset_config_en, dataset_config_en, dataset_config_pt], eval_split=True",
        "detail": "env.src.tts.tests.data_tests.test_samplers",
        "documentation": {}
    },
    {
        "label": "OUTPUT_PATH",
        "kind": 5,
        "importPath": "env.src.tts.tests.inference_tests.test_python_api",
        "description": "env.src.tts.tests.inference_tests.test_python_api",
        "peekOfCode": "OUTPUT_PATH = os.path.join(get_tests_output_path(), \"test_python_api.wav\")\ncloning_test_wav_path = os.path.join(get_tests_data_path(), \"ljspeech/wavs/LJ001-0028.wav\")\nis_coqui_available = os.environ.get(\"COQUI_STUDIO_TOKEN\")\nif is_coqui_available:\n    class CS_APITest(unittest.TestCase):\n        def test_speakers(self):\n            tts = CS_API()\n            self.assertGreater(len(tts.speakers), 1)\n        def test_emotions(self):\n            tts = CS_API()",
        "detail": "env.src.tts.tests.inference_tests.test_python_api",
        "documentation": {}
    },
    {
        "label": "cloning_test_wav_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.inference_tests.test_python_api",
        "description": "env.src.tts.tests.inference_tests.test_python_api",
        "peekOfCode": "cloning_test_wav_path = os.path.join(get_tests_data_path(), \"ljspeech/wavs/LJ001-0028.wav\")\nis_coqui_available = os.environ.get(\"COQUI_STUDIO_TOKEN\")\nif is_coqui_available:\n    class CS_APITest(unittest.TestCase):\n        def test_speakers(self):\n            tts = CS_API()\n            self.assertGreater(len(tts.speakers), 1)\n        def test_emotions(self):\n            tts = CS_API()\n            self.assertGreater(len(tts.emotions), 1)",
        "detail": "env.src.tts.tests.inference_tests.test_python_api",
        "documentation": {}
    },
    {
        "label": "is_coqui_available",
        "kind": 5,
        "importPath": "env.src.tts.tests.inference_tests.test_python_api",
        "description": "env.src.tts.tests.inference_tests.test_python_api",
        "peekOfCode": "is_coqui_available = os.environ.get(\"COQUI_STUDIO_TOKEN\")\nif is_coqui_available:\n    class CS_APITest(unittest.TestCase):\n        def test_speakers(self):\n            tts = CS_API()\n            self.assertGreater(len(tts.speakers), 1)\n        def test_emotions(self):\n            tts = CS_API()\n            self.assertGreater(len(tts.emotions), 1)\n        def test_list_calls(self):",
        "detail": "env.src.tts.tests.inference_tests.test_python_api",
        "documentation": {}
    },
    {
        "label": "test_synthesize",
        "kind": 2,
        "importPath": "env.src.tts.tests.inference_tests.test_synthesize",
        "description": "env.src.tts.tests.inference_tests.test_synthesize",
        "peekOfCode": "def test_synthesize():\n    \"\"\"Test synthesize.py with diffent arguments.\"\"\"\n    output_path = os.path.join(get_tests_output_path(), \"output.wav\")\n    run_cli(\"tts --list_models\")\n    # single speaker model\n    run_cli(f'tts --text \"This is an example.\" --out_path \"{output_path}\"')\n    run_cli(\n        \"tts --model_name tts_models/en/ljspeech/glow-tts \" f'--text \"This is an example.\" --out_path \"{output_path}\"'\n    )\n    run_cli(",
        "detail": "env.src.tts.tests.inference_tests.test_synthesize",
        "documentation": {}
    },
    {
        "label": "SynthesizerTest",
        "kind": 6,
        "importPath": "env.src.tts.tests.inference_tests.test_synthesizer",
        "description": "env.src.tts.tests.inference_tests.test_synthesizer",
        "peekOfCode": "class SynthesizerTest(unittest.TestCase):\n    # pylint: disable=R0201\n    def _create_random_model(self):\n        # pylint: disable=global-statement\n        config = load_config(os.path.join(get_tests_input_path(), \"dummy_model_config.json\"))\n        model = setup_model(config)\n        output_path = os.path.join(get_tests_input_path())\n        save_checkpoint(config, model, None, None, 10, 1, output_path)\n    def test_in_out(self):\n        self._create_random_model()",
        "detail": "env.src.tts.tests.inference_tests.test_synthesizer",
        "documentation": {}
    },
    {
        "label": "BaseVocabularyTest",
        "kind": 6,
        "importPath": "env.src.tts.tests.text_tests.test_characters",
        "description": "env.src.tts.tests.text_tests.test_characters",
        "peekOfCode": "class BaseVocabularyTest(unittest.TestCase):\n    def setUp(self):\n        self.phonemes = IPAPhonemes()\n        self.base_vocab = BaseVocabulary(\n            vocab=self.phonemes._vocab,\n            pad=self.phonemes.pad,\n            blank=self.phonemes.blank,\n            bos=self.phonemes.bos,\n            eos=self.phonemes.eos,\n        )",
        "detail": "env.src.tts.tests.text_tests.test_characters",
        "documentation": {}
    },
    {
        "label": "BaseCharacterTest",
        "kind": 6,
        "importPath": "env.src.tts.tests.text_tests.test_characters",
        "description": "env.src.tts.tests.text_tests.test_characters",
        "peekOfCode": "class BaseCharacterTest(unittest.TestCase):\n    def setUp(self):\n        self.characters_empty = BaseCharacters(\"\", \"\", pad=\"\", eos=\"\", bos=\"\", blank=\"\", is_unique=True, is_sorted=True)\n    def test_default_character_sets(self):  # pylint: disable=no-self-use\n        \"\"\"Test initiation of default character sets\"\"\"\n        _ = IPAPhonemes()\n        _ = Graphemes()\n    def test_unique(self):\n        \"\"\"Test if the unique option works\"\"\"\n        self.characters_empty.characters = \"abcc\"",
        "detail": "env.src.tts.tests.text_tests.test_characters",
        "documentation": {}
    },
    {
        "label": "TestText",
        "kind": 6,
        "importPath": "env.src.tts.tests.text_tests.test_japanese_phonemizer",
        "description": "env.src.tts.tests.text_tests.test_japanese_phonemizer",
        "peekOfCode": "class TestText(unittest.TestCase):\n    def test_japanese_text_to_phonemes(self):\n        for line in _TEST_CASES.strip().split(\"\\n\"):\n            text, phone = line.split(\"/\")\n            self.assertEqual(japanese_text_to_phonemes(text), phone)\nif __name__ == \"__main__\":\n    unittest.main()",
        "detail": "env.src.tts.tests.text_tests.test_japanese_phonemizer",
        "documentation": {}
    },
    {
        "label": "_TEST_CASES",
        "kind": 5,
        "importPath": "env.src.tts.tests.text_tests.test_japanese_phonemizer",
        "description": "env.src.tts.tests.text_tests.test_japanese_phonemizer",
        "peekOfCode": "_TEST_CASES = \"\"\"\n/dochiraniikimasuka?\n/kyo:waoNseNni,ikimasu.\nAZ/e:karazeqtomadedesu.\n/so:desune!\n/kujirawahonyu:ruidesu.\n/bidioomimasu.\n22/kyo:wahachigatsuniju:ninichidesu\nxyz/eqkusuwaizeqtotoarufabe:tagaNma\n$12.34/nedaNwaju:niteNsaNyoNdorudesu",
        "detail": "env.src.tts.tests.text_tests.test_japanese_phonemizer",
        "documentation": {}
    },
    {
        "label": "TestText",
        "kind": 6,
        "importPath": "env.src.tts.tests.text_tests.test_korean_phonemizer",
        "description": "env.src.tts.tests.text_tests.test_korean_phonemizer",
        "peekOfCode": "class TestText(unittest.TestCase):\n    def test_korean_text_to_phonemes(self):\n        for line in _TEST_CASES.strip().split(\"\\n\"):\n            text, phone = line.split(\"/\")\n            self.assertEqual(korean_text_to_phonemes(text), phone)\n        for line in _TEST_CASES_EN.strip().split(\"\\n\"):\n            text, phone = line.split(\"/\")\n            self.assertEqual(korean_text_to_phonemes(text, character=\"english\"), phone)\nif __name__ == \"__main__\":\n    unittest.main()",
        "detail": "env.src.tts.tests.text_tests.test_korean_phonemizer",
        "documentation": {}
    },
    {
        "label": "_TEST_CASES",
        "kind": 5,
        "importPath": "env.src.tts.tests.text_tests.test_korean_phonemizer",
        "description": "env.src.tts.tests.text_tests.test_korean_phonemizer",
        "peekOfCode": "_TEST_CASES = \"\"\"\n       ./       .\n 8 31 ./   .\n 100  ./   .\nA Z ./  .\n  ./  .\n\"\"\"\n_TEST_CASES_EN = \"\"\"\n  ./IJeYa IJjoGeul BoNeunGuNa.\n  cake ./KeuGo MaSinNeun KeIKeuLeul BuTaKaeYo.",
        "detail": "env.src.tts.tests.text_tests.test_korean_phonemizer",
        "documentation": {}
    },
    {
        "label": "_TEST_CASES_EN",
        "kind": 5,
        "importPath": "env.src.tts.tests.text_tests.test_korean_phonemizer",
        "description": "env.src.tts.tests.text_tests.test_korean_phonemizer",
        "peekOfCode": "_TEST_CASES_EN = \"\"\"\n  ./IJeYa IJjoGeul BoNeunGuNa.\n  cake ./KeuGo MaSinNeun KeIKeuLeul BuTaKaeYo.\n ./JeonBu GeoJinMaLiYa.\n  ./JoEun NoLaeLeul ChaJaSseoYo.\n\"\"\"\nclass TestText(unittest.TestCase):\n    def test_korean_text_to_phonemes(self):\n        for line in _TEST_CASES.strip().split(\"\\n\"):\n            text, phone = line.split(\"/\")",
        "detail": "env.src.tts.tests.text_tests.test_korean_phonemizer",
        "documentation": {}
    },
    {
        "label": "TestEspeakPhonemizer",
        "kind": 6,
        "importPath": "env.src.tts.tests.text_tests.test_phonemizer",
        "description": "env.src.tts.tests.text_tests.test_phonemizer",
        "peekOfCode": "class TestEspeakPhonemizer(unittest.TestCase):\n    def setUp(self):\n        self.phonemizer = ESpeak(language=\"en-us\", backend=\"espeak\")\n        if Version(self.phonemizer.backend_version) >= Version(\"1.48.15\"):\n            target_phonemes = EXPECTED_ESPEAK_v1_48_15_PHONEMES\n        else:\n            target_phonemes = EXPECTED_ESPEAK_PHONEMES\n        for text, ph in zip(EXAMPLE_TEXTs, target_phonemes):\n            phonemes = self.phonemizer.phonemize(text)\n            self.assertEqual(phonemes, ph)",
        "detail": "env.src.tts.tests.text_tests.test_phonemizer",
        "documentation": {}
    },
    {
        "label": "TestEspeakNgPhonemizer",
        "kind": 6,
        "importPath": "env.src.tts.tests.text_tests.test_phonemizer",
        "description": "env.src.tts.tests.text_tests.test_phonemizer",
        "peekOfCode": "class TestEspeakNgPhonemizer(unittest.TestCase):\n    def setUp(self):\n        self.phonemizer = ESpeak(language=\"en-us\", backend=\"espeak-ng\")\n        for text, ph in zip(EXAMPLE_TEXTs, EXPECTED_ESPEAKNG_PHONEMES):\n            phonemes = self.phonemizer.phonemize(text)\n            self.assertEqual(phonemes, ph)\n        # multiple punctuations\n        text = \"Be a voice, not an! echo?\"\n        gt = \"bi  vs, nt n! ko?\"\n        output = self.phonemizer.phonemize(text, separator=\"|\")",
        "detail": "env.src.tts.tests.text_tests.test_phonemizer",
        "documentation": {}
    },
    {
        "label": "TestGruutPhonemizer",
        "kind": 6,
        "importPath": "env.src.tts.tests.text_tests.test_phonemizer",
        "description": "env.src.tts.tests.text_tests.test_phonemizer",
        "peekOfCode": "class TestGruutPhonemizer(unittest.TestCase):\n    def setUp(self):\n        self.phonemizer = Gruut(language=\"en-us\", use_espeak_phonemes=True, keep_stress=False)\n        self.EXPECTED_PHONEMES = [\n            \"|i||s||n|t| ||s|||t|| || h||||v||d| h||z| |o||n| m||d||t|e||||\",\n            \"f|||| |z| l||||l| |z| e||t| w|i||k|s| k||| |k|t||u|||l|i| ||k||i||s, || ||e|| m|||\",\n            \"|n| || p||||t|s| |v| || b||e||n| ||s|p|||n|s||b||l\",\n            \"f|||| |m|o||||n||l| |||j||l|e||||n| |n|d| l|||n||!\",\n        ]\n    def test_phonemize(self):",
        "detail": "env.src.tts.tests.text_tests.test_phonemizer",
        "documentation": {}
    },
    {
        "label": "TestJA_JPPhonemizer",
        "kind": 6,
        "importPath": "env.src.tts.tests.text_tests.test_phonemizer",
        "description": "env.src.tts.tests.text_tests.test_phonemizer",
        "peekOfCode": "class TestJA_JPPhonemizer(unittest.TestCase):\n    def setUp(self):\n        self.phonemizer = JA_JP_Phonemizer()\n        self._TEST_CASES = \"\"\"\n            /dochiraniikimasuka?\n            /kyo:waoNseNni,ikimasu.\n            AZ/e:karazeqtomadedesu.\n            /so:desune!\n            /kujirawahonyu:ruidesu.\n            /bidioomimasu.",
        "detail": "env.src.tts.tests.text_tests.test_phonemizer",
        "documentation": {}
    },
    {
        "label": "TestZH_CN_Phonemizer",
        "kind": 6,
        "importPath": "env.src.tts.tests.text_tests.test_phonemizer",
        "description": "env.src.tts.tests.text_tests.test_phonemizer",
        "peekOfCode": "class TestZH_CN_Phonemizer(unittest.TestCase):\n    def setUp(self):\n        self.phonemizer = ZH_CN_Phonemizer()\n        self._TEST_CASES = \"\"\n    def test_phonemize(self):\n        # TODO: implement ZH phonemizer tests\n        pass\n    def test_name(self):\n        self.assertEqual(self.phonemizer.name(), \"zh_cn_phonemizer\")\n    def test_get_supported_languages(self):",
        "detail": "env.src.tts.tests.text_tests.test_phonemizer",
        "documentation": {}
    },
    {
        "label": "TestBN_Phonemizer",
        "kind": 6,
        "importPath": "env.src.tts.tests.text_tests.test_phonemizer",
        "description": "env.src.tts.tests.text_tests.test_phonemizer",
        "peekOfCode": "class TestBN_Phonemizer(unittest.TestCase):\n    def setUp(self):\n        self.phonemizer = BN_Phonemizer()\n        self._TEST_CASES = \"       ,       ,  \"\n        self._EXPECTED = \"                \"\n    def test_phonemize(self):\n        self.assertEqual(self.phonemizer.phonemize(self._TEST_CASES, separator=\"\"), self._EXPECTED)\n    def test_name(self):\n        self.assertEqual(self.phonemizer.name(), \"bn_phonemizer\")\n    def test_get_supported_languages(self):",
        "detail": "env.src.tts.tests.text_tests.test_phonemizer",
        "documentation": {}
    },
    {
        "label": "TestMultiPhonemizer",
        "kind": 6,
        "importPath": "env.src.tts.tests.text_tests.test_phonemizer",
        "description": "env.src.tts.tests.text_tests.test_phonemizer",
        "peekOfCode": "class TestMultiPhonemizer(unittest.TestCase):\n    def setUp(self):\n        self.phonemizer = MultiPhonemizer({\"tr\": \"espeak\", \"en-us\": \"\", \"de\": \"gruut\", \"zh-cn\": \"\"})\n    def test_phonemize(self):\n        # Enlish espeak\n        text = \"Be a voice, not an! echo?\"\n        gt = \"bi  vs, nt n! ko?\"\n        output = self.phonemizer.phonemize(text, separator=\"|\", language=\"en-us\")\n        output = output.replace(\"|\", \"\")\n        self.assertEqual(output, gt)",
        "detail": "env.src.tts.tests.text_tests.test_phonemizer",
        "documentation": {}
    },
    {
        "label": "EXAMPLE_TEXTs",
        "kind": 5,
        "importPath": "env.src.tts.tests.text_tests.test_phonemizer",
        "description": "env.src.tts.tests.text_tests.test_phonemizer",
        "peekOfCode": "EXAMPLE_TEXTs = [\n    \"Recent research at Harvard has shown meditating\",\n    \"for as little as 8 weeks can actually increase, the grey matter\",\n    \"in the parts of the brain responsible\",\n    \"for emotional regulation and learning!\",\n]\nEXPECTED_ESPEAK_PHONEMES = [\n    \"|i|s||n|t ||s||t |t h||v||d h||z |o|n m||d||t|e|||\",\n    \"f|| |z l|||l |z e|t w|i|k|s k||n |k|t|u|l|i| |n|k||i|s, | ||e m|||\",\n    \"|n|| p||t|s |v|| b||e|n ||s|p||n|s||b|l\",",
        "detail": "env.src.tts.tests.text_tests.test_phonemizer",
        "documentation": {}
    },
    {
        "label": "EXPECTED_ESPEAK_PHONEMES",
        "kind": 5,
        "importPath": "env.src.tts.tests.text_tests.test_phonemizer",
        "description": "env.src.tts.tests.text_tests.test_phonemizer",
        "peekOfCode": "EXPECTED_ESPEAK_PHONEMES = [\n    \"|i|s||n|t ||s||t |t h||v||d h||z |o|n m||d||t|e|||\",\n    \"f|| |z l|||l |z e|t w|i|k|s k||n |k|t|u|l|i| |n|k||i|s, | ||e m|||\",\n    \"|n|| p||t|s |v|| b||e|n ||s|p||n|s||b|l\",\n    \"f|| |m|o|||n|l |||j|u|l|e|||n|| |n|d l||n||!\",\n]\nEXPECTED_ESPEAK_v1_48_15_PHONEMES = [\n    \"|i|s||n|t ||s||t |t h||v||d h||z |o|n m||d||t|e|||\",\n    \"f|| |z l|||l |z e|t w|i|k|s k||n |k|t|u|l|i| |n|k||i|s, | ||e m|||\",\n    \"|n|| p||t|s |v|| b||e|n ||s|p||n|s||b|l\",",
        "detail": "env.src.tts.tests.text_tests.test_phonemizer",
        "documentation": {}
    },
    {
        "label": "EXPECTED_ESPEAK_v1_48_15_PHONEMES",
        "kind": 5,
        "importPath": "env.src.tts.tests.text_tests.test_phonemizer",
        "description": "env.src.tts.tests.text_tests.test_phonemizer",
        "peekOfCode": "EXPECTED_ESPEAK_v1_48_15_PHONEMES = [\n    \"|i|s||n|t ||s||t |t h||v||d h||z |o|n m||d||t|e|||\",\n    \"f|| |z l|||l |z e|t w|i|k|s k||n |k|t|u|l|i| |n|k||i|s, | ||e m|||\",\n    \"|n|| p||t|s |v|| b||e|n ||s|p||n|s||b|l\",\n    \"f|| |m|o|||n|l |||j|u|l|e|||n|| |n|d l||n||!\",\n]\nEXPECTED_ESPEAKNG_PHONEMES = [\n    \"|i|s||n|t ||s||t |t h||v||d h||z |o|n m||d||t|e|||\",\n    \"f|| |z l|||l |z e|t w|i|k|s k||n |k|t|u|l|i| ||k||i|s, | ||e m|||\",\n    \"|n|| p||t|s |v|| b||e|n ||s|p||n|s||b|l\",",
        "detail": "env.src.tts.tests.text_tests.test_phonemizer",
        "documentation": {}
    },
    {
        "label": "EXPECTED_ESPEAKNG_PHONEMES",
        "kind": 5,
        "importPath": "env.src.tts.tests.text_tests.test_phonemizer",
        "description": "env.src.tts.tests.text_tests.test_phonemizer",
        "peekOfCode": "EXPECTED_ESPEAKNG_PHONEMES = [\n    \"|i|s||n|t ||s||t |t h||v||d h||z |o|n m||d||t|e|||\",\n    \"f|| |z l|||l |z e|t w|i|k|s k||n |k|t|u|l|i| ||k||i|s, | ||e m|||\",\n    \"|n|| p||t|s |v|| b||e|n ||s|p||n|s||b|l\",\n    \"f|| |m|o|||n|l |||j||l|e|||n|| |n|d l||n||!\",\n]\nclass TestEspeakPhonemizer(unittest.TestCase):\n    def setUp(self):\n        self.phonemizer = ESpeak(language=\"en-us\", backend=\"espeak\")\n        if Version(self.phonemizer.backend_version) >= Version(\"1.48.15\"):",
        "detail": "env.src.tts.tests.text_tests.test_phonemizer",
        "documentation": {}
    },
    {
        "label": "PunctuationTest",
        "kind": 6,
        "importPath": "env.src.tts.tests.text_tests.test_punctuation",
        "description": "env.src.tts.tests.text_tests.test_punctuation",
        "peekOfCode": "class PunctuationTest(unittest.TestCase):\n    def setUp(self):\n        self.punctuation = Punctuation()\n        self.test_texts = [\n            (\"This, is my text ... to be striped !! from text?\", \"This is my text to be striped from text\"),\n            (\"This, is my text ... to be striped !! from text\", \"This is my text to be striped from text\"),\n            (\"This, is my text ... to be striped  from text?\", \"This is my text to be striped  from text\"),\n            (\"This, is my text to be striped from text\", \"This is my text to be striped from text\"),\n        ]\n    def test_get_set_puncs(self):",
        "detail": "env.src.tts.tests.text_tests.test_punctuation",
        "documentation": {}
    },
    {
        "label": "test_time",
        "kind": 2,
        "importPath": "env.src.tts.tests.text_tests.test_text_cleaners",
        "description": "env.src.tts.tests.text_tests.test_text_cleaners",
        "peekOfCode": "def test_time() -> None:\n    assert english_cleaners(\"It's 11:00\") == \"it's eleven a m\"\n    assert english_cleaners(\"It's 9:01\") == \"it's nine oh one a m\"\n    assert english_cleaners(\"It's 16:00\") == \"it's four p m\"\n    assert english_cleaners(\"It's 00:00 am\") == \"it's twelve a m\"\ndef test_currency() -> None:\n    assert phoneme_cleaners(\"It's $10.50\") == \"It's ten dollars fifty cents\"\n    assert phoneme_cleaners(\"1.1\") == \"one pound sterling one penny\"\n    assert phoneme_cleaners(\"1\") == \"one yen\"\ndef test_expand_numbers() -> None:",
        "detail": "env.src.tts.tests.text_tests.test_text_cleaners",
        "documentation": {}
    },
    {
        "label": "test_currency",
        "kind": 2,
        "importPath": "env.src.tts.tests.text_tests.test_text_cleaners",
        "description": "env.src.tts.tests.text_tests.test_text_cleaners",
        "peekOfCode": "def test_currency() -> None:\n    assert phoneme_cleaners(\"It's $10.50\") == \"It's ten dollars fifty cents\"\n    assert phoneme_cleaners(\"1.1\") == \"one pound sterling one penny\"\n    assert phoneme_cleaners(\"1\") == \"one yen\"\ndef test_expand_numbers() -> None:\n    assert phoneme_cleaners(\"-1\") == \"minus one\"\n    assert phoneme_cleaners(\"1\") == \"one\"",
        "detail": "env.src.tts.tests.text_tests.test_text_cleaners",
        "documentation": {}
    },
    {
        "label": "test_expand_numbers",
        "kind": 2,
        "importPath": "env.src.tts.tests.text_tests.test_text_cleaners",
        "description": "env.src.tts.tests.text_tests.test_text_cleaners",
        "peekOfCode": "def test_expand_numbers() -> None:\n    assert phoneme_cleaners(\"-1\") == \"minus one\"\n    assert phoneme_cleaners(\"1\") == \"one\"",
        "detail": "env.src.tts.tests.text_tests.test_text_cleaners",
        "documentation": {}
    },
    {
        "label": "TestTTSTokenizer",
        "kind": 6,
        "importPath": "env.src.tts.tests.text_tests.test_tokenizer",
        "description": "env.src.tts.tests.text_tests.test_tokenizer",
        "peekOfCode": "class TestTTSTokenizer(unittest.TestCase):\n    def setUp(self):\n        self.tokenizer = TTSTokenizer(use_phonemes=False, characters=Graphemes())\n        self.ph = ESpeak(\"tr\", backend=\"espeak\")\n        self.tokenizer_ph = TTSTokenizer(use_phonemes=True, characters=IPAPhonemes(), phonemizer=self.ph)\n    def test_encode_decode_graphemes(self):\n        text = \"This is, a test.\"\n        ids = self.tokenizer.encode(text)\n        test_hat = self.tokenizer.decode(ids)\n        self.assertEqual(text, test_hat)",
        "detail": "env.src.tts.tests.text_tests.test_tokenizer",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_model_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = AlignTTSConfig(\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=False,\n    phoneme_language=\"en-us\",",
        "detail": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = AlignTTSConfig(\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=False,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(get_tests_output_path(), \"train_outputs/phoneme_cache/\"),",
        "detail": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "peekOfCode": "config = AlignTTSConfig(\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=False,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(get_tests_output_path(), \"train_outputs/phoneme_cache/\"),\n    run_eval=True,",
        "detail": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"",
        "detail": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "peekOfCode": "config.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"",
        "detail": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"\n    \"--coqpit.test_delay_epochs 0 \"\n)\nrun_cli(command_train)",
        "detail": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# Inference using TTS API\ncontinue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path",
        "detail": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "documentation": {}
    },
    {
        "label": "continue_config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "peekOfCode": "continue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference",
        "detail": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "documentation": {}
    },
    {
        "label": "out_wav_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "peekOfCode": "out_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)",
        "detail": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "documentation": {}
    },
    {
        "label": "inference_command",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "peekOfCode": "inference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)\n# restore the model and continue training for one more epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_align_tts_train",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"fast_pitch_speaker_emb_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "peekOfCode": "audio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "peekOfCode": "config = FastPitchConfig(\n    audio=audio_config,\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=\"tests/data/ljspeech/phoneme_cache/\",",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.use_speaker_embedding = True\nconfig.model_args.use_speaker_embedding = True\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}'  python TTS/bin/train_tts.py --config_path {config_path}  \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.use_speaker_embedding",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "peekOfCode": "config.use_speaker_embedding = True\nconfig.model_args.use_speaker_embedding = True\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}'  python TTS/bin/train_tts.py --config_path {config_path}  \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.model_args.use_speaker_embedding",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "peekOfCode": "config.model_args.use_speaker_embedding = True\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}'  python TTS/bin/train_tts.py --config_path {config_path}  \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "peekOfCode": "config.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}'  python TTS/bin/train_tts.py --config_path {config_path}  \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}'  python TTS/bin/train_tts.py --config_path {config_path}  \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"\n    \"--coqpit.datasets.0.meta_file_attn_mask tests/data/ljspeech/metadata_attn_mask.txt \"\n    \"--coqpit.test_delay_epochs 0\"\n)",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# Inference using TTS API\ncontinue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech-1\"\ncontinue_speakers_path = os.path.join(continue_path, \"speakers.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "continue_config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "peekOfCode": "continue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech-1\"\ncontinue_speakers_path = os.path.join(continue_path, \"speakers.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "out_wav_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "peekOfCode": "out_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech-1\"\ncontinue_speakers_path = os.path.join(continue_path, \"speakers.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "speaker_id",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "peekOfCode": "speaker_id = \"ljspeech-1\"\ncontinue_speakers_path = os.path.join(continue_path, \"speakers.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "continue_speakers_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "peekOfCode": "continue_speakers_path = os.path.join(continue_path, \"speakers.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "inference_command",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "peekOfCode": "inference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)\n# restore the model and continue training for one more epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_model_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "peekOfCode": "audio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "peekOfCode": "config = FastPitchConfig(\n    audio=audio_config,\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=\"tests/data/ljspeech/phoneme_cache/\",",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.use_speaker_embedding = False\nconfig.model_args.use_speaker_embedding = False\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}'  python TTS/bin/train_tts.py --config_path {config_path}  \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "documentation": {}
    },
    {
        "label": "config.use_speaker_embedding",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "peekOfCode": "config.use_speaker_embedding = False\nconfig.model_args.use_speaker_embedding = False\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}'  python TTS/bin/train_tts.py --config_path {config_path}  \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "documentation": {}
    },
    {
        "label": "config.model_args.use_speaker_embedding",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "peekOfCode": "config.model_args.use_speaker_embedding = False\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}'  python TTS/bin/train_tts.py --config_path {config_path}  \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "peekOfCode": "config.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}'  python TTS/bin/train_tts.py --config_path {config_path}  \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}'  python TTS/bin/train_tts.py --config_path {config_path}  \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"\n    \"--coqpit.datasets.0.meta_file_attn_mask tests/data/ljspeech/metadata_attn_mask.txt \"\n    \"--coqpit.test_delay_epochs 0\"\n)",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# Inference using TTS API\ncontinue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "documentation": {}
    },
    {
        "label": "continue_config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "peekOfCode": "continue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "documentation": {}
    },
    {
        "label": "out_wav_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "peekOfCode": "out_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "documentation": {}
    },
    {
        "label": "inference_command",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "peekOfCode": "inference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)\n# restore the model and continue training for one more epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "description": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_fast_pitch_train",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"fast_pitch_speaker_emb_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "peekOfCode": "audio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "peekOfCode": "config = Fastspeech2Config(\n    audio=audio_config,\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=\"tests/data/ljspeech/phoneme_cache/\",",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.use_speaker_embedding = True\nconfig.model_args.use_speaker_embedding = True\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}'  python TTS/bin/train_tts.py --config_path {config_path}  \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.use_speaker_embedding",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "peekOfCode": "config.use_speaker_embedding = True\nconfig.model_args.use_speaker_embedding = True\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}'  python TTS/bin/train_tts.py --config_path {config_path}  \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.model_args.use_speaker_embedding",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "peekOfCode": "config.model_args.use_speaker_embedding = True\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}'  python TTS/bin/train_tts.py --config_path {config_path}  \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "peekOfCode": "config.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}'  python TTS/bin/train_tts.py --config_path {config_path}  \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}'  python TTS/bin/train_tts.py --config_path {config_path}  \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"\n    \"--coqpit.datasets.0.meta_file_attn_mask tests/data/ljspeech/metadata_attn_mask.txt \"\n    \"--coqpit.test_delay_epochs 0\"\n)",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# Inference using TTS API\ncontinue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech-1\"\ncontinue_speakers_path = os.path.join(continue_path, \"speakers.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "continue_config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "peekOfCode": "continue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech-1\"\ncontinue_speakers_path = os.path.join(continue_path, \"speakers.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "out_wav_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "peekOfCode": "out_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech-1\"\ncontinue_speakers_path = os.path.join(continue_path, \"speakers.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "speaker_id",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "peekOfCode": "speaker_id = \"ljspeech-1\"\ncontinue_speakers_path = os.path.join(continue_path, \"speakers.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "continue_speakers_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "peekOfCode": "continue_speakers_path = os.path.join(continue_path, \"speakers.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "inference_command",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "peekOfCode": "inference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)\n# restore the model and continue training for one more epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_model_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\naudio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "documentation": {}
    },
    {
        "label": "audio_config",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "peekOfCode": "audio_config = BaseAudioConfig(\n    sample_rate=22050,\n    do_trim_silence=True,\n    trim_db=60.0,\n    signal_norm=False,\n    mel_fmin=0.0,\n    mel_fmax=8000,\n    spec_gain=1.0,\n    log_func=\"np.log\",\n    ref_level_db=20,",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "peekOfCode": "config = Fastspeech2Config(\n    audio=audio_config,\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=\"tests/data/ljspeech/phoneme_cache/\",",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.use_speaker_embedding = False\nconfig.model_args.use_speaker_embedding = False\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}'  python TTS/bin/train_tts.py --config_path {config_path}  \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "documentation": {}
    },
    {
        "label": "config.use_speaker_embedding",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "peekOfCode": "config.use_speaker_embedding = False\nconfig.model_args.use_speaker_embedding = False\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}'  python TTS/bin/train_tts.py --config_path {config_path}  \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "documentation": {}
    },
    {
        "label": "config.model_args.use_speaker_embedding",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "peekOfCode": "config.model_args.use_speaker_embedding = False\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}'  python TTS/bin/train_tts.py --config_path {config_path}  \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "peekOfCode": "config.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}'  python TTS/bin/train_tts.py --config_path {config_path}  \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}'  python TTS/bin/train_tts.py --config_path {config_path}  \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"\n    \"--coqpit.datasets.0.meta_file_attn_mask tests/data/ljspeech/metadata_attn_mask.txt \"\n    \"--coqpit.test_delay_epochs 0\"\n)",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# Inference using TTS API\ncontinue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "documentation": {}
    },
    {
        "label": "continue_config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "peekOfCode": "continue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "documentation": {}
    },
    {
        "label": "out_wav_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "peekOfCode": "out_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "documentation": {}
    },
    {
        "label": "inference_command",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "peekOfCode": "inference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)\n# restore the model and continue training for one more epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "description": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_fastspeech_2_train",
        "documentation": {}
    },
    {
        "label": "test_encoder",
        "kind": 2,
        "importPath": "env.src.tts.tests.tts_tests.test_feed_forward_layers",
        "description": "env.src.tts.tests.tts_tests.test_feed_forward_layers",
        "peekOfCode": "def test_encoder():\n    input_dummy = torch.rand(8, 14, 37).to(device)\n    input_lengths = torch.randint(31, 37, (8,)).long().to(device)\n    input_lengths[-1] = 37\n    input_mask = torch.unsqueeze(sequence_mask(input_lengths, input_dummy.size(2)), 1).to(device)\n    # relative positional transformer encoder\n    layer = Encoder(\n        out_channels=11,\n        in_hidden_channels=14,\n        encoder_type=\"relative_position_transformer\",",
        "detail": "env.src.tts.tests.tts_tests.test_feed_forward_layers",
        "documentation": {}
    },
    {
        "label": "test_decoder",
        "kind": 2,
        "importPath": "env.src.tts.tests.tts_tests.test_feed_forward_layers",
        "description": "env.src.tts.tests.tts_tests.test_feed_forward_layers",
        "peekOfCode": "def test_decoder():\n    input_dummy = torch.rand(8, 128, 37).to(device)\n    input_lengths = torch.randint(31, 37, (8,)).long().to(device)\n    input_lengths[-1] = 37\n    input_mask = torch.unsqueeze(sequence_mask(input_lengths, input_dummy.size(2)), 1).to(device)\n    # residual bn conv decoder\n    layer = Decoder(out_channels=11, in_hidden_channels=128).to(device)\n    output = layer(input_dummy, input_mask)\n    assert list(output.shape) == [8, 11, 37]\n    # transformer decoder",
        "detail": "env.src.tts.tests.tts_tests.test_feed_forward_layers",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_feed_forward_layers",
        "description": "env.src.tts.tests.tts_tests.test_feed_forward_layers",
        "peekOfCode": "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ndef test_encoder():\n    input_dummy = torch.rand(8, 14, 37).to(device)\n    input_lengths = torch.randint(31, 37, (8,)).long().to(device)\n    input_lengths[-1] = 37\n    input_mask = torch.unsqueeze(sequence_mask(input_lengths, input_dummy.size(2)), 1).to(device)\n    # relative positional transformer encoder\n    layer = Encoder(\n        out_channels=11,\n        in_hidden_channels=14,",
        "detail": "env.src.tts.tests.tts_tests.test_feed_forward_layers",
        "documentation": {}
    },
    {
        "label": "expand_encoder_outputs_test",
        "kind": 2,
        "importPath": "env.src.tts.tests.tts_tests.test_forward_tts",
        "description": "env.src.tts.tests.tts_tests.test_forward_tts",
        "peekOfCode": "def expand_encoder_outputs_test():\n    model = ForwardTTS(ForwardTTSArgs(num_chars=10))\n    inputs = T.rand(2, 5, 57)\n    durations = T.randint(1, 4, (2, 57))\n    x_mask = T.ones(2, 1, 57)\n    y_mask = T.ones(2, 1, durations.sum(1).max())\n    expanded, _ = model.expand_encoder_outputs(inputs, durations, x_mask, y_mask)\n    for b in range(durations.shape[0]):\n        index = 0\n        for idx, dur in enumerate(durations[b]):",
        "detail": "env.src.tts.tests.tts_tests.test_forward_tts",
        "documentation": {}
    },
    {
        "label": "model_input_output_test",
        "kind": 2,
        "importPath": "env.src.tts.tests.tts_tests.test_forward_tts",
        "description": "env.src.tts.tests.tts_tests.test_forward_tts",
        "peekOfCode": "def model_input_output_test():\n    \"\"\"Assert the output shapes of the model in different modes\"\"\"\n    # VANILLA MODEL\n    model = ForwardTTS(ForwardTTSArgs(num_chars=10, use_pitch=False, use_aligner=False))\n    x = T.randint(0, 10, (2, 21))\n    x_lengths = T.randint(10, 22, (2,))\n    x_lengths[-1] = 21\n    x_mask = sequence_mask(x_lengths).unsqueeze(1).long()\n    durations = T.randint(1, 4, (2, 21))\n    durations = durations * x_mask.squeeze(1)",
        "detail": "env.src.tts.tests.tts_tests.test_forward_tts",
        "documentation": {}
    },
    {
        "label": "TestGlowTTS",
        "kind": 6,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts",
        "peekOfCode": "class TestGlowTTS(unittest.TestCase):\n    @staticmethod\n    def _create_inputs(batch_size=8):\n        input_dummy = torch.randint(0, 24, (batch_size, 128)).long().to(device)\n        input_lengths = torch.randint(100, 129, (batch_size,)).long().to(device)\n        input_lengths[-1] = 128\n        mel_spec = torch.rand(batch_size, 30, c.audio[\"num_mels\"]).to(device)\n        mel_lengths = torch.randint(20, 30, (batch_size,)).long().to(device)\n        speaker_ids = torch.randint(0, 5, (batch_size,)).long().to(device)\n        return input_dummy, input_lengths, mel_spec, mel_lengths, speaker_ids",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts",
        "documentation": {}
    },
    {
        "label": "count_parameters",
        "kind": 2,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts",
        "peekOfCode": "def count_parameters(model):\n    r\"\"\"Count number of trainable parameters in a network\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nclass TestGlowTTS(unittest.TestCase):\n    @staticmethod\n    def _create_inputs(batch_size=8):\n        input_dummy = torch.randint(0, 24, (batch_size, 128)).long().to(device)\n        input_lengths = torch.randint(100, 129, (batch_size,)).long().to(device)\n        input_lengths[-1] = 128\n        mel_spec = torch.rand(batch_size, 30, c.audio[\"num_mels\"]).to(device)",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts",
        "peekOfCode": "use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nc = GlowTTSConfig()\nap = AudioProcessor(**c.audio)\nWAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\nBATCH_SIZE = 3\ndef count_parameters(model):\n    r\"\"\"Count number of trainable parameters in a network\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nclass TestGlowTTS(unittest.TestCase):",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts",
        "peekOfCode": "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nc = GlowTTSConfig()\nap = AudioProcessor(**c.audio)\nWAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\nBATCH_SIZE = 3\ndef count_parameters(model):\n    r\"\"\"Count number of trainable parameters in a network\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nclass TestGlowTTS(unittest.TestCase):\n    @staticmethod",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts",
        "documentation": {}
    },
    {
        "label": "c",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts",
        "peekOfCode": "c = GlowTTSConfig()\nap = AudioProcessor(**c.audio)\nWAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\nBATCH_SIZE = 3\ndef count_parameters(model):\n    r\"\"\"Count number of trainable parameters in a network\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nclass TestGlowTTS(unittest.TestCase):\n    @staticmethod\n    def _create_inputs(batch_size=8):",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts",
        "peekOfCode": "ap = AudioProcessor(**c.audio)\nWAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\nBATCH_SIZE = 3\ndef count_parameters(model):\n    r\"\"\"Count number of trainable parameters in a network\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nclass TestGlowTTS(unittest.TestCase):\n    @staticmethod\n    def _create_inputs(batch_size=8):\n        input_dummy = torch.randint(0, 24, (batch_size, 128)).long().to(device)",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts",
        "documentation": {}
    },
    {
        "label": "WAV_FILE",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts",
        "peekOfCode": "WAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\nBATCH_SIZE = 3\ndef count_parameters(model):\n    r\"\"\"Count number of trainable parameters in a network\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nclass TestGlowTTS(unittest.TestCase):\n    @staticmethod\n    def _create_inputs(batch_size=8):\n        input_dummy = torch.randint(0, 24, (batch_size, 128)).long().to(device)\n        input_lengths = torch.randint(100, 129, (batch_size,)).long().to(device)",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts",
        "documentation": {}
    },
    {
        "label": "BATCH_SIZE",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts",
        "peekOfCode": "BATCH_SIZE = 3\ndef count_parameters(model):\n    r\"\"\"Count number of trainable parameters in a network\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nclass TestGlowTTS(unittest.TestCase):\n    @staticmethod\n    def _create_inputs(batch_size=8):\n        input_dummy = torch.randint(0, 24, (batch_size, 128)).long().to(device)\n        input_lengths = torch.randint(100, 129, (batch_size,)).long().to(device)\n        input_lengths[-1] = 128",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_model_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = GlowTTSConfig(\n    batch_size=2,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = GlowTTSConfig(\n    batch_size=2,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=\"tests/data/ljspeech/phoneme_cache/\",",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "peekOfCode": "config = GlowTTSConfig(\n    batch_size=2,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=\"tests/data/ljspeech/phoneme_cache/\",\n    run_eval=True,",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "peekOfCode": "config.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"\n    \"--coqpit.datasets.0.meta_file_attn_mask tests/data/ljspeech/metadata_attn_mask.txt \"\n    \"--coqpit.test_delay_epochs 0\"\n)",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# Inference using TTS API\ncontinue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech-1\"\ncontinue_speakers_path = config.d_vector_file\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "continue_config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "peekOfCode": "continue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech-1\"\ncontinue_speakers_path = config.d_vector_file\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "out_wav_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "peekOfCode": "out_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech-1\"\ncontinue_speakers_path = config.d_vector_file\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "speaker_id",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "peekOfCode": "speaker_id = \"ljspeech-1\"\ncontinue_speakers_path = config.d_vector_file\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "continue_speakers_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "peekOfCode": "continue_speakers_path = config.d_vector_file\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "inference_command",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "peekOfCode": "inference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)\n# restore the model and continue training for one more epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_model_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = GlowTTSConfig(\n    batch_size=2,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = GlowTTSConfig(\n    batch_size=2,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=\"tests/data/ljspeech/phoneme_cache/\",",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "peekOfCode": "config = GlowTTSConfig(\n    batch_size=2,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=\"tests/data/ljspeech/phoneme_cache/\",\n    run_eval=True,",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "peekOfCode": "config.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"\n    \"--coqpit.datasets.0.meta_file_attn_mask tests/data/ljspeech/metadata_attn_mask.txt \"\n    \"--coqpit.test_delay_epochs 0\"\n)",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# Inference using TTS API\ncontinue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech-1\"\ncontinue_speakers_path = os.path.join(continue_path, \"speakers.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "continue_config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "peekOfCode": "continue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech-1\"\ncontinue_speakers_path = os.path.join(continue_path, \"speakers.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "out_wav_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "peekOfCode": "out_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech-1\"\ncontinue_speakers_path = os.path.join(continue_path, \"speakers.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "speaker_id",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "peekOfCode": "speaker_id = \"ljspeech-1\"\ncontinue_speakers_path = os.path.join(continue_path, \"speakers.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "continue_speakers_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "peekOfCode": "continue_speakers_path = os.path.join(continue_path, \"speakers.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "inference_command",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "peekOfCode": "inference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)\n# restore the model and continue training for one more epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_model_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = GlowTTSConfig(\n    batch_size=2,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = GlowTTSConfig(\n    batch_size=2,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=\"tests/data/ljspeech/phoneme_cache/\",",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "peekOfCode": "config = GlowTTSConfig(\n    batch_size=2,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=\"tests/data/ljspeech/phoneme_cache/\",\n    run_eval=True,",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "peekOfCode": "config.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"\n    \"--coqpit.datasets.0.meta_file_attn_mask tests/data/ljspeech/metadata_attn_mask.txt \"\n    \"--coqpit.test_delay_epochs 0\"\n)",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# Inference using TTS API\ncontinue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "documentation": {}
    },
    {
        "label": "continue_config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "peekOfCode": "continue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "documentation": {}
    },
    {
        "label": "out_wav_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "peekOfCode": "out_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "documentation": {}
    },
    {
        "label": "inference_command",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "peekOfCode": "inference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)\n# restore the model and continue training for one more epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_glow_tts_train",
        "documentation": {}
    },
    {
        "label": "average_over_durations_test",
        "kind": 2,
        "importPath": "env.src.tts.tests.tts_tests.test_helpers",
        "description": "env.src.tts.tests.tts_tests.test_helpers",
        "peekOfCode": "def average_over_durations_test():  # pylint: disable=no-self-use\n    pitch = T.rand(1, 1, 128)\n    durations = T.randint(1, 5, (1, 21))\n    coeff = 128.0 / durations.sum()\n    durations = T.floor(durations * coeff)\n    diff = 128.0 - durations.sum()\n    durations[0, -1] += diff\n    durations = durations.long()\n    pitch_avg = average_over_durations(pitch, durations)\n    index = 0",
        "detail": "env.src.tts.tests.tts_tests.test_helpers",
        "documentation": {}
    },
    {
        "label": "seqeunce_mask_test",
        "kind": 2,
        "importPath": "env.src.tts.tests.tts_tests.test_helpers",
        "description": "env.src.tts.tests.tts_tests.test_helpers",
        "peekOfCode": "def seqeunce_mask_test():\n    lengths = T.randint(10, 15, (8,))\n    mask = sequence_mask(lengths)\n    for i in range(8):\n        l = lengths[i].item()\n        assert mask[i, :l].sum() == l\n        assert mask[i, l:].sum() == 0\ndef segment_test():\n    x = T.range(0, 11)\n    x = x.repeat(8, 1).unsqueeze(1)",
        "detail": "env.src.tts.tests.tts_tests.test_helpers",
        "documentation": {}
    },
    {
        "label": "segment_test",
        "kind": 2,
        "importPath": "env.src.tts.tests.tts_tests.test_helpers",
        "description": "env.src.tts.tests.tts_tests.test_helpers",
        "peekOfCode": "def segment_test():\n    x = T.range(0, 11)\n    x = x.repeat(8, 1).unsqueeze(1)\n    segment_ids = T.randint(0, 7, (8,))\n    segments = segment(x, segment_ids, segment_size=4)\n    for idx, start_indx in enumerate(segment_ids):\n        assert x[idx, :, start_indx : start_indx + 4].sum() == segments[idx, :, :].sum()\n    try:\n        segments = segment(x, segment_ids, segment_size=10)\n        raise Exception(\"Should have failed\")",
        "detail": "env.src.tts.tests.tts_tests.test_helpers",
        "documentation": {}
    },
    {
        "label": "rand_segments_test",
        "kind": 2,
        "importPath": "env.src.tts.tests.tts_tests.test_helpers",
        "description": "env.src.tts.tests.tts_tests.test_helpers",
        "peekOfCode": "def rand_segments_test():\n    x = T.rand(2, 3, 4)\n    x_lens = T.randint(3, 4, (2,))\n    segments, seg_idxs = rand_segments(x, x_lens, segment_size=3)\n    assert segments.shape == (2, 3, 3)\n    assert all(seg_idxs >= 0), seg_idxs\n    try:\n        segments, _ = rand_segments(x, x_lens, segment_size=5)\n        raise Exception(\"Should have failed\")\n    except:",
        "detail": "env.src.tts.tests.tts_tests.test_helpers",
        "documentation": {}
    },
    {
        "label": "generate_path_test",
        "kind": 2,
        "importPath": "env.src.tts.tests.tts_tests.test_helpers",
        "description": "env.src.tts.tests.tts_tests.test_helpers",
        "peekOfCode": "def generate_path_test():\n    durations = T.randint(1, 4, (10, 21))\n    x_length = T.randint(18, 22, (10,))\n    x_mask = sequence_mask(x_length).unsqueeze(1).long()\n    durations = durations * x_mask.squeeze(1)\n    y_length = durations.sum(1)\n    y_mask = sequence_mask(y_length).unsqueeze(1).long()\n    attn_mask = (T.unsqueeze(x_mask, -1) * T.unsqueeze(y_mask, 2)).squeeze(1).long()\n    print(attn_mask.shape)\n    path = generate_path(durations, attn_mask)",
        "detail": "env.src.tts.tests.tts_tests.test_helpers",
        "documentation": {}
    },
    {
        "label": "L1LossMaskedTests",
        "kind": 6,
        "importPath": "env.src.tts.tests.tts_tests.test_losses",
        "description": "env.src.tts.tests.tts_tests.test_losses",
        "peekOfCode": "class L1LossMaskedTests(unittest.TestCase):\n    def test_in_out(self):  # pylint: disable=no-self-use\n        # test input == target\n        layer = L1LossMasked(seq_len_norm=False)\n        dummy_input = T.ones(4, 8, 128).float()\n        dummy_target = T.ones(4, 8, 128).float()\n        dummy_length = (T.ones(4) * 8).long()\n        output = layer(dummy_input, dummy_target, dummy_length)\n        assert output.item() == 0.0\n        # test input != target",
        "detail": "env.src.tts.tests.tts_tests.test_losses",
        "documentation": {}
    },
    {
        "label": "MSELossMaskedTests",
        "kind": 6,
        "importPath": "env.src.tts.tests.tts_tests.test_losses",
        "description": "env.src.tts.tests.tts_tests.test_losses",
        "peekOfCode": "class MSELossMaskedTests(unittest.TestCase):\n    def test_in_out(self):  # pylint: disable=no-self-use\n        # test input == target\n        layer = MSELossMasked(seq_len_norm=False)\n        dummy_input = T.ones(4, 8, 128).float()\n        dummy_target = T.ones(4, 8, 128).float()\n        dummy_length = (T.ones(4) * 8).long()\n        output = layer(dummy_input, dummy_target, dummy_length)\n        assert output.item() == 0.0\n        # test input != target",
        "detail": "env.src.tts.tests.tts_tests.test_losses",
        "documentation": {}
    },
    {
        "label": "SSIMLossTests",
        "kind": 6,
        "importPath": "env.src.tts.tests.tts_tests.test_losses",
        "description": "env.src.tts.tests.tts_tests.test_losses",
        "peekOfCode": "class SSIMLossTests(unittest.TestCase):\n    def test_in_out(self):  # pylint: disable=no-self-use\n        # test input == target\n        layer = SSIMLoss()\n        dummy_input = T.ones(4, 57, 128).float()\n        dummy_target = T.ones(4, 57, 128).float()\n        dummy_length = (T.ones(4) * 8).long()\n        output = layer(dummy_input, dummy_target, dummy_length)\n        assert output.item() == 0.0\n        # test input != target",
        "detail": "env.src.tts.tests.tts_tests.test_losses",
        "documentation": {}
    },
    {
        "label": "BCELossTest",
        "kind": 6,
        "importPath": "env.src.tts.tests.tts_tests.test_losses",
        "description": "env.src.tts.tests.tts_tests.test_losses",
        "peekOfCode": "class BCELossTest(unittest.TestCase):\n    def test_in_out(self):  # pylint: disable=no-self-use\n        layer = BCELossMasked(pos_weight=5.0)\n        length = T.tensor([95])\n        target = (\n            1.0 - sequence_mask(length - 1, 100).float()\n        )  # [0, 0, .... 1, 1] where the first 1 is the last mel frame\n        true_x = target * 200 - 100  # creates logits of [-100, -100, ... 100, 100] corresponding to target\n        zero_x = T.zeros(target.shape) - 100.0  # simulate logits if it never stops decoding\n        early_x = -200.0 * sequence_mask(length - 3, 100).float() + 100.0  # simulate logits on early stopping",
        "detail": "env.src.tts.tests.tts_tests.test_losses",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_model_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nparameter_path = os.path.join(get_tests_output_path(), \"lj_parameters.pt\")\ntorch.save({\"mean\": -5.5138, \"std\": 2.0636, \"init_transition_prob\": 0.3212}, parameter_path)\nconfig = NeuralhmmTTSConfig(\n    batch_size=3,\n    eval_batch_size=3,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"phoneme_cleaners\",",
        "detail": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nparameter_path = os.path.join(get_tests_output_path(), \"lj_parameters.pt\")\ntorch.save({\"mean\": -5.5138, \"std\": 2.0636, \"init_transition_prob\": 0.3212}, parameter_path)\nconfig = NeuralhmmTTSConfig(\n    batch_size=3,\n    eval_batch_size=3,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=True,",
        "detail": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "documentation": {}
    },
    {
        "label": "parameter_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "peekOfCode": "parameter_path = os.path.join(get_tests_output_path(), \"lj_parameters.pt\")\ntorch.save({\"mean\": -5.5138, \"std\": 2.0636, \"init_transition_prob\": 0.3212}, parameter_path)\nconfig = NeuralhmmTTSConfig(\n    batch_size=3,\n    eval_batch_size=3,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",",
        "detail": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "peekOfCode": "config = NeuralhmmTTSConfig(\n    batch_size=3,\n    eval_batch_size=3,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(get_tests_output_path(), \"train_outputs/phoneme_cache/\"),\n    run_eval=True,",
        "detail": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch when mel parameters exists\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"",
        "detail": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "peekOfCode": "config.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch when mel parameters exists\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"",
        "detail": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"\n    \"--coqpit.test_delay_epochs 0 \"\n)\nrun_cli(command_train)",
        "detail": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"\n    \"--coqpit.test_delay_epochs 0 \"\n)\nrun_cli(command_train)",
        "detail": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# Inference using TTS API\ncontinue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path",
        "detail": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "documentation": {}
    },
    {
        "label": "continue_config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "peekOfCode": "continue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference",
        "detail": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "documentation": {}
    },
    {
        "label": "out_wav_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "peekOfCode": "out_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)",
        "detail": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "documentation": {}
    },
    {
        "label": "inference_command",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "peekOfCode": "inference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)\n# restore the model and continue training for one more epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "description": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_neuralhmm_tts_train",
        "documentation": {}
    },
    {
        "label": "TestOverflow",
        "kind": 6,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow",
        "description": "env.src.tts.tests.tts_tests.test_overflow",
        "peekOfCode": "class TestOverflow(unittest.TestCase):\n    def test_forward(self):\n        model = get_model()\n        input_dummy, input_lengths, mel_spec, mel_lengths = _create_inputs()\n        outputs = model(input_dummy, input_lengths, mel_spec, mel_lengths)\n        self.assertEqual(outputs[\"log_probs\"].shape, (input_dummy.shape[0],))\n        self.assertEqual(model.state_per_phone * max(input_lengths), outputs[\"alignments\"].shape[2])\n    def test_inference(self):\n        model = get_model()\n        input_dummy, input_lengths, mel_spec, mel_lengths = _create_inputs()",
        "detail": "env.src.tts.tests.tts_tests.test_overflow",
        "documentation": {}
    },
    {
        "label": "TestOverflowEncoder",
        "kind": 6,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow",
        "description": "env.src.tts.tests.tts_tests.test_overflow",
        "peekOfCode": "class TestOverflowEncoder(unittest.TestCase):\n    @staticmethod\n    def get_encoder(state_per_phone):\n        config = deepcopy(config_global)\n        config.state_per_phone = state_per_phone\n        config.num_chars = 24\n        return Encoder(config.num_chars, config.state_per_phone, config.prenet_dim, config.encoder_n_convolutions).to(\n            device\n        )\n    def test_forward_with_state_per_phone_multiplication(self):",
        "detail": "env.src.tts.tests.tts_tests.test_overflow",
        "documentation": {}
    },
    {
        "label": "TestOverflowUtils",
        "kind": 6,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow",
        "description": "env.src.tts.tests.tts_tests.test_overflow",
        "peekOfCode": "class TestOverflowUtils(unittest.TestCase):\n    def test_logsumexp(self):\n        a = torch.randn(10)  # random numbers\n        self.assertTrue(torch.eq(torch.logsumexp(a, dim=0), OverflowUtils.logsumexp(a, dim=0)).all())\n        a = torch.zeros(10)  # all zeros\n        self.assertTrue(torch.eq(torch.logsumexp(a, dim=0), OverflowUtils.logsumexp(a, dim=0)).all())\n        a = torch.ones(10)  # all ones\n        self.assertTrue(torch.eq(torch.logsumexp(a, dim=0), OverflowUtils.logsumexp(a, dim=0)).all())\nclass TestOverflowDecoder(unittest.TestCase):\n    @staticmethod",
        "detail": "env.src.tts.tests.tts_tests.test_overflow",
        "documentation": {}
    },
    {
        "label": "TestOverflowDecoder",
        "kind": 6,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow",
        "description": "env.src.tts.tests.tts_tests.test_overflow",
        "peekOfCode": "class TestOverflowDecoder(unittest.TestCase):\n    @staticmethod\n    def _get_decoder(num_flow_blocks_dec=None, hidden_channels_dec=None, reset_weights=True):\n        config = deepcopy(config_global)\n        config.num_flow_blocks_dec = (\n            num_flow_blocks_dec if num_flow_blocks_dec is not None else config.num_flow_blocks_dec\n        )\n        config.hidden_channels_dec = (\n            hidden_channels_dec if hidden_channels_dec is not None else config.hidden_channels_dec\n        )",
        "detail": "env.src.tts.tests.tts_tests.test_overflow",
        "documentation": {}
    },
    {
        "label": "TestNeuralHMM",
        "kind": 6,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow",
        "description": "env.src.tts.tests.tts_tests.test_overflow",
        "peekOfCode": "class TestNeuralHMM(unittest.TestCase):\n    @staticmethod\n    def _get_neural_hmm(deterministic_transition=None):\n        config = deepcopy(config_global)\n        neural_hmm = NeuralHMM(\n            config.out_channels,\n            config.ar_order,\n            config.deterministic_transition if deterministic_transition is None else deterministic_transition,\n            config.encoder_in_out_features,\n            config.prenet_type,",
        "detail": "env.src.tts.tests.tts_tests.test_overflow",
        "documentation": {}
    },
    {
        "label": "TestOverflowOutputNet",
        "kind": 6,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow",
        "description": "env.src.tts.tests.tts_tests.test_overflow",
        "peekOfCode": "class TestOverflowOutputNet(unittest.TestCase):\n    @staticmethod\n    def _get_outputnet():\n        config = deepcopy(config_global)\n        outputnet = Outputnet(\n            config.encoder_in_out_features,\n            config.memory_rnn_dim,\n            config.out_channels,\n            config.outputnet_size,\n            config.flat_start_params,",
        "detail": "env.src.tts.tests.tts_tests.test_overflow",
        "documentation": {}
    },
    {
        "label": "get_model",
        "kind": 2,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow",
        "description": "env.src.tts.tests.tts_tests.test_overflow",
        "peekOfCode": "def get_model(config=None):\n    if config is None:\n        config = config_global\n    config.mel_statistics_parameter_path = parameter_path\n    model = Overflow(config)\n    model = model.to(device)\n    return model\ndef reset_all_weights(model):\n    \"\"\"\n    refs:",
        "detail": "env.src.tts.tests.tts_tests.test_overflow",
        "documentation": {}
    },
    {
        "label": "reset_all_weights",
        "kind": 2,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow",
        "description": "env.src.tts.tests.tts_tests.test_overflow",
        "peekOfCode": "def reset_all_weights(model):\n    \"\"\"\n    refs:\n        - https://discuss.pytorch.org/t/how-to-re-set-alll-parameters-in-a-network/20819/6\n        - https://stackoverflow.com/questions/63627997/reset-parameters-of-a-neural-network-in-pytorch\n        - https://pytorch.org/docs/stable/generated/torch.nn.Module.html\n    \"\"\"\n    @torch.no_grad()\n    def weight_reset(m):\n        # - check if the current module has reset_parameters & if it's callabed called it on m",
        "detail": "env.src.tts.tests.tts_tests.test_overflow",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow",
        "description": "env.src.tts.tests.tts_tests.test_overflow",
        "peekOfCode": "use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nconfig_global = OverflowConfig(num_chars=24)\nap = AudioProcessor.init_from_config(config_global)\nconfig_path = os.path.join(get_tests_output_path(), \"test_model_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nparameter_path = os.path.join(get_tests_output_path(), \"lj_parameters.pt\")\ntorch.save({\"mean\": -5.5138, \"std\": 2.0636, \"init_transition_prob\": 0.3212}, parameter_path)\ndef _create_inputs(batch_size=8):\n    max_len_t, max_len_m = random.randint(25, 50), random.randint(50, 80)",
        "detail": "env.src.tts.tests.tts_tests.test_overflow",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow",
        "description": "env.src.tts.tests.tts_tests.test_overflow",
        "peekOfCode": "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nconfig_global = OverflowConfig(num_chars=24)\nap = AudioProcessor.init_from_config(config_global)\nconfig_path = os.path.join(get_tests_output_path(), \"test_model_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nparameter_path = os.path.join(get_tests_output_path(), \"lj_parameters.pt\")\ntorch.save({\"mean\": -5.5138, \"std\": 2.0636, \"init_transition_prob\": 0.3212}, parameter_path)\ndef _create_inputs(batch_size=8):\n    max_len_t, max_len_m = random.randint(25, 50), random.randint(50, 80)\n    input_dummy = torch.randint(0, 24, (batch_size, max_len_t)).long().to(device)",
        "detail": "env.src.tts.tests.tts_tests.test_overflow",
        "documentation": {}
    },
    {
        "label": "config_global",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow",
        "description": "env.src.tts.tests.tts_tests.test_overflow",
        "peekOfCode": "config_global = OverflowConfig(num_chars=24)\nap = AudioProcessor.init_from_config(config_global)\nconfig_path = os.path.join(get_tests_output_path(), \"test_model_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nparameter_path = os.path.join(get_tests_output_path(), \"lj_parameters.pt\")\ntorch.save({\"mean\": -5.5138, \"std\": 2.0636, \"init_transition_prob\": 0.3212}, parameter_path)\ndef _create_inputs(batch_size=8):\n    max_len_t, max_len_m = random.randint(25, 50), random.randint(50, 80)\n    input_dummy = torch.randint(0, 24, (batch_size, max_len_t)).long().to(device)\n    input_lengths = torch.randint(20, max_len_t, (batch_size,)).long().to(device).sort(descending=True)[0]",
        "detail": "env.src.tts.tests.tts_tests.test_overflow",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow",
        "description": "env.src.tts.tests.tts_tests.test_overflow",
        "peekOfCode": "ap = AudioProcessor.init_from_config(config_global)\nconfig_path = os.path.join(get_tests_output_path(), \"test_model_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nparameter_path = os.path.join(get_tests_output_path(), \"lj_parameters.pt\")\ntorch.save({\"mean\": -5.5138, \"std\": 2.0636, \"init_transition_prob\": 0.3212}, parameter_path)\ndef _create_inputs(batch_size=8):\n    max_len_t, max_len_m = random.randint(25, 50), random.randint(50, 80)\n    input_dummy = torch.randint(0, 24, (batch_size, max_len_t)).long().to(device)\n    input_lengths = torch.randint(20, max_len_t, (batch_size,)).long().to(device).sort(descending=True)[0]\n    input_lengths[0] = max_len_t",
        "detail": "env.src.tts.tests.tts_tests.test_overflow",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow",
        "description": "env.src.tts.tests.tts_tests.test_overflow",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_model_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nparameter_path = os.path.join(get_tests_output_path(), \"lj_parameters.pt\")\ntorch.save({\"mean\": -5.5138, \"std\": 2.0636, \"init_transition_prob\": 0.3212}, parameter_path)\ndef _create_inputs(batch_size=8):\n    max_len_t, max_len_m = random.randint(25, 50), random.randint(50, 80)\n    input_dummy = torch.randint(0, 24, (batch_size, max_len_t)).long().to(device)\n    input_lengths = torch.randint(20, max_len_t, (batch_size,)).long().to(device).sort(descending=True)[0]\n    input_lengths[0] = max_len_t\n    input_dummy = input_dummy * sequence_mask(input_lengths)",
        "detail": "env.src.tts.tests.tts_tests.test_overflow",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow",
        "description": "env.src.tts.tests.tts_tests.test_overflow",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nparameter_path = os.path.join(get_tests_output_path(), \"lj_parameters.pt\")\ntorch.save({\"mean\": -5.5138, \"std\": 2.0636, \"init_transition_prob\": 0.3212}, parameter_path)\ndef _create_inputs(batch_size=8):\n    max_len_t, max_len_m = random.randint(25, 50), random.randint(50, 80)\n    input_dummy = torch.randint(0, 24, (batch_size, max_len_t)).long().to(device)\n    input_lengths = torch.randint(20, max_len_t, (batch_size,)).long().to(device).sort(descending=True)[0]\n    input_lengths[0] = max_len_t\n    input_dummy = input_dummy * sequence_mask(input_lengths)\n    mel_spec = torch.randn(batch_size, max_len_m, config_global.audio[\"num_mels\"]).to(device)",
        "detail": "env.src.tts.tests.tts_tests.test_overflow",
        "documentation": {}
    },
    {
        "label": "parameter_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow",
        "description": "env.src.tts.tests.tts_tests.test_overflow",
        "peekOfCode": "parameter_path = os.path.join(get_tests_output_path(), \"lj_parameters.pt\")\ntorch.save({\"mean\": -5.5138, \"std\": 2.0636, \"init_transition_prob\": 0.3212}, parameter_path)\ndef _create_inputs(batch_size=8):\n    max_len_t, max_len_m = random.randint(25, 50), random.randint(50, 80)\n    input_dummy = torch.randint(0, 24, (batch_size, max_len_t)).long().to(device)\n    input_lengths = torch.randint(20, max_len_t, (batch_size,)).long().to(device).sort(descending=True)[0]\n    input_lengths[0] = max_len_t\n    input_dummy = input_dummy * sequence_mask(input_lengths)\n    mel_spec = torch.randn(batch_size, max_len_m, config_global.audio[\"num_mels\"]).to(device)\n    mel_lengths = torch.randint(40, max_len_m, (batch_size,)).long().to(device).sort(descending=True)[0]",
        "detail": "env.src.tts.tests.tts_tests.test_overflow",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow_train",
        "description": "env.src.tts.tests.tts_tests.test_overflow_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_model_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nparameter_path = os.path.join(get_tests_output_path(), \"lj_parameters.pt\")\ntorch.save({\"mean\": -5.5138, \"std\": 2.0636, \"init_transition_prob\": 0.3212}, parameter_path)\nconfig = OverflowConfig(\n    batch_size=3,\n    eval_batch_size=3,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"phoneme_cleaners\",",
        "detail": "env.src.tts.tests.tts_tests.test_overflow_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow_train",
        "description": "env.src.tts.tests.tts_tests.test_overflow_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nparameter_path = os.path.join(get_tests_output_path(), \"lj_parameters.pt\")\ntorch.save({\"mean\": -5.5138, \"std\": 2.0636, \"init_transition_prob\": 0.3212}, parameter_path)\nconfig = OverflowConfig(\n    batch_size=3,\n    eval_batch_size=3,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=True,",
        "detail": "env.src.tts.tests.tts_tests.test_overflow_train",
        "documentation": {}
    },
    {
        "label": "parameter_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow_train",
        "description": "env.src.tts.tests.tts_tests.test_overflow_train",
        "peekOfCode": "parameter_path = os.path.join(get_tests_output_path(), \"lj_parameters.pt\")\ntorch.save({\"mean\": -5.5138, \"std\": 2.0636, \"init_transition_prob\": 0.3212}, parameter_path)\nconfig = OverflowConfig(\n    batch_size=3,\n    eval_batch_size=3,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",",
        "detail": "env.src.tts.tests.tts_tests.test_overflow_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow_train",
        "description": "env.src.tts.tests.tts_tests.test_overflow_train",
        "peekOfCode": "config = OverflowConfig(\n    batch_size=3,\n    eval_batch_size=3,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"phoneme_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(get_tests_output_path(), \"train_outputs/phoneme_cache/\"),\n    run_eval=True,",
        "detail": "env.src.tts.tests.tts_tests.test_overflow_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow_train",
        "description": "env.src.tts.tests.tts_tests.test_overflow_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch when mel parameters exists\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"",
        "detail": "env.src.tts.tests.tts_tests.test_overflow_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow_train",
        "description": "env.src.tts.tests.tts_tests.test_overflow_train",
        "peekOfCode": "config.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch when mel parameters exists\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"",
        "detail": "env.src.tts.tests.tts_tests.test_overflow_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow_train",
        "description": "env.src.tts.tests.tts_tests.test_overflow_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"\n    \"--coqpit.test_delay_epochs 0 \"\n)\nrun_cli(command_train)",
        "detail": "env.src.tts.tests.tts_tests.test_overflow_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow_train",
        "description": "env.src.tts.tests.tts_tests.test_overflow_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"\n    \"--coqpit.test_delay_epochs 0 \"\n)\nrun_cli(command_train)",
        "detail": "env.src.tts.tests.tts_tests.test_overflow_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow_train",
        "description": "env.src.tts.tests.tts_tests.test_overflow_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# Inference using TTS API\ncontinue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path",
        "detail": "env.src.tts.tests.tts_tests.test_overflow_train",
        "documentation": {}
    },
    {
        "label": "continue_config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow_train",
        "description": "env.src.tts.tests.tts_tests.test_overflow_train",
        "peekOfCode": "continue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference",
        "detail": "env.src.tts.tests.tts_tests.test_overflow_train",
        "documentation": {}
    },
    {
        "label": "out_wav_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow_train",
        "description": "env.src.tts.tests.tts_tests.test_overflow_train",
        "peekOfCode": "out_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)",
        "detail": "env.src.tts.tests.tts_tests.test_overflow_train",
        "documentation": {}
    },
    {
        "label": "inference_command",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow_train",
        "description": "env.src.tts.tests.tts_tests.test_overflow_train",
        "peekOfCode": "inference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)\n# restore the model and continue training for one more epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_overflow_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_overflow_train",
        "description": "env.src.tts.tests.tts_tests.test_overflow_train",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_overflow_train",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "description": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_speedy_speech_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = SpeedySpeechConfig(\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",",
        "detail": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "description": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = SpeedySpeechConfig(\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=\"tests/data/ljspeech/phoneme_cache/\",",
        "detail": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "description": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "peekOfCode": "config = SpeedySpeechConfig(\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=\"tests/data/ljspeech/phoneme_cache/\",\n    run_eval=True,",
        "detail": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "description": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}'  python TTS/bin/train_tts.py --config_path {config_path}  \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"",
        "detail": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "description": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "peekOfCode": "config.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}'  python TTS/bin/train_tts.py --config_path {config_path}  \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"",
        "detail": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "description": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}'  python TTS/bin/train_tts.py --config_path {config_path}  \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"\n    \"--coqpit.datasets.0.meta_file_attn_mask tests/data/ljspeech/metadata_attn_mask.txt \"\n    \"--coqpit.test_delay_epochs 0\"\n)",
        "detail": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "description": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# Inference using TTS API\ncontinue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path",
        "detail": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "documentation": {}
    },
    {
        "label": "continue_config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "description": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "peekOfCode": "continue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference",
        "detail": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "documentation": {}
    },
    {
        "label": "out_wav_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "description": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "peekOfCode": "out_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example for it.' --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)",
        "detail": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "documentation": {}
    },
    {
        "label": "inference_command",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "description": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "peekOfCode": "inference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example for it.' --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)\n# restore the model and continue training for one more epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "description": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_speedy_speech_train",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_model_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = Tacotron2Config(\n    r=5,\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=False,",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = Tacotron2Config(\n    r=5,\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=False,\n    phoneme_language=\"en-us\",",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "peekOfCode": "config = Tacotron2Config(\n    r=5,\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=False,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(get_tests_output_path(), \"train_outputs/phoneme_cache/\"),",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "peekOfCode": "config.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"\n    \"--coqpit.test_delay_epochs 0 \"\n)\nrun_cli(command_train)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# Inference using TTS API\ncontinue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech-1\"\ncontinue_speakers_path = config.d_vector_file\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "continue_config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "peekOfCode": "continue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech-1\"\ncontinue_speakers_path = config.d_vector_file\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "out_wav_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "peekOfCode": "out_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech-1\"\ncontinue_speakers_path = config.d_vector_file\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "speaker_id",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "peekOfCode": "speaker_id = \"ljspeech-1\"\ncontinue_speakers_path = config.d_vector_file\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "continue_speakers_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "peekOfCode": "continue_speakers_path = config.d_vector_file\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "inference_command",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "peekOfCode": "inference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)\n# restore the model and continue training for one more epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "TacotronTrainTest",
        "kind": 6,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "peekOfCode": "class TacotronTrainTest(unittest.TestCase):\n    \"\"\"Test vanilla Tacotron2 model.\"\"\"\n    def test_train_step(self):  # pylint: disable=no-self-use\n        config = config_global.copy()\n        config.use_speaker_embedding = False\n        config.num_speakers = 1\n        input_dummy = torch.randint(0, 24, (8, 128)).long().to(device)\n        input_lengths = torch.randint(100, 128, (8,)).long().to(device)\n        input_lengths = torch.sort(input_lengths, descending=True)[0]\n        mel_spec = torch.rand(8, 30, config.audio[\"num_mels\"]).to(device)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "documentation": {}
    },
    {
        "label": "MultiSpeakerTacotronTrainTest",
        "kind": 6,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "peekOfCode": "class MultiSpeakerTacotronTrainTest(unittest.TestCase):\n    \"\"\"Test multi-speaker Tacotron2 with speaker embedding layer\"\"\"\n    @staticmethod\n    def test_train_step():\n        config = config_global.copy()\n        config.use_speaker_embedding = True\n        config.num_speakers = 5\n        input_dummy = torch.randint(0, 24, (8, 128)).long().to(device)\n        input_lengths = torch.randint(100, 128, (8,)).long().to(device)\n        input_lengths = torch.sort(input_lengths, descending=True)[0]",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "documentation": {}
    },
    {
        "label": "TacotronGSTTrainTest",
        "kind": 6,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "peekOfCode": "class TacotronGSTTrainTest(unittest.TestCase):\n    \"\"\"Test multi-speaker Tacotron2 with Global Style Token and Speaker Embedding\"\"\"\n    # pylint: disable=no-self-use\n    def test_train_step(self):\n        # with random gst mel style\n        config = config_global.copy()\n        config.use_speaker_embedding = True\n        config.num_speakers = 10\n        config.use_gst = True\n        config.gst = GSTConfig()",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "documentation": {}
    },
    {
        "label": "TacotronCapacitronTrainTest",
        "kind": 6,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "peekOfCode": "class TacotronCapacitronTrainTest(unittest.TestCase):\n    @staticmethod\n    def test_train_step():\n        config = Tacotron2Config(\n            num_chars=32,\n            num_speakers=10,\n            use_speaker_embedding=True,\n            out_channels=80,\n            decoder_output_dim=80,\n            use_capacitron_vae=True,",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "documentation": {}
    },
    {
        "label": "SCGSTMultiSpeakeTacotronTrainTest",
        "kind": 6,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "peekOfCode": "class SCGSTMultiSpeakeTacotronTrainTest(unittest.TestCase):\n    \"\"\"Test multi-speaker Tacotron2 with Global Style Tokens and d-vector inputs.\"\"\"\n    @staticmethod\n    def test_train_step():\n        config = config_global.copy()\n        config.use_d_vector_file = True\n        config.use_gst = True\n        config.gst = GSTConfig()\n        input_dummy = torch.randint(0, 24, (8, 128)).long().to(device)\n        input_lengths = torch.randint(100, 128, (8,)).long().to(device)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "peekOfCode": "use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nconfig_global = Tacotron2Config(num_chars=32, num_speakers=5, out_channels=80, decoder_output_dim=80)\nap = AudioProcessor(**config_global.audio)\nWAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\nclass TacotronTrainTest(unittest.TestCase):\n    \"\"\"Test vanilla Tacotron2 model.\"\"\"\n    def test_train_step(self):  # pylint: disable=no-self-use\n        config = config_global.copy()\n        config.use_speaker_embedding = False",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "peekOfCode": "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nconfig_global = Tacotron2Config(num_chars=32, num_speakers=5, out_channels=80, decoder_output_dim=80)\nap = AudioProcessor(**config_global.audio)\nWAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\nclass TacotronTrainTest(unittest.TestCase):\n    \"\"\"Test vanilla Tacotron2 model.\"\"\"\n    def test_train_step(self):  # pylint: disable=no-self-use\n        config = config_global.copy()\n        config.use_speaker_embedding = False\n        config.num_speakers = 1",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "documentation": {}
    },
    {
        "label": "config_global",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "peekOfCode": "config_global = Tacotron2Config(num_chars=32, num_speakers=5, out_channels=80, decoder_output_dim=80)\nap = AudioProcessor(**config_global.audio)\nWAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\nclass TacotronTrainTest(unittest.TestCase):\n    \"\"\"Test vanilla Tacotron2 model.\"\"\"\n    def test_train_step(self):  # pylint: disable=no-self-use\n        config = config_global.copy()\n        config.use_speaker_embedding = False\n        config.num_speakers = 1\n        input_dummy = torch.randint(0, 24, (8, 128)).long().to(device)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "peekOfCode": "ap = AudioProcessor(**config_global.audio)\nWAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\nclass TacotronTrainTest(unittest.TestCase):\n    \"\"\"Test vanilla Tacotron2 model.\"\"\"\n    def test_train_step(self):  # pylint: disable=no-self-use\n        config = config_global.copy()\n        config.use_speaker_embedding = False\n        config.num_speakers = 1\n        input_dummy = torch.randint(0, 24, (8, 128)).long().to(device)\n        input_lengths = torch.randint(100, 128, (8,)).long().to(device)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "documentation": {}
    },
    {
        "label": "WAV_FILE",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "peekOfCode": "WAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\nclass TacotronTrainTest(unittest.TestCase):\n    \"\"\"Test vanilla Tacotron2 model.\"\"\"\n    def test_train_step(self):  # pylint: disable=no-self-use\n        config = config_global.copy()\n        config.use_speaker_embedding = False\n        config.num_speakers = 1\n        input_dummy = torch.randint(0, 24, (8, 128)).long().to(device)\n        input_lengths = torch.randint(100, 128, (8,)).long().to(device)\n        input_lengths = torch.sort(input_lengths, descending=True)[0]",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_model",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_model_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = Tacotron2Config(\n    r=5,\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=False,",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = Tacotron2Config(\n    r=5,\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=False,\n    phoneme_language=\"en-us\",",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "peekOfCode": "config = Tacotron2Config(\n    r=5,\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=False,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(get_tests_output_path(), \"train_outputs/phoneme_cache/\"),",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "peekOfCode": "config.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"\n    \"--coqpit.test_delay_epochs 0 \"\n)\nrun_cli(command_train)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# Inference using TTS API\ncontinue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech-1\"\ncontinue_speakers_path = os.path.join(continue_path, \"speakers.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "continue_config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "peekOfCode": "continue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech-1\"\ncontinue_speakers_path = os.path.join(continue_path, \"speakers.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "out_wav_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "peekOfCode": "out_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech-1\"\ncontinue_speakers_path = os.path.join(continue_path, \"speakers.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "speaker_id",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "peekOfCode": "speaker_id = \"ljspeech-1\"\ncontinue_speakers_path = os.path.join(continue_path, \"speakers.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "continue_speakers_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "peekOfCode": "continue_speakers_path = os.path.join(continue_path, \"speakers.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "inference_command",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "peekOfCode": "inference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)\n# restore the model and continue training for one more epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_model_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = Tacotron2Config(\n    r=5,\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=False,",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = Tacotron2Config(\n    r=5,\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=False,\n    phoneme_language=\"en-us\",",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "peekOfCode": "config = Tacotron2Config(\n    r=5,\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=False,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(get_tests_output_path(), \"train_outputs/phoneme_cache/\"),",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "peekOfCode": "config.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"\n    \"--coqpit.test_delay_epochs 0 \"\n)\nrun_cli(command_train)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# Inference using TTS API\ncontinue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "documentation": {}
    },
    {
        "label": "continue_config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "peekOfCode": "continue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "documentation": {}
    },
    {
        "label": "out_wav_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "peekOfCode": "out_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "documentation": {}
    },
    {
        "label": "inference_command",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "peekOfCode": "inference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)\n# restore the model and continue training for one more epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron2_train",
        "documentation": {}
    },
    {
        "label": "PrenetTests",
        "kind": 6,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron_layers",
        "description": "env.src.tts.tests.tts_tests.test_tacotron_layers",
        "peekOfCode": "class PrenetTests(unittest.TestCase):\n    def test_in_out(self):  # pylint: disable=no-self-use\n        layer = Prenet(128, out_features=[256, 128])\n        dummy_input = T.rand(4, 128)\n        print(layer)\n        output = layer(dummy_input)\n        assert output.shape[0] == 4\n        assert output.shape[1] == 128\nclass CBHGTests(unittest.TestCase):\n    def test_in_out(self):",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron_layers",
        "documentation": {}
    },
    {
        "label": "CBHGTests",
        "kind": 6,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron_layers",
        "description": "env.src.tts.tests.tts_tests.test_tacotron_layers",
        "peekOfCode": "class CBHGTests(unittest.TestCase):\n    def test_in_out(self):\n        # pylint: disable=attribute-defined-outside-init\n        layer = self.cbhg = CBHG(\n            128,\n            K=8,\n            conv_bank_features=80,\n            conv_projections=[160, 128],\n            highway_features=80,\n            gru_features=80,",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron_layers",
        "documentation": {}
    },
    {
        "label": "DecoderTests",
        "kind": 6,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron_layers",
        "description": "env.src.tts.tests.tts_tests.test_tacotron_layers",
        "peekOfCode": "class DecoderTests(unittest.TestCase):\n    @staticmethod\n    def test_in_out():\n        layer = Decoder(\n            in_channels=256,\n            frame_channels=80,\n            r=2,\n            memory_size=4,\n            attn_windowing=False,\n            attn_norm=\"sigmoid\",",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron_layers",
        "documentation": {}
    },
    {
        "label": "EncoderTests",
        "kind": 6,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron_layers",
        "description": "env.src.tts.tests.tts_tests.test_tacotron_layers",
        "peekOfCode": "class EncoderTests(unittest.TestCase):\n    def test_in_out(self):  # pylint: disable=no-self-use\n        layer = Encoder(128)\n        dummy_input = T.rand(4, 8, 128)\n        print(layer)\n        output = layer(dummy_input)\n        print(output.shape)\n        assert output.shape[0] == 4\n        assert output.shape[1] == 8\n        assert output.shape[2] == 256  # 128 * 2 BiRNN",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron_layers",
        "documentation": {}
    },
    {
        "label": "TacotronTrainTest",
        "kind": 6,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "description": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "peekOfCode": "class TacotronTrainTest(unittest.TestCase):\n    @staticmethod\n    def test_train_step():\n        config = config_global.copy()\n        config.use_speaker_embedding = False\n        config.num_speakers = 1\n        input_dummy = torch.randint(0, 24, (8, 128)).long().to(device)\n        input_lengths = torch.randint(100, 129, (8,)).long().to(device)\n        input_lengths[-1] = 128\n        mel_spec = torch.rand(8, 30, config.audio[\"num_mels\"]).to(device)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "documentation": {}
    },
    {
        "label": "MultiSpeakeTacotronTrainTest",
        "kind": 6,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "description": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "peekOfCode": "class MultiSpeakeTacotronTrainTest(unittest.TestCase):\n    @staticmethod\n    def test_train_step():\n        config = config_global.copy()\n        config.use_speaker_embedding = True\n        config.num_speakers = 5\n        input_dummy = torch.randint(0, 24, (8, 128)).long().to(device)\n        input_lengths = torch.randint(100, 129, (8,)).long().to(device)\n        input_lengths[-1] = 128\n        mel_spec = torch.rand(8, 30, config.audio[\"num_mels\"]).to(device)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "documentation": {}
    },
    {
        "label": "TacotronGSTTrainTest",
        "kind": 6,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "description": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "peekOfCode": "class TacotronGSTTrainTest(unittest.TestCase):\n    @staticmethod\n    def test_train_step():\n        config = config_global.copy()\n        config.use_speaker_embedding = True\n        config.num_speakers = 10\n        config.use_gst = True\n        config.gst = GSTConfig()\n        # with random gst mel style\n        input_dummy = torch.randint(0, 24, (8, 128)).long().to(device)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "documentation": {}
    },
    {
        "label": "TacotronCapacitronTrainTest",
        "kind": 6,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "description": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "peekOfCode": "class TacotronCapacitronTrainTest(unittest.TestCase):\n    @staticmethod\n    def test_train_step():\n        config = TacotronConfig(\n            num_chars=32,\n            num_speakers=10,\n            use_speaker_embedding=True,\n            out_channels=513,\n            decoder_output_dim=80,\n            use_capacitron_vae=True,",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "documentation": {}
    },
    {
        "label": "SCGSTMultiSpeakeTacotronTrainTest",
        "kind": 6,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "description": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "peekOfCode": "class SCGSTMultiSpeakeTacotronTrainTest(unittest.TestCase):\n    @staticmethod\n    def test_train_step():\n        config = config_global.copy()\n        config.use_d_vector_file = True\n        config.use_gst = True\n        config.gst = GSTConfig()\n        input_dummy = torch.randint(0, 24, (8, 128)).long().to(device)\n        input_lengths = torch.randint(100, 129, (8,)).long().to(device)\n        input_lengths[-1] = 128",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "documentation": {}
    },
    {
        "label": "count_parameters",
        "kind": 2,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "description": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "peekOfCode": "def count_parameters(model):\n    r\"\"\"Count number of trainable parameters in a network\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nclass TacotronTrainTest(unittest.TestCase):\n    @staticmethod\n    def test_train_step():\n        config = config_global.copy()\n        config.use_speaker_embedding = False\n        config.num_speakers = 1\n        input_dummy = torch.randint(0, 24, (8, 128)).long().to(device)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "description": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "peekOfCode": "use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nconfig_global = TacotronConfig(num_chars=32, num_speakers=5, out_channels=513, decoder_output_dim=80)\nap = AudioProcessor(**config_global.audio)\nWAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\ndef count_parameters(model):\n    r\"\"\"Count number of trainable parameters in a network\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nclass TacotronTrainTest(unittest.TestCase):\n    @staticmethod",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "description": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "peekOfCode": "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nconfig_global = TacotronConfig(num_chars=32, num_speakers=5, out_channels=513, decoder_output_dim=80)\nap = AudioProcessor(**config_global.audio)\nWAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\ndef count_parameters(model):\n    r\"\"\"Count number of trainable parameters in a network\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nclass TacotronTrainTest(unittest.TestCase):\n    @staticmethod\n    def test_train_step():",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "documentation": {}
    },
    {
        "label": "config_global",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "description": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "peekOfCode": "config_global = TacotronConfig(num_chars=32, num_speakers=5, out_channels=513, decoder_output_dim=80)\nap = AudioProcessor(**config_global.audio)\nWAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\ndef count_parameters(model):\n    r\"\"\"Count number of trainable parameters in a network\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nclass TacotronTrainTest(unittest.TestCase):\n    @staticmethod\n    def test_train_step():\n        config = config_global.copy()",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "description": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "peekOfCode": "ap = AudioProcessor(**config_global.audio)\nWAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\ndef count_parameters(model):\n    r\"\"\"Count number of trainable parameters in a network\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nclass TacotronTrainTest(unittest.TestCase):\n    @staticmethod\n    def test_train_step():\n        config = config_global.copy()\n        config.use_speaker_embedding = False",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "documentation": {}
    },
    {
        "label": "WAV_FILE",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "description": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "peekOfCode": "WAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\ndef count_parameters(model):\n    r\"\"\"Count number of trainable parameters in a network\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nclass TacotronTrainTest(unittest.TestCase):\n    @staticmethod\n    def test_train_step():\n        config = config_global.copy()\n        config.use_speaker_embedding = False\n        config.num_speakers = 1",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron_model",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_model_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = TacotronConfig(\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=False,\n    phoneme_language=\"en-us\",",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = TacotronConfig(\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=False,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(get_tests_output_path(), \"train_outputs/phoneme_cache/\"),",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "peekOfCode": "config = TacotronConfig(\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=False,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=os.path.join(get_tests_output_path(), \"train_outputs/phoneme_cache/\"),\n    run_eval=True,",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "peekOfCode": "config.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"\n    \"--coqpit.test_delay_epochs 0\"\n)\nrun_cli(command_train)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# Inference using TTS API\ncontinue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)\n# restore the model and continue training for one more epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "documentation": {}
    },
    {
        "label": "continue_config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "peekOfCode": "continue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)\n# restore the model and continue training for one more epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "documentation": {}
    },
    {
        "label": "out_wav_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "peekOfCode": "out_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)\n# restore the model and continue training for one more epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "documentation": {}
    },
    {
        "label": "inference_command",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "peekOfCode": "inference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)\n# restore the model and continue training for one more epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "description": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_tacotron_train",
        "documentation": {}
    },
    {
        "label": "TestVits",
        "kind": 6,
        "importPath": "env.src.tts.tests.tts_tests.test_vits",
        "description": "env.src.tts.tests.tts_tests.test_vits",
        "peekOfCode": "class TestVits(unittest.TestCase):\n    def test_load_audio(self):\n        wav, sr = load_audio(WAV_FILE)\n        self.assertEqual(wav.shape, (1, 41885))\n        self.assertEqual(sr, 22050)\n        spec = wav_to_spec(wav, n_fft=1024, hop_length=512, win_length=1024, center=False)\n        mel = wav_to_mel(\n            wav,\n            n_fft=1024,\n            num_mels=80,",
        "detail": "env.src.tts.tests.tts_tests.test_vits",
        "documentation": {}
    },
    {
        "label": "LANG_FILE",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits",
        "description": "env.src.tts.tests.tts_tests.test_vits",
        "peekOfCode": "LANG_FILE = os.path.join(get_tests_input_path(), \"language_ids.json\")\nSPEAKER_ENCODER_CONFIG = os.path.join(get_tests_input_path(), \"test_speaker_encoder_config.json\")\nWAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\ntorch.manual_seed(1)\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# pylint: disable=no-self-use\nclass TestVits(unittest.TestCase):\n    def test_load_audio(self):\n        wav, sr = load_audio(WAV_FILE)",
        "detail": "env.src.tts.tests.tts_tests.test_vits",
        "documentation": {}
    },
    {
        "label": "SPEAKER_ENCODER_CONFIG",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits",
        "description": "env.src.tts.tests.tts_tests.test_vits",
        "peekOfCode": "SPEAKER_ENCODER_CONFIG = os.path.join(get_tests_input_path(), \"test_speaker_encoder_config.json\")\nWAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\ntorch.manual_seed(1)\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# pylint: disable=no-self-use\nclass TestVits(unittest.TestCase):\n    def test_load_audio(self):\n        wav, sr = load_audio(WAV_FILE)\n        self.assertEqual(wav.shape, (1, 41885))",
        "detail": "env.src.tts.tests.tts_tests.test_vits",
        "documentation": {}
    },
    {
        "label": "WAV_FILE",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits",
        "description": "env.src.tts.tests.tts_tests.test_vits",
        "peekOfCode": "WAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\ntorch.manual_seed(1)\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# pylint: disable=no-self-use\nclass TestVits(unittest.TestCase):\n    def test_load_audio(self):\n        wav, sr = load_audio(WAV_FILE)\n        self.assertEqual(wav.shape, (1, 41885))\n        self.assertEqual(sr, 22050)",
        "detail": "env.src.tts.tests.tts_tests.test_vits",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits",
        "description": "env.src.tts.tests.tts_tests.test_vits",
        "peekOfCode": "use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# pylint: disable=no-self-use\nclass TestVits(unittest.TestCase):\n    def test_load_audio(self):\n        wav, sr = load_audio(WAV_FILE)\n        self.assertEqual(wav.shape, (1, 41885))\n        self.assertEqual(sr, 22050)\n        spec = wav_to_spec(wav, n_fft=1024, hop_length=512, win_length=1024, center=False)\n        mel = wav_to_mel(",
        "detail": "env.src.tts.tests.tts_tests.test_vits",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits",
        "description": "env.src.tts.tests.tts_tests.test_vits",
        "peekOfCode": "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# pylint: disable=no-self-use\nclass TestVits(unittest.TestCase):\n    def test_load_audio(self):\n        wav, sr = load_audio(WAV_FILE)\n        self.assertEqual(wav.shape, (1, 41885))\n        self.assertEqual(sr, 22050)\n        spec = wav_to_spec(wav, n_fft=1024, hop_length=512, win_length=1024, center=False)\n        mel = wav_to_mel(\n            wav,",
        "detail": "env.src.tts.tests.tts_tests.test_vits",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_model_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = VitsConfig(\n    batch_size=2,\n    eval_batch_size=2,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",",
        "detail": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = VitsConfig(\n    batch_size=2,\n    eval_batch_size=2,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=\"tests/data/ljspeech/phoneme_cache/\",",
        "detail": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "peekOfCode": "config = VitsConfig(\n    batch_size=2,\n    eval_batch_size=2,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=\"tests/data/ljspeech/phoneme_cache/\",\n    run_eval=True,",
        "detail": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.audio.trim_db = 60\n# active multispeaker d-vec mode\nconfig.model_args.use_d_vector_file = True\nconfig.model_args.d_vector_file = [\"tests/data/ljspeech/speakers.json\"]\nconfig.model_args.d_vector_dim = 256\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"",
        "detail": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "peekOfCode": "config.audio.trim_db = 60\n# active multispeaker d-vec mode\nconfig.model_args.use_d_vector_file = True\nconfig.model_args.d_vector_file = [\"tests/data/ljspeech/speakers.json\"]\nconfig.model_args.d_vector_dim = 256\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"",
        "detail": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "config.model_args.use_d_vector_file",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "peekOfCode": "config.model_args.use_d_vector_file = True\nconfig.model_args.d_vector_file = [\"tests/data/ljspeech/speakers.json\"]\nconfig.model_args.d_vector_dim = 256\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"",
        "detail": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "config.model_args.d_vector_file",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "peekOfCode": "config.model_args.d_vector_file = [\"tests/data/ljspeech/speakers.json\"]\nconfig.model_args.d_vector_dim = 256\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"",
        "detail": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "config.model_args.d_vector_dim",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "peekOfCode": "config.model_args.d_vector_dim = 256\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"",
        "detail": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"\n    \"--coqpit.datasets.0.meta_file_attn_mask tests/data/ljspeech/metadata_attn_mask.txt \"\n    \"--coqpit.test_delay_epochs 0\"\n)",
        "detail": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_vits_d-vectors_train",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_model_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\ndataset_config_en = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",\n    meta_file_val=\"metadata.csv\",\n    path=\"tests/data/ljspeech\",\n    language=\"en\",\n)\ndataset_config_pt = BaseDatasetConfig(",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\ndataset_config_en = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",\n    meta_file_val=\"metadata.csv\",\n    path=\"tests/data/ljspeech\",\n    language=\"en\",\n)\ndataset_config_pt = BaseDatasetConfig(\n    formatter=\"ljspeech\",",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "dataset_config_en",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "peekOfCode": "dataset_config_en = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",\n    meta_file_val=\"metadata.csv\",\n    path=\"tests/data/ljspeech\",\n    language=\"en\",\n)\ndataset_config_pt = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "dataset_config_pt",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "peekOfCode": "dataset_config_pt = BaseDatasetConfig(\n    formatter=\"ljspeech\",\n    meta_file_train=\"metadata.csv\",\n    meta_file_val=\"metadata.csv\",\n    path=\"tests/data/ljspeech\",\n    language=\"pt-br\",\n)\nconfig = VitsConfig(\n    batch_size=2,\n    eval_batch_size=2,",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "peekOfCode": "config = VitsConfig(\n    batch_size=2,\n    eval_batch_size=2,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=\"tests/data/ljspeech/phoneme_cache/\",\n    run_eval=True,",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.audio.trim_db = 60\n# active multilingual mode\nconfig.model_args.use_language_embedding = True\nconfig.use_language_embedding = True\n# active multispeaker mode\nconfig.model_args.use_speaker_embedding = True\nconfig.use_speaker_embedding = True\n# deactivate multispeaker d-vec mode\nconfig.model_args.use_d_vector_file = False",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "peekOfCode": "config.audio.trim_db = 60\n# active multilingual mode\nconfig.model_args.use_language_embedding = True\nconfig.use_language_embedding = True\n# active multispeaker mode\nconfig.model_args.use_speaker_embedding = True\nconfig.use_speaker_embedding = True\n# deactivate multispeaker d-vec mode\nconfig.model_args.use_d_vector_file = False\nconfig.use_d_vector_file = False",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.model_args.use_language_embedding",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "peekOfCode": "config.model_args.use_language_embedding = True\nconfig.use_language_embedding = True\n# active multispeaker mode\nconfig.model_args.use_speaker_embedding = True\nconfig.use_speaker_embedding = True\n# deactivate multispeaker d-vec mode\nconfig.model_args.use_d_vector_file = False\nconfig.use_d_vector_file = False\n# duration predictor\nconfig.model_args.use_sdp = False",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.use_language_embedding",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "peekOfCode": "config.use_language_embedding = True\n# active multispeaker mode\nconfig.model_args.use_speaker_embedding = True\nconfig.use_speaker_embedding = True\n# deactivate multispeaker d-vec mode\nconfig.model_args.use_d_vector_file = False\nconfig.use_d_vector_file = False\n# duration predictor\nconfig.model_args.use_sdp = False\nconfig.use_sdp = False",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.model_args.use_speaker_embedding",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "peekOfCode": "config.model_args.use_speaker_embedding = True\nconfig.use_speaker_embedding = True\n# deactivate multispeaker d-vec mode\nconfig.model_args.use_d_vector_file = False\nconfig.use_d_vector_file = False\n# duration predictor\nconfig.model_args.use_sdp = False\nconfig.use_sdp = False\n# active language sampler\nconfig.use_language_weighted_sampler = True",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.use_speaker_embedding",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "peekOfCode": "config.use_speaker_embedding = True\n# deactivate multispeaker d-vec mode\nconfig.model_args.use_d_vector_file = False\nconfig.use_d_vector_file = False\n# duration predictor\nconfig.model_args.use_sdp = False\nconfig.use_sdp = False\n# active language sampler\nconfig.use_language_weighted_sampler = True\nconfig.save_json(config_path)",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.model_args.use_d_vector_file",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "peekOfCode": "config.model_args.use_d_vector_file = False\nconfig.use_d_vector_file = False\n# duration predictor\nconfig.model_args.use_sdp = False\nconfig.use_sdp = False\n# active language sampler\nconfig.use_language_weighted_sampler = True\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.use_d_vector_file",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "peekOfCode": "config.use_d_vector_file = False\n# duration predictor\nconfig.model_args.use_sdp = False\nconfig.use_sdp = False\n# active language sampler\nconfig.use_language_weighted_sampler = True\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.model_args.use_sdp",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "peekOfCode": "config.model_args.use_sdp = False\nconfig.use_sdp = False\n# active language sampler\nconfig.use_language_weighted_sampler = True\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.test_delay_epochs 0\"",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.use_sdp",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "peekOfCode": "config.use_sdp = False\n# active language sampler\nconfig.use_language_weighted_sampler = True\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.test_delay_epochs 0\"\n)",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.use_language_weighted_sampler",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "peekOfCode": "config.use_language_weighted_sampler = True\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.test_delay_epochs 0\"\n)\nrun_cli(command_train)\n# Find latest folder",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.test_delay_epochs 0\"\n)\nrun_cli(command_train)\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# Inference using TTS API\ncontinue_config_path = os.path.join(continue_path, \"config.json\")",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# Inference using TTS API\ncontinue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech\"\nlanguae_id = \"en\"\ncontinue_speakers_path = os.path.join(continue_path, \"speakers.json\")\ncontinue_languages_path = os.path.join(continue_path, \"language_ids.json\")\n# Check integrity of the config",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "continue_config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "peekOfCode": "continue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech\"\nlanguae_id = \"en\"\ncontinue_speakers_path = os.path.join(continue_path, \"speakers.json\")\ncontinue_languages_path = os.path.join(continue_path, \"language_ids.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "out_wav_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "peekOfCode": "out_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech\"\nlanguae_id = \"en\"\ncontinue_speakers_path = os.path.join(continue_path, \"speakers.json\")\ncontinue_languages_path = os.path.join(continue_path, \"language_ids.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "speaker_id",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "peekOfCode": "speaker_id = \"ljspeech\"\nlanguae_id = \"en\"\ncontinue_speakers_path = os.path.join(continue_path, \"speakers.json\")\ncontinue_languages_path = os.path.join(continue_path, \"language_ids.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "languae_id",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "peekOfCode": "languae_id = \"en\"\ncontinue_speakers_path = os.path.join(continue_path, \"speakers.json\")\ncontinue_languages_path = os.path.join(continue_path, \"language_ids.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "continue_speakers_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "peekOfCode": "continue_speakers_path = os.path.join(continue_path, \"speakers.json\")\ncontinue_languages_path = os.path.join(continue_path, \"language_ids.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --language_ids_file_path {continue_languages_path} --language_idx {languae_id} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "continue_languages_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "peekOfCode": "continue_languages_path = os.path.join(continue_path, \"language_ids.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --language_ids_file_path {continue_languages_path} --language_idx {languae_id} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "inference_command",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "peekOfCode": "inference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --language_ids_file_path {continue_languages_path} --language_idx {languae_id} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)\n# restore the model and continue training for one more epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_model_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\ndataset_config_en = BaseDatasetConfig(\n    formatter=\"ljspeech_test\",\n    meta_file_train=\"metadata.csv\",\n    meta_file_val=\"metadata.csv\",\n    path=\"tests/data/ljspeech\",\n    language=\"en\",\n)\ndataset_config_pt = BaseDatasetConfig(",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\ndataset_config_en = BaseDatasetConfig(\n    formatter=\"ljspeech_test\",\n    meta_file_train=\"metadata.csv\",\n    meta_file_val=\"metadata.csv\",\n    path=\"tests/data/ljspeech\",\n    language=\"en\",\n)\ndataset_config_pt = BaseDatasetConfig(\n    formatter=\"ljspeech_test\",",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "dataset_config_en",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "dataset_config_en = BaseDatasetConfig(\n    formatter=\"ljspeech_test\",\n    meta_file_train=\"metadata.csv\",\n    meta_file_val=\"metadata.csv\",\n    path=\"tests/data/ljspeech\",\n    language=\"en\",\n)\ndataset_config_pt = BaseDatasetConfig(\n    formatter=\"ljspeech_test\",\n    meta_file_train=\"metadata.csv\",",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "dataset_config_pt",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "dataset_config_pt = BaseDatasetConfig(\n    formatter=\"ljspeech_test\",\n    meta_file_train=\"metadata.csv\",\n    meta_file_val=\"metadata.csv\",\n    path=\"tests/data/ljspeech\",\n    language=\"pt-br\",\n)\nconfig = VitsConfig(\n    batch_size=2,\n    eval_batch_size=2,",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "config = VitsConfig(\n    batch_size=2,\n    eval_batch_size=2,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"multilingual_cleaners\",\n    use_phonemes=False,\n    phoneme_cache_path=\"tests/data/ljspeech/phoneme_cache/\",\n    run_eval=True,\n    test_delay_epochs=-1,",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.audio.trim_db = 60\n# active multilingual mode\nconfig.model_args.use_language_embedding = True\nconfig.use_language_embedding = True\n# deactivate multispeaker mode\nconfig.model_args.use_speaker_embedding = False\nconfig.use_speaker_embedding = False\n# active multispeaker d-vec mode\nconfig.model_args.use_d_vector_file = True",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "config.audio.trim_db = 60\n# active multilingual mode\nconfig.model_args.use_language_embedding = True\nconfig.use_language_embedding = True\n# deactivate multispeaker mode\nconfig.model_args.use_speaker_embedding = False\nconfig.use_speaker_embedding = False\n# active multispeaker d-vec mode\nconfig.model_args.use_d_vector_file = True\nconfig.use_d_vector_file = True",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "config.model_args.use_language_embedding",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "config.model_args.use_language_embedding = True\nconfig.use_language_embedding = True\n# deactivate multispeaker mode\nconfig.model_args.use_speaker_embedding = False\nconfig.use_speaker_embedding = False\n# active multispeaker d-vec mode\nconfig.model_args.use_d_vector_file = True\nconfig.use_d_vector_file = True\nconfig.model_args.d_vector_file = [\"tests/data/ljspeech/speakers.json\"]\nconfig.d_vector_file = [\"tests/data/ljspeech/speakers.json\"]",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "config.use_language_embedding",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "config.use_language_embedding = True\n# deactivate multispeaker mode\nconfig.model_args.use_speaker_embedding = False\nconfig.use_speaker_embedding = False\n# active multispeaker d-vec mode\nconfig.model_args.use_d_vector_file = True\nconfig.use_d_vector_file = True\nconfig.model_args.d_vector_file = [\"tests/data/ljspeech/speakers.json\"]\nconfig.d_vector_file = [\"tests/data/ljspeech/speakers.json\"]\nconfig.model_args.d_vector_dim = 256",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "config.model_args.use_speaker_embedding",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "config.model_args.use_speaker_embedding = False\nconfig.use_speaker_embedding = False\n# active multispeaker d-vec mode\nconfig.model_args.use_d_vector_file = True\nconfig.use_d_vector_file = True\nconfig.model_args.d_vector_file = [\"tests/data/ljspeech/speakers.json\"]\nconfig.d_vector_file = [\"tests/data/ljspeech/speakers.json\"]\nconfig.model_args.d_vector_dim = 256\nconfig.d_vector_dim = 256\n# duration predictor",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "config.use_speaker_embedding",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "config.use_speaker_embedding = False\n# active multispeaker d-vec mode\nconfig.model_args.use_d_vector_file = True\nconfig.use_d_vector_file = True\nconfig.model_args.d_vector_file = [\"tests/data/ljspeech/speakers.json\"]\nconfig.d_vector_file = [\"tests/data/ljspeech/speakers.json\"]\nconfig.model_args.d_vector_dim = 256\nconfig.d_vector_dim = 256\n# duration predictor\nconfig.model_args.use_sdp = True",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "config.model_args.use_d_vector_file",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "config.model_args.use_d_vector_file = True\nconfig.use_d_vector_file = True\nconfig.model_args.d_vector_file = [\"tests/data/ljspeech/speakers.json\"]\nconfig.d_vector_file = [\"tests/data/ljspeech/speakers.json\"]\nconfig.model_args.d_vector_dim = 256\nconfig.d_vector_dim = 256\n# duration predictor\nconfig.model_args.use_sdp = True\nconfig.use_sdp = True\n# activate language and speaker samplers",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "config.use_d_vector_file",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "config.use_d_vector_file = True\nconfig.model_args.d_vector_file = [\"tests/data/ljspeech/speakers.json\"]\nconfig.d_vector_file = [\"tests/data/ljspeech/speakers.json\"]\nconfig.model_args.d_vector_dim = 256\nconfig.d_vector_dim = 256\n# duration predictor\nconfig.model_args.use_sdp = True\nconfig.use_sdp = True\n# activate language and speaker samplers\nconfig.use_language_weighted_sampler = True",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "config.model_args.d_vector_file",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "config.model_args.d_vector_file = [\"tests/data/ljspeech/speakers.json\"]\nconfig.d_vector_file = [\"tests/data/ljspeech/speakers.json\"]\nconfig.model_args.d_vector_dim = 256\nconfig.d_vector_dim = 256\n# duration predictor\nconfig.model_args.use_sdp = True\nconfig.use_sdp = True\n# activate language and speaker samplers\nconfig.use_language_weighted_sampler = True\nconfig.language_weighted_sampler_alpha = 10",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "config.d_vector_file",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "config.d_vector_file = [\"tests/data/ljspeech/speakers.json\"]\nconfig.model_args.d_vector_dim = 256\nconfig.d_vector_dim = 256\n# duration predictor\nconfig.model_args.use_sdp = True\nconfig.use_sdp = True\n# activate language and speaker samplers\nconfig.use_language_weighted_sampler = True\nconfig.language_weighted_sampler_alpha = 10\nconfig.use_speaker_weighted_sampler = True",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "config.model_args.d_vector_dim",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "config.model_args.d_vector_dim = 256\nconfig.d_vector_dim = 256\n# duration predictor\nconfig.model_args.use_sdp = True\nconfig.use_sdp = True\n# activate language and speaker samplers\nconfig.use_language_weighted_sampler = True\nconfig.language_weighted_sampler_alpha = 10\nconfig.use_speaker_weighted_sampler = True\nconfig.speaker_weighted_sampler_alpha = 5",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "config.d_vector_dim",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "config.d_vector_dim = 256\n# duration predictor\nconfig.model_args.use_sdp = True\nconfig.use_sdp = True\n# activate language and speaker samplers\nconfig.use_language_weighted_sampler = True\nconfig.language_weighted_sampler_alpha = 10\nconfig.use_speaker_weighted_sampler = True\nconfig.speaker_weighted_sampler_alpha = 5\nconfig.save_json(config_path)",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "config.model_args.use_sdp",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "config.model_args.use_sdp = True\nconfig.use_sdp = True\n# activate language and speaker samplers\nconfig.use_language_weighted_sampler = True\nconfig.language_weighted_sampler_alpha = 10\nconfig.use_speaker_weighted_sampler = True\nconfig.speaker_weighted_sampler_alpha = 5\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "config.use_sdp",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "config.use_sdp = True\n# activate language and speaker samplers\nconfig.use_language_weighted_sampler = True\nconfig.language_weighted_sampler_alpha = 10\nconfig.use_speaker_weighted_sampler = True\nconfig.speaker_weighted_sampler_alpha = 5\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "config.use_language_weighted_sampler",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "config.use_language_weighted_sampler = True\nconfig.language_weighted_sampler_alpha = 10\nconfig.use_speaker_weighted_sampler = True\nconfig.speaker_weighted_sampler_alpha = 5\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.test_delay_epochs 0\"",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "config.language_weighted_sampler_alpha",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "config.language_weighted_sampler_alpha = 10\nconfig.use_speaker_weighted_sampler = True\nconfig.speaker_weighted_sampler_alpha = 5\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.test_delay_epochs 0\"\n)",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "config.use_speaker_weighted_sampler",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "config.use_speaker_weighted_sampler = True\nconfig.speaker_weighted_sampler_alpha = 5\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.test_delay_epochs 0\"\n)\nrun_cli(command_train)",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "config.speaker_weighted_sampler_alpha",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "config.speaker_weighted_sampler_alpha = 5\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.test_delay_epochs 0\"\n)\nrun_cli(command_train)\n# Find latest folder",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.test_delay_epochs 0\"\n)\nrun_cli(command_train)\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# Inference using TTS API\ncontinue_config_path = os.path.join(continue_path, \"config.json\")",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# Inference using TTS API\ncontinue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech-1\"\nlanguae_id = \"en\"\ncontinue_speakers_path = config.d_vector_file\ncontinue_languages_path = os.path.join(continue_path, \"language_ids.json\")\n# Check integrity of the config",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "continue_config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "continue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech-1\"\nlanguae_id = \"en\"\ncontinue_speakers_path = config.d_vector_file\ncontinue_languages_path = os.path.join(continue_path, \"language_ids.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "out_wav_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "out_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech-1\"\nlanguae_id = \"en\"\ncontinue_speakers_path = config.d_vector_file\ncontinue_languages_path = os.path.join(continue_path, \"language_ids.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "speaker_id",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "speaker_id = \"ljspeech-1\"\nlanguae_id = \"en\"\ncontinue_speakers_path = config.d_vector_file\ncontinue_languages_path = os.path.join(continue_path, \"language_ids.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "languae_id",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "languae_id = \"en\"\ncontinue_speakers_path = config.d_vector_file\ncontinue_languages_path = os.path.join(continue_path, \"language_ids.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "continue_speakers_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "continue_speakers_path = config.d_vector_file\ncontinue_languages_path = os.path.join(continue_path, \"language_ids.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --language_ids_file_path {continue_languages_path} --language_idx {languae_id} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "continue_languages_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "continue_languages_path = os.path.join(continue_path, \"language_ids.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --language_ids_file_path {continue_languages_path} --language_idx {languae_id} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "inference_command",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "inference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --language_ids_file_path {continue_languages_path} --language_idx {languae_id} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)\n# restore the model and continue training for one more epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "description": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_vits_multilingual_train-d_vectors",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_model_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = VitsConfig(\n    batch_size=2,\n    eval_batch_size=2,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",",
        "detail": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = VitsConfig(\n    batch_size=2,\n    eval_batch_size=2,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=\"tests/data/ljspeech/phoneme_cache/\",",
        "detail": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "peekOfCode": "config = VitsConfig(\n    batch_size=2,\n    eval_batch_size=2,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=\"tests/data/ljspeech/phoneme_cache/\",\n    run_eval=True,",
        "detail": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.audio.trim_db = 60\n# active multispeaker d-vec mode\nconfig.model_args.use_speaker_embedding = True\nconfig.model_args.use_d_vector_file = False\nconfig.model_args.d_vector_file = None\nconfig.model_args.d_vector_dim = 256\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (",
        "detail": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "peekOfCode": "config.audio.trim_db = 60\n# active multispeaker d-vec mode\nconfig.model_args.use_speaker_embedding = True\nconfig.model_args.use_d_vector_file = False\nconfig.model_args.d_vector_file = None\nconfig.model_args.d_vector_dim = 256\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"",
        "detail": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.model_args.use_speaker_embedding",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "peekOfCode": "config.model_args.use_speaker_embedding = True\nconfig.model_args.use_d_vector_file = False\nconfig.model_args.d_vector_file = None\nconfig.model_args.d_vector_dim = 256\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"",
        "detail": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.model_args.use_d_vector_file",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "peekOfCode": "config.model_args.use_d_vector_file = False\nconfig.model_args.d_vector_file = None\nconfig.model_args.d_vector_dim = 256\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"",
        "detail": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.model_args.d_vector_file",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "peekOfCode": "config.model_args.d_vector_file = None\nconfig.model_args.d_vector_dim = 256\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"",
        "detail": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config.model_args.d_vector_dim",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "peekOfCode": "config.model_args.d_vector_dim = 256\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"",
        "detail": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech_test \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"\n    \"--coqpit.datasets.0.meta_file_attn_mask tests/data/ljspeech/metadata_attn_mask.txt \"\n    \"--coqpit.test_delay_epochs 0\"\n)",
        "detail": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# Inference using TTS API\ncontinue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech-1\"\ncontinue_speakers_path = os.path.join(continue_path, \"speakers.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)",
        "detail": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "continue_config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "peekOfCode": "continue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech-1\"\ncontinue_speakers_path = os.path.join(continue_path, \"speakers.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path",
        "detail": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "out_wav_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "peekOfCode": "out_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\nspeaker_id = \"ljspeech-1\"\ncontinue_speakers_path = os.path.join(continue_path, \"speakers.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference",
        "detail": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "speaker_id",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "peekOfCode": "speaker_id = \"ljspeech-1\"\ncontinue_speakers_path = os.path.join(continue_path, \"speakers.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"",
        "detail": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "continue_speakers_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "peekOfCode": "continue_speakers_path = os.path.join(continue_path, \"speakers.json\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)",
        "detail": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "inference_command",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "peekOfCode": "inference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --speaker_idx {speaker_id} --speakers_file_path {continue_speakers_path} --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)\n# restore the model and continue training for one more epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_vits_speaker_emb_train",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_model_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = VitsConfig(\n    batch_size=2,\n    eval_batch_size=2,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",",
        "detail": "env.src.tts.tests.tts_tests.test_vits_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = VitsConfig(\n    batch_size=2,\n    eval_batch_size=2,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=\"tests/data/ljspeech/phoneme_cache/\",",
        "detail": "env.src.tts.tests.tts_tests.test_vits_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_train",
        "peekOfCode": "config = VitsConfig(\n    batch_size=2,\n    eval_batch_size=2,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    text_cleaner=\"english_cleaners\",\n    use_phonemes=True,\n    phoneme_language=\"en-us\",\n    phoneme_cache_path=\"tests/data/ljspeech/phoneme_cache/\",\n    run_eval=True,",
        "detail": "env.src.tts.tests.tts_tests.test_vits_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"",
        "detail": "env.src.tts.tests.tts_tests.test_vits_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_train",
        "peekOfCode": "config.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"",
        "detail": "env.src.tts.tests.tts_tests.test_vits_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --config_path {config_path} \"\n    f\"--coqpit.output_path {output_path} \"\n    \"--coqpit.datasets.0.formatter ljspeech \"\n    \"--coqpit.datasets.0.meta_file_train metadata.csv \"\n    \"--coqpit.datasets.0.meta_file_val metadata.csv \"\n    \"--coqpit.datasets.0.path tests/data/ljspeech \"\n    \"--coqpit.datasets.0.meta_file_attn_mask tests/data/ljspeech/metadata_attn_mask.txt \"\n    \"--coqpit.test_delay_epochs 0\"\n)",
        "detail": "env.src.tts.tests.tts_tests.test_vits_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# Inference using TTS API\ncontinue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path",
        "detail": "env.src.tts.tests.tts_tests.test_vits_train",
        "documentation": {}
    },
    {
        "label": "continue_config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_train",
        "peekOfCode": "continue_config_path = os.path.join(continue_path, \"config.json\")\ncontinue_restore_path, _ = get_last_checkpoint(continue_path)\nout_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference",
        "detail": "env.src.tts.tests.tts_tests.test_vits_train",
        "documentation": {}
    },
    {
        "label": "out_wav_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_train",
        "peekOfCode": "out_wav_path = os.path.join(get_tests_output_path(), \"output.wav\")\n# Check integrity of the config\nwith open(continue_config_path, \"r\", encoding=\"utf-8\") as f:\n    config_loaded = json.load(f)\nassert config_loaded[\"characters\"] is not None\nassert config_loaded[\"output_path\"] in continue_path\nassert config_loaded[\"test_delay_epochs\"] == 0\n# Load the model and run inference\ninference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)",
        "detail": "env.src.tts.tests.tts_tests.test_vits_train",
        "documentation": {}
    },
    {
        "label": "inference_command",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_train",
        "peekOfCode": "inference_command = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' tts --text 'This is an example.' --config_path {continue_config_path} --model_path {continue_restore_path} --out_path {out_wav_path}\"\nrun_cli(inference_command)\n# restore the model and continue training for one more epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_vits_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.tts_tests.test_vits_train",
        "description": "env.src.tts.tests.tts_tests.test_vits_train",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_tts.py --continue_path {continue_path} \"\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.tts_tests.test_vits_train",
        "documentation": {}
    },
    {
        "label": "TestFreeVC",
        "kind": 6,
        "importPath": "env.src.tts.tests.vc_tests.test_freevc",
        "description": "env.src.tts.tests.vc_tests.test_freevc",
        "peekOfCode": "class TestFreeVC(unittest.TestCase):\n    def _create_inputs(self, config, batch_size=2):\n        input_dummy = torch.rand(batch_size, 30 * config.audio[\"hop_length\"]).to(device)\n        input_lengths = torch.randint(100, 30 * config.audio[\"hop_length\"], (batch_size,)).long().to(device)\n        input_lengths[-1] = 30 * config.audio[\"hop_length\"]\n        spec = torch.rand(batch_size, 30, config.audio[\"filter_length\"] // 2 + 1).to(device)\n        mel = torch.rand(batch_size, 30, config.audio[\"n_mel_channels\"]).to(device)\n        spec_lengths = torch.randint(20, 30, (batch_size,)).long().to(device)\n        spec_lengths[-1] = spec.size(2)\n        waveform = torch.rand(batch_size, spec.size(2) * config.audio[\"hop_length\"]).to(device)",
        "detail": "env.src.tts.tests.vc_tests.test_freevc",
        "documentation": {}
    },
    {
        "label": "count_parameters",
        "kind": 2,
        "importPath": "env.src.tts.tests.vc_tests.test_freevc",
        "description": "env.src.tts.tests.vc_tests.test_freevc",
        "peekOfCode": "def count_parameters(model):\n    r\"\"\"Count number of trainable parameters in a network\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nclass TestFreeVC(unittest.TestCase):\n    def _create_inputs(self, config, batch_size=2):\n        input_dummy = torch.rand(batch_size, 30 * config.audio[\"hop_length\"]).to(device)\n        input_lengths = torch.randint(100, 30 * config.audio[\"hop_length\"], (batch_size,)).long().to(device)\n        input_lengths[-1] = 30 * config.audio[\"hop_length\"]\n        spec = torch.rand(batch_size, 30, config.audio[\"filter_length\"] // 2 + 1).to(device)\n        mel = torch.rand(batch_size, 30, config.audio[\"n_mel_channels\"]).to(device)",
        "detail": "env.src.tts.tests.vc_tests.test_freevc",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "env.src.tts.tests.vc_tests.test_freevc",
        "description": "env.src.tts.tests.vc_tests.test_freevc",
        "peekOfCode": "use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nc = FreeVCConfig()\nWAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\nBATCH_SIZE = 3\ndef count_parameters(model):\n    r\"\"\"Count number of trainable parameters in a network\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nclass TestFreeVC(unittest.TestCase):\n    def _create_inputs(self, config, batch_size=2):",
        "detail": "env.src.tts.tests.vc_tests.test_freevc",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "env.src.tts.tests.vc_tests.test_freevc",
        "description": "env.src.tts.tests.vc_tests.test_freevc",
        "peekOfCode": "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nc = FreeVCConfig()\nWAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\nBATCH_SIZE = 3\ndef count_parameters(model):\n    r\"\"\"Count number of trainable parameters in a network\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nclass TestFreeVC(unittest.TestCase):\n    def _create_inputs(self, config, batch_size=2):\n        input_dummy = torch.rand(batch_size, 30 * config.audio[\"hop_length\"]).to(device)",
        "detail": "env.src.tts.tests.vc_tests.test_freevc",
        "documentation": {}
    },
    {
        "label": "c",
        "kind": 5,
        "importPath": "env.src.tts.tests.vc_tests.test_freevc",
        "description": "env.src.tts.tests.vc_tests.test_freevc",
        "peekOfCode": "c = FreeVCConfig()\nWAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\nBATCH_SIZE = 3\ndef count_parameters(model):\n    r\"\"\"Count number of trainable parameters in a network\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nclass TestFreeVC(unittest.TestCase):\n    def _create_inputs(self, config, batch_size=2):\n        input_dummy = torch.rand(batch_size, 30 * config.audio[\"hop_length\"]).to(device)\n        input_lengths = torch.randint(100, 30 * config.audio[\"hop_length\"], (batch_size,)).long().to(device)",
        "detail": "env.src.tts.tests.vc_tests.test_freevc",
        "documentation": {}
    },
    {
        "label": "WAV_FILE",
        "kind": 5,
        "importPath": "env.src.tts.tests.vc_tests.test_freevc",
        "description": "env.src.tts.tests.vc_tests.test_freevc",
        "peekOfCode": "WAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\nBATCH_SIZE = 3\ndef count_parameters(model):\n    r\"\"\"Count number of trainable parameters in a network\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nclass TestFreeVC(unittest.TestCase):\n    def _create_inputs(self, config, batch_size=2):\n        input_dummy = torch.rand(batch_size, 30 * config.audio[\"hop_length\"]).to(device)\n        input_lengths = torch.randint(100, 30 * config.audio[\"hop_length\"], (batch_size,)).long().to(device)\n        input_lengths[-1] = 30 * config.audio[\"hop_length\"]",
        "detail": "env.src.tts.tests.vc_tests.test_freevc",
        "documentation": {}
    },
    {
        "label": "BATCH_SIZE",
        "kind": 5,
        "importPath": "env.src.tts.tests.vc_tests.test_freevc",
        "description": "env.src.tts.tests.vc_tests.test_freevc",
        "peekOfCode": "BATCH_SIZE = 3\ndef count_parameters(model):\n    r\"\"\"Count number of trainable parameters in a network\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\nclass TestFreeVC(unittest.TestCase):\n    def _create_inputs(self, config, batch_size=2):\n        input_dummy = torch.rand(batch_size, 30 * config.audio[\"hop_length\"]).to(device)\n        input_lengths = torch.randint(100, 30 * config.audio[\"hop_length\"], (batch_size,)).long().to(device)\n        input_lengths[-1] = 30 * config.audio[\"hop_length\"]\n        spec = torch.rand(batch_size, 30, config.audio[\"filter_length\"] // 2 + 1).to(device)",
        "detail": "env.src.tts.tests.vc_tests.test_freevc",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_fullband_melgan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_fullband_melgan_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_vocoder_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = FullbandMelganConfig(\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1,",
        "detail": "env.src.tts.tests.vocoder_tests.test_fullband_melgan_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_fullband_melgan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_fullband_melgan_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = FullbandMelganConfig(\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1,\n    seq_len=8192,",
        "detail": "env.src.tts.tests.vocoder_tests.test_fullband_melgan_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_fullband_melgan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_fullband_melgan_train",
        "peekOfCode": "config = FullbandMelganConfig(\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1,\n    seq_len=8192,\n    eval_split_size=1,",
        "detail": "env.src.tts.tests.vocoder_tests.test_fullband_melgan_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_fullband_melgan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_fullband_melgan_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --config_path {config_path} \"\nrun_cli(command_train)\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (",
        "detail": "env.src.tts.tests.vocoder_tests.test_fullband_melgan_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_fullband_melgan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_fullband_melgan_train",
        "peekOfCode": "config.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --config_path {config_path} \"\nrun_cli(command_train)\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"",
        "detail": "env.src.tts.tests.vocoder_tests.test_fullband_melgan_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_fullband_melgan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_fullband_melgan_train",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --config_path {config_path} \"\nrun_cli(command_train)\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"\n)\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.vocoder_tests.test_fullband_melgan_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_fullband_melgan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_fullband_melgan_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"\n)\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.vocoder_tests.test_fullband_melgan_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_fullband_melgan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_fullband_melgan_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"\n)\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.vocoder_tests.test_fullband_melgan_train",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_hifigan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_hifigan_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_vocoder_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = HifiganConfig(\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1,",
        "detail": "env.src.tts.tests.vocoder_tests.test_hifigan_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_hifigan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_hifigan_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = HifiganConfig(\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1,\n    seq_len=1024,",
        "detail": "env.src.tts.tests.vocoder_tests.test_hifigan_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_hifigan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_hifigan_train",
        "peekOfCode": "config = HifiganConfig(\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1,\n    seq_len=1024,\n    eval_split_size=1,",
        "detail": "env.src.tts.tests.vocoder_tests.test_hifigan_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_hifigan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_hifigan_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --config_path {config_path} \"\nrun_cli(command_train)\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (",
        "detail": "env.src.tts.tests.vocoder_tests.test_hifigan_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_hifigan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_hifigan_train",
        "peekOfCode": "config.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --config_path {config_path} \"\nrun_cli(command_train)\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"",
        "detail": "env.src.tts.tests.vocoder_tests.test_hifigan_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_hifigan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_hifigan_train",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --config_path {config_path} \"\nrun_cli(command_train)\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"\n)\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.vocoder_tests.test_hifigan_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_hifigan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_hifigan_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"\n)\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.vocoder_tests.test_hifigan_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_hifigan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_hifigan_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"\n)\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.vocoder_tests.test_hifigan_train",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_melgan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_melgan_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_vocoder_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = MelganConfig(\n    batch_size=4,\n    eval_batch_size=4,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1,",
        "detail": "env.src.tts.tests.vocoder_tests.test_melgan_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_melgan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_melgan_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = MelganConfig(\n    batch_size=4,\n    eval_batch_size=4,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1,\n    seq_len=2048,",
        "detail": "env.src.tts.tests.vocoder_tests.test_melgan_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_melgan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_melgan_train",
        "peekOfCode": "config = MelganConfig(\n    batch_size=4,\n    eval_batch_size=4,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1,\n    seq_len=2048,\n    eval_split_size=1,",
        "detail": "env.src.tts.tests.vocoder_tests.test_melgan_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_melgan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_melgan_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --config_path {config_path} \"\nrun_cli(command_train)\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (",
        "detail": "env.src.tts.tests.vocoder_tests.test_melgan_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_melgan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_melgan_train",
        "peekOfCode": "config.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --config_path {config_path} \"\nrun_cli(command_train)\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"",
        "detail": "env.src.tts.tests.vocoder_tests.test_melgan_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_melgan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_melgan_train",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --config_path {config_path} \"\nrun_cli(command_train)\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"\n)\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.vocoder_tests.test_melgan_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_melgan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_melgan_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"\n)\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.vocoder_tests.test_melgan_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_melgan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_melgan_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"\n)\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.vocoder_tests.test_melgan_train",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_multiband_melgan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_multiband_melgan_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_vocoder_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = MultibandMelganConfig(\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1,",
        "detail": "env.src.tts.tests.vocoder_tests.test_multiband_melgan_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_multiband_melgan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_multiband_melgan_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = MultibandMelganConfig(\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1,\n    seq_len=8192,",
        "detail": "env.src.tts.tests.vocoder_tests.test_multiband_melgan_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_multiband_melgan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_multiband_melgan_train",
        "peekOfCode": "config = MultibandMelganConfig(\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1,\n    seq_len=8192,\n    eval_split_size=1,",
        "detail": "env.src.tts.tests.vocoder_tests.test_multiband_melgan_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_multiband_melgan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_multiband_melgan_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --config_path {config_path} \"\nrun_cli(command_train)\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (",
        "detail": "env.src.tts.tests.vocoder_tests.test_multiband_melgan_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_multiband_melgan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_multiband_melgan_train",
        "peekOfCode": "config.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --config_path {config_path} \"\nrun_cli(command_train)\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"",
        "detail": "env.src.tts.tests.vocoder_tests.test_multiband_melgan_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_multiband_melgan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_multiband_melgan_train",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --config_path {config_path} \"\nrun_cli(command_train)\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"\n)\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.vocoder_tests.test_multiband_melgan_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_multiband_melgan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_multiband_melgan_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"\n)\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.vocoder_tests.test_multiband_melgan_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_multiband_melgan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_multiband_melgan_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"\n)\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.vocoder_tests.test_multiband_melgan_train",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_parallel_wavegan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_parallel_wavegan_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_vocoder_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = ParallelWaveganConfig(\n    batch_size=4,\n    eval_batch_size=4,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1,",
        "detail": "env.src.tts.tests.vocoder_tests.test_parallel_wavegan_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_parallel_wavegan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_parallel_wavegan_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = ParallelWaveganConfig(\n    batch_size=4,\n    eval_batch_size=4,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1,\n    seq_len=2048,",
        "detail": "env.src.tts.tests.vocoder_tests.test_parallel_wavegan_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_parallel_wavegan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_parallel_wavegan_train",
        "peekOfCode": "config = ParallelWaveganConfig(\n    batch_size=4,\n    eval_batch_size=4,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1,\n    seq_len=2048,\n    eval_split_size=1,",
        "detail": "env.src.tts.tests.vocoder_tests.test_parallel_wavegan_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_parallel_wavegan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_parallel_wavegan_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --config_path {config_path} \"\nrun_cli(command_train)\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (",
        "detail": "env.src.tts.tests.vocoder_tests.test_parallel_wavegan_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_parallel_wavegan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_parallel_wavegan_train",
        "peekOfCode": "config.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --config_path {config_path} \"\nrun_cli(command_train)\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"",
        "detail": "env.src.tts.tests.vocoder_tests.test_parallel_wavegan_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_parallel_wavegan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_parallel_wavegan_train",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --config_path {config_path} \"\nrun_cli(command_train)\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"\n)\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.vocoder_tests.test_parallel_wavegan_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_parallel_wavegan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_parallel_wavegan_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"\n)\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.vocoder_tests.test_parallel_wavegan_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_parallel_wavegan_train",
        "description": "env.src.tts.tests.vocoder_tests.test_parallel_wavegan_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"\n)\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.vocoder_tests.test_parallel_wavegan_train",
        "documentation": {}
    },
    {
        "label": "gan_dataset_case",
        "kind": 2,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_gan_datasets",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_gan_datasets",
        "peekOfCode": "def gan_dataset_case(\n    batch_size, seq_len, hop_len, conv_pad, return_pairs, return_segments, use_noise_augment, use_cache, num_workers\n):\n    \"\"\"Run dataloader with given parameters and check conditions\"\"\"\n    ap = AudioProcessor(**C.audio)\n    _, train_items = load_wav_data(test_data_path, 10)\n    dataset = GANDataset(\n        ap,\n        train_items,\n        seq_len=seq_len,",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_gan_datasets",
        "documentation": {}
    },
    {
        "label": "test_parametrized_gan_dataset",
        "kind": 2,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_gan_datasets",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_gan_datasets",
        "peekOfCode": "def test_parametrized_gan_dataset():\n    \"\"\"test dataloader with different parameters\"\"\"\n    params = [\n        [32, C.audio[\"hop_length\"] * 10, C.audio[\"hop_length\"], 0, True, True, False, True, 0],\n        [32, C.audio[\"hop_length\"] * 10, C.audio[\"hop_length\"], 0, True, True, False, True, 4],\n        [1, C.audio[\"hop_length\"] * 10, C.audio[\"hop_length\"], 0, True, True, True, True, 0],\n        [1, C.audio[\"hop_length\"], C.audio[\"hop_length\"], 0, True, True, True, True, 0],\n        [1, C.audio[\"hop_length\"] * 10, C.audio[\"hop_length\"], 2, True, True, True, True, 0],\n        [1, C.audio[\"hop_length\"] * 10, C.audio[\"hop_length\"], 0, True, False, True, True, 0],\n        [1, C.audio[\"hop_length\"] * 10, C.audio[\"hop_length\"], 0, True, True, False, True, 0],",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_gan_datasets",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_gan_datasets",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_gan_datasets",
        "peekOfCode": "file_path = os.path.dirname(os.path.realpath(__file__))\nOUTPATH = os.path.join(get_tests_output_path(), \"loader_tests/\")\nos.makedirs(OUTPATH, exist_ok=True)\nC = BaseGANVocoderConfig()\ntest_data_path = os.path.join(get_tests_path(), \"data/ljspeech/\")\nok_ljspeech = os.path.exists(test_data_path)\ndef gan_dataset_case(\n    batch_size, seq_len, hop_len, conv_pad, return_pairs, return_segments, use_noise_augment, use_cache, num_workers\n):\n    \"\"\"Run dataloader with given parameters and check conditions\"\"\"",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_gan_datasets",
        "documentation": {}
    },
    {
        "label": "OUTPATH",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_gan_datasets",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_gan_datasets",
        "peekOfCode": "OUTPATH = os.path.join(get_tests_output_path(), \"loader_tests/\")\nos.makedirs(OUTPATH, exist_ok=True)\nC = BaseGANVocoderConfig()\ntest_data_path = os.path.join(get_tests_path(), \"data/ljspeech/\")\nok_ljspeech = os.path.exists(test_data_path)\ndef gan_dataset_case(\n    batch_size, seq_len, hop_len, conv_pad, return_pairs, return_segments, use_noise_augment, use_cache, num_workers\n):\n    \"\"\"Run dataloader with given parameters and check conditions\"\"\"\n    ap = AudioProcessor(**C.audio)",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_gan_datasets",
        "documentation": {}
    },
    {
        "label": "C",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_gan_datasets",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_gan_datasets",
        "peekOfCode": "C = BaseGANVocoderConfig()\ntest_data_path = os.path.join(get_tests_path(), \"data/ljspeech/\")\nok_ljspeech = os.path.exists(test_data_path)\ndef gan_dataset_case(\n    batch_size, seq_len, hop_len, conv_pad, return_pairs, return_segments, use_noise_augment, use_cache, num_workers\n):\n    \"\"\"Run dataloader with given parameters and check conditions\"\"\"\n    ap = AudioProcessor(**C.audio)\n    _, train_items = load_wav_data(test_data_path, 10)\n    dataset = GANDataset(",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_gan_datasets",
        "documentation": {}
    },
    {
        "label": "test_data_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_gan_datasets",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_gan_datasets",
        "peekOfCode": "test_data_path = os.path.join(get_tests_path(), \"data/ljspeech/\")\nok_ljspeech = os.path.exists(test_data_path)\ndef gan_dataset_case(\n    batch_size, seq_len, hop_len, conv_pad, return_pairs, return_segments, use_noise_augment, use_cache, num_workers\n):\n    \"\"\"Run dataloader with given parameters and check conditions\"\"\"\n    ap = AudioProcessor(**C.audio)\n    _, train_items = load_wav_data(test_data_path, 10)\n    dataset = GANDataset(\n        ap,",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_gan_datasets",
        "documentation": {}
    },
    {
        "label": "ok_ljspeech",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_gan_datasets",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_gan_datasets",
        "peekOfCode": "ok_ljspeech = os.path.exists(test_data_path)\ndef gan_dataset_case(\n    batch_size, seq_len, hop_len, conv_pad, return_pairs, return_segments, use_noise_augment, use_cache, num_workers\n):\n    \"\"\"Run dataloader with given parameters and check conditions\"\"\"\n    ap = AudioProcessor(**C.audio)\n    _, train_items = load_wav_data(test_data_path, 10)\n    dataset = GANDataset(\n        ap,\n        train_items,",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_gan_datasets",
        "documentation": {}
    },
    {
        "label": "test_torch_stft",
        "kind": 2,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_losses",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_losses",
        "peekOfCode": "def test_torch_stft():\n    torch_stft = TorchSTFT(ap.fft_size, ap.hop_length, ap.win_length)\n    # librosa stft\n    wav = ap.load_wav(WAV_FILE)\n    M_librosa = abs(ap._stft(wav))  # pylint: disable=protected-access\n    # torch stft\n    wav = torch.from_numpy(wav[None, :]).float()\n    M_torch = torch_stft(wav)\n    # check the difference b/w librosa and torch outputs\n    assert (M_librosa - M_torch[0].data.numpy()).max() < 1e-5",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_losses",
        "documentation": {}
    },
    {
        "label": "test_stft_loss",
        "kind": 2,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_losses",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_losses",
        "peekOfCode": "def test_stft_loss():\n    stft_loss = STFTLoss(ap.fft_size, ap.hop_length, ap.win_length)\n    wav = ap.load_wav(WAV_FILE)\n    wav = torch.from_numpy(wav[None, :]).float()\n    loss_m, loss_sc = stft_loss(wav, wav)\n    assert loss_m + loss_sc == 0\n    loss_m, loss_sc = stft_loss(wav, torch.rand_like(wav))\n    assert loss_sc < 1.0\n    assert loss_m + loss_sc > 0\ndef test_multiscale_stft_loss():",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_losses",
        "documentation": {}
    },
    {
        "label": "test_multiscale_stft_loss",
        "kind": 2,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_losses",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_losses",
        "peekOfCode": "def test_multiscale_stft_loss():\n    stft_loss = MultiScaleSTFTLoss(\n        [ap.fft_size // 2, ap.fft_size, ap.fft_size * 2],\n        [ap.hop_length // 2, ap.hop_length, ap.hop_length * 2],\n        [ap.win_length // 2, ap.win_length, ap.win_length * 2],\n    )\n    wav = ap.load_wav(WAV_FILE)\n    wav = torch.from_numpy(wav[None, :]).float()\n    loss_m, loss_sc = stft_loss(wav, wav)\n    assert loss_m + loss_sc == 0",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_losses",
        "documentation": {}
    },
    {
        "label": "test_melgan_feature_loss",
        "kind": 2,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_losses",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_losses",
        "peekOfCode": "def test_melgan_feature_loss():\n    feats_real = []\n    feats_fake = []\n    # if all the features are different.\n    for _ in range(5):  # different scales\n        scale_feats_real = []\n        scale_feats_fake = []\n        for _ in range(4):  # different layers\n            scale_feats_real.append(torch.rand([3, 5, 7]))\n            scale_feats_fake.append(torch.rand([3, 5, 7]))",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_losses",
        "documentation": {}
    },
    {
        "label": "TESTS_PATH",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_losses",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_losses",
        "peekOfCode": "TESTS_PATH = get_tests_path()\nOUT_PATH = os.path.join(get_tests_output_path(), \"audio_tests\")\nos.makedirs(OUT_PATH, exist_ok=True)\nWAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\nap = AudioProcessor(**BaseAudioConfig().to_dict())\ndef test_torch_stft():\n    torch_stft = TorchSTFT(ap.fft_size, ap.hop_length, ap.win_length)\n    # librosa stft\n    wav = ap.load_wav(WAV_FILE)\n    M_librosa = abs(ap._stft(wav))  # pylint: disable=protected-access",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_losses",
        "documentation": {}
    },
    {
        "label": "OUT_PATH",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_losses",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_losses",
        "peekOfCode": "OUT_PATH = os.path.join(get_tests_output_path(), \"audio_tests\")\nos.makedirs(OUT_PATH, exist_ok=True)\nWAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\nap = AudioProcessor(**BaseAudioConfig().to_dict())\ndef test_torch_stft():\n    torch_stft = TorchSTFT(ap.fft_size, ap.hop_length, ap.win_length)\n    # librosa stft\n    wav = ap.load_wav(WAV_FILE)\n    M_librosa = abs(ap._stft(wav))  # pylint: disable=protected-access\n    # torch stft",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_losses",
        "documentation": {}
    },
    {
        "label": "WAV_FILE",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_losses",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_losses",
        "peekOfCode": "WAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\nap = AudioProcessor(**BaseAudioConfig().to_dict())\ndef test_torch_stft():\n    torch_stft = TorchSTFT(ap.fft_size, ap.hop_length, ap.win_length)\n    # librosa stft\n    wav = ap.load_wav(WAV_FILE)\n    M_librosa = abs(ap._stft(wav))  # pylint: disable=protected-access\n    # torch stft\n    wav = torch.from_numpy(wav[None, :]).float()\n    M_torch = torch_stft(wav)",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_losses",
        "documentation": {}
    },
    {
        "label": "ap",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_losses",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_losses",
        "peekOfCode": "ap = AudioProcessor(**BaseAudioConfig().to_dict())\ndef test_torch_stft():\n    torch_stft = TorchSTFT(ap.fft_size, ap.hop_length, ap.win_length)\n    # librosa stft\n    wav = ap.load_wav(WAV_FILE)\n    M_librosa = abs(ap._stft(wav))  # pylint: disable=protected-access\n    # torch stft\n    wav = torch.from_numpy(wav[None, :]).float()\n    M_torch = torch_stft(wav)\n    # check the difference b/w librosa and torch outputs",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_losses",
        "documentation": {}
    },
    {
        "label": "test_melgan_discriminator",
        "kind": 2,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_melgan_discriminator",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_melgan_discriminator",
        "peekOfCode": "def test_melgan_discriminator():\n    model = MelganDiscriminator()\n    print(model)\n    dummy_input = torch.rand((4, 1, 256 * 10))\n    output, _ = model(dummy_input)\n    assert np.all(output.shape == (4, 1, 10))\ndef test_melgan_multi_scale_discriminator():\n    model = MelganMultiscaleDiscriminator()\n    print(model)\n    dummy_input = torch.rand((4, 1, 256 * 16))",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_melgan_discriminator",
        "documentation": {}
    },
    {
        "label": "test_melgan_multi_scale_discriminator",
        "kind": 2,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_melgan_discriminator",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_melgan_discriminator",
        "peekOfCode": "def test_melgan_multi_scale_discriminator():\n    model = MelganMultiscaleDiscriminator()\n    print(model)\n    dummy_input = torch.rand((4, 1, 256 * 16))\n    scores, feats = model(dummy_input)\n    assert len(scores) == 3\n    assert len(scores) == len(feats)\n    assert np.all(scores[0].shape == (4, 1, 64))\n    assert np.all(feats[0][0].shape == (4, 16, 4096))\n    assert np.all(feats[0][1].shape == (4, 64, 1024))",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_melgan_discriminator",
        "documentation": {}
    },
    {
        "label": "test_melgan_generator",
        "kind": 2,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_melgan_generator",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_melgan_generator",
        "peekOfCode": "def test_melgan_generator():\n    model = MelganGenerator()\n    print(model)\n    dummy_input = torch.rand((4, 80, 64))\n    output = model(dummy_input)\n    assert np.all(output.shape == (4, 1, 64 * 256))\n    output = model.inference(dummy_input)\n    assert np.all(output.shape == (4, 1, (64 + 4) * 256))",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_melgan_generator",
        "documentation": {}
    },
    {
        "label": "test_pwgan_disciminator",
        "kind": 2,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_parallel_wavegan_discriminator",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_parallel_wavegan_discriminator",
        "peekOfCode": "def test_pwgan_disciminator():\n    model = ParallelWaveganDiscriminator(\n        in_channels=1,\n        out_channels=1,\n        kernel_size=3,\n        num_layers=10,\n        conv_channels=64,\n        dilation_factor=1,\n        nonlinear_activation=\"LeakyReLU\",\n        nonlinear_activation_params={\"negative_slope\": 0.2},",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_parallel_wavegan_discriminator",
        "documentation": {}
    },
    {
        "label": "test_redisual_pwgan_disciminator",
        "kind": 2,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_parallel_wavegan_discriminator",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_parallel_wavegan_discriminator",
        "peekOfCode": "def test_redisual_pwgan_disciminator():\n    model = ResidualParallelWaveganDiscriminator(\n        in_channels=1,\n        out_channels=1,\n        kernel_size=3,\n        num_layers=30,\n        stacks=3,\n        res_channels=64,\n        gate_channels=128,\n        skip_channels=64,",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_parallel_wavegan_discriminator",
        "documentation": {}
    },
    {
        "label": "test_pwgan_generator",
        "kind": 2,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_parallel_wavegan_generator",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_parallel_wavegan_generator",
        "peekOfCode": "def test_pwgan_generator():\n    model = ParallelWaveganGenerator(\n        in_channels=1,\n        out_channels=1,\n        kernel_size=3,\n        num_res_blocks=30,\n        stacks=3,\n        res_channels=64,\n        gate_channels=128,\n        skip_channels=64,",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_parallel_wavegan_generator",
        "documentation": {}
    },
    {
        "label": "test_pqmf",
        "kind": 2,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_pqmf",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_pqmf",
        "peekOfCode": "def test_pqmf():\n    w, sr = load(WAV_FILE)\n    layer = PQMF(N=4, taps=62, cutoff=0.15, beta=9.0)\n    w, sr = load(WAV_FILE)\n    w2 = torch.from_numpy(w[None, None, :])\n    b2 = layer.analysis(w2)\n    w2_ = layer.synthesis(b2)\n    print(w2_.max())\n    print(w2_.min())\n    print(w2_.mean())",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_pqmf",
        "documentation": {}
    },
    {
        "label": "TESTS_PATH",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_pqmf",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_pqmf",
        "peekOfCode": "TESTS_PATH = get_tests_path()\nWAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\ndef test_pqmf():\n    w, sr = load(WAV_FILE)\n    layer = PQMF(N=4, taps=62, cutoff=0.15, beta=9.0)\n    w, sr = load(WAV_FILE)\n    w2 = torch.from_numpy(w[None, None, :])\n    b2 = layer.analysis(w2)\n    w2_ = layer.synthesis(b2)\n    print(w2_.max())",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_pqmf",
        "documentation": {}
    },
    {
        "label": "WAV_FILE",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_pqmf",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_pqmf",
        "peekOfCode": "WAV_FILE = os.path.join(get_tests_input_path(), \"example_1.wav\")\ndef test_pqmf():\n    w, sr = load(WAV_FILE)\n    layer = PQMF(N=4, taps=62, cutoff=0.15, beta=9.0)\n    w, sr = load(WAV_FILE)\n    w2 = torch.from_numpy(w[None, None, :])\n    b2 = layer.analysis(w2)\n    w2_ = layer.synthesis(b2)\n    print(w2_.max())\n    print(w2_.min())",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_pqmf",
        "documentation": {}
    },
    {
        "label": "test_rwd",
        "kind": 2,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_rwd",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_rwd",
        "peekOfCode": "def test_rwd():\n    layer = RandomWindowDiscriminator(\n        cond_channels=80,\n        window_sizes=(512, 1024, 2048, 4096, 8192),\n        cond_disc_downsample_factors=[(8, 4, 2, 2, 2), (8, 4, 2, 2), (8, 4, 2), (8, 4), (4, 2, 2)],\n        hop_length=256,\n    )\n    x = torch.rand([4, 1, 22050])\n    c = torch.rand([4, 80, 22050 // 256])\n    scores, _ = layer(x, c)",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_rwd",
        "documentation": {}
    },
    {
        "label": "test_wavernn",
        "kind": 2,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn",
        "peekOfCode": "def test_wavernn():\n    config = WavernnConfig()\n    config.model_args = WavernnArgs(\n        rnn_dims=512,\n        fc_dims=512,\n        mode=\"mold\",\n        mulaw=False,\n        pad=2,\n        use_aux_net=True,\n        use_upsample_net=True,",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn",
        "documentation": {}
    },
    {
        "label": "wavernn_dataset_case",
        "kind": 2,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "peekOfCode": "def wavernn_dataset_case(batch_size, seq_len, hop_len, pad, mode, mulaw, num_workers):\n    \"\"\"run dataloader with given parameters and check conditions\"\"\"\n    ap = AudioProcessor(**C.audio)\n    C.batch_size = batch_size\n    C.mode = mode\n    C.seq_len = seq_len\n    C.data_path = test_data_path\n    preprocess_wav_files(test_data_path, C, ap)\n    _, train_items = load_wav_feat_data(test_data_path, test_mel_feat_path, 5)\n    dataset = WaveRNNDataset(",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "documentation": {}
    },
    {
        "label": "test_parametrized_wavernn_dataset",
        "kind": 2,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "peekOfCode": "def test_parametrized_wavernn_dataset():\n    \"\"\"test dataloader with different parameters\"\"\"\n    params = [\n        [16, C.audio[\"hop_length\"] * 10, C.audio[\"hop_length\"], 2, 10, True, 0],\n        [16, C.audio[\"hop_length\"] * 10, C.audio[\"hop_length\"], 2, \"mold\", False, 4],\n        [1, C.audio[\"hop_length\"] * 10, C.audio[\"hop_length\"], 2, 9, False, 0],\n        [1, C.audio[\"hop_length\"], C.audio[\"hop_length\"], 2, 10, True, 0],\n        [1, C.audio[\"hop_length\"], C.audio[\"hop_length\"], 2, \"mold\", False, 0],\n        [1, C.audio[\"hop_length\"] * 5, C.audio[\"hop_length\"], 4, 10, False, 2],\n        [1, C.audio[\"hop_length\"] * 5, C.audio[\"hop_length\"], 2, \"mold\", False, 0],",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "documentation": {}
    },
    {
        "label": "file_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "peekOfCode": "file_path = os.path.dirname(os.path.realpath(__file__))\nOUTPATH = os.path.join(get_tests_output_path(), \"loader_tests/\")\nos.makedirs(OUTPATH, exist_ok=True)\nC = WavernnConfig()\ntest_data_path = os.path.join(get_tests_path(), \"data/ljspeech/\")\ntest_mel_feat_path = os.path.join(test_data_path, \"mel\")\ntest_quant_feat_path = os.path.join(test_data_path, \"quant\")\nok_ljspeech = os.path.exists(test_data_path)\ndef wavernn_dataset_case(batch_size, seq_len, hop_len, pad, mode, mulaw, num_workers):\n    \"\"\"run dataloader with given parameters and check conditions\"\"\"",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "documentation": {}
    },
    {
        "label": "OUTPATH",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "peekOfCode": "OUTPATH = os.path.join(get_tests_output_path(), \"loader_tests/\")\nos.makedirs(OUTPATH, exist_ok=True)\nC = WavernnConfig()\ntest_data_path = os.path.join(get_tests_path(), \"data/ljspeech/\")\ntest_mel_feat_path = os.path.join(test_data_path, \"mel\")\ntest_quant_feat_path = os.path.join(test_data_path, \"quant\")\nok_ljspeech = os.path.exists(test_data_path)\ndef wavernn_dataset_case(batch_size, seq_len, hop_len, pad, mode, mulaw, num_workers):\n    \"\"\"run dataloader with given parameters and check conditions\"\"\"\n    ap = AudioProcessor(**C.audio)",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "documentation": {}
    },
    {
        "label": "C",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "peekOfCode": "C = WavernnConfig()\ntest_data_path = os.path.join(get_tests_path(), \"data/ljspeech/\")\ntest_mel_feat_path = os.path.join(test_data_path, \"mel\")\ntest_quant_feat_path = os.path.join(test_data_path, \"quant\")\nok_ljspeech = os.path.exists(test_data_path)\ndef wavernn_dataset_case(batch_size, seq_len, hop_len, pad, mode, mulaw, num_workers):\n    \"\"\"run dataloader with given parameters and check conditions\"\"\"\n    ap = AudioProcessor(**C.audio)\n    C.batch_size = batch_size\n    C.mode = mode",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "documentation": {}
    },
    {
        "label": "test_data_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "peekOfCode": "test_data_path = os.path.join(get_tests_path(), \"data/ljspeech/\")\ntest_mel_feat_path = os.path.join(test_data_path, \"mel\")\ntest_quant_feat_path = os.path.join(test_data_path, \"quant\")\nok_ljspeech = os.path.exists(test_data_path)\ndef wavernn_dataset_case(batch_size, seq_len, hop_len, pad, mode, mulaw, num_workers):\n    \"\"\"run dataloader with given parameters and check conditions\"\"\"\n    ap = AudioProcessor(**C.audio)\n    C.batch_size = batch_size\n    C.mode = mode\n    C.seq_len = seq_len",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "documentation": {}
    },
    {
        "label": "test_mel_feat_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "peekOfCode": "test_mel_feat_path = os.path.join(test_data_path, \"mel\")\ntest_quant_feat_path = os.path.join(test_data_path, \"quant\")\nok_ljspeech = os.path.exists(test_data_path)\ndef wavernn_dataset_case(batch_size, seq_len, hop_len, pad, mode, mulaw, num_workers):\n    \"\"\"run dataloader with given parameters and check conditions\"\"\"\n    ap = AudioProcessor(**C.audio)\n    C.batch_size = batch_size\n    C.mode = mode\n    C.seq_len = seq_len\n    C.data_path = test_data_path",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "documentation": {}
    },
    {
        "label": "test_quant_feat_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "peekOfCode": "test_quant_feat_path = os.path.join(test_data_path, \"quant\")\nok_ljspeech = os.path.exists(test_data_path)\ndef wavernn_dataset_case(batch_size, seq_len, hop_len, pad, mode, mulaw, num_workers):\n    \"\"\"run dataloader with given parameters and check conditions\"\"\"\n    ap = AudioProcessor(**C.audio)\n    C.batch_size = batch_size\n    C.mode = mode\n    C.seq_len = seq_len\n    C.data_path = test_data_path\n    preprocess_wav_files(test_data_path, C, ap)",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "documentation": {}
    },
    {
        "label": "ok_ljspeech",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "description": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "peekOfCode": "ok_ljspeech = os.path.exists(test_data_path)\ndef wavernn_dataset_case(batch_size, seq_len, hop_len, pad, mode, mulaw, num_workers):\n    \"\"\"run dataloader with given parameters and check conditions\"\"\"\n    ap = AudioProcessor(**C.audio)\n    C.batch_size = batch_size\n    C.mode = mode\n    C.seq_len = seq_len\n    C.data_path = test_data_path\n    preprocess_wav_files(test_data_path, C, ap)\n    _, train_items = load_wav_feat_data(test_data_path, test_mel_feat_path, 5)",
        "detail": "env.src.tts.tests.vocoder_tests.test_vocoder_wavernn_datasets",
        "documentation": {}
    },
    {
        "label": "WavegradTrainTest",
        "kind": 6,
        "importPath": "env.src.tts.tests.vocoder_tests.test_wavegrad",
        "description": "env.src.tts.tests.vocoder_tests.test_wavegrad",
        "peekOfCode": "class WavegradTrainTest(unittest.TestCase):\n    def test_train_step(self):  # pylint: disable=no-self-use\n        \"\"\"Test if all layers are updated in a basic training cycle\"\"\"\n        input_dummy = torch.rand(8, 1, 20 * 300).to(device)\n        mel_spec = torch.rand(8, 80, 20).to(device)\n        criterion = torch.nn.L1Loss().to(device)\n        args = WavegradArgs(\n            in_channels=80,\n            out_channels=1,\n            upsample_factors=[5, 5, 3, 2, 2],",
        "detail": "env.src.tts.tests.vocoder_tests.test_wavegrad",
        "documentation": {}
    },
    {
        "label": "use_cuda",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_wavegrad",
        "description": "env.src.tts.tests.vocoder_tests.test_wavegrad",
        "peekOfCode": "use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nclass WavegradTrainTest(unittest.TestCase):\n    def test_train_step(self):  # pylint: disable=no-self-use\n        \"\"\"Test if all layers are updated in a basic training cycle\"\"\"\n        input_dummy = torch.rand(8, 1, 20 * 300).to(device)\n        mel_spec = torch.rand(8, 80, 20).to(device)\n        criterion = torch.nn.L1Loss().to(device)\n        args = WavegradArgs(\n            in_channels=80,",
        "detail": "env.src.tts.tests.vocoder_tests.test_wavegrad",
        "documentation": {}
    },
    {
        "label": "device",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_wavegrad",
        "description": "env.src.tts.tests.vocoder_tests.test_wavegrad",
        "peekOfCode": "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nclass WavegradTrainTest(unittest.TestCase):\n    def test_train_step(self):  # pylint: disable=no-self-use\n        \"\"\"Test if all layers are updated in a basic training cycle\"\"\"\n        input_dummy = torch.rand(8, 1, 20 * 300).to(device)\n        mel_spec = torch.rand(8, 80, 20).to(device)\n        criterion = torch.nn.L1Loss().to(device)\n        args = WavegradArgs(\n            in_channels=80,\n            out_channels=1,",
        "detail": "env.src.tts.tests.vocoder_tests.test_wavegrad",
        "documentation": {}
    },
    {
        "label": "test_positional_encoding",
        "kind": 2,
        "importPath": "env.src.tts.tests.vocoder_tests.test_wavegrad_layers",
        "description": "env.src.tts.tests.vocoder_tests.test_wavegrad_layers",
        "peekOfCode": "def test_positional_encoding():\n    layer = PositionalEncoding(50)\n    inp = torch.rand(32, 50, 100)\n    nl = torch.rand(32)\n    o = layer(inp, nl)\n    assert o.shape[0] == 32\n    assert o.shape[1] == 50\n    assert o.shape[2] == 100\n    assert isinstance(o, torch.FloatTensor)\ndef test_film():",
        "detail": "env.src.tts.tests.vocoder_tests.test_wavegrad_layers",
        "documentation": {}
    },
    {
        "label": "test_film",
        "kind": 2,
        "importPath": "env.src.tts.tests.vocoder_tests.test_wavegrad_layers",
        "description": "env.src.tts.tests.vocoder_tests.test_wavegrad_layers",
        "peekOfCode": "def test_film():\n    layer = FiLM(50, 76)\n    inp = torch.rand(32, 50, 100)\n    nl = torch.rand(32)\n    shift, scale = layer(inp, nl)\n    assert shift.shape[0] == 32\n    assert shift.shape[1] == 76\n    assert shift.shape[2] == 100\n    assert isinstance(shift, torch.FloatTensor)\n    assert scale.shape[0] == 32",
        "detail": "env.src.tts.tests.vocoder_tests.test_wavegrad_layers",
        "documentation": {}
    },
    {
        "label": "test_ublock",
        "kind": 2,
        "importPath": "env.src.tts.tests.vocoder_tests.test_wavegrad_layers",
        "description": "env.src.tts.tests.vocoder_tests.test_wavegrad_layers",
        "peekOfCode": "def test_ublock():\n    inp1 = torch.rand(32, 50, 100)\n    inp2 = torch.rand(32, 50, 50)\n    nl = torch.rand(32)\n    layer_film = FiLM(50, 100)\n    layer = UBlock(50, 100, 2, [1, 2, 4, 8])\n    scale, shift = layer_film(inp1, nl)\n    o = layer(inp2, shift, scale)\n    assert o.shape[0] == 32\n    assert o.shape[1] == 100",
        "detail": "env.src.tts.tests.vocoder_tests.test_wavegrad_layers",
        "documentation": {}
    },
    {
        "label": "test_dblock",
        "kind": 2,
        "importPath": "env.src.tts.tests.vocoder_tests.test_wavegrad_layers",
        "description": "env.src.tts.tests.vocoder_tests.test_wavegrad_layers",
        "peekOfCode": "def test_dblock():\n    inp = torch.rand(32, 50, 130)\n    layer = DBlock(50, 100, 2)\n    o = layer(inp)\n    assert o.shape[0] == 32\n    assert o.shape[1] == 100\n    assert o.shape[2] == 65\n    assert isinstance(o, torch.FloatTensor)\n    layer.apply_weight_norm()\n    layer.remove_weight_norm()",
        "detail": "env.src.tts.tests.vocoder_tests.test_wavegrad_layers",
        "documentation": {}
    },
    {
        "label": "test_wavegrad_forward",
        "kind": 2,
        "importPath": "env.src.tts.tests.vocoder_tests.test_wavegrad_layers",
        "description": "env.src.tts.tests.vocoder_tests.test_wavegrad_layers",
        "peekOfCode": "def test_wavegrad_forward():\n    x = torch.rand(32, 1, 20 * 300)\n    c = torch.rand(32, 80, 20)\n    noise_scale = torch.rand(32)\n    args = WavegradArgs(\n        in_channels=80,\n        out_channels=1,\n        upsample_factors=[5, 5, 3, 2, 2],\n        upsample_dilations=[[1, 2, 1, 2], [1, 2, 1, 2], [1, 2, 4, 8], [1, 2, 4, 8], [1, 2, 4, 8]],\n    )",
        "detail": "env.src.tts.tests.vocoder_tests.test_wavegrad_layers",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_wavegrad_train",
        "description": "env.src.tts.tests.vocoder_tests.test_wavegrad_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_vocoder_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = WavegradConfig(\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1,",
        "detail": "env.src.tts.tests.vocoder_tests.test_wavegrad_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_wavegrad_train",
        "description": "env.src.tts.tests.vocoder_tests.test_wavegrad_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = WavegradConfig(\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1,\n    seq_len=8192,",
        "detail": "env.src.tts.tests.vocoder_tests.test_wavegrad_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_wavegrad_train",
        "description": "env.src.tts.tests.vocoder_tests.test_wavegrad_train",
        "peekOfCode": "config = WavegradConfig(\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1,\n    seq_len=8192,\n    eval_split_size=1,",
        "detail": "env.src.tts.tests.vocoder_tests.test_wavegrad_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_wavegrad_train",
        "description": "env.src.tts.tests.vocoder_tests.test_wavegrad_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --config_path {config_path} \"\nrun_cli(command_train)\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (",
        "detail": "env.src.tts.tests.vocoder_tests.test_wavegrad_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_wavegrad_train",
        "description": "env.src.tts.tests.vocoder_tests.test_wavegrad_train",
        "peekOfCode": "config.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --config_path {config_path} \"\nrun_cli(command_train)\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"",
        "detail": "env.src.tts.tests.vocoder_tests.test_wavegrad_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_wavegrad_train",
        "description": "env.src.tts.tests.vocoder_tests.test_wavegrad_train",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --config_path {config_path} \"\nrun_cli(command_train)\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"\n)\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.vocoder_tests.test_wavegrad_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_wavegrad_train",
        "description": "env.src.tts.tests.vocoder_tests.test_wavegrad_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"\n)\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.vocoder_tests.test_wavegrad_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_wavegrad_train",
        "description": "env.src.tts.tests.vocoder_tests.test_wavegrad_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"\n)\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.vocoder_tests.test_wavegrad_train",
        "documentation": {}
    },
    {
        "label": "config_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_wavernn_train",
        "description": "env.src.tts.tests.vocoder_tests.test_wavernn_train",
        "peekOfCode": "config_path = os.path.join(get_tests_output_path(), \"test_vocoder_config.json\")\noutput_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = WavernnConfig(\n    model_args=WavernnArgs(),\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    run_eval=True,\n    test_delay_epochs=-1,",
        "detail": "env.src.tts.tests.vocoder_tests.test_wavernn_train",
        "documentation": {}
    },
    {
        "label": "output_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_wavernn_train",
        "description": "env.src.tts.tests.vocoder_tests.test_wavernn_train",
        "peekOfCode": "output_path = os.path.join(get_tests_output_path(), \"train_outputs\")\nconfig = WavernnConfig(\n    model_args=WavernnArgs(),\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1,",
        "detail": "env.src.tts.tests.vocoder_tests.test_wavernn_train",
        "documentation": {}
    },
    {
        "label": "config",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_wavernn_train",
        "description": "env.src.tts.tests.vocoder_tests.test_wavernn_train",
        "peekOfCode": "config = WavernnConfig(\n    model_args=WavernnArgs(),\n    batch_size=8,\n    eval_batch_size=8,\n    num_loader_workers=0,\n    num_eval_loader_workers=0,\n    run_eval=True,\n    test_delay_epochs=-1,\n    epochs=1,\n    seq_len=256,  # for shorter test time",
        "detail": "env.src.tts.tests.vocoder_tests.test_wavernn_train",
        "documentation": {}
    },
    {
        "label": "config.audio.do_trim_silence",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_wavernn_train",
        "description": "env.src.tts.tests.vocoder_tests.test_wavernn_train",
        "peekOfCode": "config.audio.do_trim_silence = True\nconfig.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --config_path {config_path} \"\nrun_cli(command_train)\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (",
        "detail": "env.src.tts.tests.vocoder_tests.test_wavernn_train",
        "documentation": {}
    },
    {
        "label": "config.audio.trim_db",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_wavernn_train",
        "description": "env.src.tts.tests.vocoder_tests.test_wavernn_train",
        "peekOfCode": "config.audio.trim_db = 60\nconfig.save_json(config_path)\n# train the model for one epoch\ncommand_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --config_path {config_path} \"\nrun_cli(command_train)\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"",
        "detail": "env.src.tts.tests.vocoder_tests.test_wavernn_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_wavernn_train",
        "description": "env.src.tts.tests.vocoder_tests.test_wavernn_train",
        "peekOfCode": "command_train = f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --config_path {config_path} \"\nrun_cli(command_train)\n# Find latest folder\ncontinue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"\n)\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.vocoder_tests.test_wavernn_train",
        "documentation": {}
    },
    {
        "label": "continue_path",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_wavernn_train",
        "description": "env.src.tts.tests.vocoder_tests.test_wavernn_train",
        "peekOfCode": "continue_path = max(glob.glob(os.path.join(output_path, \"*/\")), key=os.path.getmtime)\n# restore the model and continue training for one more epoch\ncommand_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"\n)\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.vocoder_tests.test_wavernn_train",
        "documentation": {}
    },
    {
        "label": "command_train",
        "kind": 5,
        "importPath": "env.src.tts.tests.vocoder_tests.test_wavernn_train",
        "description": "env.src.tts.tests.vocoder_tests.test_wavernn_train",
        "peekOfCode": "command_train = (\n    f\"CUDA_VISIBLE_DEVICES='{get_device_id()}' python TTS/bin/train_vocoder.py --continue_path {continue_path} \"\n)\nrun_cli(command_train)\nshutil.rmtree(continue_path)",
        "detail": "env.src.tts.tests.vocoder_tests.test_wavernn_train",
        "documentation": {}
    },
    {
        "label": "run_models",
        "kind": 2,
        "importPath": "env.src.tts.tests.zoo_tests.test_models",
        "description": "env.src.tts.tests.zoo_tests.test_models",
        "peekOfCode": "def run_models(offset=0, step=1):\n    \"\"\"Check if all the models are downloadable and tts models run correctly.\"\"\"\n    print(\" > Run synthesizer with all the models.\")\n    output_path = os.path.join(get_tests_output_path(), \"output.wav\")\n    manager = ModelManager(output_prefix=get_tests_output_path(), progress_bar=False)\n    model_names = manager.list_models()\n    for model_name in model_names[offset::step]:\n        print(f\"\\n > Run - {model_name}\")\n        model_path, _, _ = manager.download_model(model_name)\n        if \"tts_models\" in model_name:",
        "detail": "env.src.tts.tests.zoo_tests.test_models",
        "documentation": {}
    },
    {
        "label": "test_models_offset_0_step_3",
        "kind": 2,
        "importPath": "env.src.tts.tests.zoo_tests.test_models",
        "description": "env.src.tts.tests.zoo_tests.test_models",
        "peekOfCode": "def test_models_offset_0_step_3():\n    run_models(offset=0, step=3)\ndef test_models_offset_1_step_3():\n    run_models(offset=1, step=3)\ndef test_models_offset_2_step_3():\n    run_models(offset=2, step=3)\ndef test_voice_conversion():\n    print(\" > Run voice conversion inference using YourTTS model.\")\n    model_name = \"tts_models/multilingual/multi-dataset/your_tts\"\n    language_id = \"en\"",
        "detail": "env.src.tts.tests.zoo_tests.test_models",
        "documentation": {}
    },
    {
        "label": "test_models_offset_1_step_3",
        "kind": 2,
        "importPath": "env.src.tts.tests.zoo_tests.test_models",
        "description": "env.src.tts.tests.zoo_tests.test_models",
        "peekOfCode": "def test_models_offset_1_step_3():\n    run_models(offset=1, step=3)\ndef test_models_offset_2_step_3():\n    run_models(offset=2, step=3)\ndef test_voice_conversion():\n    print(\" > Run voice conversion inference using YourTTS model.\")\n    model_name = \"tts_models/multilingual/multi-dataset/your_tts\"\n    language_id = \"en\"\n    speaker_wav = os.path.join(get_tests_data_path(), \"ljspeech\", \"wavs\", \"LJ001-0001.wav\")\n    reference_wav = os.path.join(get_tests_data_path(), \"ljspeech\", \"wavs\", \"LJ001-0032.wav\")",
        "detail": "env.src.tts.tests.zoo_tests.test_models",
        "documentation": {}
    },
    {
        "label": "test_models_offset_2_step_3",
        "kind": 2,
        "importPath": "env.src.tts.tests.zoo_tests.test_models",
        "description": "env.src.tts.tests.zoo_tests.test_models",
        "peekOfCode": "def test_models_offset_2_step_3():\n    run_models(offset=2, step=3)\ndef test_voice_conversion():\n    print(\" > Run voice conversion inference using YourTTS model.\")\n    model_name = \"tts_models/multilingual/multi-dataset/your_tts\"\n    language_id = \"en\"\n    speaker_wav = os.path.join(get_tests_data_path(), \"ljspeech\", \"wavs\", \"LJ001-0001.wav\")\n    reference_wav = os.path.join(get_tests_data_path(), \"ljspeech\", \"wavs\", \"LJ001-0032.wav\")\n    output_path = os.path.join(get_tests_output_path(), \"output.wav\")\n    run_cli(",
        "detail": "env.src.tts.tests.zoo_tests.test_models",
        "documentation": {}
    },
    {
        "label": "test_voice_conversion",
        "kind": 2,
        "importPath": "env.src.tts.tests.zoo_tests.test_models",
        "description": "env.src.tts.tests.zoo_tests.test_models",
        "peekOfCode": "def test_voice_conversion():\n    print(\" > Run voice conversion inference using YourTTS model.\")\n    model_name = \"tts_models/multilingual/multi-dataset/your_tts\"\n    language_id = \"en\"\n    speaker_wav = os.path.join(get_tests_data_path(), \"ljspeech\", \"wavs\", \"LJ001-0001.wav\")\n    reference_wav = os.path.join(get_tests_data_path(), \"ljspeech\", \"wavs\", \"LJ001-0032.wav\")\n    output_path = os.path.join(get_tests_output_path(), \"output.wav\")\n    run_cli(\n        f\"tts --model_name  {model_name}\"\n        f\" --out_path {output_path} --speaker_wav {speaker_wav} --reference_wav {reference_wav} --language_idx {language_id} --progress_bar False\"",
        "detail": "env.src.tts.tests.zoo_tests.test_models",
        "documentation": {}
    },
    {
        "label": "tts",
        "kind": 2,
        "importPath": "env.src.tts.hubconf",
        "description": "env.src.tts.hubconf",
        "peekOfCode": "def tts(model_name='tts_models/en/ljspeech/tacotron2-DCA',\n        vocoder_name=None,\n        use_cuda=False):\n    \"\"\"TTS entry point for PyTorch Hub that provides a Synthesizer object to synthesize speech from a give text.\n    Example:\n        >>> synthesizer = torch.hub.load('coqui-ai/TTS', 'tts', source='github')\n        >>> wavs = synthesizer.tts(\"This is a test! This is also a test!!\")\n            wavs - is a list of values of the synthesized speech.\n    Args:\n        model_name (str, optional): One of the model names from .model.json. Defaults to 'tts_models/en/ljspeech/tacotron2-DCA'.",
        "detail": "env.src.tts.hubconf",
        "documentation": {}
    },
    {
        "label": "dependencies",
        "kind": 5,
        "importPath": "env.src.tts.hubconf",
        "description": "env.src.tts.hubconf",
        "peekOfCode": "dependencies = [\n    'torch', 'gdown', 'pysbd', 'gruut', 'anyascii', 'pypinyin', 'coqpit', 'mecab-python3', 'unidic-lite'\n]\nimport torch\nfrom TTS.utils.manage import ModelManager\nfrom TTS.utils.synthesizer import Synthesizer\ndef tts(model_name='tts_models/en/ljspeech/tacotron2-DCA',\n        vocoder_name=None,\n        use_cuda=False):\n    \"\"\"TTS entry point for PyTorch Hub that provides a Synthesizer object to synthesize speech from a give text.",
        "detail": "env.src.tts.hubconf",
        "documentation": {}
    },
    {
        "label": "build_py",
        "kind": 6,
        "importPath": "env.src.tts.setup",
        "description": "env.src.tts.setup",
        "peekOfCode": "class build_py(setuptools.command.build_py.build_py):  # pylint: disable=too-many-ancestors\n    def run(self):\n        setuptools.command.build_py.build_py.run(self)\nclass develop(setuptools.command.develop.develop):\n    def run(self):\n        setuptools.command.develop.develop.run(self)\n# The documentation for this feature is in server/README.md\npackage_data = [\"TTS/server/templates/*\"]\ndef pip_install(package_name):\n    subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", package_name])",
        "detail": "env.src.tts.setup",
        "documentation": {}
    },
    {
        "label": "develop",
        "kind": 6,
        "importPath": "env.src.tts.setup",
        "description": "env.src.tts.setup",
        "peekOfCode": "class develop(setuptools.command.develop.develop):\n    def run(self):\n        setuptools.command.develop.develop.run(self)\n# The documentation for this feature is in server/README.md\npackage_data = [\"TTS/server/templates/*\"]\ndef pip_install(package_name):\n    subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\nrequirements = open(os.path.join(cwd, \"requirements.txt\"), \"r\").readlines()\nwith open(os.path.join(cwd, \"requirements.notebooks.txt\"), \"r\") as f:\n    requirements_notebooks = f.readlines()",
        "detail": "env.src.tts.setup",
        "documentation": {}
    },
    {
        "label": "pip_install",
        "kind": 2,
        "importPath": "env.src.tts.setup",
        "description": "env.src.tts.setup",
        "peekOfCode": "def pip_install(package_name):\n    subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\nrequirements = open(os.path.join(cwd, \"requirements.txt\"), \"r\").readlines()\nwith open(os.path.join(cwd, \"requirements.notebooks.txt\"), \"r\") as f:\n    requirements_notebooks = f.readlines()\nwith open(os.path.join(cwd, \"requirements.dev.txt\"), \"r\") as f:\n    requirements_dev = f.readlines()\nrequirements_all = requirements_dev + requirements_notebooks\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as readme_file:\n    README = readme_file.read()",
        "detail": "env.src.tts.setup",
        "documentation": {}
    },
    {
        "label": "python_version",
        "kind": 5,
        "importPath": "env.src.tts.setup",
        "description": "env.src.tts.setup",
        "peekOfCode": "python_version = sys.version.split()[0]\nif Version(python_version) < Version(\"3.7\") or Version(python_version) >= Version(\"3.11\"):\n    raise RuntimeError(\"TTS requires python >= 3.7 and < 3.11 \" \"but your Python version is {}\".format(sys.version))\ncwd = os.path.dirname(os.path.abspath(__file__))\nwith open(os.path.join(cwd, \"TTS\", \"VERSION\")) as fin:\n    version = fin.read().strip()\nclass build_py(setuptools.command.build_py.build_py):  # pylint: disable=too-many-ancestors\n    def run(self):\n        setuptools.command.build_py.build_py.run(self)\nclass develop(setuptools.command.develop.develop):",
        "detail": "env.src.tts.setup",
        "documentation": {}
    },
    {
        "label": "cwd",
        "kind": 5,
        "importPath": "env.src.tts.setup",
        "description": "env.src.tts.setup",
        "peekOfCode": "cwd = os.path.dirname(os.path.abspath(__file__))\nwith open(os.path.join(cwd, \"TTS\", \"VERSION\")) as fin:\n    version = fin.read().strip()\nclass build_py(setuptools.command.build_py.build_py):  # pylint: disable=too-many-ancestors\n    def run(self):\n        setuptools.command.build_py.build_py.run(self)\nclass develop(setuptools.command.develop.develop):\n    def run(self):\n        setuptools.command.develop.develop.run(self)\n# The documentation for this feature is in server/README.md",
        "detail": "env.src.tts.setup",
        "documentation": {}
    },
    {
        "label": "package_data",
        "kind": 5,
        "importPath": "env.src.tts.setup",
        "description": "env.src.tts.setup",
        "peekOfCode": "package_data = [\"TTS/server/templates/*\"]\ndef pip_install(package_name):\n    subprocess.call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\nrequirements = open(os.path.join(cwd, \"requirements.txt\"), \"r\").readlines()\nwith open(os.path.join(cwd, \"requirements.notebooks.txt\"), \"r\") as f:\n    requirements_notebooks = f.readlines()\nwith open(os.path.join(cwd, \"requirements.dev.txt\"), \"r\") as f:\n    requirements_dev = f.readlines()\nrequirements_all = requirements_dev + requirements_notebooks\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as readme_file:",
        "detail": "env.src.tts.setup",
        "documentation": {}
    },
    {
        "label": "requirements",
        "kind": 5,
        "importPath": "env.src.tts.setup",
        "description": "env.src.tts.setup",
        "peekOfCode": "requirements = open(os.path.join(cwd, \"requirements.txt\"), \"r\").readlines()\nwith open(os.path.join(cwd, \"requirements.notebooks.txt\"), \"r\") as f:\n    requirements_notebooks = f.readlines()\nwith open(os.path.join(cwd, \"requirements.dev.txt\"), \"r\") as f:\n    requirements_dev = f.readlines()\nrequirements_all = requirements_dev + requirements_notebooks\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as readme_file:\n    README = readme_file.read()\nexts = [\n    Extension(",
        "detail": "env.src.tts.setup",
        "documentation": {}
    },
    {
        "label": "requirements_all",
        "kind": 5,
        "importPath": "env.src.tts.setup",
        "description": "env.src.tts.setup",
        "peekOfCode": "requirements_all = requirements_dev + requirements_notebooks\nwith open(\"README.md\", \"r\", encoding=\"utf-8\") as readme_file:\n    README = readme_file.read()\nexts = [\n    Extension(\n        name=\"TTS.tts.utils.monotonic_align.core\",\n        sources=[\"TTS/tts/utils/monotonic_align/core.pyx\"],\n    )\n]\nsetup(",
        "detail": "env.src.tts.setup",
        "documentation": {}
    },
    {
        "label": "exts",
        "kind": 5,
        "importPath": "env.src.tts.setup",
        "description": "env.src.tts.setup",
        "peekOfCode": "exts = [\n    Extension(\n        name=\"TTS.tts.utils.monotonic_align.core\",\n        sources=[\"TTS/tts/utils/monotonic_align/core.pyx\"],\n    )\n]\nsetup(\n    name=\"TTS\",\n    version=version,\n    url=\"https://github.com/coqui-ai/TTS\",",
        "detail": "env.src.tts.setup",
        "documentation": {}
    },
    {
        "label": "Tag",
        "kind": 6,
        "importPath": "tag.models.tag_model",
        "description": "tag.models.tag_model",
        "peekOfCode": "class Tag(Base):\n    __tablename__ = \"Tag\"\n    id = Column(Integer, primary_key=True, index=True)\n    name = Column(String(50), nullable=False)\n    def __str__(self):\n        return self.name",
        "detail": "tag.models.tag_model",
        "documentation": {}
    },
    {
        "label": "Settings",
        "kind": 6,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "class Settings(BaseSettings):\n    POSTGRES_USER: str = os.getenv(\"POSTGRES_USER\", \"\")\n    POSTGRES_PASSWORD: str = os.getenv(\"POSTGRES_PASSWORD\", \"\")\n    POSTGRES_HOST: str = os.getenv(\"POSTGRES_HOST\", \"\")\n    POSTGRES_DATABASE: str = os.getenv(\"POSTGRES_DATABASE\", \"\")\n    JWT_SECRET_KEY: str = os.getenv(\"JWT_SECRET_KEY\", \"\")\n    JWT_REFRESH_SECRET_KEY: str = os.getenv(\"JWT_REFRESH_SECRET_KEY\", \"\")\n    JWT_ALGORITHM: str = os.getenv(\"JWT_ALGORITHM\", \"\")\n    MJ_APIKEY_PUBLIC: str = os.getenv(\"MJ_APIKEY_PUBLIC\", \"\")\n    MJ_APIKEY_PRIVATE: str = os.getenv(\"MJ_APIKEY_PRIVATE\", \"\")",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "settings",
        "kind": 5,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "settings = Settings()",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "get_db",
        "kind": 2,
        "importPath": "database",
        "description": "database",
        "peekOfCode": "def get_db():\n    db = SessionLocal()\n    try:\n        return db\n    finally:\n        db.close()",
        "detail": "database",
        "documentation": {}
    },
    {
        "label": "SQLALCHEMY_DATABASE_URL",
        "kind": 5,
        "importPath": "database",
        "description": "database",
        "peekOfCode": "SQLALCHEMY_DATABASE_URL = (\n    f\"postgresql://{settings.POSTGRES_USER}:{settings.POSTGRES_PASSWORD}@{settings.POSTGRES_HOST}/{settings.POSTGRES_DATABASE}\"\n)\nengine = create_engine(SQLALCHEMY_DATABASE_URL, echo=True)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\nBase = declarative_base()\ndef get_db():\n    db = SessionLocal()\n    try:\n        return db",
        "detail": "database",
        "documentation": {}
    },
    {
        "label": "engine",
        "kind": 5,
        "importPath": "database",
        "description": "database",
        "peekOfCode": "engine = create_engine(SQLALCHEMY_DATABASE_URL, echo=True)\nSessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\nBase = declarative_base()\ndef get_db():\n    db = SessionLocal()\n    try:\n        return db\n    finally:\n        db.close()",
        "detail": "database",
        "documentation": {}
    },
    {
        "label": "SessionLocal",
        "kind": 5,
        "importPath": "database",
        "description": "database",
        "peekOfCode": "SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)\nBase = declarative_base()\ndef get_db():\n    db = SessionLocal()\n    try:\n        return db\n    finally:\n        db.close()",
        "detail": "database",
        "documentation": {}
    },
    {
        "label": "Base",
        "kind": 5,
        "importPath": "database",
        "description": "database",
        "peekOfCode": "Base = declarative_base()\ndef get_db():\n    db = SessionLocal()\n    try:\n        return db\n    finally:\n        db.close()",
        "detail": "database",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "app = FastAPI(title=\"ShopTaiChe Admin\")\nauthentication_backend = AdminAuth(secret_key=\"29050fe68c6509b99c14b53faae016b1f8bcd73021b69f037fa0d85ec43cf5c1\")\nadmin = AdminBackend(app, engine, authentication_backend=authentication_backend)\nadmin.add_view(TagAdmin)\napp.add_middleware(ExceptionMiddleware)\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Hello World\"}\n@app.get(\"/test\")\nasync def protected_route(request: Request, token: str = Depends(HTTPBearer())):",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "authentication_backend",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "authentication_backend = AdminAuth(secret_key=\"29050fe68c6509b99c14b53faae016b1f8bcd73021b69f037fa0d85ec43cf5c1\")\nadmin = AdminBackend(app, engine, authentication_backend=authentication_backend)\nadmin.add_view(TagAdmin)\napp.add_middleware(ExceptionMiddleware)\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Hello World\"}\n@app.get(\"/test\")\nasync def protected_route(request: Request, token: str = Depends(HTTPBearer())):\n    # Use the authenticated user for further processing",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "admin",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "admin = AdminBackend(app, engine, authentication_backend=authentication_backend)\nadmin.add_view(TagAdmin)\napp.add_middleware(ExceptionMiddleware)\n@app.get(\"/\")\nasync def root():\n    return {\"message\": \"Hello World\"}\n@app.get(\"/test\")\nasync def protected_route(request: Request, token: str = Depends(HTTPBearer())):\n    # Use the authenticated user for further processing\n    return {\"message\": f\"Hello!\"}",
        "detail": "main",
        "documentation": {}
    }
]